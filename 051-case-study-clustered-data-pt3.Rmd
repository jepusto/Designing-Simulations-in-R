---
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r, include=FALSE} 
library( tidyverse )
options(list(dplyr.summarise.inform = FALSE))


### Code from the prior chapter
source( "case_study_code/clustered_data_simulation.R" )

dat <- gen_dat_model( n=5, J=3, p=0.5, 
                        gamma_0=0, gamma_1=0.2, gamma_2=0.2,
                        sigma2_u = 0.4, sigma2_e = 1,
                      alpha = 0.5 )

```


# Case study: A simulation with clustered data, part III

We now extend our case study in Chapter @case_cluster to a multifactor simulation.
Previously, we reinforced our basic message that making functions to wrap parts of simulation substantially eases code construction, but so far have only investigated a single scenario.
How do our findings generalize?  When are the different methods differently appropriate? 
To answer this, we need to extend to a multifactor simulation to systematically explore trends across contexts for our three estimators.
We begin by identifying some questions we might have given our preliminary results.

Regarding bias, in our initial simulation, we noticed that Linear Regression is estimating a person-weighted quantity, and so would be considered biased for the site-average ATE.
We might next ask, how much does bias change if we change the site-size by impact relationship?




For precision, we also saw that Linear Regression has a higher standard error.
But is this a general finding?  When does this occur?
Are there contexts where linear regression will do better than the others?
Originally we thought aggregation would lose information becuase little sites will have the same weight as big sites, but be more imprecisely estimated.
Were we wrong? Or perhaps if site size was even more variable, Agg might do worse and worse.

Finally, the estimated SEs all appeared to be good, although they were rather variable, relative to the true SE.
We might then ask, is this always the case?  Will the estimated SEs fall apart (e.g., be way too large or way too small, in  general) in different contexts?

To answer these questions we need to more systematically explore the space of models.  But we have a lot of knobs to turn.
In our simulation, we can generate fake cluster randomized data with the following features:

 - The treatment impact of the site can vary, and vary with the site size
 - We can have sites of different sizes if we want
 - We can also vary:
 
     - the site intercept variance
     - the residual variance, 
     - the treatment impact
     - the site size
     - the number of sites, ...

We cannot easily vary all of these.
We instead reflect on our research questions, speculate as to what is likely to matter, and then consider varying the following:

 - Average site size: Does the number of students/site matter?
 - Number of sites: Do cluster-robust SEs work with fewer sites?
 - Variation in site size: Varying site sizes cause bias or break things?
 - Correlation of site size and site impact: Will correlation cause bias?
 - Cross site variation: Does the amount of site variation matter?

Even so, we have a problem: How do we index cross site variation?
If we simply add more cross site variation, our total variation will increase.
If methods deteriorate, we then have a confound: is it the cross site variation causing the problem, or is it the total variation?
We therefore want to vary site variation while controlling total variation.


In extending our simulation we will also come across all sorts of concerns such as how to handle convergence issues in the modeling.
We also need to think about how to deal with nuisance factors and how to summarize complex simulations.
Finally, how do we choose appropriate factors and not become beholden to the parameters in our model?



## Standardization in a data generating process

Given our model, we can generate data by specifying our parameters and variables of $\gamma_{0}, \gamma_{1}, \gamma_{2}, \sigma^2_\epsilon, \sigma^2_u, \bar{n}, \alpha, J, p$.

Now, as discussed above, we want to manipulate within vs. between variation.  If we just add more between variation (increase $\sigma^2_u$), our overall variation of $Y$ will increase.
This will make it hard to think about, e.g., power, since we have confounded within vs. between variation with overall variation (which is itself bad for power).
It also impacts interpretation of coefficients.  A treatment effect of 0.2 on our outcome scale is "smaller" if there is more overall variation.

To handle this we first (1) Standardize our data and then (2) reparameterize, so we have human-selected parameters that we can interpret that we then _translate_ to our list of data generation parameters.
This allows us to, for exmaple, operate in standard quantities such as effect size units.
It also allows us ot index our DGP with more interpretable parameters such as the Intra-Class Correlation (ICC).

Our model is 
$$ Y_{ij} = \gamma_{0} + \gamma_{1} Z_j + \gamma_2 Z_j \left(\frac{n_j - \bar{n}}{\bar{n}} \right)  + u_j + \epsilon_{ij}  $$

The variance of our control-side outcomes is
$$ 
\begin{aligned}
var( Y_{ij}(0) ) &= var( \beta_{0j} + \epsilon_{ij} ) \\
 &= var( \gamma_{0} + \gamma_{1} Z_j + \gamma_{2}Z_j \tilde{n}_j + u_j + \epsilon_{ij} ) \\
&= \sigma^2_u + \sigma^2_\epsilon
\end{aligned}
$$ 
The effect size of an impact is defined as the impact over the control-side standard deviation.
(Sometimes people use the pooled standard deviation, but this is usually a bad choice if one suspects treatment variation.  More treatment variation should not reduce the effect size for the same absolute average impact.)

$$ ES = \frac{\gamma_1}{SD( Y | Z_j = 0 )} = \frac{\gamma_1}{\sqrt{ \sigma^2_u + \sigma^2_\epsilon } } $$

The way we think about how "big" $\gamma_1$ is depends on how much site variation and residual variation there is.
But it is also easier to detect effects when the residual variation is small.
Effect sizes "standardize out" these sorts of tensions.  We can use that.

In particular, we will use the Intraclass Correlation Coeffiicent (ICC), defined as
$$ ICC = \frac{ \sigma^2_u }{ \sigma^2_\epsilon + \sigma^2_u } . $$
The ICC is a measure of within vs. between variation.

What we then do is first standardized our data, meaning we ensure the control side variance equals 1.
Using the above, this means $\sigma^2_u + \sigma^2_\epsilon = 1$.
It also gives us $ICC = \sigma^2_u$, and $\sigma^2_\epsilon = 1 - ICC$.

Our two model parameters are now tied together by our single ICC tuning parameter.
The core idea is we can now manipulate the aspects of the DGP we want while holding other aspects of the DGP constant.
Given our standardized scale, we have dropped a parameter from our set we might want to vary, and ensured varying the other parameter (now the ICC) is varying only one aspect of the DGP, not both.
Before, increasing $\sigma^2_u$ had two consequences: total variation and relative amount of variation at the school level.
Manipulating ICC only does the latter.

Our revised code is then, at the simulation driver level:

```{r revised_run_CRT_sim}
run_CRT_sim <- function(reps, 
                        n_bar = 10, J = 30, p = 0.5,
                        ATE = 0, ICC = 0.4,
                        size_coef = 0, alpha = 0,
                        seed = NULL, aggregate = TRUE) {

  stopifnot( ICC >= 0 && ICC < 1 )
  
  scat( "Running n=%d, J=%d, ICC=%.2f, ATE=%.2f (%d replicates)\n", n_bar, J, ICC, ATE, reps)

  if (!is.null(seed)) set.seed(seed)

  res <- 
    purrr::rerun( reps, {
      dat <- gen_dat_model( n_bar = n_bar, J = J, p = p,
                            gamma_0 = 0, gamma_1 = ATE, gamma_2 = size_coef,
                            sigma2_u = ICC, sigma2_e = 1 - ICC,
                            alpha = alpha )
      analyze_data(dat)
    }) %>%
    bind_rows( .id="runID" )
}
```

Note the `stopifnot`: it is wise to ensure our parameter transforms are all reasonable, so we don't get unexplained errors or strange results.

Also note how we are transforming our ICC parameter into specific other parameters to maintain our effect size interpretation of our
simulation.
We don't need to modify the `gen_dat_model` method: we are just specifying the constellation of parameters as a function of the parameters we want to directly control in the simulation.


## Making analyze_data() quiet

If we run our simulation when there is little cluster variation, we start getting a lot of warnings from our MLM estimator:
```{r}
# TBD
```

When we scale up to our full simulations, these warnings can become a nuisance.
Furthermore, the `lmer` command can sometimes just fails (we believe there is some bug in the optimizer that fails if things are just perfectly wrong).
If this was on simulation run 944 out of 1000, we would lose everything!
To protect ourselves, we trap messages and warnings as so (see Chapter \@(#safe_code) for more on this):

```
quiet_lmer = quietly( lmer )
analyze_data <- function( dat ) {
    
    # MLM
    M1 <- quiet_lmer( Yobs ~ 1 + Z + (1|sid), data=dat )
    message1 = ifelse( length( M1$message ) > 0, 1, 0 )
    warning1 = ifelse( length( M1$warning ) > 0, 1, 0 )

   ...

    # Compile our results
    tibble( 
      method = c( "MLM", "LR", "Agg" ),
      ATE_hat = c( est1, est2, est3 ),
      SE_hat = c( se1, se2, se3 ),
      p_value = c( pv1, pv2, pv3 ),
      message = c( message1, 0, 0 ),
      warning = c( warning1, 0, 0 )
    )
}
```

We now get a note about the message regarding convergence saved in our results:

```{r demo_analyze_CRT_data}
analyze_data(dat)
```

## Where to compute performance measures: inside vs. outside?


*INSIDE (aggregate as you simulate):*
For each scenario we get a tidy result of our performance measures
Less data to store, easier to compartmentalize
No ability to add new performance measures on the fly

*OUTSIDE (keep all simulation runs):*
You can dynamically add or change how you calculate performance measures
End up with massive amounts of data to store and manipulate


## Run the simulation

Running our simulation is the exact same code as we have used before.
Simulations take awhile to run so we save them so we can analyze at our leisure.
Here we are storing the individual runs, not the analyzed results!

```{r run_the_CRT_simulation, eval=FALSE}
params <- 
  cross_df(design_factors) %>%
  mutate(
    reps = 100,
    seed = 20200320 + 1:n()
  )
params$res = pmap(params, .f = one_CRT_sim )
res = params %>% unnest( cols=c(data) )
saveRDS( res, file = "results/simulation_CRT.rds" )
```


## Analyzing our results

Now that we have run our simulation, we turn to our questions listed above, extending our initial findings from our initial scenario to assess trends across our simulation factors.


### Checking on convergence issues

First, we explore how often we get a convergence message:

```{r examine_convergence_rates}
res <- readRDS( "results/simulation_CRT.rds" )
res %>% 
  group_by( method, ICC ) %>%
  summarise( message = mean( message ) ) %>%
  pivot_wider( names_from = "method", values_from="message" )
```

We see that when the ICC is 0 we get a lot of convergence issues, but as soon as we pull away from 0 it drops off considerably.
At this point we might decide to drop those runs with a message or keep them.
In this case, we decide to keep (it shouldn't matter much in any case except the ICC = 0 case).
We might eventually want to do a separate analysis of the ICC = 0 context to see if the MLM approach actually falls apart, or if it is just throwing error messages.

### Calculating standard metrics

Once we have our individual runs for our difference scenarios, we group and calculate our performance metrics for each group.

```{r calc_CRT_performance_metrics}
sres <- 
  res %>% 
  group_by( n_bar, J, ATE, size_coef, ICC, alpha, method ) %>%
  summarise( 
    bias = mean(ATE_hat - ATE),
    SE = sd( ATE_hat ),
    RMSE = sqrt( mean( (ATE_hat - ATE )^2 ) ),
    ESE_hat = sqrt( mean( SE_hat^2 ) ),
    SD_SE_hat = sqrt( sd( SE_hat^2 ) ),
    power = mean( p_value <= 0.05 ),
    R = n(),
    .groups = "drop"
  )
```

### Bias analysis

As a first step to understanding bias, we might bundle our results by ICC.
In this code we are making groups of method by ICC level so we get side-by-side boxplots for each ICC level considered:

```{r clusterRCT_plot_bias_v1}
ggplot( sres, aes( ICC, bias, col=method, group=paste0(ICC,method) ) ) +
  facet_grid( alpha ~ size_coef, labeller = label_both ) +
  geom_boxplot(coef = Inf) +
  geom_hline( yintercept = 0 ) +
  theme_minimal() +
  scale_x_continuous( breaks = unique( sres$ICC) )
```

Each box is a collection of simulation trials. E.g., for `ICC = 0.6`, `size_coef = 0.2`, and `alpha = 0.8` we have 9 scenarios representing the varying level 1 and level 2 sample sizes:
```{r}
filter( sres, ICC == 0.6, size_coef == 0.2,
        alpha == 0.8, method=="Agg" ) %>%
  dplyr::select( n_bar:alpha, bias )
```

We are seeing a few outliers for some of the boxplots, suggesting that there are other factors driving bias.  We could try bundling along different aspects to see:

```{r clusterRCT_plot_bias_v2}
ggplot( sres, aes( as.factor(n_bar), bias, col=method, group=paste0(n_bar,method) ) ) +
  facet_grid( alpha ~  size_coef, labeller = label_both ) +
  geom_boxplot(coef = Inf) +
  geom_hline( yintercept = 0 ) +
  theme_minimal()
```

No progress there.  Perhaps it is instability or MCSE.

The boxplots are hard for seeing trends.
Instead of bundling, we can therefore aggregate:
```{r}
ssres <- 
  sres %>% 
  group_by( ICC, method, alpha, size_coef ) %>%
  summarise( bias = mean( bias ) )

ggplot( ssres, aes( ICC, bias, col=method ) ) +
  facet_grid( alpha ~  size_coef, labeller = label_both ) +
  geom_point( alpha=0.75 ) + 
  geom_line( alpha=0.75 ) +
  geom_hline( yintercept = 0 ) +
  theme_minimal()
```

This shows that site variation leads to greater bias, but only if the coefficient for size is nonzero.
We also see that all the estimators must be the same if site variation is 0, with the overplotted lines on the top row of the figure.









