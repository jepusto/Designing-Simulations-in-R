---
output: html_document
editor_options: 
  chunk_output_type: console
---


```{r setup_exp_design_analysis, include=FALSE}
library( tidyverse )
library( purrr )
options(list(dplyr.summarise.inform = FALSE))
theme_set( theme_classic() )



### Code for one of the running examples
source( "case_study_code/clustered_data_simulation.R" )
source( "case_study_code/cronbach_alpha_simulation.R" )

res <- readRDS( file = "results/simulation_CRT.rds" )
res

sres <- 
  res %>% 
  group_by( n_bar, J, ATE, size_coef, ICC, alpha, method ) %>%
  summarise( 
    bias = mean(ATE_hat - ATE),
    SE = sd( ATE_hat ),
    RMSE = sqrt( mean( (ATE_hat - ATE )^2 ) ),
    ESE_hat = sqrt( mean( SE_hat^2 ) ),
    SD_SE_hat = sqrt( sd( SE_hat^2 ) ),
    power = mean( p_value <= 0.05 ),
    R = n(),
    .groups = "drop"
  )
sres

# 100 iterations per factor
summary( sres$R )
```

# Building good vizualizations

In the prior chapter, we saw a series of visualizations that showed overall trends across a variety of examples.
These visualizations are not the initial ones that were created, for those research projects.
In practice, making a visualization often requires creating a _bunch_ of graphs to look at different aspects of the data.
From that pile of graphs, you would then refine ones that communicate the overall results most cleanly, and include those in your main write-up.
In our work, we find we often generate a series of R Markdown reports that load the simulation results and generate these visualizations.
These initial documents are then discussed internally by the research team.

In this chapter we dive into how to create visualizations, and discuss a set of common tools that we use to explore our simulation results.
In particular, we focus on four essential tools:

1. **Subsetting**: Multifactor simulations can be complex and confusing. Sometimes it is easier to first explore a subset of the simulation results, such as a single factor level.
2. **Many small multiples**: plotting all of the results in a single plot, with facets to break up the results by simulation factors.
3. **Bundling**: grouping the results by a primary factor of interest, and then plotting the performance measure as a boxplot so you can see how much variation there is within that factor level.
4. **Aggregation**: averaging the performance measure across some of the simulation factors, so you can see overall trends.

<!--These tools are often used in combination.
To get a sense of an overall trend, you can aggregate across factors, averaging your performance metrics and then making line charts or scatterplots.

For all of these plots, if you have a lot of factors, you might also want to use small multiples (faceting) to break up the plots into manageable chunks.
Do this especially if entire sets of simulation results are on radically different scales of performance (e.g., you have a range of outcomes explored, each on a different scale).
You can further use color, shape, and line type to encode different factors.
-->

We next walk through these techniques using our running example of comparing methods for analyzing a Cluster RCT.
We will start with an investigation of bias.

As a reminder, in our Cluster RCT example, we have three methods for estimating the average treatment effect: linear regression of the student-level outcome onto treatment (with cluster-robust standard errors); aggregation, where we regress the cluster-level average outcome onto treatment (with heteroskedastic robust standard errors); and multilevel modeling with random effects.
We want to know if these methods are biased for our defined estimand, which is the cluster-average effect.

## Subsetting

As an initial exploration, you can simply filter the simulation results to a single factor level for some nuisance parameter.
For example, we might examine an ICC of 0.20 only, as this is a "reasonable" value that, given our substance matter knowledge, we know is frequently found in empirical data.
We would then consider the other levels as a "sensitivity" analysis vaguely alluded to in our main report and placed elsewhere, such as an online supplemental appendix.

It would be our job, in this case, to verify that our reported findings on the main results indeed were echoed in our other, set-aside, simulation runs.
In our case the more complex plots explored earlier, such as the aggregation plot, showed mostly horizontal lines with respect to the ICC value; we thus might be able to safely ignore the ICC factor in our main report.


## Many small multiples

If you do not have too many different factors, you can simply plot all of your results at once.
For example, say we only ran simulations for our Cluster RCT simulation with `n_bar = 320` and `J = 20`.
Then we could plot all our simulation results like so:

```{r}
sres_sub <- sres %>%
  filter( n_bar == 320, J == 20 )
ggplot( sres_sub, aes( as.factor(alpha), bias, 
                       col=method, pch=method, group=method ) ) +
  facet_grid( size_coef ~ ICC, labeller = label_both ) +
  geom_point() + geom_line() +
  geom_hline( yintercept = 0 ) +
  theme_minimal() 
```

Each point is one of our methods in one of our simulation scenarios.
We are looking at the raw results.
We connect the points with lines to help us see trends within each of the small multiples.
The lines help us visually track which group of points goes with which.

We are showing five variables: our facets are organized by two (ICC and the size coefficient), and within each facet we have three (alpha, our outcome of bias, and the methods themselves).
Unfortunately, we quickly reach our limits of this approach if we have more factors than five, which, in this case, we do (recall we subset to single values of our remaining two factors of `n_bar` and `J`).
We next present bundling, a way of using this core idea of showing raw results, but in a semi-aggregated way.


## Bundling

When faced with many simulation factors, we can _bundle_ bundle the simulations into groups defined by a selected primary factor of interest, and then plot each bundle with a boxplot of a selected performance criteria.
Each boxplot then shows the central measure of how well an estimator worked across those scenarios, along with a sense of how much that performance varied across those simulations.
If the boxes are narrow, then we know that the variation across simulations within the box did not impact performance much.
If the boxes are wide, then we know that the factors that vary within the box matter a lot for performance.

With bundling, we generally need a good number of simulation runs per scenario, so that the MCSE in the performance measures does not make our boxplots look substantially more variable (wider) than the truth.
(Consider a case where the simulations within a box all would have the same performance, but the MCSE is large; in this case we would see a wide boxplot when we should not.)

To illustrate, group our Cluster RCT results by method, ICC, the size coefficient (how strong the cluster size to treatment impact relationship is), and alpha (how much the cluster sizes vary).
For a specific ICC, size, and alpha, we will put the boxes for the three methods side-by-side to directly compare the methods:

```{r clusterRCT_plot_bias_v1}
ggplot( sres, aes( as.factor(alpha), bias, col=method, group=paste0(ICC,method) ) ) +
  facet_grid( size_coef ~ ICC, labeller = label_both ) +
  geom_boxplot(coef = Inf) +
  geom_hline( yintercept = 0 ) +
  theme_minimal() 
```

Each box is a collection of simulation trials. E.g., for `ICC = 0.6`, `size_coef = 0.2`, and `alpha = 0.8` each of the three boxes contains 9 scenarios representing the varying level 1 and level 2 sample sizes.
Here are the 9 for the Aggregation method:
```{r}
filter( sres, 
        ICC == 0.6, 
        size_coef == 0.2,
        alpha == 0.8, method=="Agg" ) %>%
  dplyr::select( n_bar:alpha, bias ) %>%
  knitr::kable( digits = 2 )
```

Our bias boxplot makes some trends clear.
For example, we see that there is virtually no bias for any method when the size coefficient is 0 and the ICC is 0. 
It is a bit more unclear, but it seems there is also virtually no bias when the size coefficient is 0 regardless of ICC, but the boxes get wider as ICC increases, making us wonder if something else is potentially going on.
When alpha is 0 and the size coefficient is 0.2, all methods have a negative bias for most scenarios considered, as all boxes and almost all of the whiskers are below the 0 line (when ICC is 0.6 or 0.8 we may have some instances of 0 or positive bias, if that is not MCSE giving long tails).

The apparent outliers (long tails) for some of the boxplots suggest that the other two factors (cluster size and number of clusters) do relate to the degree of bias.  We could try bundling along different aspects to see if that explains these differences:

```{r clusterRCT_plot_bias_v2}
ggplot( sres, aes( as.factor(n_bar), bias, col=method, group=paste0(n_bar,method) ) ) +
  facet_grid( alpha ~  size_coef, labeller = label_both ) +
  geom_boxplot(coef = Inf) +
  geom_hline( yintercept = 0 ) +
  theme_minimal()
```

No progress there; we have long tails suggesting something is allowing for large bias in some contexts.
This could be MCSE, with some of our bias estimates being large due to random chance.
Or it could be some specific combination of factors allows for large bias (e.g., perhaps small sample sizes makes our estimators more vulnerable to bias).
In an actual analysis, we would make a note to investigate these anomalies a bit further, later on.

In general, playing around with factors so that the boxes are generally narrow is a good idea; it means that you have found a representation of the data where the variation within your bundles is less important.
This might not always be possible, if all your factors matter; in this case the width of your boxes tells you to what extent the bundled factors matter relative to the factors explicitly present in your plot.



## Aggregation

Boxplots can make seeing trends more difficult, as the eye is drawn to the boxes and tails, and the range of your plot axes can be large due to needing to accommodate the full tails and outliers of your results; this can compress the mean differences between groups, making them look small.
Instead of bundling, we can therefore aggregate, where we average all the scenarios within a box to get a single number of average performance.
This will show us overall trends rather than individual simulation variation.

When we aggregate, and average over some of the factors, we collapse our simulation results down to fewer moving parts.
Aggregation across factors is better than having not had those factors in the first place!
A performance measure averaged over a factor is a more general answer of how things work in practice than having not varied the factor at all.

For example, if we average across ICC and site variation, and see that our methods had different degrees of bias as a function of $J$, we would know that the found trend is a general trend across a range of scenarios defined by different ICC and site variation levels, rather than a specific one tied to a single ICC and amount of site variation.
Our conclusions would then be more general: if we had not explored more scenarios, we would not have any sense of how general our found trend might be.

That said, if some of our scenarios had no bias, and some had large bias, when we aggregated we would report that there is generally a moderate amount of bias.
This would not be entirely faithful to the actual results.
If, however, the initial boxplots show ranges generally positive or generally negative, then aggregation will be more faithful to the spirit of the results.

Also, aggregated results can be misleading if you have scaling issues or extreme outliers.
With bias, our scale is fairly well set, so we are good.
But if we were aggregating standard errors over different sample sizes, then the larger standard errors of the smaller sample size simulations (and the greater variability in estimating those standard errors) would swamp the standard errors of the larger sample sizes.
Usually, with aggregation, we want to average over something we believe does not change massively over the marginalized-out factors.
To achieve this, we can often average over a relative measure (such as standard error divided by the standard error of some baseline method), which tend to be more invariant and comparable across scenarios.

A major advantage of aggregation over the bundling approach is we can have a smaller number of replications per factor combination with aggregation.
If the number of replicates within each scenario is small, then the performance measures for each scenario is estimated with a lot of error; the aggregate, by contrast, will be an average across many more replicates and thus give a good sense of _average_ performance.
The averaging, in effect, gives a lot more replications per aggregated performance measure.


For our cluster RCT, we might aggregate our bias across our sample sizes as follows:
```{r}
ssres <- 
  sres %>% 
  group_by( method, ICC, alpha, size_coef ) %>%
  summarise( bias = mean( bias ),
             n = n() )
ssres
```

We now have a single bias estimate for each combination of ICC, alpha, and size_coef; we have collapsed 9 scenarios into one overall scenario that generalizes bias across different sizes of experiment.
We can then plot, using many small multiples:

```{r}
ggplot( ssres, aes( ICC, bias, col=method ) ) +
  facet_grid( size_coef ~  alpha, labeller = label_both ) +
  geom_point( alpha=0.75 ) + 
  geom_line( alpha=0.75 ) +
  geom_hline( yintercept = 0 ) +
  theme_minimal()
```

We see more clearly that greater variation in cluster size (alpha) leads to greater bias for the linear regression estimator, but only if the coefficient for size is nonzero (which makes sense given our theoretical understanding of the problem---if size is not related to treatment effect, it is hard to imagine how varying cluster sizes would cause much bias).
We are looking at an interaction between our simulation factors: we only see bias for linear regression when cluster size relates to impact and there is variation in cluster size.
As ICC increases, we are not seeing any major differences in the pattern of our results
We also see that all the estimators have near zero bias when there is no variation in cluster size, with the overplotted lines on the top row of the figure.

If you have many levels of a factor, as we do with ICC, you can let ggplot aggregate directly by taking advantage of the smoothing options:

```{r, messages=FALSE}
ggplot( sres, aes( ICC, bias, col=method ) ) +
  facet_grid( alpha ~  size_coef, labeller = label_both ) +
    geom_point( alpha=0.15,
              position = position_dodge(width = 0.04) ) +
  geom_smooth( se=FALSE ) + 
  geom_hline( yintercept = 0 ) +
  theme_minimal()

```

In the above, we let the original points show faintly as well, to give a sense of the variation across simulation trials.


#### A note on how to aggregate

Some performance measures are biased with respect to the Monte Carlo uncertainty.
The estimated standard error, for example, is biased; the variance, by contrast, is not.
The RMSE is biased, the MSE is not.

When aggregating, therefore, it is often best to aggregate the unbiased performance measures, and then calculate the biased ones from those.
For example, to estimate aggregated standard error you might do the following:
```{r}
agg_perf <- sres %>% 
  group_by( ICC, method, alpha, size_coef ) %>%
  summarise( SE = sqrt( mean( SE^2 ) ) )
```

Because bias is linear, you do not need to worry about the bias of the standard error.
But if you are looking at the magnitude of bias ($|bias|$), then you can run into issues when the biases are close to zero, if they are measured noisily.
In this case, looking at average bias, not $|bias|$, is safer.



### Regression Summarization

One can treat the simulation results as a dataset in its own right.
In this case we can regress a performance measure against the methods and various factor levels to get "main effects" of how the different levels impact performance, holding the other levels constant.
This type of regression is called a "meta regression" (@kleijnen1981regression,@friedman1988metamodel,@gilbert2024multilevel), as we are regressing on already processed results.
It also has ties to meta analysis (see, e.g., @borenstein2021introduction), where we look for trends across sets of experiments.

In a meta regression, the main effect estimated for each method will tell us if a method is, on average, higher or lower than the baseline method, averaging across all the simulation scenarios.
The main effect of the factors will tell us if that factor impacts the performance measure.

These regressions can also include interactions between method and factor, to see if some factors impact different methods differently.
They can also include interactions between factors, which allows us to explore how the impact of a factor can matter more or less, depending on other aspects of the context.

For our cluster RCT, we might have, for example:

```{r}
# Make our simulation factors factors rather tha numeric values
sres_f = sres %>%
  mutate( across( c( n_bar, J, size_coef, ICC, alpha ), factor ) )

# Run the regression
M <- lm( bias ~ (n_bar + J + size_coef + ICC + alpha) * method, 
         data = sres_f )

# View the results
stargazer::stargazer(M, type = "text",
                     single.row = TRUE )
```

We can quickly get a lot of features, making a meta-regression somewhat hard to interpret.
This one does not even have interactions of the simulation factors--which, given the found interactions from our plots, is a major concern!
That said, picking out the significant coefficients is a quick way to obtain a lot of clues as to what is driving performance.
E.g., some features interact with the LR method for bias.
The other two methods seem less impacted.



## Analyzing results with few iterations per scenario

When your simulation iterations are expensive to run (i.e., when each model fitting takes several minutes, then running thousands of iterations for many scenarios may no longer be computationally feasible) then you can run into serious issues with noisy estimates of performance.

First, this is why understanding how large your Monte Carlo Standard Errors (MCSEs) are is so important.
If the methods being evaluated are substantially different, then you may be able to demonstrate what you need to, even with few iterations.

If, however, your MCSEs are too large, then you can use the visualization methods that average across scenarios, giving you more precise estimates of aggregated performance.
Do not, by contrast, trust the boxplot approaches--the MCSEs will make your boxes wider, and give the impression that there is more variation than there really is.

Regression approaches can be particularly useful: the regressions are effectively again averaging performance across scenario, looking for overall trends.
You can even fit random effects regression, specifially accounting for the noise in the scenario-specific performance measures.
For more on this approach see @gilbert2024multilevel.


## Analyzing results when some trials have failed

If your methods being evaluated sometimes fail, then when they tend to fail is something to investigate in its own right.
Ideally, failure is not too common, meaning we could drop those trials, or keep them, without really impacting our overall results.
But one should at least know what one is ignoring.

For example, in our cluster RCT, we know we have, at least sometimes, convergence issues.
We also know that ICC is an important feature, so we can explore how often we get a convergence message by ICC level:

```{r examine_convergence_rates}
res %>% 
  group_by( method, ICC ) %>%
  summarise( message = mean( message ) ) %>%
  pivot_wider( names_from = "method", values_from="message" )
```

We see that when the ICC is 0 we get a lot of convergence issues, but as soon as we pull away from 0 it drops off considerably.
At this point we might decide to drop those runs with a message or keep them.
In this case, we decide to keep.
It shouldn't matter much, except possibly when ICC = 0, and we know the convergence issues are driven by trying to estimate a 0 variance, and thus is in some sense expected.
Furthermore, we know people using these methods would likely ignore these messages, and thus we are faithfully capturing how these methods would be used in practice.
We might eventually, however, want to do a separate analysis of the ICC = 0 context to see if the MLM approach is actually falling apart, or if it is just throwing warnings.


## Case study: power for a randomized trial

We next walk through a case study where we compare different visualizations of the same performance metric (in this case, power).
The goal is to see how to examine a metric from several perspectives, and to see how to explore simulation results across scenarios.

For this example, we are going to look at a randomized experiment.
We will generate control potential outcomes with a normal distribution, and then add a treatment effect to the treated units.

For our analytic approach, we will estimate the
treatment effect by taking the difference in means.
We will calculate the associated standard error and generate $p$-values using the normal
approximation.
(As we will see, this is not a good idea for small sample sizes since we should be using a $t$-test style approach.)

Violating our usual modular approach, we are going to have a single function that does an entire step: our function will generate the data and then analyze it all in one go.
Our function also calculates and returns the true effect size of the DGP as the true treatment
effect divided by the control standard deviation (this true effect size will be useful for understanding power, as we show later on).

```{r}
run.one = function( nC, nT, sdC, tau, mu = 5, sdTau = 0 ) {
  Y0 = mu + rnorm( nC, sd=sdC )
  Y1 = mu + rnorm( nT, sd=sdC ) + tau + rnorm( nT, sd=sdTau )

  tau.hat = mean( Y1 ) - mean( Y0 )
  SE.hat = sqrt( var( Y0 ) / ( nC ) + var( Y1 ) / ( nT ) )

  z = tau.hat / SE.hat
  pv = 2 * (1 - pnorm( abs( z ) ))

  data.frame( tau.hat = tau.hat, SE.hat = SE.hat, 
              z = z, p.value = pv )
}
```

Our function generates a data set, analyzes it, and give us back a variety
of results as a one-row dataframe, as per usual:

```{r}
run.one( nT=5, nC=10, sdC=1, tau=0.5 )
```

In this case, our results are a mix of the parameters and estimated quantities.

We then write a function that runs our single trial multiple times and
summarizes the results:

```{r}
run.experiment = function( nC, nT, sdC, tau, mu = 5, sdTau = 0, R = 500 ) {
  
  eres = replicate( R, 
                    run.one( nC, nT, sdC, tau, sdTau=sdTau, mu=mu ), 
                    simplify=FALSE ) %>%
    bind_rows()
  
  eres %>% 
    summarise( E.tau.hat = mean( tau.hat ),
               E.SE.hat = mean( SE.hat ),
               power = mean( p.value <= 0.05 ) ) %>%
    mutate( nC=nC, nT=nT, sdC=sdC, tau=tau, mu=mu, sdTau=sdTau, R=R )
}
```

For performance, we have the average average treatment effect estimate `E.tau.hat`,
the average Standard Error estimate `E.SE.hat`, 
and the power `power` (defined as the percent of time we reject at
alpha=0.05, i.e., the percent of times our $p$-value was less than our 0.05
threshold):

Our function also adds in the details of the simulation (the parameters we passed
to the `run.one()` call). This is an easy way to keep track of things.

We test our function to see what we get:
```{r, cache=TRUE}
run.experiment( 10, 3, 1, 0.5, 5, 0.2 )
```

We next use the above to run our multi-factor simulation experiment.
We are going to vary four factors: control group size, treatment group size,
standard deviation of the units, and the treatment effect.

```{r}
nC = c( 2, 4, 7, 10, 50, 500 )
nT = c( 2, 4, 7, 10, 50, 500 )
sdC = c( 1 )
tau = c( 0, 0.5, 1 )
sdTau = c( 0, 0.5 )

experiments = expand_grid( nC=nC, nT=nT, sdC=sdC, tau=tau, sdTau = sdTau )
experiments
```

We next run an experiment for each row of our dataframe of experiment factor
combinations, and save the results.

```{r secret_run_Neyman_block, include=FALSE}
if ( !file.exists("results/Neyman_RCT_results.rds")) {
  library( furrr )
  plan(multisession)
  exp.res <- experiments %>% future_pmap_dfr( run.experiment, R=2000,
                                             .options = furrr_options(seed = NULL) )
  saveRDS( exp.res, file="results/Neyman_RCT_results.rds" )

} else {
  exp.res = readRDS( file="results/Neyman_RCT_results.rds" )
}
```

```{r, do_power_sim, eval=FALSE}
exp.res <- experiments %>% 
  pmap_df( run.experiment, R=2000 )
dir.create("results", showWarnings = FALSE )
saveRDS( exp.res, file="results/Neyman_RCT_results.rds" )
```

The `R=2000` after `run.experiment` passes the *same* parameter of $R=2000$ to each
run (we run the same number of trials for each experiment).
We can put it there rather than have it be a column in our list of factors to run.

Here is a peek at our results:
```{r}
head( exp.res )
```


### The initial analysis

We are ready to analyze, and we start with plotting.
Plotting is always a good way to visualize simulation results.
We first make our tau into a factor, so `ggplot` behaves, and then plot all our
experiments as two rows based on one factor (`sdTau`) with the columns being
another (`nT`).
Unlike our Cluster RCT, we only have one method, so we can use our color for a different simulation factor.
Within each plot we have the x-axis for one factor (`nC`) and multiple lines for the final factor (`tau`).
The $y$-axis is our outcome of interest, power. We add a 0.05 line to show
when we are rejecting at rates above our nominal $\alpha$. This plot shows
the relationship of five variables.

```{r, messages=FALSE}
exp.res = exp.res %>% 
  mutate( tau = as.factor( tau ) )
ggplot( exp.res, aes( x=nC, y=power, group=tau, col=tau ) ) +
  facet_grid( sdTau ~ nT, labeller=label_both ) +
  geom_point() + geom_line() +
  scale_x_log10() +
  geom_hline( yintercept=0.05, col="black", lty=2)
```

We are looking at power for different control and treatment group sizes.  The tau is our treatment effect, and so for $\tau = 0$ we are looking at validity (false rejection of the null) and for the other $\tau$ power (noticing an effect when it is there).
Notice that we are seeing elevated rejection rates (the tau=0 line is well above 0.05) under the null for small and even moderate sample sizes!


### Focusing on validity

Especially when considering power, we might want to separate rejection rates under the null vs. rejection rates when there is a true thing to reject.
In particular, we can subset to specific simulation runs with no treatment impact to assess the validity. 

First, we can filter our simulations to large samples to make sure we are seeing rejection rates (`power`) of around 0.05, which is what we want:

```{r}
filter( exp.res, tau==0, nT >= 50, nC >= 50 ) %>%
  knitr::kable(digits=2)
```

This does check out, which is a nice sanity check that we have implemented everything correctly.
It is ok to use simulation of simple, known, cases to check that our methods are implemented correctly and working as expected.

We can then get fancy and look at false rejection rate (power under `tau = 0`) as a
function of both nC and nT using an interaction-style plot where we average over the other variables:

```{r plot_interaction_Results, warning=FALSE}
exp.res.rej <- exp.res %>% 
  filter( tau == 0 ) %>%
  group_by( nC, nT ) %>%
  summarize( power = mean( power ),
             n = n() )

exp.res.rej = mutate( exp.res.rej, power = round( power * 100 ) )

ggplot( exp.res.rej, aes( x=nC, y=power, group=nT, col=as.numeric(as.factor(nT)) ) ) +
  geom_point() + geom_line( size=0.5 ) +
  geom_hline( yintercept = 5 ) +
  scale_y_continuous( limits = c( 0, 40 ) ) +
  scale_x_log10( breaks = unique( exp.res.rej$nC ) ) +
  scale_color_gradient( low = "black", high="red",
                        labels = unique( exp.res.rej$nT ) ) +
  labs( x = "# C", y = "Power", colour = "# T" )
```

This plot focuses on the validity of our test.
It shows that we can have massively elevated rates when either the number of treated or control units is small (below 7).
It also shows that as the size of one group increases, if the other is small our rejection rates climb!  Note how for 4 control units, the $n_T = 500$ line is above the others (except for the $n_T = 2$ line).



### Aggregate to look at main effects

We can ignore all the other factors while we look at one specific factor of interest.
This is looking at the **main effect** or **marginal effect** of the factor.

The easy way to do this is to aggregate, letting `ggplot` smooth our individual points on a
plot.
Be sure to also plot the individual points to see variation, however.

```{r plotPool, warning=FALSE, message=FALSE}
ggplot( exp.res, aes( x=nC, y=power, group=tau, col=tau ) ) +
  geom_jitter( width=0.02, height=0, alpha=0.5 ) +
  geom_smooth( se = FALSE ) +
  scale_x_log10( breaks=nC) +
  geom_hline( yintercept=0.05, col="black", lty=2)
```

Note how we see our individual runs that we marginalize over as the dots.

To look at our main effects we can also summarize our results, averaging our
experimental runs across other factor levels. For example, in the code below
we average over the different treatment group sizes and standard deviations,
and plot the marginalized results.

To marginalize, we group by the things we want to keep and let `summarise()` 
average over the things we want to get rid of.

```{r}
exp.res.sum = exp.res %>% group_by( nC, tau ) %>%
  summarise( power = mean( power ) )
head( exp.res.sum )
```

```{r plotCollapse, fig.height=3, warning=FALSE, message=FALSE}
ggplot( exp.res.sum, aes( x=nC, y=power, group=tau, col=tau ) ) +
  geom_line() + geom_point() +
  scale_x_log10( breaks=nC) +
  geom_hline( yintercept=0.05, col="black", lty=2)
```

We can try to get clever and look at other aspects of our experimental runs.
The above suggests that the smaller of the two groups is dictating when things
going awry, in terms of elevated rejection rates under the null.

We can also look at our results in terms of some more easily interpretable parameters, such as effect size instead of raw treatment effect.

To try and simplify the story, we can try and look at total sample size and the smaller of
the two groups sample size and make two different plots that way.
<!--We also subset to just the `sd=1` cases as there is nothing really different about the two
options; we probably should average across but this could reduce clarity of
the presentation of results:-->


```{r}
exp.res <- exp.res %>% 
  mutate( n = nC + nT,
          n.min = pmin( nC, nT ) )
```

```{r plotA, fig.height=4, warning=FALSE, message=FALSE, fig.cap="Power as a function of total sample size.", fig.subcap=c( "Total Size", "Minimum Size" ), out.width="45%", fig.ncol=2}

ggplot( exp.res, aes( x=n, y=power, group=tau, col=tau ) ) +
  geom_jitter( width=0.05, height=0) +
  geom_smooth( se = FALSE, span = 1) +
  scale_x_log10() +
  geom_hline( yintercept=0.05, col="black", lty=2) +
  labs( title = "Total sample size" )

ggplot( exp.res, aes( x=n.min, y=power, group=tau, col=tau ) ) +
  geom_jitter( width=0.05, height=0) +
  geom_smooth( se = FALSE, span = 1) +
  scale_x_log10() +
  geom_hline( yintercept=0.05, col="black", lty=2) +
  labs( title = "Minimum group size" )
```

Note the few observations out in the high `n.min` region for the second
plot---this plot is a bit strange in that the different levels along the
x-axis are assymetric with respect to each other. It is not balanced.


### Recap
Overall, this exploration demonstrates the process of looking at a single performance metric (power) and refining a series of plots to get a sense of what the simulation is taking us.
There are many different plots we might choose, and this depends on the messages we are trying to convey.

The key is to explore, and see what you can learn!


## Exercises

1) For our cluster RCT, use the simulation results to assess how much better (or worse) the different methods are to each other in terms of confidence interval coverage. What scenarios tend to result in the worst coverage?









