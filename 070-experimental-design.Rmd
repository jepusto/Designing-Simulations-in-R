---
output: html_document
editor_options: 
  chunk_output_type: console
---


```{r setup_exp_design, include=FALSE}
library( tidyverse )
library( purrr )
options(list(dplyr.summarise.inform = FALSE))
theme_set( theme_classic() )

### Code for the running examples
source( "case_study_code/r_bivariate_Poisson.R" )
source( "case_study_code/r_and_z.R" )
source( "case_study_code/evaluate_CIs.R" )

source( "case_study_code/clustered_data_simulation.R" )

dat <- gen_cluster_RCT( n=5, J=3, p=0.5, 
                      gamma_0=0, gamma_1=0.2, gamma_2=0.2,
                      sigma2_u = 0.4, sigma2_e = 1,
                      alpha = 0.5 )

```

# (PART) Multifactor Simulations {-}

# Designing the multifactor simulation experiment {#exp-design}

Thus far, we have created code that will run a simulation for a single combination of parameter values.
In practice, simulation studies typically examine a range of different values, including varying the levels of the focal parameter values, auxiliary parameters, sample size, and possibly other design parameters, to explore a range of different scenarios.
We either want reassurance that our findings are general, or we want to understand what aspects of the context affect the performance of the estimator or estimators we are studying.
A single simulation gives us no hint as to either of these questions.
It is only by looking across a range of settings that we can hope to understand trade-offs, general rules, and limits.
Let's now look at the remaining piece of the simulation puzzle: the study's experimental design. 

Simulation studies often take the form of __full factorial__ designed experiments. In full factorials, each factor (a particular knob a researcher might turn to change the simulation conditions) is varied across multiple levels, and the design includes _every_ possible combination of the levels of every factor. One way to represent such a design is as a list of the factors and levels being explored. 

For example, consider a simulation study examining the performance of confidence intervals for Pearson's correlation coefficient under a bivariate Poisson distribution. 
We examined this data-generating model in Section \@ref(BVPois-example), implementing it in the function `r_bivariate_Poisson()`. The model has three parameters (the means of each variate, $\mu_1, \mu_2$ and the correlation $\rho$) and there is one design parameter (sample size, $N$). 
Thus, we might in principle examine up to four factors. 

Using these parameters as factors in the simulation design will lead to considerable redundancy because of the symmetry of the variates: generating data with $\mu_1 = 10$ and $\mu_2 = 5$ would lead to identical correlations as using $\mu_1 = 5$ and $\mu_2 = 10$.
It is useful to re-parameterize to reduce redundancy and simply things.
We will therefore define the simulation conditions by always treating $\mu1$ as the larger variate and by specifying the ratio of the smaller to the larger mean as $\lambda = \mu_2 / \mu_1$.
We might then examine the following factors:

* the sample size, with values of $N = 10, 20$, or $30$
* the mean of the larger variate, with values of $\mu_1 = 4, 8$, or $12$
* the ratio of means, with values of $\lambda = 0.5$ or $1.0$.
* the true correlation, with values ranging from $\rho = 0.0$ to $0.7$ in steps of $0.1$

The above parameters describe a $3 \times 3 \times 2 \times 8$ factorial design, where each element is the number of levels for that factor. This is a four-factor experiment, because we have four different things we are varying.

To implement this design in code, we first save the simulation parameters as a list with one entry per factor, and each entry consists of the levels that we would like to explore.
We will run a simulation for every possible combination of these values.
Here is code that generates all of the scenarios given the above design, storing these combinations in a data frame, `params`, that represents the full experimental design:

```{r make_Pearson_sim_dataframe}
design_factors <- list(
  N = c(10, 20, 30),
  mu1 = c(4, 8, 12),
  lambda = c(0.5, 1.0),
  rho = seq(0.0, 0.7, 0.1)
)

lengths(design_factors)

params <- expand_grid( !!!design_factors )
params
```

We use `expand_grid()` from the `tidyr` package to create all possible combinations of the four factors.[^expand-grid-wtf]
We have a total of $`r paste(lengths(design_factors), collapse = " \\times ")` = `r nrow(params)`$ rows, each row corresponding to a simulation scenario to explore.
With multifactor experiments, it is easy to end up running a lot of experiments!

[^expand-grid-wtf]: `expand_grid()` is set up to take one argument per factor of the design. A clearer example of its natural syntax is:
    ```{r}
    params <- expand_grid(
      N = c(10, 20, 30),
      mu1 = c(4, 8, 12),
      lambda = c(0.5, 1.0),
      rho = seq(0.0, 0.7, 0.1)
    )
    ```
    However, we generally find it useful to create a list of design factors before creating the full grid of parameter values, so we prefer to make `design_factors` first. To use `expand_grid()` on a list, we need to use `!!!`, the splice operator from the `rlang` package, which treats `design_factors` as a set of arguments to be passed to `expand_grid`. The syntax may look a bit wacky, but it's succinct and useful. 


## Choosing parameter combinations

How do we go about choosing parameter values to examine?
Choosing which parameters to use is a central part of good simulation design because the primary limitation of simulation studies is always their _generalizability_.
On the one hand, it is difficult to extrapolate findings from a simulation study beyond the set of simulation conditions that were examined. On the other hand, it is often difficult or impossible to examine the full space of all possible parameter values, except for very simple problems.
Even in the relatively straightforward Pearson correlation simulation, we have four factors, and the last three could, in principle, each take an infinite number of different levels.
How can we come up with a defensible set of levels to examine?

The choice of simulation conditions needs to be made in the context of the problem or model that you are studying, so it is a bit difficult to offer valid, decontextualized advice.
We provide a few observations all the same:

1. For research simulations, it often is important to be able to relate your findings to previous research. This suggests that you should select parameter levels to make this possible, such as by looking at sample sizes similar to those examined in previous studies. That said, previous simulation studies are not always perfect (actually, there are a lot of really crummy ones out there!), and so prior work should not generally be your sole guide or justification.

2. Generally, it is better to err on the side of being more comprehensive. You learn more by looking at a broader range of conditions, and you can always boil down your results to a more limited set of conditions for purposes of presentation.

3. It is also important to explore breakdown points (e.g., what sample size is too small for a method to work?) rather than focusing only on conditions where a method might be expected to work well. Pushing the boundaries and identifying conditions where estimation methods break will help you to provide better guidance for how the methods should be used in practice.

An important point regarding (2) is that you can be more comprehensive and then have fewer replications per scenario.
For example, say you were planning on doing 1000 simulations per scenario, but then you realize there is some new factor that you do not think matters, but that you believe other researchers will worry about.
You could add in that factor, say with four levels, and then do 250 simulations per scenario.
The total work remains the same.

When analyzing the final simulation you would first verify you do not see trends along this new factor, and then marginalize out that factor in your summaries of results.
Marginalizing out a factor (i.e., averaging your performance metrics across the additional factor) is a powerful technique of making a claim about how your methods work _on average_ across a _range_ of scenarios, rather than for a specific scenario; this comes up in the next chapter on presenting results.

Overall, you generally want to vary parameters that you believe matter, or that you think other people will believe matter.
The first is so you can learn.
The second is to build your case.

Once you have identified your parameters, you then have to decide on the levels of the parameter you will include in the simulation.
There are three strategies you might take:

1. Vary a parameter over as much of its range as you can.
2. Choose parameter levels to represent a realistic practical range. These ranges would ideally be empirically justified based on systematic reviews of prior applications. They may also simply come from ones informal impressions of what is realistic in practice.
3. Choose parameters to emulate an important known application or context.

Of these choices, (1) is the most general---but is also the most computationally intensive.
Option (2) focuses attention, ideally, on what is of practical relevance to a practitioner.
Option (3) is usually coupled with a subsequent applied data analysis, and in this case the simulation is often used to enrich that analysis.
In particular, if the simulation shows the methods work for data with the given form of the target application, people may be more willing to believe the application's findings.

Regardless of how you select your primary parameters, you should also vary nuisance parameters (at least a little) to test the sensitivity of your results to these other aspects.
While simulations will (generally) never be fully generalizable, you can certainly make them so they avoid the obvious things a critic might identify as an easy dismissal of your findings.

To recap, as you think about your parameter selection, always keep the following design principles and acknowledgements:

- The primary limitation of simulation studies is __generalizability__.
- Choose conditions that allow you to relate your findings to previous work.
- Err towards being comprehensive. Your goal should be to build an understanding of the major moving parts, and you can always tailor your final presentation of results to give the simplified story, once you find it.
- Explore breakdown points (e.g., what sample size is too small for applying a given method?).

Finally, you should fully expect to add and subtract from your set of simulation factors as you get your initial simulation results!  No one ever runs just a single simulation.


## Using pmap to run multifactor simulations

Once we have selected factors and levels for simulation, we now need to run the simulation code across all of our factor combinations.
One way to do this is by using `pmap()` from the `purrr` package.
`pmap()` marches down a set of lists, running a function on each $p$-tuple of elements, taking the $i^{th}$ element from each list for iteration $i$, and passing them as parameters to the specified function.
`pmap()` then returns the results of this sequence of function calls as a list of results.[^pmap-variants]
Because R's `data.frame` objects are also sets of lists (where each variable is a vector, which is a simple form of list), `pmap()` also works seemlessly on `data.frame` or `tibble` objects.

[^pmap-variants]: Just like `map()` or `map2()`, `pmap()` has variants such as `_dbl` or `_df`.
These variants automatically stack or convert the list of things returned into a tidier collection (for `_dbl` it will convert to a vector of numbers, for `_df` it will stack the results to make a large dataframe, assuming each thing returned is a little dataframe).

Here is a small illustration of `pmap()` in action:
```{r}

some_function <- function( a, b, theta, scale ) {
    scale * (a + theta*(b-a))
}

args_data <- tibble( a = 1:3, b = 5:7, theta = c(0.2, 0.3, 0.7) )
purrr::pmap( args_data, .f = some_function, scale = 10 )
```

One important constraint of `pmap()` is that the variable names over which to iterate over must correspond exactly to arguments of the function to be evaluated.
In the above example, `args_data` must have column names that correspond to the arguments of `some_function`.
For functions with additional arguments that are not manipulated, extra parameters can be passed after the function name (as in the `scale` argument in this example). 
These will also be passed to each function call, but will be the same for all calls.

Let's now implement this technique for our simulation of confidence intervals for Pearson's correlation coefficient.
In Section \@ref(estimation-functions), we developed a function called `r_and_z()` for computing confidence intervals for Pearson's correlation using Fisher's $z$ transformation;
then in Section \@ref(assessing-confidence-intervals), we wrote a function called `evaluate_CIs()` for evaluating confidence interval coverage and average width. 
We can bundle `r_bivariate_Poisson()`, `r_and_z()`, and `evaluate_CIs()` into a simulation driver function by taking
```{r}
library(simhelpers)

Pearson_sim <- bundle_sim(
  f_generate = r_bivariate_Poisson, f_analyze = r_and_z, f_summarize = evaluate_CIs
)
args(Pearson_sim)
```
This function will run a simulation for a given scenario:
```{r}
Pearson_sim(1000, N = 10, mu1 = 5, mu2 = 5, rho = 0.3)
```

In order to call `Pearson_sim()`, we will need to ensure that the columns of the `params` dataset correspond to the arguments of the function.
Because we re-parameterized the model in terms of $\lambda$, we will first need to compute the parameter value for $\mu_2$ and remove the `lambda` variable because it is not an argument of `Pearson_sim()`:
```{r}
params_mod <- 
  params %>%
  mutate(mu2 = mu1 * lambda) %>%
  dplyr::select(-lambda)
```

Now we can use `pmap()` to run the simulation for all `r nrow(params)` parameter settings:
```{r secret-run-Pearson-Poisson-sims, include=FALSE}
# (See below this block for book code)

if ( !file.exists( "results/Pearson_Poisson_results.rds" ) ) {
  # Secret Run code in parallel for speedup
  library(future)
  library(furrr)
  plan(multisession)
  set.seed(20250718)
  sim_results <- 
    params_mod %>%
    mutate(res = future_pmap(., .f = Pearson_sim, reps = 1000, .options = furrr_options(seed = TRUE) ) )
  
  write_rds( sim_results, file = "results/Pearson_Poisson_results.rds" )
} else {
  sim_results <- read_rds("results/Pearson_Poisson_results.rds")
}

```

```{r run-Pearson-sims, eval = FALSE}
sim_results <- params
sim_results$res <- pmap(params_mod, Pearson_sim, reps = 1000 )
```

The above code calls our `run_alpha_sim()` method for each row of our list of scenarios we want to explore.
Even better, we are storing the results __as a new variable in the same dataset__.

```{r}
sim_results
```

The above code may look a bit peculiar: we are storing a set of dataframes (our result) in our original dataframe.
This is actually ok in R: our results will be in what is called a __list-column__, where each element in our list column is the little summary of our simulation results for that scenario.
Here is the third scenario, for example:

```{r}
sim_results$res[[3]]
```

List columns are neat, but hard to work with.
To turn into normal data we can use `unnest()` to expand the `res` variable, replicating the values of the main variables once for each row in the nested dataset:

```{r}
sim_results <- unnest(sim_results, cols = res)
sim_results
```

Putting all of this together into a tidy workflow leads to the following:

```{r, eval = FALSE}
sim_results <- 
  params %>%
  mutate(
    mu2 = mu1 * lambda,
    reps = 1000
  ) %>%
  mutate(
    res = pmap(dplyr::select(., -lambda), .f = Pearson_sim)
  ) %>%
  unnest(cols = res)
```

If you like, you can simply use the `simhelpers` package:

```{r, eval=FALSE}
sim_results <- evaluate_by_row(params_mod, Pearson_sim, reps = 1000)
```
One advantage of `evaluate_by_row()` is that it makes it easy to run the calculations in parallel; see Section \@ref(parallel-processing).

We finally save our results using tidyverse's `write_rds()`; see [R for Data Science, Section 7.5](https://r4ds.hadley.nz/data-import.html#sec-writing-to-a-file).
We first ensure we have a directory by making one via `dir.create()` (see Section \@ref(saving-files) for more on files):

```{r, eval=FALSE}
dir.create("results", showWarnings = FALSE )
write_rds( sim_results, file = "results/Pearson_Poisson_results.rds" )
```

Our simulation results for all our scenarios are now saved!

## When to calculate performance metrics when running multiple simulations

For a single scenario simulation, we repeatedly generate and analyze data, and then assess the performance across the repetitions.
When we extend to multifactor simulations, we have a choice: do we compute performance measures for each simulation scenario as we go (inside) or do we compute all of them after we get all of our individual results (outside)?
There are pros and cons to each approach.

### Aggregate as you simulate (inside)

The *inside* approach runs a stand-alone simulation for each scenario of interest. For each combination of factors, we simulate data, apply our estimators, assess performance, and return a table with summary performance measures. We can then stack these tables to get a dataset with all of the results, ready for analysis.

This is the approach we illustrated above. It is straightforward and streamlined: we already have a method to run simulations for a single scenario, and we just repeat it across multiple scenarios and combine the outputs.
After calling `pmap()` (or `evaluate_by_row()`) and stacking the results, we end up with a dataset containing all the simulation conditions, one simulation context per row (or maybe we have sets of several rows for each simulation context, with one row for each method), with the columns being our factors and measured performance outcomes.
This table of performance is ideally all we need to conduct further analysis and write up the results.

The primary advantages of the inside strategy are that it is easy to modularize the simulation code and it produces a compact dataset of results, minimizing the number and size of files that need to be stored.
On the con side, calculating summary performance measures inside of the simulation driver limits our ability to add new performance measures on the fly or to examine the distribution of individual estimates.
For example, say we wanted to check if the distribution of Fisher-z estimates in a particular scenario was right skewed, perhaps because we are worried that the estimator sometimes breaks down.
We might want to make a histogram of the point estimates, or calculate the skew of the estimates as a performance measure.
Because the individual estimates are not saved, we would have no way of investigating these questions without rerunning the simulation for that condition.
In short, the inside strategy minimizes disk space but constrains our ability to explore or revise performance calculations.

### Keep all simulation runs (outside)

The _outside_ approach involves retaining the entire set of estimates from every replication, with each row corresponding to an estimate for a given simulated dataset.
The benefit of the outside approach is that it allows us to add or change how we calculate performance measures without re-running the entire simulation.
This is especially important if the simulation is time-intensive, such as when the estimators being evaluated are computationally expensive.
The primary disadvantage the outside approach is that it produces large amounts of data that need to be stored and further manipulated.
Thus, the outside strategy maximizes flexibility, at the cost of increased dataset size.

In our Pearson correlation simulation, we initially followed the inside strategy. To move to the outside strategy, we can set the `summarize` argument of `Pearson_sim()` to `FALSE` so that the simulation driver returns a row for every replication:
```{r do_power_sim_full, cache=TRUE}
Pearson_sim(reps = 4, N = 15, mu1 = 5, mu2 = 5, rho = 0.5, summarize = FALSE)
```

We then save the entire set of estimates, rather than the performance summaries.
This result file will have $R$ times as many rows as the older file. In practice, these results can quickly get  to be extremely large.
But disk space is cheap!
Here we run the same experiment with our more complete storage.

Note how the `pmap_df` stacks the multiple rows from each run, giving us everything nicely bundled up:

```{r secret_Pearson_full, include=FALSE}
# (See below this block for book code)

if ( !file.exists( "results/Pearson_Poisson_results_full.rds" ) ) {
  # Secret Run code in parallel for speedup
  library(future)
  library(furrr)
  plan(multisession)
  set.seed(20250718)
  sim_results_full <- 
    params_mod %>%
    mutate(
      res = future_pmap(
        ., .f = Pearson_sim, 
        reps = 1000, summarize = FALSE, 
        .options = furrr_options(seed = TRUE)
      ) 
    ) %>%
    unnest(res)
  
  write_rds( sim_results_full, file = "results/Pearson_Poisson_results_full.rds" )
} else {
  sim_results_full <- read_rds("results/Pearson_Poisson_results_full.rds")
}
```

```{r Pearson_alpha_sim_all_rows, eval=FALSE}
params$res <- params %>% 
    pmap( run_alpha_sim_raw, iterations = 500 )
sim_results_full <- unnest( params,
                            cols = res ) 
write_rds( sim_results_full, file = "results/Pearson_Poisson_results_full.rds" )
```

We end up with a lot more rows.
Here is the number of rows for the outside vs inside approach:
```{r}
c(inside = nrow( sim_results ), outside = nrow( sim_results_full ))
```

Comparing the file sizes on the disk: 
```{r}
c(
  inside = file.size("results/Pearson_Poisson_results.rds"),
  outside = file.size("results/Pearson_Poisson_results_full.rds")
) / 2^10 # Kb
```
The first is several kilobytes, the second is several megabytes.

### Getting raw results ready for analysis

If we generated raw results then we need to do the performance calculations across replications within each simulation context so that we can explore the trends across the simulation factors.

One way to do this is to use `group_by()` and `summarize()` to carry out the performance calculations:
```{r}
sim_results_full %>%
  group_by( N, mu1, mu2, rho ) %>%
  summarise( 
    calc_coverage(lower_bound = CI_lo, upper_bound = CI_hi, true_param = rho)
  )
```

If we want to use our full performance measure function `evaluate_CIs()` to get additional metrics such as MCSEs, we would _nest_ our data into a series of mini-datasets (one for each simulation), and then process each element.
As we saw above, nesting collapses a larger dataset into one where one of the variables consists of a list of datasets:

```{r}
results <- 
  sim_results_full |>
  group_by( N, mu1, mu2, rho ) %>%
  nest( .key = "res" )
results
```

Note how each row of our nested data has a little tibble containing the results for that context, with 1000 rows each.
Once nested, we can then use `map2()` to apply a function to each element of `res`:

```{r}
results <- 
  results %>%
  mutate( performance = map2( res, rho, evaluate_CIs ) ) %>%
  dplyr::select( -res ) %>%
  unnest( cols="performance" ) 
results
```

We have built our final performance table _after_ running the entire simulation, rather than running it on each simulation scenario in turn.

Now, if we want to add a performance metric, we can simply change `evaluate_CIs` and recalculate, without having to recompute the entire simulation.
Summarizing during the simulation vs. after, as we just did, leads to the same set of results.
In fact, if we use the same seed, we would _exactly_ the same results.
Allowing yourself the flexibility to re-calculate performance measures can be very advantageous, and we tend to follow this outside strategy for any simulations involving more complex estimation procedures.

## A multifactor evaluation of Cluster RCT estimators

Let us return to the case study presented in Section \@ref(case-cluster) and consider how to expand it into a multi-factor simulation.

### Choosing parameters for the Clustered RCT

So far, we have only investigated a single scenario at a time, but the modular functions that we have designed make it relatively straightforward to explore a range of scenarios by re-calling our simulation function.
But how do our findings generalize? Under what conditions do the various estimation methods perform better or worse? 
To answer these questions, we need to extend to a multifactor simulation to _systematically_ explore how our three estimators behave across a range of contexts.

We begin by identifying some potential research questions suggested by our preliminary exploration.
Regarding bias, we noticed in our initial simulation that Linear Regression targets a person-weighted average effect, so it would be considered biased for the cluster-average average treatment effect.
We might then ask, how large is bias in practice, and how much does bias change as we change the cluster-size by impact relationship?
Considering precision, we saw that Linear Regression has a higher standard error than the other estimators.
But is this a general finding?  If not, are there contexts where linear regression will have a lower standard error than the others?
Further, we originally thought that aggregation would lose information because smaller clusters would have the same weight as larger clusters, but be more imprecisely estimated.
Were we wrong? Or perhaps if cluster size was even more variable, aggregation might do worse and worse.
Finally, the estimated SEs for all three methods all appeared to be good, although they were rather variable, relative to the true SE.
We might then ask, are the standard errors always the right size, on average?  Will the estimated SEs fall apart (i.e., be far too large or far too small) in some contexts?  If so, which ones?

To answer all of these questions we need to more systematically explore the space of models.
But we have a lot of knobs to turn.
In particular, our data-generating process will produce artificial cluster-randomized experiments where we can vary any of the following features:

 - the number of clusters;
 - the proportion of clusters that receive treatment;
 - the average cluster size and degree of variation in cluster sizes;
 - how much the average impact varies across clusters, and how strongly that is connected to cluster size;
 - how much the cluster intercepts vary (degree of cross-cluster variation); and
 - the degree of residual variation.
 
Manipulating all of these factors would lead to a huge and unwieldy number of simulation conditions to evaluate.
Before proceeding, we reflect on our research questions, speculate as to what is likely to matter, and then consider varying the following:

 - Number of clusters: Do cluster-robust SEs work with fewer clusters?
 - Average cluster size: Does the number of students/cluster matter?
 - Variation in cluster size: Do varying cluster sizes cause bias or break things?
 - Correlation of cluster size and cluster impact: Will correlation cause bias?
 - Cross cluster variation: Does the amount of cluster variation matter?
 
When selecting factors to manipulate, it is important to ensure each factor is isolated, so that changing one of them should not change other aspects of the data-generating process that might impact performance.
For example, if we simply added more cross-cluster variation by directly increasing the random effects for the clusters, the total variation in the outcome will also increase.
If we then see that the performance of an estimator deteriorates as variation increases, we have a confound: is the cross-cluster variation causing the problem, or is it the total variation?
To avoid this confound, we should vary cluster variation while holding the total variation fixed; this is why we use the ICC parameterization, as discussed in Section \@ref(case-cluster).

Given our research questions and the way we parameterize the DGP, we end up with the following factors and levels:

```{r CRT_factors}
crt_design_factors <- list(
  n_bar = c( 20, 80, 320 ),
  J = c( 5, 20, 80 ),
  ATE = c( 0.2 ),
  size_coef = c( 0, 0.2 ),
  ICC = c( 0, 0.2, 0.4, 0.6, 0.8 ),
  alpha = c( 0, 0.5, 0.8 )
)
```

The ATE factor only has one level. We could later expand this if we wanted to, but based on theoretical concerns we are fairly confident the ATE will not impact our research questions, so we leave it as it is, set at a value we would consider plausible in real life.

### Redundant factor combinations

There is some redundancy in the parameter combinations that we have selected.
In particular, if `size_coef` is nonzero, but `alpha` is 0, then the cluster size will not impact the cluster average ATE because there is no variation in cluster size.
We could drop one of the redundant conditions to save some simulation runs---in essence we are running the same simulation for `alpha=0` two times, once for each `size_coef` value, for each level of `ICC`, `n_bar` and `J`.
However, doing so would mean that our simulation is no longer fully crossed.
We will leave the redundant conditions in as a sanity check for our results. 
We know we should get the same results and if we do not, then we either have uncontrolled uncertainty in our simulation or an error in the code.
If computation is cheap, then keeping the fully crossed structure will make for easier analysis.
Otherwise our experiment is not balanced, and so when we average performance across some factors to see the effect of others, we might end up with surprising and misleading results.

### Running the simulations

We will run our cluster RCT simulation using the same code pattern as we used with the Pearson correlation simulations.
Because we are not exactly sure which performance metrics we will want to use, we will save the individual replications and then calculate performance metrics after generating the simulation results.
That is, we will use the outside strategy.

We first make a table of scenarios:
```{r}
params <- 
  expand_grid( !!!crt_design_factors ) %>%
  mutate(
    seed = 20200320 + 17 * 1:n()
  )
```

To allow reproducibility, we specify a different seed for each scenario just to avoid anything confusing about shared randomness across scenarios (see Section \@ref(seeds-and-pseudo-RNGs) for further discussion).
We then run the simulation 1000 times each for scenario, then unnest to get our final data:

```{r run_the_CRT_simulation, eval=FALSE}
params$res <- pmap(params, .f = run_CRT_sim, reps = 1000 )
res <- unnest(params, cols=data)
saveRDS( res, file = "results/simulation_CRT.rds" )
```


```{r secret_run_full_CRT, include=FALSE}
if ( !file.exists( "results/simulation_CRT.rds" ) ) {
 
  source( here::here( "case_study_code/clustered_data_simulation_runner.R" ) )
} else {
  res = readRDS("results/simulation_CRT.rds")
}
```

Normally, we would speed up these calculations using parallel processing; we discuss how to do so in Chapter \@ref(parallel-processing).
Even under parallel processing, this simulation took overnight to run.
Our final results look like this:

```{r}
res
```

We have a lot of rows of data!

### Calculating performance metrics

Our next step is to group our results by our simulation factors and calculate all our performance metrics at once.
For example, here we calculate our primary performance measures by hand:

```{r, messages=FALSE}
res <- readRDS( file = "results/simulation_CRT.rds" )

sres <- 
  res %>% 
  group_by( n_bar, J, ATE, size_coef, ICC, alpha, method ) %>%
  summarise( 
    bias = mean(ATE_hat - ATE),
    SE = sd( ATE_hat ),
    RMSE = sqrt( mean( (ATE_hat - ATE )^2 ) ),
    ESE_hat = sqrt( mean( SE_hat^2 ) ),
    SD_SE_hat = sqrt( sd( SE_hat^2 ) ),
    power = mean( p_value <= 0.05 ),
    R = n(),
    .groups = "drop"
  )
```

If we want MCSEs (as we usually do), then we could do that by hand or use the `simhelpers` package as so:
```{r}
library( simhelpers )

sres <- 
  res %>% 
  group_by( n_bar, J, ATE, size_coef, ICC, alpha, method ) %>%
  summarise( 
    calc_absolute( 
      estimates = ATE_hat, true_param = ATE, 
      criteria = c("bias","stddev","rmse")
    ),
    calc_relative_var( 
      estimates = ATE_hat, var_estimates = SE_hat^2,
      criteria = "relative bias"
    ) 
  ) %>%
  rename( SE = stddev, SE_mcse = stddev_mcse ) %>%
  dplyr::select( -K_absolute, -K_relvar ) %>%
  ungroup()

glimpse( sres )
```

## Exercises

### Brown and Forsythe redux

Take another look at Table \@ref(tab:BF-Scenarios), which is excerpted from @brown1974SmallSampleBehavior. 
Create a tibble with one row for each of the 20 scenarios that they evaluated. 
Then create a function for running the full simulation process (see Exercise \@ref(Welch-simulation)).
Use `pmap()` or `evaluate_by_row()` to run simulations of all 20 scenarios and reproduce the results in Table \@ref(tab:BF-table1) of Chapter \@ref(case-ANOVA).

### Meta-regression

Exercise \@ref(meta-regression-DGP) described the random effects meta-regression model. 
List the focal, auxiliary, and structural parameters of this model, and propose a set of design factors to use in a multifactor simulation of the model.
Create a list with one entry per factor, then create a dataset with one row for each simulation context that you propose to evaluate.


### Comparing the trimmed mean, median and mean {#exercise:trimmed-mean}

In this exercise, you will write a simulation to compare several different
estimators of a common parameter.
In particular, you will compare the  mean, trimmed mean, and median as estimators of the center of a symmetric distribution (such that the mean and median parameters are identical).  
To do this, you should break building this simulation evaluation down into functions for each component of the simulation. 
This will allow you to extend the same framework to more complicated simulation studies.
This extended exercise illustrates how methodologists might compare different estimation strategies, as you might see in the "simulation" section of a stats paper.

As the data-generation function, use a scaled $t$-distribution so that the standard deviation will always be 1 but will have different fatness of tails (high chance of outliers):

```{r}
gen_scaled_t <- function( n, mu, df0 ) {
    mu + rt( n, df=df0 ) / sqrt( df0 / (df0-2) )
}
```

The variance of a $t$ distribution is $df/(df-2)$, so when we divide our observations by the
square root of this, we standardize them so they have unit variance.

1. Verify that `gen_scaled_t()` produces data with mean `mu` and standard deviation 1 for various `df0` values.

2. Write a method to calculate the mean, trimmed mean, and median of a vector of data.
   The trimmed mean should trim 10% of the data from each end.
   The method should return a data frame with the three estimates, one row per estimator.

3. Verify your estimation method works by analyzing a dataset generated with `gen_scaled_t()`.
   For example, you can generate a dataset of size 100 with `gen_scaled_t(100, 0, 3)` and then analyze it.
   
4. Use `bundle_sim()` to create a simulation function that generates data and then analyzes it.
   The function should take `n` and `df0` as arguments, and return the estimates from your analysis method.
   Use `id` to give each simulation run an ID.

5. Run your simulation function for 1000 datasets of size 10, with `mu=0` and `df0=5`.
   Store the results in a variable called `raw_exps`.
   
6. Write a function to calculate the RMSE, bias, and standard error for your three estimators, given the results.

7. Make a single function that takes `df0` and `n`, and runs a simulation and returns the performances of your three methods.

8. Now make a grid of $n = 10, 50, 250, 1250$ and $df_0 =  3, 5, 15, 30$, and generate results for your multi-factor simulation.

9. Make a plot showing how SE changes as a function of sample size for each estimator. Do the three estimator seem to follow the same pattern? Or do they work differently?



