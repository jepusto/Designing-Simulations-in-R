---
output: html_document
editor_options: 
  chunk_output_type: console
---


```{r setup_exp_design, include=FALSE}
library( tidyverse )
library( purrr )
options(list(dplyr.summarise.inform = FALSE))
theme_set( theme_classic() )



### Code for one of the running examples
source( "case_study_code/clustered_data_simulation.R" )
source( "case_study_code/cronbach_alpha_simulation.R" )

dat <- gen_dat_model( n=5, J=3, p=0.5, 
                      gamma_0=0, gamma_1=0.2, gamma_2=0.2,
                      sigma2_u = 0.4, sigma2_e = 1,
                      alpha = 0.5 )

```

# (PART) Multifactor Simulations {-}

# Designing the multifactor simulation experiment {#exp-design}

So far, we have created code that will run a simulation for a single combination of parameter values.
In practice, simulation studies typically examine a range of different values, including varying the levels of the true parameter values and perhaps also varying sample sizes, to explore a range of different scenarios.
We either want reassurance that our findings are general, or we want to understand what aspects of the context lead to our found results.
A single simulation gives us no hint as to either of these questions.
It is only by looking across a range of settings that we can fully understand trade-offs, general rules, and limits.
Let's now look at the remaining piece of the simulation puzzle: the study's experimental design. 

Simulation studies often take the form of __full factorial__ designed experiments. In full factorials, each factor (a particular knob a researcher might turn to change the simulation conditions) is varied across multiple levels, and the design includes _every_ possible combination of the levels of every factor. One way to represent such a design is as a list of the factors and levels being explored. 

For example, for the Cronbach alpha simulation discussed in the prior chapter, we might want to vary:

* the sample size, with values of 50 or 100
* the number of items, with values of 4 or 8
* the true value of alpha, with values ranging from 0.1 to 0.9
* the degrees of freedom of the multivariate $t$ distribution, with values of 5, 10, 20, or 100

The above parameters describe a $2 \times 2 \times 9 \times 4$ factorial design, where each element is the number of options for that factor. This is a four factor experiment, because we have four different things we are varying.

To implement in code, we first save the simulation parameters as a list of factors, each factor having a list of values to explore.
We then run a simulation for every possible combination of these values.
Here is code that generates all the scenarios we will run given the above design, storing these combinations in a data frame, `params`, that represents the full experimental design:

```{r make_cron_sim_dataframe}
design_factors <- list(
  n = c(50, 100),
  p = c(4, 8),
  alpha = seq(0.1, 0.9, 0.1),
  df = c(5, 10, 20, 100)
)

params <- cross_df(design_factors)
params
```

See what we get?

Last chapter, we wrote a `run.experiment()` method to run a simulation for a given scenario.
The parameters we would pass to `run.experiment()` correspond to the columns of `params`.
We have a total of $`r paste(lengths(design_factors), collapse = " \\times ")` = `r nrow(params)`$ rows, each row corresponding to a simulation scenario to explore.
With multifactor experiments, it is easy to end up running a lot of experiments!



## Choosing parameter combinations

How do we go about choosing parameter values to examine?
Choosing which parameters to use is a central part of good simulation design because the primary limitation of simulation studies is always their _generalizability_.
On the one hand, it's difficult to extrapolate findings from a simulation study beyond the set of simulation conditions that were examined. On the other hand, it is often difficult or impossible to examine the full space of all possible parameter values, except for very simple problems.
Even in the relatively straightforward Cronbach alpha simulation, we have four factors, and the last three could, in principle, each take an infinite number of different levels.
How can we come up with a defensible set of levels to examine?

The choice of simulation conditions needs to be made in the context of the problem or model that you are studying, so it is a bit difficult to offer valid, decontextualized advice.
We provide a few observations all the same:

1. For research simulations, it often is important to be able to relate your findings to previous research. This suggests that you should select parameter levels to make this possible, such as by looking at sample sizes similar to those examined in previous studies. That said, previous simulation studies are not always perfect (actually, there are a lot of really crummy ones out there!), and so prior work should not generally be your sole guide or justification.

2. Generally, it is better to err on the side of being more comprehensive. You learn more by looking at a broader range of conditions, and you can always boil down your results to a more limited set of conditions for purposes of presentation.

3. It is also important to explore breakdown points (e.g., what sample size is too small for a method to work?) rather than focusing only on conditions where a method might be expected to work well. Pushing the boundaries and identifying conditions where estimation methods break will help you to provide better guidance for how the methods should be used in practice.

An important point regarding (2) is that you can be more comprehensive and then have fewer replications per scenario.
For example, say you were planning on doing 1000 simulations per scenario, but then you realize there is some new factor that you do not think matters, but that you believe other researchers will worry about.
You could add in that factor, say with four levels, and then do 250 simulations per scenario.
The total work remains the same.

When analyzing the final simulation you would first verify you do not see trends along this new factor, and then marginalize out that factor in your summaries of results.
Marginalizing out a factor (i.e., averaging your performance metrics across the additional factor) is a powerful technique of making a claim about how your methods work _on average_ across a _range_ of scenarios, rather than for a specific scenario; this comes up in the next chapter on presenting results.

Overall, you generally want to vary parameters that you believe matter, or that you think other people will believe matter.
The first is so you can learn.
The second is to build your case.

Once you have identified your parameters, you then have to decide on the levels of the parameter you will include in the simulation.
There are three strategies you might take:

1. Vary a parameter over as much of its range as you can.
2. Choose parameter levels to represent a realistic practical range. These ranges would ideally be empirically justified based on systematic reviews of prior applications. They may also simply come from ones informal impressions of what is realistic in practice.
3. Choose parameters to emulate an important known application or context.

Of these choices, (1) is the most general---but is also the most computationally intensive.
Option (2) focuses attention, ideally, on what is of practical relevance to a practitioner.
Option (3) is usually coupled with a subsequent applied data analysis, and in this case the simulation is often used to enrich that analysis.
In particular, if the simulation shows the methods work for data with the given form of the target application, people may be more willing to believe the application's findings.

Regardless of how you select your primary parameters, you should also vary nuisance parameters (at least a little) to test the sensitivity of your results to these other aspects.
While simulations will (generally) never be fully generalizable, you can certainly make them so they avoid the obvious things a critic might identify as an easy dismissal of your findings.

To recap, as you think about your parameter selection, always keep the following design principles and acknowledgements:

- The primary limitation of simulation studies is __generalizability__.
- Choose conditions that allow you to relate your findings to previous work.
- Err towards being comprehensive. Your goal should be to build an understanding of the major moving parts, and you can always tailor your final presentation of results to give the simplified story, once you find it.
- Explore breakdown points (e.g., what sample size is too small for applying a given method?).

Finally, you should fully expect to add and subtract from your set of simulation factors as you get your initial simulation results!  No one ever runs just a single simulation.




### Choosing parameters for the Clustered RCT

Extending our case study presented in Section \@ref(case-cluster) to a multifactor simulation, we next design our full experiment.

So far, we have only investigated a single scenario at a time, although our modular approach does make exploring a range of scenarios by re-calling our simulation function relatively straightforward.
But how do our findings generalize?  When are the different methods differently appropriate? 
To answer this, we need to extend to a multifactor simulation to _systematically_ explore how our three estimators behave across a range of contexts.
We begin by identifying some research questions we might have, given our preliminary results.

Regarding bias, in our initial simulation, we noticed that Linear Regression is estimating a person-weighted quantity, and so would be considered biased for the cluster-average ATE.
We might next ask, how large is bias in practice, and much does bias change as we change the cluster-size by impact relationship?

For precision, we saw that Linear Regression has a higher standard error.
But is this a general finding?  If not, are there contexts where linear regression will have a lower standard error than the others?

Again for precision, we originally thought that aggregation would lose information because the little clusters would have the same weight as big clusters, but be more imprecisely estimated.
Were we wrong? Or perhaps if cluster size was even more variable, aggregation might do worse and worse.

Finally, the estimated SEs for all three methods all appeared to be good, although they were rather variable, relative to the true SE.
We might then ask, are the standard errors always the right size, on average?  Will the estimated SEs fall apart (e.g., be way too large or way too small, in  general) for some contexts?  If so, which ones?

To answer all of these questions we need to more systematically explore the space of models.
But we have a lot of knobs to turn.
In particular, with our DGP we can generate fake cluster randomized data where we vary any of the following features:

 - How much the average impact varies across clusters, and how strongly that is connected to cluster size
 - How much the cluster intercepts vary (degree of cross cluster variation)
 - How much residual variation there is
 - What is the average cluster size and the variation in cluster sizes
 - How many clusters do we have
 - What proportion of clusters are treated
 
We cannot easily vary all of these factors without having too many simulations to easily peform.
We instead reflect on our research questions, speculate as to what is likely to matter, and then consider varying the following:

 - Average cluster size: Does the number of students/cluster matter?
 - Number of clusters: Do cluster-robust SEs work with fewer clusters?
 - Variation in cluster size: Do varying cluster sizes cause bias or break things?
 - Correlation of cluster size and cluster impact: Will correlation cause bias?
 - Cross cluster variation: Does the amount of cluster variation matter?
 
When designing the final factors, it is important to ensure each factor is isolated, in that changing one of them should not change a host of other things that might impact performance.
For example, in our case, if we simply added more cross cluster variation by directly increasing the random effects for the clusters, our total variation will also increase.
If we then see that a method deteriorates as variation increases, we then have a confound: is it the cross cluster variation is causing the problem, or is it the total variation?
We therefore want to vary cluster variation while controlling total variation; this is why we use the ICC knob discussed in Section \@ref(case-cluster).the section on the data generation process.

Given our research interests and the way we parameterize our DGP, we end up with the following:

```{r CRT_factors}
crt_design_factors <- list(
  n_bar = c( 20, 80, 320 ),
  J = c( 5, 20, 80 ),
  ATE = c( 0.2 ),
  size_coef = c( 0, 0.2 ),
  ICC = c( 0, 0.2, 0.4, 0.6, 0.8 ),
  alpha = c( 0, 0.5, 0.8 )
)
```

Note our ATE factor only has one level: we could later expand this if we wanted to, but based on theoretical concerns we are fairly confident the ATE will not impact our research questions, so we leave it as it is, picked to capture what is a plausible "realistic" ATE.


## Using pmap to run multifactor simulations

To run simulations across all of our factor combinations, we are going to use a very useful method in the `purrr` package called `pmap()`.
`pmap()` marches down a set of lists, running a function on each $p$-tuple of elements, taking the $i^{th}$ element from each list for iteration $i$, and passing them as parameters to the specified function.
`pmap()` then returns the results of this sequence of function calls as a list of results.

Here is a small illustration of `pmap()` in action:
```{r}
my_function <- function( a, b, theta, scale ) {
    scale * (a + theta*(b-a))
}

args = list( a = 1:3, 
             b = 5:7, 
             theta = c(0.2, 0.3, 0.7) )
purrr::pmap_dbl(  args, my_function, scale = 10 )
```

One important note is the variable names for the lists being iterated over must correspond exactly to function arguments of the called function.
Extra parameters can be passed after the function name; these will also be passed to each function call, but will be the same for all calls.
See how `scale` is the same for all calls in the above example.

Just like `map()` or `map2()`, `pmap()` has variants such as `_dbl` or `_df`.
These variants automatically stack or convert the list of things returned into a tidier collection (for `_dbl` it will convert to a vector of numbers, for `_df` it will stack the results to make a large dataframe, assuming each thing returned is a little dataframe).

So far, this is great, but it does not quite look like what we want: our factors are stored as a dataframe, not three lists.
This is where R gets interesting: in R, the columns of a dataframe are actually stored as a list of vectors or lists (with each of the vectors or lists having the exact same length).
This works beautifully with `pmap()`.
Witness:

```{r}
args[[2]]

a_df = as_tibble(args)
a_df
a_df[[2]]

purrr::pmap_dbl( a_df, my_function, scale = 10)
```

When we pass our data frame `a_df` to `pmap()`, `pmap()` takes it as a list of lists, and therefore does exactly what it did before.

All of this means `pmap()` can run a specified function on each row of a dataset.
Continuing the Cronbach Alpha simulation from above, we would have the following:

```{r secret-run-cronbach-sims_code, include=FALSE}
# (See below this block for book code)
params$iterations <- 500
if ( !file.exists( "results/cronbach_results.csv" ) ) {
  # Secret Run code in parallel for speedup
  library(future)
  library(furrr)
  source( "case_study_code/cronbach_alpha_simulation.R")
  plan(multisession)
  params$seed = 1:nrow(params) * 17 + 100000
  sim_results <- 
    params %>%
    mutate(res = future_pmap(., .f = run_alpha_sim,
                              .options = furrr_options(seed = NULL) ) )
  write_csv( sim_results, file = "results/cronbach_results.csv" )
} else {
  sim_results = read_csv("results/cronbach_results.csv")
}
sim_results <- sim_results %>%
  dplyr::select( -seed )
```

```{r run-cronbach-sims, eval = FALSE}
params$iterations <- 500
sim_results <-  params %>%
  mutate(res = pmap(., run_alpha_sim ) )
```

The above code calls our `run_alpha_sim()` method for each row of our list of scenarios we want to explore.
Even better, we are storing the results __as a new variable in the same dataset__.

```{r}
sim_results
```

The above code may look a bit peculiar: we are storing a set of dataframes (our result) in our original dataframe.
This is actually ok in R: our results will be in what is called a __list-column__, where each element in our list column is the little summary of our simulation results for that scenario.
Here is the third scenario, for example:

```{r}
sim_results$res[[3]]
```

List columns are neat, but hard to work with.
To turn into normal data we can use `unnest()` to expand the `res` variable, replicating the values of the main variables once for each row in the nested dataset:

```{r}
library(tidyr)
sim_results <- unnest(sim_results, cols = res) %>%
  dplyr::select( -iterations )
sim_results
```

We can put all of this together in a a tidy workflow as follows:

```{r, eval = FALSE}
sim_results <- 
  params %>%
  mutate(res = pmap(., .f = run_alpha_sim)) %>%
  unnest(cols = res)
```

If you like, you can simply use the `simhelpers` package:

```{r, eval=FALSE}
sim_results <- evaluate_by_row(params, run_alpha_sim)
```
The above code is auto-generated by the `create_skeleton()` method as well.
One advantage of `simhelpers` is it is easy to then do the above in parallel; see Section \@ref(parallel-processing).

We finally save our results using tidyverse's `write_csv()`; see "R for Data Science" textbook, 11.5.
We first ensure we have a directory by making one via `dir.create()` (see Section \@ref(saving-files) for more on files):

```{r, eval=FALSE}
dir.create("results", showWarnings = FALSE )
write_csv( sim_results, file = "results/cronbach_results.csv" )
```

Our simulation results for all our scenarios are now saved!


## When to calculate performance metrics when running multiple simulations

For a single scenario simulation, we repeatedly generate and analyze data, and then assess the performance across the repetitions.
When we extend to multifactor simulations, we have a choice: do we compute performance measures for each simulation scenario as we go (inside) or do we compute all of them after we get all of our individual results (outside)?
We discuss the pros and cons of each approach next.

### Aggregate as you simulate (inside)

The *inside* approach runs a stand-alone simulation for each scenario of interest. For each combination of factors, we simulate data, assess performance, and return a summary table. We then stack these tables to get the final results.

This approach, which is what we illustrated above, is straightforward: we already have a method to run simulations for a single scenario, and we just repeat it across multiple scenarios and combine the outputs.
After the `pmap()` call and the stacking of results, we end up with a dataframe with all our simulations represented, one simulation context per row (or maybe we have bundles of rows for each simulation context, with one row for each method), with the columns being our factors and measured performance outcomes.
This table of performance is ideally all we need to write up our results.

With "inside," we do not have much data to store, and it is easier to modularize the simulation code.
On the cons side, we have no ability to add new performance measures on the fly or look at the individual estimates.
For example, say we wanted to see if the distribution of estimates in a scenario across iterations was right skewed, because we were worried the estimator sometimes broken down and have extreme answers?
We might want to make a histogram of the point estimates, or calculate the skew of the estimates as a performance measure.
As the individual estimates are lost, we would have no way of doing this without rerunning the entire simulation.


### Keep all simulation runs (outside)

In the _outside_ approach, when we finish each single scenario we do not calculate the performance, but instead return the entire set of individual estimates as a dataframe, with each row being a method's estimate for a given dataset.
The benefit of the outside approach is, given these raw estimates, we can dynamically add or change how we calculate performance measures without re-running the entire simulation.
This is especially important if the simulation is time intensive (e.g., if the estimators being evaluated are computationally expensive).
With the outside approach, however, we end up with massive amounts of data to store and manipulate.

For our Cronbach simulation, to move from inside to outside, we just take the summarizing step out of `run_alpha_sim()`.
E.g.,:

```{r}
run_alpha_sim_raw <- function(iterations, n, p, alpha, df, coverage = 0.95 ) {
  
  results <- 
    replicate(n = iterations, {
      dat <- r_mvt_items(n = n, p = p, alpha = alpha, df = df)
      estimate_alpha(dat, coverage = coverage)
    }, simplify = FALSE) %>%
    bind_rows()
  
  results
}
```

Each call to `run_alpha_sim_raw()` now gives one row per simulation trial:

```{r do_power_sim_full, cache=TRUE}
run_alpha_sim_raw( iterations = 4, 
                   50, 6, 0.5, 3 )
```

We then save the entire set of estimates, rather than the performance.
The primary advantage of the outside approach is we can then generate new performance measures, as they
occur to us, later on.
The disadvantage is this result file will be $R$ times as many rows as the older file, which can get quite, quite large.

But disk space is cheap!
Here we run the same experiment with our more complete storage.
Note how the `pmap_df` stacks the multiple rows from each run, giving us everything nicely bundled up:

```{r secret_cronback_full, include=FALSE}
if ( !file.exists("results/cronbach_results_full.csv" ) ) {
  source( "case_study_code/cronbach_alpha_simulation.R" )
  library(future)
  library(furrr)
  plan(multisession)
  params$seed = 1:nrow(params) * 17 + 100000
  params$res <- params %>% 
    future_pmap( run_alpha_sim_raw,
                 .options = furrr_options(seed = NULL) )
  sim_results_full <- unnest( params,
                              cols = res ) 
  write_csv( sim_results_full, "results/cronbach_results_full.csv" )
}
sim_results_full = read_csv("results/cronbach_results_full.csv")
```

```{r cronbach_alpha_sim_all_rows, eval=FALSE}
params$res <- params %>% 
    pmap( run_alpha_sim_raw, iterations = 500 )
sim_results_full <- unnest( params,
                            cols = res ) 
write_csv( sim_results_full, "results/cronbach_results_full.csv" )
```

We end up with a lot more rows.
Here is the number of rows for the outside vs inside approach:
```{r}
nrow( sim_results_full )
nrow( sim_results )
```

Comparing the file sizes on the disk, one is several k, the other is several megabytes:
```{r}
file.size("results/cronbach_results.csv") / 1024
file.size("results/cronbach_results_full.csv") / 1024
```


### Getting raw results ready for analysis

If we generated raw results then we need to collapse them by experimental run
before calculating performance measures so we can explore the trends across the
experiments.

One way to do this is to use `group_by()` and `summarize()` to summrize our raw results.
```{r}
sim_results_full %>%
    group_by( n, p, alpha, df ) %>%
  summarise( 
    bias = mean(A - alpha),
    RMSE = sqrt(mean((A - alpha)^2))
  )
```

If we want to use our full performance measure function `alpha_performance()` to get additional metrics such as MCSEs, we would _nest_ our data into a series of mini-datasets (one for each simulation), and then process each element.
As we saw above, nesting collapses a larger dataframe into a dataframe of dataframes:

```{r}
results <- sim_results_full |>
  dplyr::select( -iterations, -seed ) |>
  group_by( n, p, alpha, df ) |>
  nest( .key = "alpha_sims" )
results
```

Note how each row of our nested data has a little dataframe of the individual simulation results, with 500 rows each.
Once nested, we can then use `map()` to apply a function to each element of the nested dataframe.

```{r}
results <- results %>%
  mutate( performance = map2( alpha_sims, alpha, 
                              alpha_performance ) )
results <- results %>%
  dplyr::select( -alpha_sims ) %>%
  unnest( cols="performance" ) 
results
```

We have built our final performance table _after_ running the entire simulation, rather than running it on each simulation scenario in turn.

Now, if we want to add a performance metric, we can simply change `alpha_performance` and recalculate, without running the time-intensive simulations.
Being able to re-analyze your results is generally a far easier fix than running all the simulations again.
Summarizing during the simulation vs. after, as we just did, leads to the same set of results.
In fact, if we use the same seed, we would _exactly_ the same results.
The inside vs. outside choice is one of using disk space in exchange for later flexibility.


## Running the Cluster RCT multifactor experiment

We run our cluster RCT simulation using the same code pattern as we used with Cronbach alpha.
Because we are not exactly sure what performance metrics we want, we will save our individual results, and then calculate performance metrics on the full data.
I.e., we are using the outside approach, not inside.

We first make our table of scenarios:

```{r}
params <- 
  cross_df(crt_design_factors) %>%
  mutate(
    seed = 20200320 + 17 * 1:n()
  )
```

To allow reproducibility, we give a different seed for each scenario just to avoid anything confusing about shared randomness across scenarios.
<!--For more on is for reproducibility; we discuss this more in Section \@ref(sec-reproducability).-->
<!-- LWM: Need to reference new location of reprod.  Does this section exist anymore? -->
We then run the simulation 1000 times each for all our scenarios, and unnest to get our final data:

```{r run_the_CRT_simulation, eval=FALSE}
params$res = pmap(params, .f = run_CRT_sim, reps = 1000 )
res = params %>% 
  unnest( cols=c(data) )
saveRDS( res, file = "results/simulation_CRT.rds" )
```


```{r secret_run_full_CRT, include=FALSE}
if ( !file.exists( "results/simulation_CRT.rds" ) ) {
 
  source( here::here( "case_study_code/clustered_data_simulation_runner.R" ) )
} else {
  res = readRDS("results/simulation_CRT.rds")
}
```

Normally we would do this in parallel for speed; we discuss how to do that in Section \@ref(parallel-processing).
Even under parallel processing, this simulation took overnight to run.
Our final results look like this:

```{r}
res
```

We have a lot of rows of data!
Our next step is to group our results by our simulation factors and calculate all our performance metrics at once.
For example, here we calculate our main performance measures by hand:

```{r, messages=FALSE}
res <- readRDS( file = "results/simulation_CRT.rds" )

sres <- 
  res %>% 
  group_by( n_bar, J, ATE, size_coef, ICC, alpha, method ) %>%
  summarise( 
    bias = mean(ATE_hat - ATE),
    SE = sd( ATE_hat ),
    RMSE = sqrt( mean( (ATE_hat - ATE )^2 ) ),
    ESE_hat = sqrt( mean( SE_hat^2 ) ),
    SD_SE_hat = sqrt( sd( SE_hat^2 ) ),
    power = mean( p_value <= 0.05 ),
    R = n(),
    .groups = "drop"
  )
sres
```

If we want MCSEs (we do), then we can do that by hand as well, or use the `simhelpers` package as so:
```{r}
library( simhelpers )
sres <- res %>% 
  group_by( n_bar, J, ATE, size_coef, ICC, alpha, method ) %>%
  summarise( calc_absolute( estimates = ATE_hat,
                            true_param = ATE,
                            criteria = c("bias","stddev",
                                         "rmse")),
             calc_relative_var( estimates = ATE_hat,
                                var_estimates = SE_hat^2,
                                criteria = "relative rmse" ) ) %>%
  rename( SE = stddev,
          SE_mcse = stddev_mcse ) %>%
  dplyr::select( -K_absolute, -K_relvar )
sres
```


### Making analyze_data() quiet

<!-- LWM: Does this section need to be reworked in light of revision of chapter 8 (I think 8?) on making things quiet and trapping errors? -->

```{r, include=FALSE}

set.seed( 42544 )
dat <- gen_dat_model( n=5, J=3, p=0.5, 
                      gamma_0=0, gamma_1=0.2, gamma_2=0.2,
                      sigma2_u = 0, sigma2_e = 1,
                      alpha = 0.5 )
res <- analyze_data(dat)

```


If we run our Cluster RCT simulation when there is little cluster variation, we will get a lot of messages and warnings from our MLM estimator.
For example, from a single call we get:
```{r}
res <- analyze_data(dat)
```

When we scale up to our full simulations, these messages and warnings can become a nuisance.
<!--Furthermore, on our machine, the `lmer()` command can sometimes just fail (we believe there is some bug in the optimizer that fails if things are just perfectly wrong).
If our code failed on simulation run 944 out of 1000, we would lose everything!-->
To keep track of these issues, we trap messages and warnings using the `quietly()` method of the `purrr` package as so (see Chapter \@(#safe_code) for more on capturing warnings and trapping errors):

```
quiet_lmer = quietly( lmer )

quiet_analyze_data <- function( dat ) {
    
    # MLM
    M1 <- quiet_lmer( Yobs ~ 1 + Z + (1|sid), data=dat )
    M = M1$result
    message1 = ifelse( length( M1$message ) > 0, 1, 0 )
    warning1 = ifelse( length( M1$warning ) > 0, 1, 0 )

   ...

    # Compile our results
    tibble( 
      method = c( "MLM", "LR", "Agg" ),
      ATE_hat = c( est1, est2, est3 ),
      SE_hat = c( se1, se2, se3 ),
      p_value = c( pv1, pv2, pv3 ),
      message = c( message1, 0, 0 ),
      warning = c( warning1, 0, 0 )
    )
}
```

We now get a note about the message regarding convergence saved in our results:

```{r demo_analyze_CRT_data}
res <- quiet_analyze_data(dat)
res
```

See?  No more warnings are printed to the console, but we do see the message as a variable in our results.




## Exercises

### Comparing the trimmed mean, median and mean {#exercise:trimmed-mean}

In this exercise, you will write a simulation to compare different forms of an
estimator for estimating the same thing.
In particular, you will compare estimation of the
center of a symmetric distribution via mean, trimmed mean, and median (so the
mean and median are the same).  These are the three estimation strategies
that we might be comparing in a paper (pretend you "invented" the trimmed
mean and want to demonstrate its utility).

To do this, you should break building this simulation evaluation down into lots of
functions to show the general framework. This framework can readily be
extended to more complicated simulation studies.
This extended exercise illustrates how methodologists might compare different strategies for estimation, and is what we might see in the "simulation" section of a stats paper.

For our data-generation function use the scaled $t$-distribution so the standard deviation will always be 1 but we will have different fatness of tails (high chance of outliers):

```{r}
gen.data = function( n, mu, df0 ) {
    mu + rt( n, df=df0 ) / sqrt( df0 / (df0-2) )
}
```

The variance of a $t$ distribution is $df/(df-2)$, so when we divide our observations by the
square root of this, we standardize them so they have unit variance.

1) Verify this generates data with mean `mu` and standard deviation 1 for various `df0` values.

2) Write a method to calculate the mean, trimmed mean, and median of a vector of data.
   The trimmed mean should trim 10% of the data from each end.
   The method should return a data frame with the three estimates, with each row being one of our estimators.

3) Verify your method works by analyzing a dataset generated with `gen.data()`.
   For example, you can generate a dataset of size 100 with `gen.data(100, 0, 3)` and then analyze it.
   
4) Use `bundle_sim()` to create a simulation function that generates data and then analyzes it.
   The function should take `n` and `df0` as arguments, and return the estimates from your analysis method.
   Use `id` to give each simulation run an ID.

5) Run your simulation function for 1000 datasets of size 10, with `mu=0` and `df0=5`.
   Store the results in a variable called `raw.exps`.
   
6) Write a function to calculate the RMSE, bias, and standard error for your three estimators, given the results.

7) Make a single function that takes $df0$ and $n$, and runs a simulation and returns the performances of your three methods.

8) Now make a grid of $n =10, 50, 250, 1250$ and $df_0 =  3, 5, 15, 30$, and generate results for your multifactor simulation.

9) Make a plot showing how SE changes as a function of sample size for each estimator.  Are they different?



