# Estimation procedures

In the abstract, a function that implements an estimation procedure should have the following form:
```{r}
estimate <- function(sim_data) {

  # calculations/model-fitting/estimation procedures
  
  return(estimates)
}
```
The function takes a set of simulated data as input, fits a model or otherwise calculates a set of estimates, and produces as output a set estimates. The estimates could be point-estimates of parameters, standard errors, confidence intervals, etc. Depending on the research question, this function might involve use of a combination of several procedures (e.g., a diagnostic test for heteroskedasticity, followed by the conventional formula or heteroskedasticity-robust formula for standard errors). Also depending on the research question, we might need to create _several_ functions that implement different estimation procedures to be compared. 

Brown and Forsythe considered four different hypothesis testing procedures for heteroskedastic ANOVA. For starters, let's look at the simplest one, which is just to use a conventional one-way ANOVA (while mistakenly assuming homoskedasticity). The `oneway.test` function will calculate this test automatically:

```{r}
sim_data <- generate_data(mu = mu, sigma_sq = sigma_sq, sample_size = sample_size)
oneway.test(x ~ factor(group), data = sim_data, var.equal = TRUE)
```

The main result we need here is the p-value, which will let us assess the test's Type-I error and power for a given nominal $\alpha$-level. The following function takes simulated data as input and returns as output the p-value from a one-way ANOVA:

```{r}
ANOVA_F_aov <- function(sim_data) {
  oneway_anova <- oneway.test(x ~ factor(group), data = sim_data, var.equal = TRUE)
  return(oneway_anova$p.value)
}

ANOVA_F_aov(sim_data)
```

An alternative approach would be to program the ANOVA F statistic and test directly. Following the formulas on p. 129 of Brown and Forsythe (1974): 

```{r}
ANOVA_F_direct <- function(sim_data) {
  
  x_bar <- with(sim_data, tapply(x, group, mean))
  s_sq <- with(sim_data, tapply(x, group, var))
  n <- table(sim_data$group)
  g <- length(x_bar)
  
  df1 <- g - 1
  df2 <- sum(n) - g
  
  msbtw <- sum(n * (x_bar - mean(sim_data$x))^2) / df1
  mswn <- sum((n - 1) * s_sq) / df2
  fstat <- msbtw / mswn
  pval <- pf(fstat, df1, df2, lower.tail = FALSE)
 
  return(pval)
}

ANOVA_F_direct(sim_data)
```

This approach takes more work to program, but will end up being quicker to compute. To see the difference, we'll use an R package called `microbenchmark` to test how long the computations take for each version of the function. The `microbenchmark` function runs each expression 100 times (by default) and tracks how long the computations take. It then summarizes the distribution of timings:

```{r}
library(microbenchmark)
timings <- microbenchmark(Rfunction = ANOVA_F_aov(sim_data),
                          direct    = ANOVA_F_direct(sim_data))
timings
```

The direct function is `r with(summary(timings), round(mean[1] / mean[2], 1))` times faster than the built-in R function.

This result is pretty typical. Built-in R functions usually include lots of checks and error-handling, which take time to compute. These checks are crucial for messy, real-world data analysis but unnecessary with our pristine, simulated data. Here we can skip them by doing the calculations directly. 

Now let's consider another one of the tests considered by Brown and Forsythe. Here is a function that calculates the Welch test, again following the notation and formulas from the paper:

```{r}

Welch_F <- function(sim_data) {
   
  x_bar <- with(sim_data, tapply(x, group, mean))
  s_sq <- with(sim_data, tapply(x, group, var))
  n <- table(sim_data$group)
  g <- length(x_bar)

  w <- n / s_sq
  u <- sum(w)
  x_tilde <- sum(w * x_bar) / u
  msbtw <- sum(w * (x_bar - x_tilde)^2) / (g - 1)

  G <- sum((1 - w / u)^2 / (n - 1))
  denom <- 1 +  G * 2 * (g - 2) / (g^2 - 1)
  W <- msbtw / denom
  f <- (g^2 - 1) / (3 * G)

  pval <- pf(W, df1 = g - 1, df2 = f, lower.tail = FALSE)

  return(pval)
}

Welch_F(sim_data)
```

## Efficiency considerations

Computational efficiency is usually a secondary consideration when you're starting to design a simulation study. It's better to produce accurate code, even if it's a bit slow, than to use write code that is speedy but hard to follow (or even worse, that produces incorrect results). All that said, there is some glaring redundancy in the two functions we've looked at so far. Both of them start by taking the simulated data and calculating summary statistics for each group, using the following code:

```{r}
x_bar <- with(sim_data, tapply(x, group, mean))
s_sq <- with(sim_data, tapply(x, group, var))
n <- table(sim_data$group)
g <- length(x_bar)
```

In the interest of not repeating ourselves, it would better to pull this code out as a separate function and then re-write the `ANOVA_F` and `Welch_F` functions to take the summary statistics as input. Here is a function that takes simulated data and returns a list of summary statistics:

```{r}
summarize_data <- function(sim_data) {
  
  x_bar <- with(sim_data, tapply(x, group, mean))
  s_sq <- with(sim_data, tapply(x, group, var))
  n <- table(sim_data$group)
  g <- length(x_bar)
  
  return(list(x_bar = x_bar, s_sq = s_sq, n = n, g = g))
}

summarize_data(sim_data)
```

Now we can re-write the F-test functions to use the output of this function:

```{r}
ANOVA_F <- function(x_bar, s_sq, n, g) {
  
  df1 <- g - 1
  df2 <- sum(n) - g
  
  msbtw <- sum(n * (x_bar - weighted.mean(x_bar, w = n))^2) / df1
  mswn <- sum((n - 1) * s_sq) / df2
  fstat <- msbtw / mswn
  pval <- pf(fstat, df1, df2, lower.tail = FALSE)
 
  return(pval)
}

summary_stats <- summarize_data(sim_data)
with(summary_stats, ANOVA_F(x_bar = x_bar, s_sq = s_sq, n = n, g = g))

Welch_F <- function(x_bar, s_sq, n, g) {
   
  w <- n / s_sq
  u <- sum(w)
  x_tilde <- sum(w * x_bar) / u
  msbtw <- sum(w * (x_bar - x_tilde)^2) / (g - 1)

  G <- sum((1 - w / u)^2 / (n - 1))
  denom <- 1 +  G * 2 * (g - 2) / (g^2 - 1)
  W <- msbtw / denom
  f <- (g^2 - 1) / (3 * G)

  pval <- pf(W, df1 = g - 1, df2 = f, lower.tail = FALSE)

  return(data.frame(W = W, f = f, pval = pval))
}

with(summary_stats, Welch_F(x_bar = x_bar, s_sq = s_sq, n = n, g = g))
```

The results are the same as before. 

## Checking the estimation function(s)

Just as with the data-generating function, it is important to verify the accuracy of the estimation functions. For the ANOVA-F test, this can be done simply by checking the result of `ANOVA_F` against the built-in `oneway.test` function. Let's do that with a fresh set of data:

```{r}
sim_data <- generate_data(mu = mu, sigma_sq = sigma_sq, sample_size = sample_size)
aov_results <- oneway.test(x ~ factor(group), data = sim_data, var.equal = TRUE)
aov_results

summary_stats <- summarize_data(sim_data)
F_results <- with(summary_stats, ANOVA_F(x_bar = x_bar, s_sq = s_sq, n = n, g = g))
F_results
all.equal(aov_results$p.value, F_results)

```

We can follow the same approach to check the results of the Welch test because it is also implemented in `oneway.test`:

```{r}
aov_results <- oneway.test(x ~ factor(group), data = sim_data, var.equal = FALSE)
aov_results

W_results <- with(summary_stats, Welch_F(x_bar = x_bar, s_sq = s_sq, n = n, g = g))
W_results
all.equal(aov_results$p.value, W_results$pval)
```

Checking estimation functions can be a bit more difficult for procedures that are not already implemented in R. For example, the two other procedures examined by Brown and Forsythe, the James' test and Brown and Forsythe's $F*$ test, are not available in base R. They are available in the user-contributed package `onewaytests` (I found this by searching for "Brown-Forsythe" at http://rseek.org/). We could benchmark our calculations against this package, but of course there is some risk that the package might not be correct. Another route is to verify your results on numerical examples reported in authoritative papers, on the assumption that there's less risk of an error there. In the original paper that proposed the test, Welch (1951) provides a worked numerical example of the procedure. He reports the following summary statistics:

```{r}
g <- 3
x_bar <- c(27.8, 24.1, 22.2)
s_sq <- c(60.1, 6.3, 15.4)
n <- c(20, 20, 10)
```

He also reports $W = 3.35$ and $f = 22.6$. Replicating the calculations with our `Welch_F` function:

```{r}
Welch_F(x_bar = x_bar, s_sq = s_sq, n = n, g = g)
```

We get slightly different results! But we know that our function is correct---or at least consistent with `oneway.test`---so what's going on? It turns out that there was an error in some of Welch's intermediate calculations, which can only be spotted because he reported all of his work in the paper. 

## Exercises

The following exercises involve exploring and tweaking the simulation code we've developed to replicate the results of Brown and Forsythe (1974). Here are the key functions for the data-generating process and estimation procedures:
```{r}

generate_data <- function(mu, sigma_sq, sample_size) {

  N <- sum(sample_size) 
  g <- length(sample_size) 
  
  group <- rep(1:g, times = sample_size) 
  mu_long <- rep(mu, times = sample_size)
  sigma_long <- rep(sqrt(sigma_sq), times = sample_size) 
  
  x <- rnorm(N, mean = mu_long, sd = sigma_long)
  sim_data <- data.frame(group = group, x = x)
    
  return(sim_data)
}

summarize_data <- function(sim_data) {
  
  x_bar <- with(sim_data, tapply(x, group, mean))
  s_sq <- with(sim_data, tapply(x, group, var))
  n <- table(sim_data$group)
  g <- length(x_bar)
  
  return(list(x_bar = x_bar, s_sq = s_sq, n = n, g = g))
}

ANOVA_F <- function(x_bar, s_sq, n, g) {
  
  df1 <- g - 1
  df2 <- sum(n) - g
  
  msbtw <- sum(n * (x_bar - weighted.mean(x_bar, w = n))^2) / df1
  mswn <- sum((n - 1) * s_sq) / df2
  fstat <- msbtw / mswn
  pval <- pf(fstat, df1, df2, lower.tail = FALSE)
 
  return(pval)
}

Welch_F <- function(x_bar, s_sq, n, g) {
   
  w <- n / s_sq
  u <- sum(w)
  x_tilde <- sum(w * x_bar) / u
  msbtw <- sum(w * (x_bar - x_tilde)^2) / (g - 1)

  G <- sum((1 - w / u)^2 / (n - 1))
  denom <- 1 +  G * 2 * (g - 2) / (g^2 - 1)
  W <- msbtw / denom
  f <- (g^2 - 1) / (3 * G)

  pval <- pf(W, df1 = g - 1, df2 = f, lower.tail = FALSE)

  return(data.frame(W = W, f = f, pval = pval))
}
```

### Estimation helper function

Write a helper function that rolls up the `summarize_data`, `ANOVA_F`, and `Welch_F` functions and outputs p-values for the ANOVA F-test and the Welch test. Here's a skeleton to fill in:
```{r}
calculate_pvals <- function(sim_data) {
  
  # fill in the guts here
  
  return(pvals)
}
```

Use the helper function to simplify the following `replicate` call (you'll need to change `eval` to `TRUE` in the header of this code chunk for the code to compile when you knit):

```{r, eval = FALSE}
mu <- rep(0, 4)
sample_size <- c(4, 8, 10, 12)
sigma_sq <- c(3, 2, 2, 1)^2
p_vals <- replicate(n = 10000, {
  sim_data <- generate_data(mu = mu, sigma_sq = sigma_sq, sample_size = sample_size) 
  summary_stats <- summarize_data(sim_data)
  anova_p <- with(summary_stats, ANOVA_F(x_bar = x_bar, s_sq = s_sq, n = n, g = g))
  Welch <- with(summary_stats, Welch_F(x_bar = x_bar, s_sq = s_sq, n = n, g = g))
  data.frame(ANOVA = anova_p, Welch = Welch$pval)
}, simplify = FALSE)
```

### The BFF\* test 

Write a function that implements the Brown-Forsythe F\*-test (the BFF\* test!) as described on p. 130 of Brown and Forsythe (1974). Incorporate the function into the `calculate_pvals` function from the previous question, and use it to estimate rejection rates of the BFF\* test for the parameter values in the fifth line of Table 1 (which are the same as those used in the previous question). 

```{r}
BF_F <- function(x_bar, s_sq, n, g) {
  
  # fill in the guts here
  
  return(pval = pval)
}

# Further R code here
```
