
```{r, include=FALSE}
library( tidyverse )
```

# (PART) An Introductory Look  {-}

# Introduction {#introduction}

Monte Carlo simulations are computational experiments that involve using random number generators to study the behavior of statistical or mathematical models. 
Monte Carlo studies are an essential tool in many different fields of science---climate science, engineering, education research, as three examples---and are used for a variety of different purposes. 
Simulations are used to model complex stochastic processes such as weather patterns [@jones2012IntroductionScientificProgramming; @robert2010IntroducingMonteCarlo]; 
to generate parameter estimates from complex statistical models, as in Markov Chain Monte Carlo sampling [@gelman2013BayesianDataAnalysis]; 
and even to estimate uncertainty in statistical summaries, as in bootstrapping [@davison1997BootstrapMethodsTheir].
Within the fields of statistics and quantitative methodology, simulation studies play a critical role in the development and validation of methods for data analysis. 
In this book, we examine the use of simulation for this last purpose.
<!--of using Monte Carlo simulation methods to understand the properties of data analysis methods. -->

Within quantitatively oriented fields, researchers developing new statistical methods or evaluating the use of existing methods nearly always use Monte Carlo simulations as part of their research process. 
In this context, researchers use simulations in a way analogous to how a chef would use their test kitchen to develop new recipes before putting them on the menu, how a manufacturer would use a materials testing laboratory to evaluate the safety and durability of a new consumer good before bringing it to market, or how an astronaut would prepare for a spacewalk by practicing the process in an underwater mock-up.
Simulation studies provide a clean and controlled environment for testing out data analysis approaches before putting them to use with real empirical data.

At a very high level, Monte Carlo simulation provides a method for understanding the performance of a statistical model or data analysis method under conditions where the truth is known and can be controlled.
The basic approach for doing so is as follows:

1. Create artificial data using random number generators based on a specific statistical model, or more generally, a _Data-Generating Process_ (DGP).
2. Apply one or more data-analysis procedures, such as estimating a particular statistical model, to the artificial data. (This might be something as simple as calculating a difference in sample means, or it might be an involved, multi-step procedure involving cleansing the data of apparent outliers, imputing missing values, applying a machine-learning algorithm, and carrying out further calculations on the predictions of the algorithm.)
3. Repeat Steps 1 and 2 many times. 
4. Summarize the results across these repetitions in order to understand the general trends or patterns in how the method works. 

This approach is useful because one can control the data-generating process and therefore fully know the truth---something that is almost always uncertain when analyzing real, empirical data. 
Having full control of the data-generating process makes it possible to assess how well a procedure works by comparing the estimates produced by the data analysis procedure against this known truth.
For instance, one can see if estimates from a statistical procedure are consistently too high or too low.
Using simulation, one can also compare multiple data analyis procedures by comparing the degree of error in each set of results to determine which procedure is generally more accurate when applied to the same collection of artificial datasets.

In this book, we focus on this core purpose of simulation, but even this core purpose has many possible variants.
To seed the field, we next give a high level overview of a variety of different uses for simulation studies in quantitative research, and then we discuss some of the limitations and pitfalls of simulation studies that we should keep in mind as we proceed.
<!-- JEP: I feel like we need a closing thought in this paragraph. -->
<!-- LWM: I tried, it is not great. -->

## Some of simulation's many uses

Monte Carlo simulations allow for rapid exploration of different data analysis procedures and, even more broadly, different approaches to designing studies and collecting measurements. Simulations are especially useful because they provide a means to answer questions that are difficult or impossible to answer by other means.
Many statistical models and estimation methods _can_ be analyzed mathematically, but only by using asymptotic approximations that describe how the methods work as the sample sizes increases towards infinity. 
In contrast, simulation methods provide answers for specific, finite sample sizes.
Thus, they allow researchers to study models and estimation methods where relevant algebraic formulas are not available, not easily applied, or not sufficiently accurate.

Circumstances where simulations are helpful---or even essential---occur in a range of different situations within quantitative research. 
To set the stage for our subsequent presentation, consider the following areas where one might find need of simulation.

### Comparing statistical approaches

Perhaps one of the most common uses of Monte Carlo simulation is to compare alternative statistical approaches to analyzing the same type of data.
In the academic literature on statistical methodology, authors frequently report simulation studies  comparing a newly proposed method against more traditional approaches, to make a case for the utility of their method.
A classic example of such work is @brown1974SmallSampleBehavior, who compared four different procedures for conducting a hypothesis test for equality of means in several populations (i.e., one-way ANOVA) when the population variances are not equal.
A subsequent study by @mehrotra1997ImprovingBrownforsytheSolution built on Brown and Forsythe's work, proposing a more refined method and using simulations to demonstrate that it is superior to the existing methods.
We revisit the @brown1974SmallSampleBehavior example in Chapter \@ref(case-ANOVA).

Comparative simulation can also have a practical application: In many situations, more than one approach or data analysis methods is possible for addressing a given research question (or estimating a specified target parameter).
Simulations comparing the costs of multiple approaches can be quite informative and can help to guide the design of analytic plans (such as plans included in pre-registered study protocols).
As an example of the type of questions that researchers might encounter in designing analytic plans: what are the practical benefits and costs of using a model that allows for cross-site impact variation for a multi-site trial [@miratrix2021applied]?
In the ideal case, simulations can identify best practices for how to approach analysis of a certain type of data and can surface trade-offs between competing approaches that occur in practice.

### Assessing performance of complex pipelines

In practice, statistical methods are often used in as part of a multi-step workflow.
For instance, in a regression model, one might first use a statistical test for heteroskedasticity (e.g., the White test or the Breusch-Pagan test) and then determine whether to use conventional or heteroskedasticity-robust standard errors depending on the result of the test. 
This combination of an initial diagnostic test followed by contingent use of different statistical procedures is all but impossible to analyze mathematically, but it is straight-forward to simulate [see, for example, @longUsingHeteroscedasticityConsistent2000].
In particular, simulations are a straight-foward way to assess whether a proposed workflow is _valid_---that is, whether the conclusions from a pipeline are correct at a given level of certainty.

Beyond just evaluating the performance characteristics of a workflow, simulating a multi-step workflow can actually be used as a technique for _conducting_ statistical inference with real data.
Data analysis approaches such as randomization inference or bootstrapping [@davison1997BootstrapMethodsTheir] involve repeatedly simulating data and putting it through an analytic pipeline, in order to assess the uncertainty of an estimate based on real data.
In bootstrapping, the variation in a point estimate across replications of the simulation is used as the standard error for the context being simulated; 
an argument by analogy (the bootstrap analogy) is what connects this to inference on the original data and point estimate (see, e.g., the first few chapters of @davison1997BootstrapMethodsTheir or @Efron1999 for discussion of this principle, and see @good2013permutation or @lehmann1975statistical for more on permutation inference).

### Assessing performance under misspecification

Many statistical estimation procedures are known to perform well when the assumptions they entail are correct.
However, data analysts must also be concerned with the _robustness_ of estimation procedures---that is, their performance when one or more of the assumptions is violated to some degree.
For example, in a multilevel model, how important is the assumption that the random effects are normally distributed? What about normality of the individual-level error terms? What about homoskedasticity of the individual-level error terms?
Quantitative researchers routinely contend with such questions when analyzing empirical data.

Similar concerns arise for researchers considering the trade-offs between methods that make relatively stringent assumptions versus methods that are more flexible or adaptive. When the true data-generating process meets stringent assumptions (e.g., a treatment effect that is constant across the population of participants), what are the potential gain of exploiting such structure in the estimation process?
Conversely, what are the costs (in terms of computation time and precision) of using more flexible methods that do not impose strong assumptions?
A researcher designing an analytic plan would want to be well-informed of such trade-offs and, ideally, would want to situate their understanding in the context of the empirical phenomena that they study.
Simulation allows for such investigation and comparison.

### Assessing the finite sample performance of a statistical approach

Many statistical estimation procedures can be shown (through mathematical analysis) to work well _asymptotically_---that is, given an infinite amount of data---but their performance for data of a given, finite size is more difficult to quantify.
Although mathematical theory can inform us about "asymptopia,"  researchers live in a world of finite sample sizes, where it can be difficult to gauge if one's real data is large enough that the asymptotic approximations apply.
This is of particular concern with hierarchical data structures that include only 20 to 40 clusters---a common circumstance in many randomized field trials in education research. 
<!-- JEP: Citation for this? -->
<!-- LWM: I will have one in 6 months?  But I don't know of one off the top of my head. -->
Simulation is a tractable approach for assessing the small-sample performance of such estimation methods or for determining minimum required sample sizes for adequate performance. 

One example of a simulation investigating questions of finite-sample behavior comes from @longUsingHeteroscedasticityConsistent2000, whose evaluated the performance of heteroskedasticity-robust standard errors (HRSE) in linear regression models.
Asymptotic analysis indicates that HRSEs work well in sufficiently large samples [that is, it can be shown that they provide correct assessments of uncertainty when $N$ is infinite; see @White1980heteroskedasticity], but what about in realistic contexts?
@longUsingHeteroscedasticityConsistent2000 use extensive simulations to investigate the properties of different versions of HRSEs for linear regression across a range of sample sizes, demonstrating that the most commonly used form of these estimators often does _not_ work well with sample sizes found in typical social science applications.
<!-- Simulation could answer what asymptotics could not: how these estimators work in actual practice. -->
<!-- JEP: Don't like the above sentence because there actually was a fair bit of mathematical analysis that addressed this question too. It's just obscure and inaccessible to most researchers. -->
<!-- LWM: Happy to cut, but saying somehting about avoiding math might be nice? -->

<!-- LWM: Do we need another example or are we good as it stands, do you think? -->

### Conducting Power Analyses

During the process of proposing, seeking funding for, and planning an empirical research study, researchers need to justify the design of the study, including the size of the sample that they aim to collect. 
Part of such justifications may involve a _power analysis_, or an approximation of the probability that the study will show a statistically significant effect, given assumptions about the magnitude of true effects and other aspects of the data-generating process.

Many guidelines and tools are available for conducting power analysis for various research designs, including software such as [PowerUp!](https://www.causalevaluation.org/power-analysis.html) [@dong2013PowerUpToolCalculating], [the Generalizer](https://www.thegeneralizer.org/) [@tipton2014stratified], and [G*Power](https://www.psychologie.hhu.de/arbeitsgruppen/allgemeine-psychologie-und-arbeitspsychologie/gpower) [@faul2009StatisticalPowerAnalyses].
These tools use analytic formulas for power, which are often derived using approximations and simplifying assumptions about a planned design. Simulation provides a very general-purpose alternative for power calculations, which can avoid such approximations and simplifications.
By repeatedly simulating data based on a hypothetical process and then analyzing data following a specific protocol, one can _computationally_ approximate the power to detect the an effect of a specified size.

Using simulation instead of analytic formulas allows for power analyses that are more nuanced and more tailored to the researcher's circumstance than what can be obtained from available software. In particular, simulation can be useful for the following:

 * When estimating power in multi-site, block- or cluster-randomized trials, the formulas implemented in available software assume that sites are of equal size and that outcome distributions are unrelated to the size of each site. 
 Small deviations from these assumptions are unlikely to change the results, but in practice, researchers may face situations where sites vary quite widely in size or where site-level outcomes are related to site size.
 Simulation can estimate power in this case. 

 * Available software such as [PowerUp!](https://www.causalevaluation.org/power-analysis.html) allows investigators to build in assumptions about anticipated rates of attrition in cluster-randomized trials, under the assumption that attrition is completely at random. 
 However, researchers might anticipate that attrition will be related to baseline characteristics, leading to data that is missing at random but not completely at random. 
 Simulation can be used to assess how this affect the power of a planned study.
 
* There are some closed-form expressions for power to test mediational relations (i.e., indirect and direct effects) in a variety of different experimental designs, and these formulas are now available in [PowerUp!](https://www.causalevaluation.org/power-analysis.html). 
 However, the formulas involve a large number of parameters (including some where it may be difficult to develop credible assumptions) and they apply only to a specific analytic model for the mediating relationships.
 Researchers planning a study to investigate mediation might therefore find it useful to generate realistic data structures and conduct power analysis via simulation.

### Simulating processess

Yet another common use for Monte Carlo simulation is as a way to emulate a complex process as a means to better understand it or to evaluate the consequences of modifying it.
A famous area of process simulation are climate models, where researchers simulate the process of climate change.
These physical simulations mimic very complex systems to try and understand how perturbations (e.g., more carbon release) will impact downstream trends.

Another example of process simulation arises in education research.
Some large school districts such as New York City have centralized lotteries for school assignment, which entail having families rank schools by order of preference.
The central office then assigns students to schools via a lottery procedure where each student gets a lottery number that breaks ties when there are too many students desiring to go to a certain school.
Students' school assignments are therefore based in part on random chance, but the the process is quite complex: each student has some probability of assignment to each school on their list, but the probabilities depend on their choices and the choices of other students.

The school lottery process creates a natural experiment, based on which researchers can estimate the causal impact of being assigned to one school vs. another.
However, a defensible analysis of the process requires knowing the probabilities of school assignment. @abdulkadirouglu2017research conducted such an evaluation using the school lottery process in New York City. 
They calculated school assignment probabilities via simulation, by running the school lottery over and over, changing only students' lottery numbers, and recording students' school assignments in each repetition of the process.
Simulating the lottery process a large number of times provided precise estimates of each students' assignment probabilities, based on which @abdulkadirouglu2017research were able to estimate causal impacts of school assignment.

For another example, one that possibly illustrates the perils of simulation as taking us away from results that pass face validity, @staiger2010searching simulate the process of firing teachers depending on their estimated value added scores.
Based on their simulations, which model firing different proportions of teachers, they suggest that firing substantial portions of the teacher workforce annually would substantially improve student test scores.
Their work offers a clear illustration of how simulations can be used to examine the potential consequences of various policy decisions, assuming the underlying assumptions hold true.
This example also brings home a core concern of simulation: we only learn about the world we are simulating, and the relevance of simulation evidence to the real world is by no means guaranteed.
We will explore this concern more thoroughly in the following section.

<!-- JEP: The theme of "perils of simualtion" hasn't been introduced yet, so perhaps we should save this example for later? -->
<!-- LWM: I tweaked.  Thoughts? -->

## The perils of simulation as evidence

Simulation has the potential to be a powerful tool for investigating quantitative methods.
However, evidence from simulation studies is also fundamentally limited in certain ways, and thus very susceptible to critique.
The core advantage of simulation studies is that they allow for evaluation of data analysis methods under _specific_ and exact conditions, avoiding the need for approximation.
The core limitation of simulations stems from this same property: they provide information about the performance of data analysis methods under specified conditions, but no guarantee that patterns of performance hold in general.
One can partially address questions of generalization by examining a wide range of conditions, looking to see whether a pattern holds consistently or changes depending on features of the data-generating process.
Even this strategy has limitations, though.
Except for very simple processes, we can seldom consider every possible set of conditions. 

Critiques of simulation studies often revolve around the _realism_, _relevance_, or _generality_ of the data generating process.
Are the simulated data realistic, in the sense that they follow similar patterns to what one would see in real empirical data?
Was the simulation systematic in exploring a wide variety of scenarios, so that general conclusions are warranted?

We see at least three principles for addressing questions like this.
Perhaps most fundamental is to be transparent in one's methods and reasoning:
explicitly state what was done, and provide code so that others can reproduce one's results or tweak them to test variations of the data-generating process or alternative analysis strategies.
Another important component of a robust argument is to systematically  vary the conditions under examination. This is faciliated by writing code in a way to make it easy to simulate across a range of different data-generating scenarios.
Once that is in place, one can systematically explore myriad scenarios and report all of the results.
Finally, one can draw on relevant statistical theory to support the design of a simulation and interpretation of its results. 
Mathematical analysis might indicate that some features of a data-generating process will have a strong influence on the performance of a method, while other features will not matter at all when sample sizes are sufficiently large. 
Well designed simulations will choose conditions to examine that are motivated by or complement what is known based on existing statistical theory.

## Simulation-hacking 

<!-- LWM: Not following section title here -->
<!-- JEP: Was trying to play off of "p-hacking" but probably too obscure/confusing. -->

As we will see in later chapters, the design of a simulation study entails myriad choices, often over a very large space of possibilities. 
This flexibility leaves lots of room for discretion and judgement.  
Due to this flexibility, simulation findings are held in great skepticism by many.
The following motto summarizes the skeptic's concern:

> Simulations are doomed to succeed.

Simulations are alluring: once a simulation framework is set up, it is easy to tweak and adjust.
It is natural for us all to continue to do this until the simulation works "as it should."
If our goal is to show something that we already believe is correct (e.g., that our fancy new estimation procedure is better than existing methods), we will eventually find a way to align our simulation with our intuition.[^guest-speakers]
This is, simply put, a version of fishing.
To counteract this possibility, we believe it is critical to push ourselves to design scenarios where things do not work as expected.
An aspiration of simulation studies should be to explore the boundary conditions that separate where preferred methods work and where they break or fail.

[^guest-speakers]: A comment from James: I recall attending seminars in the statistics department during graduate school, where guest speakers usually presented both some theory and some simulation results. A few years into my graduate studies, I realized that the simulation part of the presentation could nearly always be summarized as "we did some simulations and showed that our new method works better than old methods under conditions that we have cleverly selected to be favorable for our approach." I fervently hope that my own work is not as boring or predictable as my memory of these seminars.

## Simulating to learn

Thus far, we have focused on using Monte Carlo simulations for methodological research.
Readers who do not identify as methodologists may question whether there is any benefit to learning how to do simulations if they are never going to conduct methodological research studies or use simulation to aid in planning an empirical study.
We think the answer to this is an emphatic _YES_, for at least two reasons.

First, readers who do any sort of quantitative data analysis need to make decisions about what methods to use in their own practice. 
Across fields, existing guidance about data analysis practice is almost certainly informed by simulation research of some form, whether well-designed and thorough or haphazard and poorly reasoned. 
Consequently, having a high-level understanding of the logic and limitations of simulation will help you to be a critical consumer of methods research, even if you do not intend to conduct methods research of your own.

Second, conducting simulations deepens one's understanding of the logic of statistical modeling and statistical inference.
Learning a new statistical model (such as generalized linear mixed models) or analytic technique (such as multiple imputation by chained equations) requires taking in _a lot_ of detailed information, from the assumptions of the model to the interpretation of parameter estimates to the best practices for estimation and what to do if some part of the process goes off. 
To thoroughly digest all these details, we have found it invaluable to _simulate_ data based on the model under consideration. 
This usually requires translating mathematical notation into computer code, an exercise which makes the components of the model more tangible than just a jumble of Greek letters.
The simulated data is then available for inspection, summarizing, graphing, and further calculation, all of which can aid interpretation.
Furthermore, the process of simulating yields a dataset which can then be used to practice implementing the analysis procedure and interpreting the results.
We have found that following this type of process is a highly effective way to learn new models and methods, worthwhile even if one has no intention of carrying out methodological research. 
We might even go so far as to argue that _whatever you might think, you don't really understand a statistical model until you've done a simulation of it._

## Organization of the text

This book leans heavily into the "spiral curriculum" idea, where we introduce a concept, then revisit it in more depth later on.
In particular, we will kick off with a tiny simulation, provide a high-level overview of the simulation process, and then dive into the details of each step in subsequent chapters.
We have many case studies throughout the book; these are designed to make the topics under discussion salient, and also designed to provide chunks of code that you can copy and use for your own purposes.

We devided the book into several parts.
Part I (which you are reading now) makes a case for simulation, and gives an initial simulation to set the stage of our approach.
Part II lays out the core components (generating artificial data, bundling analytic procedures, running a simulation, analyzing the simulation results) for simulating for a single context, and closes with some more involved case studies that illustrate the principles of modular, tidy simulation design.
Part III moves to multifactor simulations, i.e., simulations across multiple types of scenario.
This is the heart of good simulation design: by evaluating or comparing estimation procedures across multiple scenarios we can begin to understand general properties of these procedures.

We then close with two final parts.
Part IV provides extensions to the simulation process, covering topics such as reproducibility, parallelization, and handling errors.
Our final Part V gives three extensions to specialized forms of simulation (simulating to calculate power, simulating within a potential outcomes framework for causal inference, and the parametric bootstrap) to help show how simulation can be used to answer a wide variety of questions in a wide variety of contexts, thus connecting back to the many purposes discussed above.
We have some appendices with some coding tidbits and further resources as well.


