
```{r, include=FALSE}
library( tidyverse )
```

# (PART) An Introductory Look  {-}

# Introduction {#introduction}

Monte Carlo simulations are a tool for studying the behavior of random processes, such as the behavior of a statistical estimation procedure when applied to a sample of data. 
Within quantitatively oriented fields, researchers developing new statistical methods or evaluating the use of existing methods nearly always use Monte Carlo simulations as part of their research process. 
In the context of methodological development, researchers use simulations in a way analogous to how a chef would use their test kitchen to develop new recipes before putting them on the menu, how a manufacturer would use a materials testing laboratory to evaluate the safety and durability of a new consumer good before bringing it to market, or how an astronaut would prepare for a spacewalk by practicing the process in an underwater mock-up.
Simulation studies provide a clean and controlled environment for testing out data analysis approaches before putting them to use with real empirical data.

More broadly, Monte Carlo studies are an essential tool in many different fields of science---climate science, engineering, and education research are three examples---and are used for a variety of different purposes. 
Simulations are used to model complex stochastic processes such as weather patterns [@jones2012IntroductionScientificProgramming; @robert2010IntroducingMonteCarlo]; 
to generate parameter estimates from complex statistical models, as in Markov Chain Monte Carlo sampling [@gelman2013BayesianDataAnalysis]; 
and even to estimate uncertainty in statistical summaries, as in bootstrapping [@davison1997BootstrapMethodsTheir].
In this book, we shall focus on using simulation for the development and validation of methods for data analysis, which are everyday concerns within the fields of statistics and quantitative methodology.
However, we also believe that many of the general principles of simulation design and execution that we will discuss are broadly applicable to these other purposes, and we note connections as they occur.

At a very high level, Monte Carlo simulation provides a method for understanding the performance of a statistical model or data analysis method under conditions where the truth is known and can be controlled.
The basic approach for doing so is as follows:

1. Create artificial data using random number generators based on a specific statistical model, or more generally, a _Data-Generating Process_ (DGP).
2. Apply one or more data-analysis procedures to the artificial data. (These procedures might be something as simple as calculating a difference in sample means or fitting a regression model, or it might be an involved, multi-step procedure involving cleansing the data of apparent outliers, imputing missing values, applying a machine-learning algorithm, and carrying out further calculations on the predictions of the algorithm.)
3. Repeat Steps 1 and 2 many times. 
4. Summarize the results across these repetitions in order to understand the general trends or patterns in how the method works. 

Simulation is useful because one can control the data-generating process and therefore fully know the truth---something that is almost always uncertain when analyzing real, empirical data. 
Having full control of the data-generating process makes it possible to assess how well a procedure works by comparing the estimates produced by the data analysis procedure against this known truth.
For instance, we can see if estimates from a statistical procedure are consistently too high or too low (i.e., whether an estimator is systematically biased).
We can also compare multiple data analysis procedures by assessing the degree of error in each set of results to determine which procedure is generally more accurate when applied to the same collection of artificial datasets.


This basic process of simulation can be used to investigate an array of different questions that arise in statistics and quantitative methodology.
To seed the field, we now give a high-level overview of some of the major use cases. We then discuss some of the major limitations and common pitfalls of simulation studies, which are important to keep in mind as we proceed.

## Some of simulation's many uses

Monte Carlo simulations allow for rapid exploration of different data analysis procedures and, even more broadly, different approaches to designing studies and collecting measurements. Simulations are especially useful because they provide a means to answer questions that are difficult or impossible to answer by other means.
Many statistical models and estimation methods _can_ be analyzed mathematically, but only by using asymptotic approximations that describe how the methods work as sample size increases towards infinity. 
In contrast, simulation methods provide answers for specific, finite sample sizes.
Thus, they allow researchers to study models and estimation methods where relevant mathematical formulas are not available, not easily applied, or not sufficiently accurate.

Circumstances where simulations are helpful---or even essential---occur in a range of different situations within quantitative research. 
To set the stage for our subsequent presentation, consider the following areas where one might find need of simulation.

### Comparing statistical approaches

One of the more common uses of Monte Carlo simulation is to compare alternative statistical approaches to analyzing the same type of data.
In the academic literature on statistical methodology, authors frequently report simulation studies  comparing a newly proposed method against more traditional approaches, to make a case for the utility of their method.
As Dr. Little puts it in his article _Ten simple powerful ideas for the statistical scientist_ (with "Embrace Well-Designed Simulation Experiments" being one of the ten!), "thoughtfully designed frequentist simulation experiments can cast useful light on the properties of a method" [@little2013praise].

A classic example of simulation for such evaluation is @brown1974SmallSampleBehavior, who compared four different procedures for conducting a hypothesis test for equality of means in several populations (i.e., one-way ANOVA) when the population variances are not equal.
A subsequent study by @mehrotra1997ImprovingBrownforsytheSolution built on Brown and Forsythe's work, proposing a more refined method and using simulations to demonstrate that it is superior to the existing methods.
We explore the @brown1974SmallSampleBehavior study in the case study of Chapter \@ref(case-ANOVA).

Comparative simulation can also have a practical application: In many situations, more than one data analysis approach is possible for addressing a given research question (or estimating a specified target parameter).
Simulations comparing multiple approaches can be quite informative and can help to guide the design of an analytic plan (such as plans included in a pre-registered study protocol).
For instance, researchers designing a multi-site randomized experiment might wonder whether they should use an analytic model that allows for variation in the site-specific impact estimates [@miratrix2021applied] or a simpler model that treats the impact as homogeneous across sites. 
What are the practical benefits and costs of using the more complex model?
In the ideal case, simulations can identify best practices for how to approach analysis of a certain type of data and can surface trade-offs between competing approaches that occur in practice.

### Assessing performance of complex pipelines

In practice, statistical methods are often used as part of a multi-step workflow.
For instance, in a regression model, one might first use a statistical test for heteroskedasticity (e.g., the White test or the Breusch-Pagan test) and then determine whether to use conventional or heteroskedasticity-robust standard errors depending on the result of the test. 
This combination of an initial diagnostic test followed by contingent use of different statistical procedures is quite difficult to analyze mathematically, but it is straight-forward to simulate [see, for example, @longUsingHeteroscedasticityConsistent2000].
In particular, simulations are a straight-foward way to assess whether a proposed workflow is _valid_---that is, whether the conclusions from a pipeline are correct at a given level of certainty.

Beyond just evaluating the performance characteristics of a workflow, simulating a multi-step workflow can actually be used as a technique for _conducting_ statistical inference with real data.
Data analysis approaches such as randomization inference and bootstrapping involve repeatedly simulating data and putting it through an analytic pipeline, in order to assess the uncertainty of the original estimate based on real data.
In bootstrapping, the variation in a point estimate across replications of the simulation is used as the standard error for the context being simulated; 
an argument by analogy (the bootstrap analogy) is what connects this to inference on the original data and point estimate.
See the first few chapters of @davison1997BootstrapMethodsTheir or @efron2000BootstrapModernStatistics for further discussion of bootstrapping, and see @good2013permutation or @lehmann1975statistical for more on permutation inference.

### Assessing performance under misspecification

Many statistical estimation procedures are known to perform well when the assumptions they entail are correct.
However, data analysts must also be concerned with the _robustness_ of estimation procedures---that is, their performance when one or more of the assumptions is violated to some degree.
For example, in a multilevel model, how important is the assumption that the random effects are normally distributed? What about normality of the individual-level error terms? What about homoskedasticity of the individual-level error terms?
Quantitative researchers routinely contend with such questions when analyzing empirical data, and simulation can provide some answers.

Similar concerns arise for researchers considering the trade-offs between methods that make relatively stringent assumptions versus methods that are more flexible or adaptive. When the true data-generating process meets stringent assumptions (e.g., a treatment effect that is constant across the population of participants), what are the potential gain of exploiting such structure in the estimation process?
Conversely, what are the costs (in terms of computation time or precision) of using more flexible methods that do not impose strong assumptions?
A researcher designing an analytic plan would want to be well-informed of such trade-offs and, ideally, would want to situate their understanding in the context of the empirical phenomena that they study.
Simulation allows for such investigation and comparison.

### Assessing the finite-sample performance of a statistical approach

Many statistical estimation procedures can be shown (through mathematical analysis) to work well _asymptotically_---that is, given an infinite amount of data---but their performance for data of a given, finite size is more difficult to quantify.
Although mathematical theory can inform us about "asymptopia," empirical researchers live in a world of finite sample sizes, where it can be difficult to gauge if one's real data is large enough that the asymptotic approximations apply.
For example, this is of particular concern with hierarchical data structures that include only 20 to 40 clusters---a common circumstance in many randomized field trials in education research. 
<!-- JEP: Citation for this? -->
<!-- LWM: I will have one in 6 months?  But I don't know of one off the top of my head. -->
Simulation is a tractable approach for assessing the small-sample performance of such estimation methods or for determining minimum required sample sizes for adequate performance. 

One example of a simulation investigating questions of finite-sample behavior comes from @longUsingHeteroscedasticityConsistent2000, whose evaluated the performance of heteroskedasticity-robust standard errors (HRSE) in linear regression models.
Asymptotic analysis indicates that HRSEs work well (in the sense of providing correct assessments of uncertainty) in sufficiently large samples [@White1980heteroskedasticity], but what about in realistic contexts where small samples occur?
@longUsingHeteroscedasticityConsistent2000 use extensive simulations to investigate the properties of different versions of HRSEs for linear regression across a range of sample sizes, demonstrating that the most commonly used form of these estimators often does _not_ work well with sample sizes found in typical social science applications.
Via simulation, they provided compelling evidence about a problem without having to wade into a technical (and potentially inaccessible) mathematical analysis of the problem. 

<!-- LWM: Do we need another example or are we good as it stands, do you think? -->
<!-- JEP: Would be nice to have another example at some point, about something other than heteroskedasticity. -->

### Conducting Power Analyses

During the process of proposing, seeking funding for, and planning an empirical research study, researchers need to justify the design of the study, including the size of the sample that they aim to collect. 
Part of such justifications may involve a _power analysis_, or an approximation of the probability that the study will show a statistically significant effect, given assumptions about the magnitude of true effects and other aspects of the data-generating process.
Researchers may also wish to compare the power of different possible designs in order to inform decisions about how to carry out the proposed study given a set of monetary and temporal constraints.

Many guidelines and tools are available for conducting power analysis for various research designs, including software such as [PowerUp!](https://www.causalevaluation.org/power-analysis.html) [@dong2013PowerUpToolCalculating], [the Generalizer](https://www.thegeneralizer.org/) [@tipton2014stratified],  [G*Power](https://www.psychologie.hhu.de/arbeitsgruppen/allgemeine-psychologie-und-arbeitspsychologie/gpower) [@faul2009StatisticalPowerAnalyses], and [PUMP](https://cran.r-project.org/web//packages/PUMP/index.html) [@hunter2023PowerMultiplicityProject].
These tools use analytic formulas for power, which are often derived using approximations and simplifying assumptions about a planned design. Simulation provides a very general-purpose alternative for power calculations, which can avoid such approximations and simplifications.
By repeatedly simulating data based on a hypothetical process and then analyzing data following a specific protocol, one can _computationally_ approximate the power to detect an effect of a specified size.

Using simulation instead of analytic formulas allows for power analyses that are more nuanced and more tailored to the researcher's circumstance than what can be obtained from available software.
For example, simulation can be useful for the following:

 * When estimating power in multi-site, block- or cluster-randomized trials, the formulas implemented in available software assume that sites are of equal size and that outcome distributions are unrelated to the size of each site. 
 Small deviations from these assumptions are unlikely to change the results, but in practice, researchers may face situations where sites vary quite widely in size or where site-level outcomes are related to site size.
 Simulation can estimate power in this case. 

 * Available software such as [PowerUp!](https://www.causalevaluation.org/power-analysis.html) allows investigators to build in assumptions about anticipated rates of attrition in cluster-randomized trials, under the assumption that attrition is completely at random and unrelated to anything. 
 However, researchers might anticipate that, in practice, attrition will be related to baseline characteristics. 
 Simulation can be used to assess how this might affect the power of a planned study.
 
* There are some closed-form expressions for power to test mediational relations (i.e., indirect and direct effects) in a variety of different experimental designs, and these formulas are now available in [PowerUp!](https://www.causalevaluation.org/power-analysis.html). 
 However, the formulas involve a large number of parameters (including some where it may be difficult to develop credible assumptions) and they apply only to a specific analytic model for the mediating relationships.
 Researchers planning a study to investigate mediation might therefore find it useful to generate realistic data structures and conduct power analysis via simulation.

### Simulating processess

Yet another common use for Monte Carlo simulation is as a way to emulate a complex process as a means to better understand it or to evaluate the consequences of modifying it.
A famous area of process simulation are climate models, where researchers simulate the process of climate change.
These physical simulations mimic very complex systems to try and understand how perturbations (e.g., more carbon release) will impact downstream trends.

Another example of process simulation arises in education research.
Some large school districts such as New York City have centralized lotteries for school assignment, which entail having families rank schools by order of preference.
The central office then assigns students to schools via a lottery procedure where each student gets a lottery number that breaks ties when there are too many students desiring to go to certain schools.
Students' school assignments are therefore based in part on random chance, but the the process is quite complex: each student has some probability of assignment to each school on their list, but the probabilities depend on their choices and the choices of other students.

The school lottery process creates a natural experiment, based on which researchers can estimate the causal impact of being assigned to one school vs. another.
A defensible analysis of the process requires knowing the probabilities of school assignment. @abdulkadirouglu2017research conducted such an evaluation using the school lottery process in New York City. 
They calculated school assignment probabilities via simulation, by running the school lottery over and over, changing only students' lottery numbers, and recording students' school assignments in each repetition of the process.
Simulating the lottery process a large number of times provided precise estimates of each students' assignment probabilities, based on which @abdulkadirouglu2017research were able to estimate causal impacts of school assignment.

For another example, one that possibly illustrates the perils of simulation as taking us away from results that pass face validity, @staiger2010searching simulated the process of firing teachers depending on their estimated value-added scores.
Based on their simulations, which model firing different proportions of teachers, they suggest that firing substantial portions of the teacher workforce annually would substantially improve student test scores.
Their work offers a clear illustration of how simulations can be used to examine the potential consequences of various policy decisions, assuming the underlying assumptions hold true.
This example also brings home a core concern of simulation: we only learn about the world we are simulating, and the relevance of simulation evidence to the real world is by no means guaranteed.

## The perils of simulation as evidence

Simulation has the potential to be a powerful tool for investigating quantitative methods.
However, evidence from simulation studies is also fundamentally limited in certain ways, and thus very susceptible to critique.
The core advantage of simulation studies is that they allow for evaluation of data analysis methods under _specific and exact conditions_, avoiding the need for approximation.
The core limitation of simulations stems from this same property: they provide information about the performance of data analysis methods under specified conditions, but provide no guarantee that patterns of performance hold in general.
One can partially address questions of generalization by examining a wide range of conditions, looking to see whether a pattern holds consistently or changes depending on features of the data-generating process.
Even this strategy has limitations, though.
Except for very simple processes, we can seldom consider every possible set of conditions. 

As we will see in later chapters, the design of a simulation study typically entails making choices over very large spaces of possibility. 
This flexibility leaves lots of room for discretion and judgement, and even for personal or professional biases [@boulesteix2020Replication].
Due to this flexibility, simulation findings are held in great skepticism by many.
The following motto summarizes the skeptic's concern:

> Simulations are doomed to succeed.

As this motto captures, simulations are alluring: once a simulation framework is set up, it is easy to tweak and adjust.
It is natural for us all to continue to do this until the simulation works "as it should."
If our goal is to show something that we already believe is correct (e.g., that our fancy new estimation procedure is better than existing methods), we could probably find a way to align our simulation with our intuition.[^guest-speakers]

[^guest-speakers]: A comment from James: I recall attending seminars in the statistics department during graduate school, where guest speakers usually presented both some theory and some simulation results. A few years into my graduate studies, I realized that the simulation part of the presentation could nearly always be replaced with a single slide that said "we did some simulations and showed that our new method works better than old methods under conditions that we have cleverly selected to be favorable for our approach." I hope that my own work is not as boring or predictable as my memory of these seminars.

Critiques of simulation studies often revolve around the _realism_, _relevance_, or _generality_ of the data generating process.
Are the simulated data realistic, in the sense that they follow similar patterns to what one would see in real empirical data?
Are the explored aspects of the simulation relevant to what we would expect to find in practice?
Was the simulation systematic in exploring a wide variety of scenarios, so that general conclusions are warranted?

We see at least three principles for addressing such questions in one's own work.
Perhaps most fundamental is to be transparent in one's methods and reasoning:
explicitly state what was done, and provide code so that others can reproduce one's results or tweak them to test variations of the data-generating process or alternative analysis strategies.
Another important component of a robust argument is to systematically  vary the conditions under examination. 
This is facilitated by writing code in a way to make it easy to simulate across a range of different data-generating scenarios.
Once that is in place, one can systematically explore myriad scenarios and report all of the results.
An aspiration of the simulation architect should be to explore the boundary conditions that separate where preferred methods work and where they break or fail.
Finally, one can draw on relevant statistical theory to support the design of a simulation and interpretation of its results. 
Mathematical analysis might indicate that some features of a data-generating process will have a strong influence on the performance of a method, while other features will not matter at all when sample sizes are sufficiently large. 
Well designed simulations will examine conditions that are motivated by or complement what is known based on existing statistical theory.

In addition to these principles, methodologists have proposed broader changes in practice to counter the potential for bias in methodological simulation studies.
@morris2019UsingSimulationStudies introduced a formal framework, called ADEMP, to guide the reporting of methodological simulations.
@boulesteix2013Plea argued for greater use of neutral comparison studies, in which the performance of alternative statistical methods are compared under a range of relevant conditions by methodological researchers who do not have vested interests in any specific method [see also @boulesteix2017evidencebased].
Further, @siepe2024SimulationStudiesMethodological argue for more routine pre-registration of methodological simulations to bring greater transparency and reduce the possibility of bias arising from flexibility in their design. 


## Simulating to learn

Most of the examples of Monte Carlo simulation that we have mentioned thus far are drawn from formal methodological research, published in methodologically focused research journals.
If you do not identify as a methodologist, you may be wondering whether there is any benefit to learning how to do simulations---what's the point, if you are never going to conduct methodological research studies or use simulation to aid in planning an empirical study? 
However, we believe that simulation is an incredibly useful tool---and well worth learning, even outside the context of formal methodological research---for at least two reasons.

First, in order to do any sort of quantitative data analysis, you will need to make decisions about what methods to use. 
Across fields, existing guidance about data analysis practice is almost certainly informed by simulation research of some form, whether well-designed and thorough or haphazard and poorly reasoned. 
Consequently, having a high-level understanding of the logic and limitations of simulation will help you to be a critical consumer of methods research, even if you do not intend to conduct methods research of your own.

Second, we believe conducting simulations deepens one's understanding of the logic of statistical modeling and statistical inference.
Learning a new statistical model (such as generalized linear mixed models) or analytic technique (such as multiple imputation by chained equations) requires taking in _a lot_ of detailed information, from the assumptions of the model to the interpretation of parameter estimates to the best practices for estimation and what to do if some part of the process goes off. 
To thoroughly digest all these details, we have found it invaluable to _simulate_ data based on the model under consideration. 
This usually requires translating mathematical notation into computer code, an exercise which makes the components of the model more tangible than just a jumble of Greek letters.
The simulated data is then available for inspection, summarizing, graphing, and further calculation, all of which can aid comprehension and interpretation.
Moreover, the process of simulating yields a dataset which can then be used to practice implementing the analysis procedure and interpreting the results.
We have found that building a habit of simulating is a highly effective way to learn new models and methods, worthwhile even if one has no intention of carrying out methodological research. 
We might even go so far as to argue that _whatever you might think, you don't really understand a statistical model until you've done a simulation of it._


## Why R? 

This book aims not only to introduce the conceptual principles of Monte Carlo simulation, but also to provide a practical guide to actually _conducting_ simulation studies (whether for personal learning purposes or for formal methodological research).
And conducting simulations requires writing computer code (sometimes, lots of code!).
The computational principles and practices that we will describe are very general, not specific to any particular programming language, but for purposes of demonstrating, presenting examples, and practicing the process of developing a simulation, it helps to be specific. 
To that end, we will be using R, a popular programming language that is widely used among statisticians, quantitative methodologists, and data scientists. 
Our presentation will assume that readers are comfortable with writing R scripts to carry out tasks such as cleaning variables, summarizing data, creating data-based graphics, and running regression models (or more generally, estimating statistical models). 

We have chosen to focus on R (rather than some other programming language) because both of us are intimately familiar with R and use it extensively in our day-to-day work. 
Simply put, it is much easier to write in your native language than in one in which you are less fluent.
But beyond our own habits and preferences, there are several more principled reasons for using R. 
<!-- TRIM? Subsequent paragraphs could probably be shortened into a single paragraph or maybe removed all together.-->

R is free and open source software, which can be run under many different operating systems (Windows, Mac, Linux, etc.).
This is advantageous not only because of the cost, but also because it means that anyone with a computer---anywhere in the world---can access the software and could, if they wanted, re-run our provided code for themselves. 
This makes R a good choice for practicing transparent and open science processes. 

There is a very large, active, and diverse community of people who use, teach, and develop R. 
It is used widely for applied data analysis and statistical work in such fields as education, psychology, economics, epidemiology, public health, and political science, 
and is widely taught in quantitative methods and applied statistics courses.
Integral to the appeal of R is that it includes tens of thousands of contributed packages, which extend the core functionality of the language in myriad ways.
New statistical techniques are often quickly available in R, or can be accessed through R interfaces.
Increasingly, R can also be used to interface with other languages and platforms, such as running Python code via the [`reticulate`](https://rstudio.github.io/reticulate/) package, running Stan programs for Bayesian modeling via [`RStan`](https://mc-stan.org/users/interfaces/rstan), or calling the h2o machine learning library using the [`h2o` package](https://cran.r-project.org/package=h2o) [@fryda2014H2oInterfaceH2O]. 
The huge variety of statistical tools available in R makes it a fascinating place to learn and practice.

R does have a persistent reputation as being a challenging and difficult language to use. 
This reputation might be partly attributable to its early roots, having been developed by highly technical statisticians who did not necessarily prioritize accessibility, legibility of code, or ease of use. 
However, as the R community has grown, the availability of introductory documentation and learning materials has improved drastically, so that it is now much easier to access pedagogical materials and find help. 

R's reputation also probably partly stems from being a decentralized, open source project with many, many contributors. 
Contributed R packages vary hugely in quality and depth of development; there are some amazingly powerful tools available but also much that is half-baked, poorly executed, or flat out wrong. 
Because there is no central oversight or quality control, the onus is on the user to critically evaluate the packages that they use.
For newer users especially, we recommend focusing on more established and widely used packages, seeking input and feedback from more knowledgeable users, and taking time to validate functionality against other packages or software when possible. 

A final contributor to R's intimidating reputation might be its extreme flexibility.
As both a statistical analysis package and a fully functional programming language, R can do many things that other software packages cannot, but this also means that there are often many different ways to accomplish the same task. 
In light of this situation, it is good to keep in mind that knowing a single way to do something is usually adequate---there is no need to learn six different words for hello, when one is enough to start a conversation. 

On balance, we think that the many strengths of R make it worthwhile to learn and continue exploring. For simulation, in particular, R's facility to easily write functions (bundles of commands that you can easily call in different manners), to work with multiple datasets in play at the same time, and to leverage the vast array of other people's work all make it a very attractive language.

## Organization of the text

We think of simulation studies as falling on a spectrum of formality. 
On the least formal end, we may use simulations to learn a statistical model, investigating questions purely to satisfy our own curiosity. 
On the most formal end, we may conduct carefully designed, pre-registered simulations that compare the performance of competing statistical methods across an array of data-generating conditions designed to inform practice in a particular research area.

Our central goal is to help you learn to work anywhere along this spectrum, from the most casual to the most principled.
To support that goal, the book is designed with a spiral structure, where we first present simpler and less formal examples that illustrate high-level principles, then revisit the component pieces, dissecting and exploring them in greater depth.
We defer discussion of concepts that are relevant only to more formal use-cases (such as the ADEMP framework of @morris2019UsingSimulationStudies) until later chapters.
We have also included many case studies throughout the book; these are designed to make the topics under discussion tangible, and are also designed to provide chunks of code that you emulate or even directly copy and use for your own purposes.

We divided the book into several parts.
In Part I (which you are reading now), we lay out our case for learning simulation, introduces some guidance on programming, and presents an initial, very simple simulation to set the stage for later discussion of design principles.
Part II lays out the core components (generating artificial data, applying analytic procedures, executing the simulations, and analyzing the simulation results) for simulating a single scenario. It then presents some more involved case studies that illustrate the principles of modular, tidy simulation design.
<!-- Need to define modular, tidy simulation design. -->
Part III moves to multifactor simulations, meaning simulations that look at more than one scenario or context.
Multifactor simulation is central to the design of more formal simulation studies because it is only by evaluating or comparing estimation procedures across multiple scenarios that we can begin to understand their general properties.

The book closes with two final parts.
Part IV covers some technical challenges that are commonly encountered when programming simulations, including reproducibility, parallel computing, and error handling.
Part V covers three extensions to specialized forms of simulation: simulating to calculate power, simulating within a potential outcomes framework for causal inference, and the parametric bootstrap. The specialized applications underscore how simulation can be used to answer a wide variety of questions across a range of contexts, thus connecting back to the broader purposes discussed above.
The book also includes appendices with some further guidance on writing R code and pointers to further resources.
