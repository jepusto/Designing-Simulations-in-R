```{r, include=FALSE}
library( tidyverse )
library( blkvar )
```

# (APPENDIX) Appendices {-}

# Coding tidbits {#coding-tidbits}

This chapter is not specifically about simulation, but it does have a few tips and tricks regarding coding that are worth attending to.

## How to repeat yourself {#repeating-oneself}

At the heart of simulation is replication: we want to do the same task over and over. In this book we have showcased a variety of tools to replicate a random process. In this section we give a formal presentation of these tools.

### Using `replicate()`

The `replicate( n, expr, simplify )` method is a base-R function, which takes two arguments: a number `n` and an expression `expr` to run repeatedly. You can set `simplify = FALSE` to get the output of the function as a list, and if you set `simplify = TRUE` then R will try to simplify your results into an array.

For simple tasks where your expression gives you a single number, replicate will produce a vector of numbers:
```{r}
replicate( 5, mean( rpois( 3, lambda = 1 ) ) )
```

If you do not simplify, you then will need to massage your results:

```{r}
one_run <- function() {
  dd = rpois( 3, lambda = 1 )
  tibble( mean = mean( dd ), sd = sd( dd ) )
}
rps <- replicate( 2, one_run(), simplify = FALSE )
rps
```
In particular, you will probably stack all your tibbles to make one large dataset:
```{r}
rps <- bind_rows( rps )
rps
```

Note that you give replicate a full piece of code that would run on its own.  You can even give a whole block of code in curly braces.
This is exactly the same code as before:

```{r}
rps <- replicate( 2, {
  dd = rpois( 3, lambda = 1 )
  tibble( mean = mean( dd ), sd = sd( dd ) )
}, simplify = FALSE ) %>%
  bind_rows()
```


The `replicate()` method is good for simple tasks, but for more general use, you will probably want to use `map()`.

### Using `map()`

The tidyverse way of repeating oneself is the `map()` method.  The nice thing about `map()` is you map over a list of values, and thus can call a function repeatedly, but with a shifting set of inputs.

```{r}
one_run_v2 <- function( N ) {
  dd = rpois( N, lambda = 1 )
  tibble( mean = mean( dd ), sd = sd( dd ) )
}
n_list = c(2, 5)
rps <- map( n_list, one_run_v2 )
rps
```
You again would want to stack your results:
```{r}
bind_rows(rps)
```
We have a small issue here, however, which is we lost what we _gave_ `map()` for each call.
If we know we only get one row back from each call, we can add the column directly:
```{r}
rps$n = n_list
```

A better approach is to _name_ your list of input parameters, and then your `map` function can add those names for you as a new column when you stack:
```{r}
n_list %>%
  set_names() %>%
  map( one_run_v2 ) %>%
  bind_rows( .id = "n" )
```
An advantage here is if you are returning multiple rows (e.g., one row for each estimator tested in a more complex simulation), all the rows will get named correctly and automatically.

In older tidyverse worlds, you will see methods such as `map_dbl()` or `map_dfr()`.  These will automatically massage your output into the target type.  `map_dfr()` will automatically bind rows, and `map_dbl()` will try to simplify the output into a list of doubles.  Modern tidyverse no longer likes this, which we find somewhat sad.

To read more about `map()`, check out out [Section 21.5 of R for Data Science (1st edition)](https://r4ds.had.co.nz/iteration.html#the-map-functions), which provides a more thorough introduction to mapping.

### map with no inputs

If you do not have parameters, but still want to use `map()`, you can.  E.g.,

```{r}
map_dfr( 1:3, \(.) one_run() )
```
The weird "\(.)" is a shorthand for a function that takes one argument and then calls `one_run()` with no arguments.  We are using the 1:3 notation to just make a list of the right length (3 replicates, in this case) to map over.  A lot of fuss!  Just use `replicate()`

To make all of this more clear, consider passing arguments that you manipulate on the fly:
```{r anony_rep_2}
map_dfr( n_list, \(x) one_run_v2( x*x ) )
```
Anonymous functions, as these are called, can be useful to connect your pieces of simulation together.

### Other approaches for repetition

In the past, there was a tidyverse method called `rerun()`, but it is currently out of favor.
Originally, `rerun()` did exactly that: you gave it a number and a block of code, and it would rerun the block of code that many times, giving you the results as a list.
`rerun()` and `replicate()` are near equivalents.
As we saw, `replicate()` does what its name suggests---it replicates the result of an expression a specified number of times. Setting `simplify = FALSE` returns the output as a list (just like `rerun()`.

## Default arguments for functions {#default-arguments}

To write functions that are both easy to use and configurable, we find it helpful to set default arguments.
For example,

```{r demo_default_args}
my_function = function( a = 10, b = 20 ) {
     100 * a + b
}

my_function()
my_function( 5 )
my_function( b = 5 )
my_function( b = 5, a = 1 )
```

We can still call `my_function()` when we don't know what the arguments are, but then when we know more about the function, we can specify things of interest.
Lots of R commands work exactly this way, and for good reason.

Especially for code to generate random datasets, default arguments can be a lifesaver as you can then call the method before you know exactly what everything means.

For example, consider the `blkvar` package that has some code to generate blocked randomized datasets.
We might locate a promising method, and type it in:

```{r, error=TRUE}
library( blkvar )
generate_blocked_data()
```

That didn't work, but let's provide some block sizes and see what happens:

```{r}
generate_blocked_data( n_k = c( 3, 2 ) )
```

Nice!  We see that we have a block ID and the control and treatment potential outcomes.  We also don't see a random assignment variable, so that tells us we probably need some other methods as well.
But we can play with this as it stands right away.

Next we can see that there are many things we might tune:
```{r}
args( generate_blocked_data )
```

The documentation will tell us more, but if we just need some sample data, we can quickly assess our method before having to do much reading and understanding.
Only once we have identified what we need do we have to turn to the documentation itself.


## Testing and debugging code in your scripts

If you have an extended script with a list of functions, you might have a lot of code that runs each function in turn, so you can easily remind yourself of what it does, or what the output looks like.
One way to keep this code around, but not have it run all the time when you run your script, is to put the code inside a "FALSE block," that might look like so:

```{r}
if ( FALSE ) {
  res <- my_function( 10, 20, 30 )
  res
  # Some notes as to what I want to see.
  
  sd( res )
  # This should be around 20
}
```

You can then, when looking at the script, paste the code inside the block into the console when you want to run it.
If you source the script, however, it will not run at all, and thus your code will source faster and not print out any extraneous output.





## The source command and keeping things organized

Simulations have two general phases: generate your results and analyze your results.
The ending of the first phase should be to save the generated results.
The beginning of the second phase should then be to load the results from a file and analyze them.
These phases can be in a separate '.R' files.
Dividing your simulations in this way allows for easily changing how one _analyzes_ an experiment without re-running the entire thing.

This is the simplest version of a general principle of a larger project: put code for different purposes in different files.

For example, at the minimum, for a complex multifactor simulation, you will likely have three general collections of code, not including the code to run the multifactor simulation itself:

 * Code for generating data
 * Code for analyzing data
 * Code for running a single simulation scenario

If each of these pieces is large and complex, you might consider putting them in three different `.R` files.
Then, in your primary simulation driver, you could source these files.
E.g.,

```{r demo_source_multiple, eval=FALSE}
source( "pack_data_generators.R" )
source( "pack_estimators.R" )
source( "pack_simulation_support.R" )
```

The `source()` command essentially "cuts and pastes" the contents of the given file into your R work session.
If the file has code to run, it will run it.
If the file has a list of methods, those methods will now be available for use.

Alternatively, `pack_simulation_support.R` could, inside it, source the other two files.
You would then only source the single simulation support file in your primay file.

One reason for putting code in individual files is you can then have testing code in each of your files (in False blocks, like described above), testing each of your components.
Then, when you are not focused on that component, you don't have to look at that testing code.

Another good reason for this type of modular organizing is you can then have a variety of data generators, forming a library of options.
You can then easily create different simulations that use different pieces, in a larger project.

For example, in one recent simulation project on estimators for an Instrumental Variable analysis, we had several different data generators for generating different types of compliance patterns (IVs are often used to handle noncompliance in randomized experiments).
Our data generation code file then had several methods:

```
> ls()
[1] "describe_sim_data"  "make_dat"           "make.dat.1side"     
[4] "make.dat.1side.old" "make.dat.orig"      "make.dat.simple"
[7] "make.dat.tuned"     "rand.exp"           "summarize_sim_data"
```

The describe and summarize methods printed various statistics about a sample dataset; these are used to debug and understand how the generated data looks.
We also had a variety of different DGP methods because we had different versions that came up as we were trying to chase down errors in our estimators and understand strange behavior.

Putting the estimators in a different file also had a nice additional purpose: we also had an applied data example in our work, and we could simply source that file and use those estimators on our actual data.
This ensured our simulation and applied analysis were perfectly aligned in terms of the estimators we were using.
Also, as we debugged our estimators and tweaked them, we immediately could re-run our applied analysis to update those results with minimal effort.

Modular programming is key.


## Debugging with browser

Consider the following code taken from a simulation:

```{r demo_browser, eval=FALSE}
    if ( any( is.na( rs$estimate ) ) ) {
        browser()
    }
```

The `browser()` command stops your code and puts you in an interactive console where you can look at different objects and see what is happening.
Having it triggered when something bad happens (in this case when a set of estimates has an unexpected NA) can help untangle what is driving a rare event.

