---
output: html_document
editor_options: 
  chunk_output_type: console
---


```{r setup-multiple-scenarios, include=FALSE}
library( tidyverse )
library( purrr )
library( simhelpers )

options(list(dplyr.summarise.inform = FALSE))
theme_set( theme_classic() )

### Code for the running examples
source("case_study_code/generate_ANOVA_data.R")
source("case_study_code/ANOVA_Welch_F.R")
source( "case_study_code/r_bivariate_Poisson.R" )
source( "case_study_code/r_and_z.R" )
source( "case_study_code/evaluate_CIs.R" )

source( "case_study_code/clustered_data_simulation.R" )

dat <- gen_cluster_RCT( n=5, J=3, p=0.5, 
                      gamma_0=0, gamma_1=0.2, gamma_2=0.2,
                      sigma2_u = 0.4, sigma2_e = 1,
                      alpha = 0.5 )

```

# (PART) Systematic Simulations {-}

# Simulating across multiple scenarios {#simulating-multiple-scenarios}

In Chapter \@ref(simulation-structure), we described the general structure of basic simulations as following four steps: generate, analyze, repeat, and summarize. 
The principles of tidy simulation suggest that each of these steps should be represented by its own function or set of code.
For any particular simulation we have a data-generating function and a data-analysis function, which can be bundled together into a simulation driver that repeatedly executes the generate-and-analyze process; we also have a summarization function (or set of code) that computes performance measures across the replications of the simulation process.
In the previous section of the book, we focused on creating code that will run a simulation for a single scenario, going from a set of parameter values to a set of performance measures. 

In practice, simulation studies often involve examining a range of different values, such as multiple levels of a focal parameter value and potentially also multiple levels for auxiliary parameters, sample size, and other design parameters.
In this chapter, we demonstrate an approach for executing simulations across multiple scenarios and organizing the results for further analysis. 
Our focus here is on the programming techniques and computational structure. 
In the next chapter, we discuss some of the deeper theoretical challenges of designing multifactor simulations.
Then in subsequent chapters, we examine tools for analyzing and making sense of results from more complex, multifactor simulation designs.

In Chapter \@ref(simulation-structure), we described three further steps involved in systematic simulations: _designing_ a set of scenarios to examine, _executing_ across multiple scenarios, and _synthesizing_ the performance results across scenarios. 
The same principles of tidy simulation apply to these steps as well.
Thus, we will demonstrate how to create a dataset representing the experimental design of the simulation, how to execute a simulation driver across multiple scenarios, and how to organize results for synthesis. 

## Simulating across levels of a single factor

Even if we are only using simulation in an ad hoc, exploratory way, we will often be interested in examining the performance of a model or estimation method in more than one scenario.
We have already seen examples of this in Chapter \@ref(t-test-simulation), where we looked at the coverage rate of a confidence interval for the mean of an exponential distribution.
In Section \@ref(simulating-across-different-scenarios), we applied a simulation driver function across a set of sample sizes ranging from 10 to 300, finding that the coverage rate improves towards the desired level as sample size increases.
Simple forms of systematic exploration such as this are useful in many situations.
For instance, when using Monte Carlo simulation for study planning, we might examine simulated power over a range of the target parameter to identify the smallest parameter for power is above a desired level.
If we are using simulation simply to study an unfamiliar model, we might vary a key parameter over a wide range to see how the performance of an estimator changes.
These forms of exploration can be understood as single-factor simulations.

To demonstrate a single-factor simulation, we revisit case study on heteroskedastic analysis of variance, as studied by @brown1974SmallSampleBehavior and developed in Chapter \@ref(case-ANOVA).
Suppose that we want to understand how the power of Welch's test varies as a function of the maximum distance between group means.
The data-generating function `generate_ANOVA_data()` that we developed previously was set up to take a vector of means per group, so we re-parameterize the function to define the group means based on the maximum difference (`max_diff`), under the assumption that the means are equally spaced between zero and the maximum difference.
We will also re-parameterize the function in terms of the total sample size and the fraction of observations allocated to each group.
The revised function is 
```{r}
generate_ANOVA_new <- function(
  G, max_diff, sigma_sq = 1, N = 20, allocation = "equal"
) {
  
  mu <- seq(0, max_diff, length.out = G)
  if (allocation == "equal") allocation <- rep(1 / G, times = G)
  N_g <- round(N * allocation)  
  
  group <- factor(rep(1:G, times = N_g))
  mu_long <- rep(mu, times = N_g)
  sigma_long <- rep(sqrt(sigma_sq), times = N_g)
  
  x <- rnorm(N, mean = mu_long, sd = sigma_long)
  sim_data <- tibble(group = group, x = x)
  
  return(sim_data)
}
```
Now we can create a simulation driver by combining this new data-generating function with the data-analysis function we created in Section \@ref()
```{r}
sim_ANOVA <- bundle_sim(f_generate = generate_ANOVA_new, f_analyze = ANOVA_Welch_F)
```
To compute power, we generate a set of simulated $p$ values and then summarize the rejection rate of the Welch test at $\alpha$ levels of .01 and .05:
```{r}
sim_ANOVA(100, G = 4, max_diff = 0.5, sigma_sq = c(1, 2, 2, 3), N = 40) |>
  calc_rejection(p_values = Welch, alpha = c(.01, .05))
```
Now we can apply this process for several different scenarios with different values of `mu1`.

Following the principles of tidy simulation, it is useful to represent the design of a systematic simulation as a dataset with a row for each scenario to be considered. 
For a single-factor simulation, the experimental design consists of a dataset with just a single variable: 
```{r}
Welch_design <- tibble(max_diff = seq(0, 0.8, 0.1))
str(Welch_design)
```
To compute simulation results for each of these scenarios, we can use the `map()` function from `purrr()`.
This function takes a list of values as the input, then calls a function on each value.
Our `sim_ANOVA()` function has several further arguments that need to be specified.
Because these will be the same for every value of `max_diff`, we can include them as additional arguments in `map()`, and they will be used every time `sim_ANOVA()` is called.
Here is one way to code this:
```{r, eval = FALSE}
Welch_results <- 
  Welch_design %>%
  mutate(
    pvals = map(max_diff, sim_ANOVA, reps = 100, G = 4, 
                sigma_sq = c(1, 2, 2, 3), N = 40)
  )
```
Another way to accomplish the same thing is to specify an anonymous function (also called a lambda) in the `map()` call.
This makes it clearer that the additional arguments are getting called in every evaluation of `sim_ANOVA()`:
```{r}
Welch_results <- 
  Welch_design %>%
  mutate(
    pvals = map(max_diff, ~ sim_ANOVA(100, G = 4, max_diff = .x, 
                                      sigma_sq = c(1, 2, 2, 3), 
                                      N = 40))
  )
```
In the resulting dataset, the `pvals` variable is a list, with each entry consisting of a tibble of simulated p-values. 
Using the `unnest()` function simplies the structure of the results, making it easier to do performance calculations:
```{r}
Welch_results %>% unnest(pvals)
```
The resulting dataset has `r nrow(Welch_results)` rows, consisting of `r nrow(Welch_results) / nrow(Welch_design)` replications for each of `r nrow(Welch_design)` scenarios.
To compute power levels, we use `calc_rejection()` after grouping the results by scenario:
```{r}
Welch_power <- 
  Welch_results %>% 
  unnest(pvals) %>%
  group_by(max_diff) %>%
  summarize(
    calc_rejection(p_values = Welch, alpha = c(.01,.05))
  )

Welch_power
```
The power levels are quite low, with the $\alpha = .05$-level tests reaching a maximum power of `r round(max(Welch_power$rej_rate_05),2)` when `max_diff` is `r round(max(Welch_power$max_diff),2)`.
The lower power levels max sense because we are looking at a scenario with a very small sample size of just 10 observations per group.

### A performance summary function

These performance calculations focus only on the results for the Welch test, when we might be interested in comparing Welch's test to the conventional ANOVA $F$. 
One way to carry out the performance calculations for both measures is to write a small function that encapsulates the performance calculations, then use it in place of `calc_rejection()`. 
The function should take a set of simulation results as input and provide a dataset of performance measures as output.
Here is one possible implementation, which uses `map()` to apply the performance calculations to each set of simulated p-values:
```{r}
summarize_power <- function(data, alpha = c(.01,.05)) {
  ANOVA <- calc_rejection(data, p_values = ANOVA, alpha = alpha, format = "long")
  Welch <- calc_rejection(data, p_values = Welch, alpha = alpha, format = "long")
  bind_rows(
    ANOVA = ANOVA, 
    Welch = Welch,
    .id = "test"
  )
} 

power_levels <- 
  Welch_results %>%
  mutate(
    power = map(pvals, summarize_power, alpha = c(.01, .05))
  ) %>%
  dplyr::select(-pvals) %>%
  unnest(power)

power_levels
```

### Adding performance calculations to the simulation driver

Now that we have a function for carrying out the performance calculations, we could consider incorporating this step into the simulation driver function. 
That way, we can call the simulation driver function with a set of parameter values and it will return a table of performance summaries.
The `bundle_sim()` function from `simhelpers` will create such a function for us, by combining a performance calculation function with the data-generating and data-analysis functions:
```{r}
sim_ANOVA_full <- bundle_sim(
  f_generate = generate_ANOVA_new, 
  f_analyze = ANOVA_Welch_F,
  f_summarize = summarize_power
)

args(sim_ANOVA_full)
```
The resulting function includes an input argument that controls which alpha levels to use in the rejection rate calculations.
The bundled simulation driver also includes an additional option called `summarize`, which allows the user to control whether to apply the performance calculation function to the simulation output.
The default value of `TRUE` means that calling the function will compute rejection rates:
```{r}
sim_ANOVA_full(
  reps = 100, G = 4, max_diff = 0.5, 
  sigma_sq = c(1, 2, 2, 3), N = 40, 
  alpha = c(.01, .05)
)
```
Setting `summarize = FALSE` will produce a dataset with the raw simulation output, with one row per replication, ignoring the additional inputs related to the performance calculations:
```{r}
sim_ANOVA_full(
  reps = 4, G = 4, max_diff = 0.5, 
  sigma_sq = c(1, 2, 2, 3), N = 40, 
  summarize = FALSE
)
```

This more elaborate simulation driver makes execution of the simulations a bit more streamlined.
The full set of performance summaries can now be computed by calling `map()` with the full driver:
```{r}
set.seed(20251031)

power_levels <- 
  Welch_design %>%
  mutate(
    res = map(max_diff, sim_ANOVA_full, reps = 500, G = 4, 
                sigma_sq = c(1, 2, 2, 3), N = 40,
                alpha = c(.01, .05))
  ) %>%
  unnest(res)
```

The results are organized in a way that facilitates visualization of the power levels:

```{r hetero-ANOVA-power, fig.width = 7, fig.height = 3.5, out.width = "100%"}

ggplot(power_levels) + 
  aes(max_diff, rej_rate, color = test) + 
  geom_point() + geom_line() + 
  scale_y_continuous(limits = c(0, NA), expand = expansion(0,c(0,0.01))) + 
  facet_wrap(~ alpha, scales = "free", labeller = label_bquote(alpha == .(alpha))) + 
  labs(x = "Maximum mean difference", y = "Power") + 
  theme_minimal() + 
  theme(legend.position  ="inside", legend.position.inside = c(0.1,0.9))
```

Under the conditions examined here, both tests appear to have similar power.
At the .05 $\alpha$ level, the power of the Welch test is nearly identical to that of the ANOVA $F$ test. 
At the .01 $\alpha$ level, there may be a discrepancy at when `max_diff` is 0.8, although the apparent difference could be attributable to Monte Carlo error.
Although the tests appear to work similarly here, these results are based on a very specific set of conditions, including equally sized groups and a specific configuration of within-group variances.
A natural further question is whether this pattern holds under other configurations of sample allocations, total sample size, or within-group variances.
These questions can be examined by expanding the simulation design to further scenarios.

## Simulating across multiple factors

For example, consider a simulation study examining the performance of confidence intervals for Pearson's correlation coefficient under a bivariate Poisson distribution. 
We examined this data-generating model in Section \@ref(BVPois-example), implementing it in the function `r_bivariate_Poisson()`. The model has three parameters (the means of each variate, $\mu_1, \mu_2$ and the correlation $\rho$) and there is one design parameter (sample size, $N$). 
Thus, we could in principle examine up to four factors. 

Using these parameters directly as factors in the simulation design will lead to considerable redundancy because of the symmetry of the model: generating data with $\mu_1 = 10$ and $\mu_2 = 5$ would lead to identical correlations as using $\mu_1 = 5$ and $\mu_2 = 10$.
It is useful to re-parameterize to reduce redundancy and simply things.
We will therefore define the simulation conditions by always treating $\mu_1$ as the larger variate and by specifying the ratio of the smaller to the larger mean as $\lambda = \mu_2 / \mu_1$.
We might then examine the following factors:

* the sample size, with values of $N = 10, 20$, or $30$
* the mean of the larger variate, with values of $\mu_1 = 4, 8$, or $12$
* the ratio of means, with values of $\lambda = 0.5$ or $1.0$.
* the true correlation, with values ranging from $\rho = 0.0$ to $0.7$ in steps of $0.1$

The above parameters describe a $3 \times 3 \times 2 \times 8$ factorial design, where each element is the number of levels for that factor. This is a four-factor experiment, because we have four different things we are varying.

To implement this design in code, we first save the simulation parameters as a list with one entry per factor, where each entry consists of the levels that we would like to explore.
We will run a simulation for every possible combination of these values.
Here is code that generates all of the scenarios given the above design, storing these combinations in a data frame, `params`, that represents the full experimental design:

```{r make_Pearson_sim_dataframe}
design_factors <- list(
  N = c(10, 20, 30),
  mu1 = c(4, 8, 12),
  lambda = c(0.5, 1.0),
  rho = seq(0.0, 0.7, 0.1)
)

lengths(design_factors)

params <- expand_grid( !!!design_factors )
params
```

We use `expand_grid()` from the `tidyr` package to create all possible combinations of the four factors.[^expand-grid-wtf]
We have a total of $`r paste(lengths(design_factors), collapse = " \\times ")` = `r nrow(params)`$ rows, each row corresponding to a simulation scenario to explore.
With multifactor experiments, it is easy to end up running a lot of experiments!

[^expand-grid-wtf]: `expand_grid()` is set up to take one argument per factor of the design. A clearer example of its natural syntax is:
    ```{r}
    params <- expand_grid(
      N = c(10, 20, 30),
      mu1 = c(4, 8, 12),
      lambda = c(0.5, 1.0),
      rho = seq(0.0, 0.7, 0.1)
    )
    ```
    However, we generally find it useful to create a list of design factors before creating the full grid of parameter values, so we prefer to make `design_factors` first. To use `expand_grid()` on a list, we need to use `!!!`, the splice operator from the `rlang` package, which treats `design_factors` as a set of arguments to be passed to `expand_grid`. The syntax may look a bit wacky, but it is succinct and useful. 

## Using pmap to run multifactor simulations {#using-pmap-to-run-multifactor-simulations}

Once we have selected factors and levels for simulation, we now need to run the simulation code across all of our factor combinations.
Conceptually, each row of our `params` dataset represents a single simulation scenario, and we want to run our simulation code for each of these scenarios.
We would thus call our simulation function, using all the values in that row as parameters to pass to the function.

One way to call a function on each row of a dataset in this manner is by using `pmap()` from the `purrr` package.
`pmap()` marches down a set of lists, running a function on each $p$-tuple of elements, taking the $i^{th}$ element from each list for iteration $i$, and passing them as parameters to the specified function.
`pmap()` then returns the results of this sequence of function calls as a list of results.[^pmap-variants]
Because R's `data.frame` objects are also sets of lists (where each variable is a vector, which is a simple form of list), `pmap()` also works seemlessly on `data.frame` or `tibble` objects.

[^pmap-variants]: Just like `map()` or `map2()`, `pmap()` has variants such as `_dbl` or `_df`.
These variants automatically stack or convert the list of things returned into a tidier collection (for `_dbl` it will convert to a vector of numbers, for `_df` it will stack the results to make a large dataframe, assuming each thing returned is a little dataframe).

Here is a small illustration of `pmap()` in action:
```{r}

some_function <- function( a, b, theta, scale ) {
    scale * (a + theta*(b-a))
}

args_data <- tibble( a = 1:3, b = 5:7, theta = c(0.2, 0.3, 0.7) )
purrr::pmap( args_data, .f = some_function, scale = 10 )
```

One important constraint of `pmap()` is that the variable names over which to iterate over must correspond exactly to arguments of the function to be evaluated.
In the above example, `args_data` must have column names that correspond to the arguments of `some_function`.
For functions with additional arguments that are not manipulated, extra parameters can be passed after the function name (as in the `scale` argument in this example). 
These will also be passed to each function call, but will be the same for all calls.

Let's now implement this technique for our simulation of confidence intervals for Pearson's correlation coefficient.
In Section \@ref(estimation-functions), we developed a function called `r_and_z()` for computing confidence intervals for Pearson's correlation using Fisher's $z$ transformation;
then in Section \@ref(assessing-confidence-intervals), we wrote a function called `evaluate_CIs()` for evaluating confidence interval coverage and average width. 
We can bundle `r_bivariate_Poisson()`, `r_and_z()`, and `evaluate_CIs()` into a simulation driver function by taking
```{r}
library(simhelpers)

Pearson_sim <- bundle_sim(
  f_generate = r_bivariate_Poisson, f_analyze = r_and_z, f_summarize = evaluate_CIs
)
args(Pearson_sim)
```
This function will run a simulation for a given scenario:
```{r}
Pearson_sim(1000, N = 10, mu1 = 5, mu2 = 5, rho = 0.3)
```

In order to call `Pearson_sim()`, we will need to ensure that the columns of the `params` dataset correspond to the arguments of the function.
Because we re-parameterized the model in terms of $\lambda$, we will first need to compute the parameter value for $\mu_2$ and remove the `lambda` variable because it is not an argument of `Pearson_sim()`:
```{r}
params_mod <- 
  params %>%
  mutate(mu2 = mu1 * lambda) %>%
  dplyr::select(-lambda)
```

Now we can use `pmap()` to run the simulation for all `r nrow(params)` parameter settings:
```{r secret-run-Pearson-Poisson-sims, include=FALSE}
# (See below this block for book code)

if ( !file.exists( "results/Pearson_Poisson_results.rds" ) ) {
  # Secret Run code in parallel for speedup
  library(future)
  library(furrr)
  plan(multisession)
  set.seed(20250718)
  sim_results <- 
    params_mod %>%
    mutate(res = future_pmap(., .f = Pearson_sim, reps = 1000, .options = furrr_options(seed = TRUE) ) )
  
  write_rds( sim_results, file = "results/Pearson_Poisson_results.rds" )
} else {
  sim_results <- read_rds("results/Pearson_Poisson_results.rds")
}

```

```{r run-Pearson-sims, eval = FALSE}
sim_results <- params
sim_results$res <- pmap(params_mod, Pearson_sim, reps = 1000 )
```

The above code calls our `run_alpha_sim()` method for each row in the list of scenarios we want to explore.
Conveniently, we can store the results __as a new variable in the same dataset__.

```{r}
sim_results
```

The above code may look a bit peculiar: we are storing a set of dataframes (our result) in our original dataframe.
This is actually ok in R: our results will be in what is called a __list-column__, where each element in our list column is the little summary of our simulation results for that scenario.
Here is the third scenario, for example:

```{r}
sim_results$res[[3]]
```

List columns are neat, but hard to work with.
To turn the list-column into normal data, we can use `unnest()` to expand the `res` variable, replicating the values of the main variables once for each row in the nested dataset:

```{r}
sim_results <- unnest(sim_results, cols = res)
sim_results
```

Putting all of this together into a tidy workflow leads to the following:

```{r, eval = FALSE}
sim_results <- 
  params %>%
  mutate(
    mu2 = mu1 * lambda,
    reps = 1000
  ) %>%
  mutate(
    res = pmap(dplyr::select(., -lambda), .f = Pearson_sim)
  ) %>%
  unnest(cols = res)
```

If you like, you can simply use the `evaluate_by_row()` function from the `simhelpers` package:

```{r, eval=FALSE}
sim_results <- 
  params %>%
  mutate( mu2 = mu1 * lambda ) %>%
  evaluate_by_row( Pearson_sim, reps = 1000 )
```
An advantage of `evaluate_by_row()` is that the input dataset can include extra variables (such as `lambda`).
Another advantage is that it is easy to run the calculations in parallel; see Chapter \@ref(parallel-processing).

As a final step, we save our results using tidyverse's `write_rds()`; see [R for Data Science, Section 7.5](https://r4ds.hadley.nz/data-import.html#sec-writing-to-a-file).
We first ensure we have a directory by making one via `dir.create()` (see Chapter \@ref(saving-files) for more on files):

```{r, eval=FALSE}
dir.create( "results", showWarnings = FALSE )
write_rds( sim_results, file = "results/Pearson_Poisson_results.rds" )
```

We now have a complete set of simulation results for all of the scenarios we specified.

## When to calculate performance metrics

For a single-scenario simulation, we repeatedly generate and analyze data, and then assess the performance across the repetitions.
When we extend this process to multifactor simulations, we have a choice: do we compute performance measures for each simulation scenario as we go (inside) or do we compute all of them after we get all of our individual results (outside)?
There are pros and cons to each approach.

### Aggregate as you simulate (inside)

The *inside* approach runs a stand-alone simulation for each scenario of interest. For each combination of factors, we simulate data, apply our estimators, assess performance, and return a table with summary performance measures. We can then stack these tables to get a dataset with all of the results, ready for analysis.

This is the approach we illustrated above. It is straightforward and streamlined: we already have a method to run simulations for a single scenario, and we just repeat it across multiple scenarios and combine the outputs.
After calling `pmap()` (or `evaluate_by_row()`) and stacking the results, we end up with a dataset containing all the simulation conditions, one simulation context per row (or maybe we have sets of several rows for each simulation context, with one row for each method), with the columns consisting of the simulation factors and measured performance outcomes.
This table of performance is ideally all we need to conduct further analysis and write up the results.

The primary advantages of the inside strategy are that it is easy to modularize the simulation code and it produces a compact dataset of results, minimizing the number and size of files that need to be stored.
On the con side, calculating summary performance measures inside of the simulation driver limits our ability to add new performance measures on the fly or to examine the distribution of individual estimates.
For example, say we wanted to check if the distribution of Fisher-z estimates in a particular scenario was right-skewed, perhaps because we are worried that the estimator sometimes breaks down.
We might want to make a histogram of the point estimates, or calculate the skew of the estimates as a performance measure.
Because the individual estimates are not saved, we would have no way of investigating these questions without rerunning the simulation for that condition.
In short, the inside strategy minimizes disk space but constrains our ability to explore or revise performance calculations.

### Keep all simulation runs (outside)

The _outside_ approach involves retaining the entire set of estimates from every replication, with each row corresponding to an estimate for a given simulated dataset.
The benefit of the outside approach is that it allows us to add or change how we calculate performance measures without re-running the entire simulation.
This is especially important if the simulation is time-intensive, such as when the estimators being evaluated are computationally expensive.
The primary disadvantage the outside approach is that it produces large amounts of data that need to be stored and further manipulated.
Thus, the outside strategy maximizes flexibility, at the cost of increased dataset size.

In our Pearson correlation simulation, we initially followed the inside strategy. To move to the outside strategy, we can set the `summarize` argument of `Pearson_sim()` to `FALSE` so that the simulation driver returns a row for every replication:
```{r do_power_sim_full, cache=TRUE}
Pearson_sim(reps = 4, N = 15, mu1 = 5, mu2 = 5, rho = 0.5, summarize = FALSE)
```

We then save the entire set of estimates, rather than the performance summaries.
This result file will have $R$ times as many rows as the older file. In practice, these results can quickly get  to be extremely large.
But disk space is cheap!
Here we run the same experiment as in Section \@ref(using-pmap-to-run-multifactor-simulations), but storing the individual replications instead of just the summarized results:

```{r secret_Pearson_full, include=FALSE}
# (See below this block for book code)

if ( !file.exists( "results/Pearson_Poisson_results_full.rds" ) ) {
  # Secret Run code in parallel for speedup
  library(future)
  library(furrr)
  plan(multisession)
  set.seed(20250718)
  sim_results_full <- 
    params_mod %>%
    mutate(
      res = future_pmap(
        ., .f = Pearson_sim, 
        reps = 1000, summarize = FALSE, 
        .options = furrr_options(seed = TRUE)
      ) 
    ) %>%
    unnest(res)
  
  write_rds( sim_results_full, file = "results/Pearson_Poisson_results_full.rds" )
} else {
  sim_results_full <- read_rds("results/Pearson_Poisson_results_full.rds")
}
```

```{r Pearson_all_rows, eval=FALSE}
sim_results_full <- 
  params %>%
  mutate( mu2 = mu1 * lambda ) %>%
  evaluate_by_row( Pearson_sim, reps = 1000, summarize = FALSE )

write_rds( sim_results_full, file = "results/Pearson_Poisson_results_full.rds" )
```

We end up with many more rows.
Here is the number of rows for the outside vs inside approach:
```{r}
c(inside = nrow( sim_results ), outside = nrow( sim_results_full ))
```

Comparing the file sizes on the disk: 
```{r}
c(
  inside = file.size("results/Pearson_Poisson_results.rds"),
  outside = file.size("results/Pearson_Poisson_results_full.rds")
) / 2^10 # Kb
```
The first is several kilobytes, the second is several megabytes.

### Getting raw results ready for analysis

If we generate raw results, we then need to do the performance calculations across replications within each simulation context so that we can explore the trends across simulation factors.

One way to do this is to use `group_by()` and `summarize()` to carry out the performance calculations:
```{r}
sim_results_full %>%
  group_by( N, mu1, mu2, rho ) %>%
  summarise( 
    calc_coverage(lower_bound = CI_lo, upper_bound = CI_hi, true_param = rho)
  )
```

If we want to use our full performance measure function `evaluate_CIs()` to get additional metrics such as MCSEs, we would _nest_ our data into a series of mini-datasets (one for each simulation), and then process each element.
As we saw above, nesting collapses a larger dataset into one where one of the variables consists of a list of datasets:

```{r}
results <- 
  sim_results_full |>
  group_by( N, mu1, mu2, rho ) %>%
  nest( .key = "res" )
results
```

Note how each row of our nested data has a little tibble containing the results for that context, with 1000 rows each.
Once nested, we can then use `map2()` to apply a function to each element of `res`:

```{r}
results_summary <- 
  results %>%
  mutate( performance = map2( res, rho, evaluate_CIs ) ) %>%
  dplyr::select( -res ) %>%
  unnest( cols="performance" ) 
results_summary
```

We have built our final performance table _after_ running the entire simulation, rather than running it on each simulation scenario in turn.

Now, if we want to add a performance metric, we can simply change `evaluate_CIs` and recalculate, without having to recompute the entire simulation.
Summarizing during the simulation vs. after, as we just did, leads to the same set of results.^[In fact, if we use the same seed, we should obtain _exactly_ the same results.]
Allowing yourself the flexibility to re-calculate performance measures can be very advantageous, and we tend to follow this outside strategy for any simulations involving more complex estimation procedures.


## Summary

Multifactor simulations are simply a systematically generated series of individual scenario simulations.
The overall workflow is to first identify the factors and levels to explore (which we store as a dataset of all the combinations of the factors desired).
Think of this as a menu, or checklist, of simulations to run.
The next step is to then to walk down the list, running each simulation in turn.

Each individual simulation will generate its own set of results.
These can be the raw results (individual simulation iterations) or summary results (performance measures).
We stack all of these results, connecting them to the simulation factors they came from, to get a single massive dataset.
The key question is then how to explore this full set of results; without much difficulty, the amount of results generated can be kind of overwhelming!

The next chapters dive into how to take on this final task.

## Exercises

### Brown and Forsythe redux

Take another look at Table [5.1](case-ANOVA.html#tab:BF-Scenarios), which is excerpted from @brown1974SmallSampleBehavior. 
Create a tibble with one row for each of the 20 scenarios that they evaluated. 
Then create a function for running the full simulation process (see Exercise \@ref(Welch-simulation)).
Use `pmap()` or `evaluate_by_row()` to run simulations of all 20 scenarios and reproduce the results in Table [5.2](case-ANOVA.html#tab:BF-table1) of Chapter \@ref(case-ANOVA).

### Meta-regression

Exercise \@ref(meta-regression-DGP) described the random effects meta-regression model. 
List the focal, auxiliary, and structural parameters of this model, and propose a set of design factors to use in a multifactor simulation of the model.
Create a list with one entry per factor, then create a dataset with one row for each simulation context that you propose to evaluate.


### Comparing the trimmed mean, median and mean {#exercise:trimmed-mean}

In this exercise, you will write a simulation to compare several different
estimators of a common parameter.
In particular, you will compare the  mean, trimmed mean, and median as estimators of the center of a symmetric distribution (such that the mean and median parameters are identical).  
To do this, you should break building this simulation evaluation down into functions for each component of the simulation. 
This will allow you to extend the same framework to more complicated simulation studies.
This extended exercise illustrates how methodologists might compare different estimation strategies, as you might see in the "simulation" section of a stats paper.

As the data-generation function, use a scaled $t$-distribution so that the standard deviation will always be 1 but will have different fatness of tails (high chance of outliers):

```{r}
gen_scaled_t <- function( n, mu, df0 ) {
    mu + rt( n, df=df0 ) / sqrt( df0 / (df0-2) )
}
```

The variance of a $t$ distribution is $df/(df-2)$, so when we divide our observations by the
square root of this, we standardize them so they have unit variance.

1. Verify that `gen_scaled_t()` produces data with mean `mu` and standard deviation 1 for various `df0` values.

2. Write a method to calculate the mean, trimmed mean, and median of a vector of data.
   The trimmed mean should trim 10% of the data from each end.
   The method should return a data frame with the three estimates, one row per estimator.

3. Verify your estimation method works by analyzing a dataset generated with `gen_scaled_t()`.
   For example, you can generate a dataset of size 100 with `gen_scaled_t(100, 0, 3)` and then analyze it.
   
4. Use `bundle_sim()` to create a simulation function that generates data and then analyzes it.
   The function should take `n` and `df0` as arguments, and return the estimates from your analysis method.
   Use `id` to give each simulation run an ID.

5. Run your simulation function for 1000 datasets of size 10, with `mu=0` and `df0=5`.
   Store the results in a variable called `raw_exps`.
   
6. Write a function to calculate the RMSE, bias, and standard error for your three estimators, given the results.

7. Make a single function that takes `df0` and `n`, and runs a simulation and returns the performances of your three methods.

8. Now make a grid of $n = 10, 50, 250, 1250$ and $df_0 =  3, 5, 15, 30$, and generate results for your multi-factor simulation.

9. Make a plot showing how SE changes as a function of sample size for each estimator. Do the three estimator seem to follow the same pattern? Or do they work differently?

