# Estimation procedures


```{r, include=FALSE}
library(tidyverse)

# From prior chapter on ANOVA case study.
generate_data <- function(mu, sigma_sq, sample_size) {

  N <- sum(sample_size)
  g <- length(sample_size)

  group <- rep(1:g, times = sample_size)
  mu_long <- rep(mu, times = sample_size)
  sigma_long <- rep(sqrt(sigma_sq), times = sample_size)

  x <- rnorm(N, mean = mu_long, sd = sigma_long)
  sim_data <- data.frame(group = group, x = x)

  return(sim_data)
}

mu <- c(1, 2, 5, 6)
sigma_sq <- c(3, 2, 5, 1)
sample_size <- c(3, 6, 2, 4)


one_run = function( mu, sigma_sq, sample_size ) {
  sim_data <- generate_data(mu = mu, sigma_sq = sigma_sq, sample_size = sample_size)
  anova_p <- ANOVA_F(sim_data)
  Welch_p <- Welch_F(sim_data)
  tibble(ANOVA = anova_p, Welch = Welch_p)
}

```


In the abstract, a function that implements an estimation procedure should have the following form:
```{r}
estimate <- function(data) {

  # calculations/model-fitting/estimation procedures
  
  return(estimates)
}
```

The function takes a set of data as input, fits a model or otherwise calculates an estimate, possibly with associated standard errors and so forth, and produces as output these estimates.
In principle, you should be able to run your function on real data as well as simulated.

The estimates could be point-estimates of parameters, standard errors, confidence intervals, etc.
Depending on the research question, this function might involve a combination of several procedures (e.g., a diagnostic test for heteroskedasticity, followed by the conventional formula or heteroskedasticity-robust formula for standard errors).
Also depending on the research question, we might need to create _several_ functions that implement different estimation procedures to be compared. 

In Chapter @case_ANOVA, for example, we saw different functions for some of the methods Brown and Forsythe considered for heteroskedastic ANOVA.
We re-print them here, taking full advantage of our digital-bookness:

```{r}
ANOVA_F <- function(sim_data) {

  x_bar <- with(sim_data, tapply(x, group, mean))
  s_sq <- with(sim_data, tapply(x, group, var))
  n <- table(sim_data$group)
  g <- length(x_bar)

  df1 <- g - 1
  df2 <- sum(n) - g

  msbtw <- sum(n * (x_bar - mean(sim_data$x))^2) / df1
  mswn <- sum((n - 1) * s_sq) / df2
  fstat <- msbtw / mswn
  pval <- pf(fstat, df1, df2, lower.tail = FALSE)

  return(pval)
}


Welch_F <- function(sim_data) {

  x_bar <- with(sim_data, tapply(x, group, mean))
  s_sq <- with(sim_data, tapply(x, group, var))
  n <- table(sim_data$group)
  g <- length(x_bar)

  w <- n / s_sq
  u <- sum(w)
  x_tilde <- sum(w * x_bar) / u
  msbtw <- sum(w * (x_bar - x_tilde)^2) / (g - 1)

  G <- sum((1 - w / u)^2 / (n - 1))
  denom <- 1 +  G * 2 * (g - 2) / (g^2 - 1)
  W <- msbtw / denom
  f <- (g^2 - 1) / (3 * G)

  pval <- pf(W, df1 = g - 1, df2 = f, lower.tail = FALSE)

  return(pval)
}
```




## Checking the estimation function

Just as with the data-generating function, it is important to verify the accuracy of the estimation functions. For the ANOVA-F test, this can be done simply by checking the result of our `ANOVA_F` against the built-in `oneway.test` function. Let's do that with a fresh set of data:

```{r}
sim_data <- generate_data(mu = mu, sigma_sq = sigma_sq,
                          sample_size = sample_size)
aov_results <- oneway.test(x ~ factor(group), data = sim_data, 
                           var.equal = TRUE)
aov_results

F_results <- ANOVA_F(sim_data)
all.equal(aov_results$p.value, F_results)
```

We use `all.equal()` because it will check equality up to a tolerance in R, which can avoid some weird floating point errors due to rounding in R.

We can follow the same approach to check the results of the Welch test because it is also implemented in `oneway.test`:

```{r}
aov_results <- oneway.test(x ~ factor(group),
                           data = sim_data, 
                           var.equal = FALSE)
aov_results
Welch_results <- Welch_F(sim_data)
all.equal(aov_results$p.value, Welch_results)

```

See @sec_comp_efficiency for a bit more on checking functions, and a side-note about transparency in the writing of papers.



## Checking via simulation

If your estimation procedure truly is new, how would you check it?
Well, one obvious answer is simulation!

In principle, for large samples and data generated under the assumptions required by your new procedure, you should have a fairly good sense that it should work and how it should work.
It is often the case that as you design your simulation, and then start analyzing the results, 



## Exercises (adding the BFF test)

Let's continue to explore and tweak the simulation code we've developed to replicate the results of Brown and Forsythe (1974).
Below is the key functions for the data-generating process (taken from the prior chapter).
The estimation procedures are above.

```{r}
generate_data <- function(mu, sigma_sq, sample_size) {

  N <- sum(sample_size) 
  g <- length(sample_size) 
  
  group <- rep(1:g, times = sample_size) 
  mu_long <- rep(mu, times = sample_size)
  sigma_long <- rep(sqrt(sigma_sq), times = sample_size) 
  
  x <- rnorm(N, mean = mu_long, sd = sigma_long)
  sim_data <- data.frame(group = group, x = x)
    
  return(sim_data)
}
```




1. Write a function that implements the Brown-Forsythe F\*-test (the BFF\* test!) as described on p. 130 of Brown and Forsythe (1974).
Call it on a sample dataset to check it.

2. Now incorporate the function into the `one_run()` function from the previous question, and use it to estimate rejection rates of the BFF\* test for the parameter values in the fifth line of Table 1 of Brown and Forsythe (1974).

```{r}
BF_F <- function(x_bar, s_sq, n, g) {
  
  # fill in the guts here
  
  return(pval = pval)
}

# Further R code here
```

