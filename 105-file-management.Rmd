---
output: html_document
editor_options: 
  chunk_output_type: console
---
```{r, include =FALSE}
library( purrr )
```

# (PART) Computational Considerations {-}

# Organizing a simulation project


Full, multifactor simulations often grow into quite complex projects with many moving pieces. 
As a project grows in complexity, the steady accretion of functions, scripts, stored results, and analysis can easily become overwhelming---to the point that you would dread sharing these work products with anyone.
Fortunately, a bit of forethought and advance planning can make these types of projects much more tractable and manageable.

In the next several chapters, we describe some organizing principles and programming practices that will make it easier to handle complex simulation projects.
In this chapter, we discuss project organization, file management, and storing simulation results. 
In Chapter \@ref(parallel-processing), we demonstrate  parallel processing methods that will help you to speed up the computation involved in multifactor simulations. 
Finally, in Chapter \@ref(debugging-and-testing), we introduce some basic debugging and testing techniques that we have found useful for keeping on top of this type of sprawling complexity.
<!-- JEP: This isn't really an accurate characterization of the debugging-and-testing chapter? -->

## Simulation project structure 

In a very simple simulation that does not take long to execute, you might keep all of the code, output, and analysis in a single file.
However, if any of the computations take substantial time to execute, keeping everything in one file will become unwieldy.
For more complex or more computationally demanding simulations, there are big benefits to organizing your code across more than one file.

Multifactor simulation studies usually involve three distinct phases of work.
The first phase involves developing functions for generating data, applying estimators or data analysis procedures to the data, calculating performance measures, and running the full simulation for a single scenario.
The end-product of this phase is a set of functions or methods for executing a simulation.
The second phase involves running the simulations across multiple scenarios, as discussed in Chapter \@ref(simulating-multiple-scenarios).
The end-product of this phase is a set of results, containing estimates of performance measures for one or more methods under each condition.
The third phase involves analyzing the simulation results and drawing conclusions about the performance of the method or methods under evaluation.
The end-product of this phase is often a set of figures, tables, and text (perhaps taking the form of a memo, blog post, slide deck, or manuscript) that summarizes what you find.
Each of the three phases involves a distinct set of processes and different types of code. 

To stay organized when conducting a larger, multifactor simulation, we find it generally useful to keep a clear separation between phases. 
In practice, this means keeping the code for each phase in its own, separate `.R` file and storing the results from each phase so that it can be re-run or revised without re-executing the entire set of calculations. 
This approach is in keeping with a general principle for organizing large computational projects: keep the code for distinct tasks in different files. 

In this chapter, we describe strategies for organizing the code and work products involved in a multifactor simulation study. 
We start by describing a directory structure that encourages a clear separation between phases of a project.
We then give examples and recommendations for how to structure individual files and how to store results after executing a simulation.
The advice and recommendations we offer are drawn from our own experience working on large simulation projects (including the many mistakes we have made and lessons we have learned through trial-and-error).
Of course, we readily acknowledge that our approach here is not the only way to do things, and we make no claims of optimality.
As you gain experience with your own work, you will develop your own habits and strategies.
<!-- JEP: Does this need one more closing thought / transition? -->

## Principled directory structures

_Tidy home, tidy mind_, as the saying goes. 
With any computational project, our home is usually a directory on our computer (or in the cloud).
For multifactor simulation studies, we _strongly_ advocate using an organized and clearly labelled directory structure, which will facilitate more intentional---and easier to follow---coding practices.
We recommend using an integrated development environment such as RStudio, Positron, or VS Code and building a directory structure as follows:

```
my_project/
  proj.Rproj
  README.md
  R/
  test/
  data/
  scripts/
  results/
```

If you have ever looked at the source code for or developed your own R package, this structure will look familiar.

* The `R/` directory is where to put the core R code for your simulation.
  It should contain one or more `.R` scripts that hold the main functions for implementing a tidy simulation, including data-generating functions, analysis functions, performance calculation functions, and a simulation driver that pulls all the pieces together. 
  With all of these functions saved in the same directory, you can then run the scripts as needed to gain access to your functions.
  You _should not_ put the scripts that actually execute the simulation or analyze the results in this folder.
  Instead, the `R/` folder is reserved for the building blocks of the simulation.

* The `test/` directory is where to put any code for testing the functions and methods you have developed.
  You could even write formal unit tests for your methods using a testing framework such as the `testthat` package, and put those in the `test/` directory.

* The `data/` directory is where to put any data files used in executing the simulation.
  For instance, a simulation of a cluster-randomized experiment might make use of an empirical dataset containing features such as cluster sizes or covariate values, which are sampled when generating artificial datasets.
  This directory is reserved for _source_ data, which may be read in as part of the simulation.
  It should not contain any files that are _created_ as part of executing the simulation.

* The `scripts/` directory is where to put scripts that actually run the simulations and analyze simulation results.
  Some of these scripts will make use of functions saved in the `R/` directory.
  You will likely have at least one script that runs the simulation and at least one script for analyzing the results.

* The `results/` directory is where to save any generated results of your simulation.
  This directory should _only_ contain files that have been created by running code in the `scripts/` directory.
  Sometimes it might be worth having separate directories `raw_results` and `results`, where `raw_results` holds output created from running the simulation driver, and `results` holds final results produced from merging and summarizing the raw results.
<!-- JEP: This last sentence could be about directories or it could be applied to files within a directory. I've edited to make it about directories---what did you have in mind? Any preference? -->

Using this structure will help you to maintain reproducibility of the full project.
If any files in `data/`, `R/`, or `scripts/` are changed, then some or all of the files in `results/` may need to be updated.
In principle, it should be possible to delete all of the files in `results/` and recreate them in full by re-running the files in the `scripts/` directory.

## Well structured code files

With a directory structure in place, conducting a simulation will require writing, testing, and revising code (potentially quite a bit of code!).
There are two main types of files that analysts use to hold R code; plain `.R` scripts and R markdown (`.Rmd`) or Quarto (`.qmd`) notebooks.
The former can only contain R syntax and comments, whereas the latter can hold a mix of R code chunks and written text.
The latter work as the source code for creating reproducible reports. 
Compiling a notebook will automatically run the embedded code and interweave the results with the text, producing a formatted document that can display figures, tables, and other forms of output.

For very small projects, it may be possible to store your entire code base in a single notebook, containing all of your functions, code for creating a simulation design and executing the simulations, and code for analyzing the results.
An advantage of using a notebook here is that you can add plain text explanations and descriptions of the component pieces mixed in with the actual source code, so that rationales and design decisions are fully documented.
Further, you can re-run the entire simulation at the click of a button, simply by re-compiling the the notebook.
However, it is difficult to work this way if your simulations involve more intensive computations.

For larger projects, you will be more dependant on the first type of file, the `.R` script.
Ideally, you should follow the principles of modular programming for the code in these files.
Each `.R` file should hold a collection of code that is related to a single task.
Accordingly, we distinguish between two types of of `.R` scripts: those that _just_ have functions that can be used for other purposes and those that carry out numerical calculations.
The former type only contain functions, so that the only consequence of running them from top to bottom will be to load some new functions into your workspace.
This type should be stored in the `R/` directory.
The latter type are traditional scripts that do calculations, store or load data files, and create graphs and other summaries of data.
This type should be stored in the `scripts/` directory.

### Putting headers in your .R file

When you write an `.R` script, it is a good idea to put a header at the top of the file that describes the file's purpose.
Within the file, you can also describe the contents with shorter section headers.
For instance, a very simple section header might look like this:

```
#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#
# Data generating functions ----
#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#
```

Note the `----` at the end of the middle line.
Using four trailing dashes trailing dashes indicates a section of the code that RStudio's parser can interpret.
If you click on the dropdown at the bottom of the RStudio source pane, you will see a pop-up table of contents that allows you to quickly navigate to different parts of your source file.
You can also see the same table of contents by clicking the `Outline` button in the top right corner of the source pane.

### The source command {#about-source-command}

If your code is organized across more than one file, you will need a way to call one file from another. 
This is the purpose of the `source()` command, which effectively "cuts and pastes" the contents of the given file into your R work session.
For example, the following code runs three separate R scripts in turn:

```{r demo_source_multiple, eval=FALSE}
source( here::here( "R/data_generators.R" ) )
source( here::here( "R/estimators.R" ) )
source( here::here( "R/simulation_support.R" ) )
```

If the named file has code to run, it will run it.
If the named file has a list of methods, those methods will now be available for use.
The `here::here()` command is a convenience function that allows you to specify a file path relative to your R project root directory, so you can easily find your files.[^working-directory-weirdness]

[^working-directory-weirdness]: If you are used to using RStudio projects, you might wonder why you should use `here::here()`. If you always open your project from the `.Rproj` file, then the working directory will always start out set to the root directory, but it will not necessarily remain so (if a script calls `setwd()`, for instance). Further, when `.Rmd` or `.qmd` notebooks are compiled, the working directory is temporarily changed to the location of the notebook, so relative file references will work differently than in plain `.R` scripts. Wrapping file names in `here::here()` ensures that they will always be evaluated relative to the root directory of the project.

The `source()` command is simple and literal---it runs a specified script from top to bottom, regardless of the contents. 
In particular, if the sourced script includes further calls of `source()`, the referenced code will be run. 
For example, the `simulation_support.R` script could include calls to source the other two files.
In this case, you would only need to source the single `simulation_support.R` file to load the contents of all three files. 
Although this might seem convenient, it does create some constraints as well.
With this structure, there would be no way to source the contents of `simulation_support.R` without also sourcing `data_generators.R` and `estimators.R`.
For this reason, we tend to avoid using `source()` in scripts that might themselves be sourced.


One reason for storing code in individual files and using `source()` is that you can then include testing code in each of your files to both demonstrate the syntax and check the correctness of your functions.
Then, when you are not focused on that particular function or component, you don't have to look at that testing code.
Another good reason for this type of modular organization is that is supports developing a broader simulation universe, potentially with a variety of data generating functions.
With a whole library of options, you can then readily run multiple simulations that involve some different components and some shared features.

We followed this approach in one recent simulation project examining estimators for an Instrumental Variable analysis, a common approach to handling non-compliance in randomized experiments. 
In this project, we wrote several different data generating functions that implemented different types of compliance patterns.
Our `data_generators.R` code file then had several methods.
When we sourced it, we end up wit the following methods available to call:

```
> ls()
[1] "describe_sim_data"  "make_dat"           "make.dat_1side"     
[4] "make_dat_1side_old" "make_dat_orig"      "make_dat_simple"
[7] "make_dat_tuned"     "rand_exp"           "summarize_sim_data"
```

The `describe()` and `summarize()` methods printed various statistics about a sample dataset; we used used these to examine generated datasets and debug the data generating functions.
We also had a variety of different data generating methods because we had different versions that came up as we were trying to chase down errors in our estimators and understand strange behavior.

For this project, we also stored estimation functions in a different file (also in the `/R` directory).
Doing so made it easier to use these functions for purposes other than running simulations.
We also did some empirical data analysis as part of the project, and so we could simply source the file with our estimation functions and apply them to a real dataset. 
This ensured that our simulation and applied analysis were exactly aligned in terms of the estimators we were using.
As we debugged and tweaked our estimators, we could immediately re-run our applied analysis to update the results, without worrying about whether the calculations were consistent with the simulation code.

### Storing testing code in your scripts {#about-keeping-tests-with-FALSE}

If you have an extended `.R` file with a list of functions, you might also want to store a lot of code that runs each function in turn, so you can easily remind yourself of what it does, or what the output looks like.
One way to keep this code around, but not have it run all the time when you run your script, is to put the code inside a "FALSE block," that might look like so:

```{r}
# My testing code ----
if ( FALSE ) {
  res <- my_function( 10, 20, 30 )
  res
  # Some notes as to what I want to see.
  
  sd( res )
  # This should be around 20
}
```

You can then, when you open and look at the script, paste the code inside the block into the console when you want to run it.
If you source the script, however, it will not run at all, and thus your code will source faster and not print out any extraneous output.
This is a good way to keep your testing code with the code it is testing.
When you want to work on just the part of the project captured by your script, you can work inside the single file very easily, ignoring the other parts of your project.

You can also (or instead) write testing code, which we will talk about further below.


## Saving simulation results {#saving-files}


Always save your simulation results to a file.
Simulations are painful and time consuming to run, and you will invariably want to analyze the results of them in a variety of different ways, once you have looked at your preliminary analysis.
We advocate saving your simulation as soon as it is complete.
But there are some ways to do better than that, such as saving as you go.
This can protect you if your simulation occasionally crashes, or if you want to rerun only parts of your simulation for some reason.


### Saving simulations in general

Once your simulation has completed, you can save it like so:
```{r, eval=FALSE}
dir.create("results", showWarnings = FALSE )
write_csv( res, "results/simulation_CRT.csv" )
```

`write_csv()` is a tidyverse file-writing command; see "R for Data Science"
textbook, 11.5.

You can then load it, just before analysis, as so:
```{r, eval=FALSE}
res = read_csv( "results/simulation_CRT.csv" )
```

There are two general tools for saving.  The `read/write_csv` methods save your file in a way where you can open it with a spreadsheet program and look at it.
But your results should be in a vanilla, rectangular format (non-fancy data frame without list columns).

Alternatively, you can use the `saveRDS()` and `readRDS()` methods; these save objects to a file such that when you load them, they are as you left them.
The RDS saving keeps your R object as given.
The simpler format of a csv file means your factors, if you have them, may not preserve as factors, and so forth.



### Saving simulations as you go

If you are not sure you have time to run your entire simulation, or you think your computer might crash half way through, or something similar, you can save each chunk you run as you go, in its own file.  You then stack those files at the end to get your final results.
With clever design, you can even then selectively delete files to rerun only parts of your larger simulation---but be sure to rerun everything from scratch before you run off and publish your results, to avoid embarrassing errors.

Here, for example, is a script from a research project examining how one might use post-stratification to improve the precision of an IV estimate.
This is the script that runs the simulation.
Note the sourcing of other scripts that have all the relevant functions; these are not important here.
Due to modular programming, we can see what this script does, even without those detail.

```{r IV_simulation_driver, eval=FALSE}
source( "R/simulation_functions.R" )

if ( !file.exists("results/frags" ) ) {
    dir.create("results/frags")
}

# Number of simulation replicates per scenario
R = 1000

# Do simulation breaking up R into this many chunks
M_CHUNK = 10

###### Set up the multifactor simulation #######

# chunkNo is a hack to make a bunch of smaller chunks for doing parallel more
# efficiently.
factors = expand_grid( chunkNo = 1:M_CHUNK,
                       N = c( 500, 1000, 2000 ),
                       pi_c = c( 0.05, 0.075, 0.10 ),
                       nt_shift = c( -1, 0, 1 ),
                       pred_comp = c( "yes", "no" ),
                       pred_Y = c( "yes", "no" ),
                       het_tx = c( "yes", "no" ),
                       sd0 = 1
                       )
factors <- factors %>% mutate(
    reps = R / M_CHUNK,
    seed = 16200320 + 1:n()
)
```

This generates a data frame of all our factor combinations.
This is our list of "tasks" (each row of factors).
These tasks have repeats: the "chunks" means we do a portion of each scenario, as specified by our simulation factors, as a process.
This would allow for greater parallelization (e.g., if we had more cores), and also lets us save our work without finishing an entire scenario of, in this case, 1000 iterations.

To set up our simulation we make a little helper method to do one row.
With each row, once we have run it, we save it to disk.
This means if we kill our simulation half-way through, most of the work would be saved.
Our function is then going to either do the simulation (and save the result to disk immediately), or, if it can find the file with the results from a previous run, load those results from disk:

```{r, eval=FALSE}
safe_run_sim = safely( run_sim )
file_saving_sim = function( chunkNo, seed, ... ) {
    fname = paste0( "results/frags/fragment_", chunkNo, "_", seed, ".rds" )
    res = NA
    if ( !file.exists(fname) ) {
        res = safe_run_sim( chunkNo=chunkNo, seed=seed, ... )
        saveRDS(res, file = fname )
    } else {
        res = readRDS( file=fname )
    }
    return( res )
}
```

Note how we wrap our core `run_sim` method (that takes all our simulation factors and runs a simulation for those factors) in `safely`; `run_sim()` was crashing very occasionally, and so to make the code more robust, we wrapped it so we could see any error messages.
Our method cleverly either loads a saved result, or generates it, for a given chunk.
This means from whatever is calling the function, it will look exactly the same whether it is loading a saved result or generating a new one.

We next run the simulation by calling `file_saving_sim()` for all of our simulation scenarios.

```{r demo_run_on_fly_code, eval=FALSE}
# Shuffle the rows so we run in random order to load balance.
factors = sample_n(factors, nrow(factors) )

if ( TRUE ) {
    # Run in parallel
    parallel::detectCores()
    
    library(future)
    library(furrr)
    
    #plan(multiprocess) # choose an appropriate plan from future package
    #plan(multicore)
    plan(multisession, workers = parallel::detectCores() - 2 )
    
    factors$res <- future_pmap(factors, .f = file_saving_sim,
                          .options = furrr_options(seed = NULL),
                          .progress = TRUE )
    
} else {
  # Run not in parallel, used for debugging
  factors$res <- pmap(factors, .f = file_saving_sim )
}

tictoc::toc()
```

Note how we shuffle the rows of our task list so that which process gets what task is randomized.
If some tasks are much longer (e.g., due to larger sample size) then this will get balanced out across our processes.
See \@ref(parallel-processing) for more on parallel processing.

The `if-then` structure allows us to easily switch between parallel and nonparallel code.
This makes debugging easier: when running in parallel, stuff printed to the console does not show until the simulation is over.
Plus it would be all mixed up since multiple processes are working simultaneously.

The above overall structure allows the researcher to delete one of the "fragment" files from the disk, run the simulation code, and have it just do one tiny piece of the simulation.
This means the researcher can insert a `browser()` command somewhere inside the code, and debug the code, in the natural context of how the simulation is being run.

The seed setting ensures reproducibility.
Once we are done, we need to clean up our results:

```{r run_sim_demo_code, eval=FALSE}
sim_results <- 
    factors %>% 
    unnest(cols = res)

# Cut apart the results and error messages
sim_results$sr = rep( c("res","err"), nrow(sim_results)/2)
sim_results = pivot_wider( sim_results, names_from = sr, values_from = res )

saveRDS( sim_results, file="results/simulation_results.rds" )
```

Our final `simulation_results.rds` file will have all the results from our simulation, made by stacking all of the fragments of our simulation together.


### Dynamically making directories

If you are generating a lot of files, then you should put them somewhere.
But where?
It can be nice to dynamically generate a directory for your files on fly.
One way to do this is to write a function that will make any needed directory, if it doesn't exist, and then put your file in that spot.
For example, you might have your own version of `write_csv` as:

```{r my_write_csv_function}
my_write_csv <- function( data, path, file ) {
  
  if ( !dir.exists( here::here( path ) ) ) {
    dir.create( here::here( path ), recursive=TRUE ) 
  }
  write_csv( data, paste0( path, file ) )
}
```

This will look for a path (starting from your R Project, by taking advantage of the `here` package), and put your data file in that spot.
If the spot doesn't exist, it will make it for you.




### Loading and combining files of simulation results

Once your simulation files are all generated, the following code will stack them all into a giant set of results, assuming all the files are themselves data frames stored in RDS objects.
This function will try and stack all files found in a given directory; for it to work, you should ensure there are no other files stored there.


```{r load_all_sims_function, eval=FALSE}
load.all.sims = function( filehead="raw_results/" ) {
  
  files = list.files( filehead, full.names=TRUE)
  
  res = map_df( files, function( fname ) {
    cat( "Reading results from ", fname, "\n" )
    rs = readRDS( file = fname )
    rs$filename = fname
    rs
  })
  res
}
```

You would use as so:
```{r use_load_all_sims, eval=FALSE}
results <- load.all.sims( filehead="raw_results/" )
results <- bind_rows( results )
```



