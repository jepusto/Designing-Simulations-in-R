---
output: html_document
editor_options: 
  chunk_output_type: console
---
```{r, include =FALSE}
library( purrr )
```

# (PART) Computational Considerations {-}

# Organizing a simulation project


Full, multifactor simulations often grow into quite complex projects with many moving pieces. 
As a project grows in complexity, you may end up producing a _lot_ of output, which can become hard to navigate and manage.
The steady accretion of functions, scripts, stored results, and analysis can easily become overwhelming---to the point that you would dread sharing these work products with anyone.
However, a bit of forethought and advance planning can make these types of projects much more tractable and manageable.

In this section, we describe some organizing principles and programming practices that will make it easier to handle complex simulation projects.
In this chapter, we discuss project organization, file management, and storing simulation results. 
In Chapter \@ref(parallel-processing), we demonstrate  parallel processing methods that will help you to speed up the computation involved in multifactor simulations. 
Finally, Chapter \@ref(debugging-and-testing) covers some basic debugging and testing techniques that we have found useful for keeping on top of this type of sprawling complexity.
<!-- JEP: This isn't really an accurate characterization of the debugging-and-testing chapter? -->

## Simulation project structure 

Multifactor simulation studies usually involve three distinct phases of work.
The first phase involves developing functions for generating data, applying estimators or data analysis procedures to the data, calculating performance measures, and running the full simulation for a single scenario.
The end-product of this phase is a set of functions or methods for executing a simulation.
The second phase involves running the simulations across multiple scenarios, as discussed in Chapter \@ref(simulating-multiple-scenarios).
The end-product of this phase is a set of results, containing estimates of performance measures for one or more methods under each condition.
The third phase involves analyzing the simulation results and drawing conclusions about the performance of the method or methods under evaluation.
The end-product of this phase is often a set of figures, tables, and text (perhaps taking the form of a memo, blog post, slide deck, or manuscript) that summarizes what you find.
Each of the three phases involves a distinct set of processes and a different type of code. 

As a bare-bones example, a multi-factor simulation will likely have three distinct pieces of code:

* A set of functions for running the simulation,
* Code for creating the simulation design matrix, executing the calculations, and compiling all the results, and
* Code for analyzing the results.

In a very simple simulation that does not take long to execute, you might keep all of this code in a single file.
However, if any of these components is more complex or takes time to execute, keeping everything in one file will become unwieldy.

To stay organized when conducting a larger, multifactor simulation, we find it generally useful to keep a clear separation between phases. 
In practice, this means keeping the code for each phase in its own, separate `.R` file and storing the results from each phase so that it can be re-run or revised without re-executing the entire set of calculations. 
This approach is in keeping with a general principle for organizing large computational projects: keep the code for distinct tasks in different files. 

In this chapter, we describe strategies for organizing the code and work products involved in a multifactor simulation study. 
We start by describing a directory structure that encourages a clear separation between phases of a project.
We then offer examples and recommendations for how to structure individual files and how to store results after executing a simulation.
The advice and recommendations we offer are drawn from our own experience working on large simulation projects (including the many mistakes we have made and lessons we have learned through trial-and-error).
Of course, we readily acknowledge that our approach here is not the only way to approach things, and we make no claims of optimality.
As you gain experience with your own work, you will develop your own habits and strategies. 

## Principled directory structures

_Tidy home, tidy mind_, as the saying goes. 
With any computational project, our home is usually a file directory on your computer (or in the cloud).
For multifactor simulation studies, we _strongly_ advocate using a well-organized directory structure, which will facilitate more organized---and easier to follow---coding practices.
We recommend using an integrated development environment such as RStudio, Positron, or VS Code, using a directory structure organized as follows:

```
my_project/
  proj.Rproj
  README.md
  R/
  test/
  data/
  scripts/
  results/
```

If you have ever looked at the source code for or developed your own R package, this structure will look familiar.

* The `R/` directory is where you put the core R code for your simulation.
  It should contain one or more `.R` scripts that hold the main functions for implementing a tidy simulation, including data-generating functions, analysis functions, performance calculation functions, and a simulation driver that pulls all the pieces together. 
  With all of these functions saved in the same directory, you can then load these scripts into your workspace as needed to gain access to those functions.
  You _should not_ put the scripts that actually execute the simulation or analyze the results in this folder.
  Instead, the `R/` folder is reserved for the building blocks of the simulation.

* The `test/` directory is where you put any code for testing the functions and methods you have developed.
  You can even use the `testthat` package to write unit tests for your methods, and put those in the `test/` directory.

* The `data/` directory is where you put any data files used in executing the simulation.
  For instance, a simulation of a cluster-randomized experiment might make use of an empirical dataset containing features such as cluster sizes or covariate values, which are sampled when generating artificial datasets.
  This directory is reserved for _source_ data, which may be read in as part of the simulation.
  It should not contain any files that are _created_ as part of executing the simulation.

* The `scripts/` directory is where you put your scripts that actually run the simulations and analyze simulation results.
  Some of these scripts will make use of the functions saved in the `R/` directory.
  You will likely have at least one script that runs the simulation and at least one script for analyzing the results.

* The `results/` directory is where you will save any generated results of your simulation.
  This directory should _only_ contain files that have been created by running code in the `scripts/` directory.
  Sometimes it might be worth having `raw_results` and `results`, where `raw_results` are what you save as soon as you finish running the simulation, and `results` have final results where you may have merged and summarized the raw results.
<!-- JEP: This last sentence could be about directories or it could be applied to files within a directory---what did you have in mind? Any preference? -->

Using this structure will help you to maintain reproducibility of the full project.
If any files in `data/`, `R/`, or `scripts/` are changed, then some or all of the files in `results/` may need to be updated.
In principle, it should be possible to delete all of the files in `results/` and recreate them in full by re-running the files in the `scripts/` directory.

## Well structured code files

With a directory structure in place, conducting a simulation will require writing, testing, and revising code (potentially a lot of code!).
There are two main types of files that analysts use to hold R code; plain `.R` scripts and R markdown (`.Rmd`) or Quarto (`.qmd`) notebooks.
The former can only contain R syntax and comments, whereas the latter can hold a mix of R code chunks and written text.
The latter work as the source code for creating reproducible reports; compiling them will automatically run the embedded code and interweave the results with the text, producing a formatted document that can display figures, tables, and other forms of output.

For very small projects, it may be possible to put your entire code base in a single notebook, containing all of your functions, code for creating a simulation design and executing the simulations, and code for analyzing the results.
An advantage of using a notebook here is that you can add plain text explanations and descriptions of the component pieces mixed in with the actual source code, so that rationales and design decisions are fully documented.
Further, you can re-run the entire simulation at the click of a button, simply by re-compiling the the notebook.
However, it is difficult to work this way if your simulations involve more intensive computations.

For larger projects, you will be more dependant on the first type of file, the `.R` script.
Ideally, the principle of modular programming would be applied to these files.
Each `.R` file would hold a collection of code that is related to a single task.
There are two main types of of `.R` script: those that _just_ have functions that can be used for other purposes (meaning that if you run them, the only thing that happens is you end up with some new functions in your current workspace), and those that are traditional scripts such that when you run them, R will do a variety of specified tasks.


### The source command {#about-source-command}

Inside of an R script you can "source" other `.R` files.
The `source()` command essentially "cuts and pastes" the contents of the given file into your R work session.
E.g.,

```{r demo_source_multiple, eval=FALSE}
source( here::here( "R/data_generators.R" ) )
source( here::here( "R/estimators.R" ) )
source( here::here( "R/simulation_support.R" ) )
```

If the named file has code to run, it will run it.
If the named file has a list of methods, those methods will now be available for use.
The `here::here()` command is a convenience function that allows you to specify a file path relative to your R project root directory, so you can easily find your files.

You can even source files inside files that are sourced.
For example, `simulation_support.R` could, inside it, source the other two files.
You would then only source the single simulation support file in your primary simulation script.

One reason for putting code in individual files is you can then have testing code in each of your files (in False blocks, see below), testing each of your components.
Then, when you are not focused on that component, you don't have to look at that testing code.

Another good reason for this type of modular organizing is you can then allow for a whole simulation universe, writing a variety of data generators that together form a library of options.
You can then easily create different simulations that use your different pieces, in your larger project.

For example, in one recent simulation project on estimators for an Instrumental Variable analysis, we had several different data generators for generating different types of compliance patterns (IVs are often used to handle noncompliance in randomized experiments).
Our `data_generators.R` code file then had several methods.
When we sourced it, we end up wit the following list of methods:

```
> ls()
[1] "describe_sim_data"  "make_dat"           "make.dat_1side"     
[4] "make_dat_1side_old" "make_dat_orig"      "make_dat_simple"
[7] "make_dat_tuned"     "rand_exp"           "summarize_sim_data"
```

The `describe()` and `summarize()` methods printed various statistics about a sample dataset; these are used to debug and understand how the generated data looks.
We also had a variety of different DGP methods because we had different versions that came up as we were trying to chase down errors in our estimators and understand strange behavior.

Putting the estimators in a different file also had a nice additional purpose: we also had an applied data example in our work, and we could simply source that single file and use those estimators on our actual data.
This ensured our simulation and applied analysis were perfectly aligned in terms of the estimators we were using.
Also, as we debugged our estimators and tweaked them, we immediately could re-run our applied analysis to update those results with minimal effort.

Modular programming is key.


### Putting headers in your .R file

When you write a `.R` script, it is a good idea to put a header at the top of the file, giving a description of the file's purpose.
Then, you can also put dividers in your file, e.g.,

```
#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#
# Data generating functions ----
#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#
```

Note the `----` at the end of the middle line: if you add those trailing dashes, then if you click on the dropdown at the bottom of your RStudio, you will see a pop-up table of contents that allows you to quickly navigate to different parts of your source file.


### Storing testing code in your scripts {#about-keeping-tests-with-FALSE}

If you have an extended `.R` file with a list of functions, you might also want to store a lot of code that runs each function in turn, so you can easily remind yourself of what it does, or what the output looks like.
One way to keep this code around, but not have it run all the time when you run your script, is to put the code inside a "FALSE block," that might look like so:

```{r}
# My testing code ----
if ( FALSE ) {
  res <- my_function( 10, 20, 30 )
  res
  # Some notes as to what I want to see.
  
  sd( res )
  # This should be around 20
}
```

You can then, when you open and look at the script, paste the code inside the block into the console when you want to run it.
If you source the script, however, it will not run at all, and thus your code will source faster and not print out any extraneous output.
This is a good way to keep your testing code with the code it is testing.
When you want to work on just the part of the project captured by your script, you can work inside the single file very easily, ignoring the other parts of your project.

You can also (or instead) write testing code, which we will talk about further below.


## Saving simulation results {#saving-files}


Always save your simulation results to a file.
Simulations are painful and time consuming to run, and you will invariably want to analyze the results of them in a variety of different ways, once you have looked at your preliminary analysis.
We advocate saving your simulation as soon as it is complete.
But there are some ways to do better than that, such as saving as you go.
This can protect you if your simulation occasionally crashes, or if you want to rerun only parts of your simulation for some reason.


### Saving simulations in general

Once your simulation has completed, you can save it like so:
```{r, eval=FALSE}
dir.create("results", showWarnings = FALSE )
write_csv( res, "results/simulation_CRT.csv" )
```

`write_csv()` is a tidyverse file-writing command; see "R for Data Science"
textbook, 11.5.

You can then load it, just before analysis, as so:
```{r, eval=FALSE}
res = read_csv( "results/simulation_CRT.csv" )
```

There are two general tools for saving.  The `read/write_csv` methods save your file in a way where you can open it with a spreadsheet program and look at it.
But your results should be in a vanilla, rectangular format (non-fancy data frame without list columns).

Alternatively, you can use the `saveRDS()` and `readRDS()` methods; these save objects to a file such that when you load them, they are as you left them.
The RDS saving keeps your R object as given.
The simpler format of a csv file means your factors, if you have them, may not preserve as factors, and so forth.



### Saving simulations as you go

If you are not sure you have time to run your entire simulation, or you think your computer might crash half way through, or something similar, you can save each chunk you run as you go, in its own file.  You then stack those files at the end to get your final results.
With clever design, you can even then selectively delete files to rerun only parts of your larger simulation---but be sure to rerun everything from scratch before you run off and publish your results, to avoid embarrassing errors.

Here, for example, is a script from a research project examining how one might use post-stratification to improve the precision of an IV estimate.
This is the script that runs the simulation.
Note the sourcing of other scripts that have all the relevant functions; these are not important here.
Due to modular programming, we can see what this script does, even without those detail.

```{r IV_simulation_driver, eval=FALSE}
source( "R/simulation_functions.R" )

if ( !file.exists("results/frags" ) ) {
    dir.create("results/frags")
}

# Number of simulation replicates per scenario
R = 1000

# Do simulation breaking up R into this many chunks
M_CHUNK = 10

###### Set up the multifactor simulation #######

# chunkNo is a hack to make a bunch of smaller chunks for doing parallel more
# efficiently.
factors = expand_grid( chunkNo = 1:M_CHUNK,
                       N = c( 500, 1000, 2000 ),
                       pi_c = c( 0.05, 0.075, 0.10 ),
                       nt_shift = c( -1, 0, 1 ),
                       pred_comp = c( "yes", "no" ),
                       pred_Y = c( "yes", "no" ),
                       het_tx = c( "yes", "no" ),
                       sd0 = 1
                       )
factors <- factors %>% mutate(
    reps = R / M_CHUNK,
    seed = 16200320 + 1:n()
)
```

This generates a data frame of all our factor combinations.
This is our list of "tasks" (each row of factors).
These tasks have repeats: the "chunks" means we do a portion of each scenario, as specified by our simulation factors, as a process.
This would allow for greater parallelization (e.g., if we had more cores), and also lets us save our work without finishing an entire scenario of, in this case, 1000 iterations.

To set up our simulation we make a little helper method to do one row.
With each row, once we have run it, we save it to disk.
This means if we kill our simulation half-way through, most of the work would be saved.
Our function is then going to either do the simulation (and save the result to disk immediately), or, if it can find the file with the results from a previous run, load those results from disk:

```{r, eval=FALSE}
safe_run_sim = safely( run_sim )
file_saving_sim = function( chunkNo, seed, ... ) {
    fname = paste0( "results/frags/fragment_", chunkNo, "_", seed, ".rds" )
    res = NA
    if ( !file.exists(fname) ) {
        res = safe_run_sim( chunkNo=chunkNo, seed=seed, ... )
        saveRDS(res, file = fname )
    } else {
        res = readRDS( file=fname )
    }
    return( res )
}
```

Note how we wrap our core `run_sim` method (that takes all our simulation factors and runs a simulation for those factors) in `safely`; `run_sim()` was crashing very occasionally, and so to make the code more robust, we wrapped it so we could see any error messages.
Our method cleverly either loads a saved result, or generates it, for a given chunk.
This means from whatever is calling the function, it will look exactly the same whether it is loading a saved result or generating a new one.

We next run the simulation by calling `file_saving_sim()` for all of our simulation scenarios.

```{r demo_run_on_fly_code, eval=FALSE}
# Shuffle the rows so we run in random order to load balance.
factors = sample_n(factors, nrow(factors) )

if ( TRUE ) {
    # Run in parallel
    parallel::detectCores()
    
    library(future)
    library(furrr)
    
    #plan(multiprocess) # choose an appropriate plan from future package
    #plan(multicore)
    plan(multisession, workers = parallel::detectCores() - 2 )
    
    factors$res <- future_pmap(factors, .f = file_saving_sim,
                          .options = furrr_options(seed = NULL),
                          .progress = TRUE )
    
} else {
  # Run not in parallel, used for debugging
  factors$res <- pmap(factors, .f = file_saving_sim )
}

tictoc::toc()
```

Note how we shuffle the rows of our task list so that which process gets what task is randomized.
If some tasks are much longer (e.g., due to larger sample size) then this will get balanced out across our processes.
See \@ref(parallel-processing) for more on parallel processing.

The `if-then` structure allows us to easily switch between parallel and nonparallel code.
This makes debugging easier: when running in parallel, stuff printed to the console does not show until the simulation is over.
Plus it would be all mixed up since multiple processes are working simultaneously.

The above overall structure allows the researcher to delete one of the "fragment" files from the disk, run the simulation code, and have it just do one tiny piece of the simulation.
This means the researcher can insert a `browser()` command somewhere inside the code, and debug the code, in the natural context of how the simulation is being run.

The seed setting ensures reproducibility.
Once we are done, we need to clean up our results:

```{r run_sim_demo_code, eval=FALSE}
sim_results <- 
    factors %>% 
    unnest(cols = res)

# Cut apart the results and error messages
sim_results$sr = rep( c("res","err"), nrow(sim_results)/2)
sim_results = pivot_wider( sim_results, names_from = sr, values_from = res )

saveRDS( sim_results, file="results/simulation_results.rds" )
```

Our final `simulation_results.rds` file will have all the results from our simulation, made by stacking all of the fragments of our simulation together.


### Dynamically making directories

If you are generating a lot of files, then you should put them somewhere.
But where?
It can be nice to dynamically generate a directory for your files on fly.
One way to do this is to write a function that will make any needed directory, if it doesn't exist, and then put your file in that spot.
For example, you might have your own version of `write_csv` as:

```{r my_write_csv_function}
my_write_csv <- function( data, path, file ) {
  
  if ( !dir.exists( here::here( path ) ) ) {
    dir.create( here::here( path ), recursive=TRUE ) 
  }
  write_csv( data, paste0( path, file ) )
}
```

This will look for a path (starting from your R Project, by taking advantage of the `here` package), and put your data file in that spot.
If the spot doesn't exist, it will make it for you.




### Loading and combining files of simulation results

Once your simulation files are all generated, the following code will stack them all into a giant set of results, assuming all the files are themselves data frames stored in RDS objects.
This function will try and stack all files found in a given directory; for it to work, you should ensure there are no other files stored there.


```{r load_all_sims_function, eval=FALSE}
load.all.sims = function( filehead="raw_results/" ) {
  
  files = list.files( filehead, full.names=TRUE)
  
  res = map_df( files, function( fname ) {
    cat( "Reading results from ", fname, "\n" )
    rs = readRDS( file = fname )
    rs$filename = fname
    rs
  })
  res
}
```

You would use as so:
```{r use_load_all_sims, eval=FALSE}
results <- load.all.sims( filehead="raw_results/" )
results <- bind_rows( results )
```



