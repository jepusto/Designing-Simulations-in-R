---
output: html_document
editor_options: 
  chunk_output_type: console
---
```{r, include =FALSE}
library( purrr )
```

# (PART) Computational Considerations {-}

# Organizing a simulation project


Multi-factor simulations can easily grow into quite complex projects with many moving pieces. 
As a project grows in complexity, the steady accretion of functions, scripts, stored results, and analysis can become overwhelming---to the point that you would dread sharing these work products with anyone.
Fortunately, a bit of forethought and advance planning can make these types of projects much more tractable and manageable.

In the next several chapters, we describe some organizing principles and programming practices that will make it easier to handle complex simulation projects.
In this chapter, we discuss project organization, file management, and storing simulation results. 
In Chapter \@ref(parallel-processing), we demonstrate  parallel processing methods that will help you to speed up the computation involved in multifactor simulations. 
Finally, in Chapter \@ref(debugging-and-testing), we introduce some basic debugging and testing techniques that we have found useful for keeping on top of complex code bases.

## Simulation project structure 

In a very simple simulation that does not take long to execute, you might keep all of the code, output, and analysis in a single file.
However, if any of the computations take substantial time to execute, keeping everything in one file will become unwieldy.
For more complex or more computationally demanding simulations, there are big benefits to organizing your code across more than one file.

Multifactor simulation studies usually involve three distinct phases of work, each involving a different set of processes and different types of code.
The first phase involves developing functions for generating data, applying estimators or data analysis procedures to the data, calculating performance measures, and running the full simulation for a single scenario.
The end-product of this phase is a set of functions or methods for executing a simulation.
The second phase involves running the simulations across multiple scenarios, as discussed in Chapter \@ref(simulating-multiple-scenarios).
The end-product of this phase is a set of results, containing estimates of performance measures for one or more methods under each condition.
The third phase involves analyzing the simulation results and drawing conclusions about the performance of the method or methods under evaluation.
The end-product of this phase is often a set of figures, tables, and text (perhaps taking the form of a memo, blog post, slide deck, or manuscript) that summarizes what you find.

To stay organized when conducting a larger, multifactor simulation, we find it generally useful to keep a clear separation between phases. 
In practice, this means keeping the code for each phase in its own, separate file and storing the results from each phase so that it can be re-run or revised without re-executing the entire set of calculations. 
This approach is in keeping with a general principle for organizing large computational projects: keep the code for distinct tasks in different files. 

In this chapter, we describe strategies for organizing the code and work products involved in a multifactor simulation study. 
We start by offering guidance and recommendations for how to structure individual files and make use of code that is organized across more than one file.
We then describe a directory structure that encourages a clear separation between phases of a project.
Finally, we discuss strategies for storing and organizing results when executing a simulation.
The advice and recommendations we offer are drawn from our own experience working on large simulation projects (including the many mistakes we have made and lessons we have learned through trial-and-error).
Of course, we readily acknowledge that the approach we describe here is not the only way to do things, and we make no claims of optimality.
As you gain experience with your own work, you will develop your own systems, habits, and strategies.
We offer this guidance as a starting point from which to build.

## Well structured code files

Conducting a simulation will require writing, testing, and revising code (potentially quite a bit of code!).
There are two main types of files that analysts use to hold R code; plain `.R` scripts and R markdown (`.Rmd`) or Quarto (`.qmd`) notebooks.
The former can only contain R syntax and comments, whereas the latter can hold a mix of R code chunks and written text.
The latter work as the source code for creating reproducible reports. 
Compiling a notebook will automatically run the embedded code and interweave the results with the text, producing a formatted document that can display figures, tables, and other forms of output.

For very small projects, it may be possible to store your entire code base in a single notebook, containing all of your functions, code for creating a simulation design and executing the simulations, and code for analyzing the results.
An advantage of using a notebook here is that you can add plain text explanations and descriptions of the component pieces mixed in with the actual source code, so that rationales and design decisions are fully documented.
Further, you can re-run the entire simulation at the click of a button, simply by re-compiling the the notebook.
However, it is difficult to work this way if the simulation involves more intensive computation.

For larger or more computationally demanding projects, we find it useful to make more use of the first type of file, the humble `.R` script.
Ideally, you should follow the principles of modular programming for the code in these files.
Each `.R` file should hold a collection of code that is related to a single task.
Accordingly, we distinguish between two types of of `.R` scripts: those that _just_ have functions that can be used for other purposes and those that carry out numerical calculations.
The former type only contain functions, so that the only consequence of running them from top to bottom will be to load some new functions into your workspace.
The latter type are traditional scripts that do calculations, store or load data files, and create graphs and other summaries of data.

### Putting headers in your .R file

When you write an `.R` script, it is a good idea to put a header at the top of the file that describes the file's purpose.
Within the file, you can also describe the contents with shorter section headers.
For instance, a very simple section header might look like this:

```
#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#
# Data generating functions ----
#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#
```

Note the `----` at the end of the middle line.
Using four trailing dashes trailing dashes indicates a section of the code that RStudio's parser can interpret.
If you click on the dropdown at the bottom of the RStudio source pane, you will see a pop-up table of contents that allows you to quickly navigate to different parts of your source file.
You can also see the same table of contents by clicking the `Outline` button in the top right corner of the source pane.

### The source command {#about-source-command}

If your code is organized across more than one file, you will need a way to call one file from another. 
This is the purpose of the `source()` command, which effectively "cuts and pastes" the contents of the given file into your R work session.
For example, the following code runs three separate R scripts in turn:

```{r demo_source_multiple, eval=FALSE}
source( here::here( "R/data_generators.R" ) )
source( here::here( "R/estimators.R" ) )
source( here::here( "R/simulation_support.R" ) )
```

If the named file has code to run, it will run it.
If the named file has a list of methods, those methods will now be available for use.
The `here::here()` command is a convenience function that allows you to specify a file path relative to your R project root directory, so you can easily find your files.[^working-directory-weirdness]

[^working-directory-weirdness]: If you are used to using RStudio projects, you might wonder why you should use `here::here()`. If you always open your project from the `.Rproj` file, then the working directory will always start out set to the root directory, but it will not necessarily remain so (if a script calls `setwd()`, for instance). Further, when `.Rmd` or `.qmd` notebooks are compiled, the working directory is temporarily changed to the location of the notebook, so relative file references will work differently than in plain `.R` scripts. Wrapping file names in `here::here()` ensures that they will always be evaluated relative to the root directory of the project.

The `source()` command is simple and literal---it runs a specified script from top to bottom, regardless of the contents. 
In particular, if the sourced script includes further calls of `source()`, the referenced code will be run. 
For example, the `simulation_support.R` script could include calls to source the other two files.
In this case, you would only need to source the single `simulation_support.R` file to load the contents of all three files. 
Although this might seem convenient, it does create some constraints as well.
With this structure, there would be no way to source the contents of `simulation_support.R` without also sourcing `data_generators.R` and `estimators.R`.
For this reason, we tend to avoid using `source()` in scripts that might themselves be sourced.


One reason for storing code in individual files and using `source()` is that you can then include testing code in each of your files to both demonstrate the syntax and check the correctness of your functions.
Then, when you are not focused on that particular function or component, you don't have to look at that testing code.
Another good reason for this type of modular organization is that is supports developing a broader simulation universe, potentially with a variety of data generating functions.
With a whole library of options, you can then readily run multiple simulations that involve some different components and some shared features.

We followed this approach in one recent simulation project examining estimators for an instrumental variable analysis, a common approach to handling non-compliance in randomized experiments. 
In this project, we wrote several different data generating functions that implemented different types of non-compliance patterns.
Our `data_generators.R` code file then had several methods.
When we sourced it, we end up wit the following methods available to call:

```
> ls()
[1] "describe_sim_data"  "make_dat"           "make.dat_1side"     
[4] "make_dat_1side_old" "make_dat_orig"      "make_dat_simple"
[7] "make_dat_tuned"     "rand_exp"           "summarize_sim_data"
```

The `describe()` and `summarize()` methods printed various statistics about a sample dataset; we used used these to examine generated datasets and debug the data generating functions.
We also had a variety of different data generating methods, which we developed over the course of the project as we were trying to chase down errors in our estimators and understand strange behavior.

For this project, we also stored estimation functions in a different file.
Doing so made it easier to use these functions for purposes other than running simulations.
We also did some empirical data analysis as part of the project, and so we could simply source the file with our estimation functions and apply them to a real dataset. 
This ensured that our simulation and applied analysis were exactly aligned in terms of the estimators we were using.
As we debugged and tweaked our estimators, we could immediately re-run our applied analysis to update the results, without worrying about whether the calculations were consistent with the simulation code.

### Storing testing code {#storing-testing-code}

If you have an extended `.R` file with a list of functions, you might also want to store a lot of code that runs each function in turn, so you can easily remind yourself of what it does, or what the output looks like.
One way to keep this code around, but not have it run all the time when you run your script, is to put the code inside a "FALSE block," that might look like so:

```{r}
# My testing code ----
if ( FALSE ) {
  res <- my_function( 10, 20, 30 )
  res
  # Some notes as to what I want to see.
  
  sd( res )
  # This should be around 20
}
```

When you open and look at this script, you can then paste the code inside the block into the console when you want to run it.
However, if you `source()` the script, the `FALSE` block will not run at all, so you can avoid extraneous output or computations that are not actually needed.
This is a good way to keep simple demo code or testing code in close proximity to the function it is testing.
When you want to work on just the part of the project captured by your script, you can work inside the single file very easily, ignoring the other parts of your project.

In previous chapters, we have emphasized the importance of writing code to validate that your functions work as intended. 
Sometimes, such validation code will involve more than just a quick call to the function.
In such cases, it can be useful to organize validation code in its own file, separate from the function or functions that it tests.
You would then need to `source()` the files containing the function to be tested.
This approach does have the drawback that the testing code is not in close proximity to the function to be tested, so you need to work across multiple files when developing and running the test code. 
However, unlike with the `FALSE` blocks, organizing your test code in a separate file makes it easier to re-run a collection tests.
It becomes more appealing when the validation code involves calls to functions that are themselves stored in separate files or when it involves invoking packages that are not needed for the main calculations involved in the simulation. 
This approach is also closer to how you might work when developing code for an R package, with unit tests that check the correctness of critical functions.[^unit-testing] 

[^unit-testing]: For more on unit testings, see Chapters 13 through 15 of @Wickham2023packages.

## Principled directory structures

_Tidy home, tidy mind_, as the saying goes. 
With any computational project, our home is usually a directory on our computer (or in a cloud).
For multifactor simulation studies, we _strongly_ advocate using an organized and clearly labelled directory structure, which will facilitate more intentional---and easier to follow---coding practices.
We recommend using an integrated development environment such as RStudio, Positron, or VS Code and building a directory structure as follows:

```
my_project/
  proj.Rproj
  README.md
  R/
  test/
  data/
  scripts/
  results/
```

If you have ever looked at the source code for or developed your own R package, this structure will look familiar.

* The `R/` directory is where to put the core R code for your simulation.
  It should contain one or more `.R` scripts that hold the main functions for implementing a tidy simulation, including data-generating functions, analysis functions, performance calculation functions, and a simulation driver that pulls all the pieces together. 
  With all of these functions saved in the same directory, you can then `source()` the scripts as needed to gain access to your functions.
  You _should not_ put the scripts that actually execute the simulation or analyze the results in this folder.
  Instead, the `R/` folder is reserved for the building blocks of the simulation.

* The `test/` directory is where to put any code for testing the functions and methods you have developed.
  You could even write formal unit tests for your methods using a testing framework such as the `testthat` package, and put those in the `test/` directory.

* The `data/` directory is where to put any data files used in executing the simulation.
  For instance, a simulation of a cluster-randomized experiment might make use of an empirical dataset containing features such as cluster sizes or covariate values, which are sampled when generating artificial datasets.
  This directory is reserved for _source_ data, which may be read in as part of the simulation.
  It should not contain any files that are _created_ as part of executing the simulation.

* The `scripts/` directory is where to put scripts that actually run the simulations and analyze simulation results.
  Some of these scripts will make use of functions saved in the `R/` directory.
  You will likely have at least one script that runs the simulation and at least one script for analyzing the results.

* The `results/` directory is where to save any generated results of your simulation.
  This directory should _only_ contain files that have been created by running code in the `scripts/` directory.
  Sometimes it might be worth having separate directories `raw_results` and `results`, where `raw_results` holds output created from running the simulation driver, and `results` holds final results produced from merging and summarizing the raw results.
<!-- JEP: This last sentence could be about directories or it could be applied to files within a directory. I've edited to make it about directories---what did you have in mind? Any preference? -->

Using this structure will help you to maintain reproducibility of the full project.
If any files in `data/`, `R/`, or `scripts/` are changed, then some or all of the files in `results/` may need to be updated.
In principle, it should be possible to delete all of the files in `results/` and recreate them in full by re-running the files in the `scripts/` directory.


## Saving simulation results {#saving-files}

Multifactor simulations can be error-prone and time-consuming to run, and analyzing raw simulation results often involves several rounds of iteration and refinement.
Because of this, it pays to be cautious and save raw simulation results, so that they can be accessed and analyzed without having to re-run the code that generated them.
As a bare minimum for any multifactor simulation, we recommend saving the complete set of results to a file. 
In many instances, it may make sense to go further by saving results for each unique condition as soon as it is complete. 
This approach provides an additional level of protection in the event that you need to re-run parts of the simulation or if the code crashes out in rare circumstances or for only a subset of the conditions.

### File formats

An initial consideration is what format to use when saving simulation results. 
Two main candidates are the generic `.csv` format or the R-specific `.rds` format.

The first option involves saving a dataset as a comma-separated value file.
For instance, after your simulation has completed, you could save it in the file `results/simulation_CRT.csv")` like so:
```{r, eval=FALSE}
dir.create("results", showWarnings = FALSE)
write_csv( res, "results/simulation_CRT.csv" )
```
The `write_csv()` function comes from the `readr` package, which is part of the `tidyverse` suite.
It takes a dataset and a file name as input and creates a comma-separated value file containing the contents of the dataset.[^write-csv]
Once stored, you can load the dataset back into R for analysis using the `read_csv()` function:
```{r, eval=FALSE}
res <- read_csv( "results/simulation_CRT.csv" )
```
A key advantage of storing results as `.csv` is that it is a general format, so the file can be read by others even if they are not familiar with R.
The format also makes for convenient viewing in a spreadsheet program.
However, storing results in this format will only work for plain, rectangular datasets with no special features (such as list-columns).
Further, saving a dataset as `.csv` and then reading it back into R results in a loss of meta-data, such as variable labels and column data-types.[^factor-to-char]

[^write-csv]: See Section 11.5 of the _R for Data Science_ textbook [@Wickham2023data] for further details.

[^factor-to-char]: For example, a variable that was initially created with `factor()` will be converted into a plain character vector, and the ordering of the factor levels will be lost.

Alternatively, you can use the `saveRDS()` and `readRDS()` methods; these save objects to a file such that when you load them, they are as you left them.
The RDS saving keeps your R object as given.



### Saving simulations as you go

If you are not sure you have time to run your entire simulation, or you think your computer might crash half way through, or something similar, you can save each chunk you run as you go, in its own file.  You then stack those files at the end to get your final results.
With clever design, you can even then selectively delete files to rerun only parts of your larger simulation---but be sure to rerun everything from scratch before you run off and publish your results, to avoid embarrassing errors.

Here, for example, is a script from a research project examining how one might use post-stratification to improve the precision of an IV estimate.
This is the script that runs the simulation.
Note the sourcing of other scripts that have all the relevant functions; these are not important here.
Due to modular programming, we can see what this script does, even without those detail.

```{r IV_simulation_driver, eval=FALSE}
source( "R/simulation_functions.R" )

if ( !file.exists("results/frags" ) ) {
    dir.create("results/frags")
}

# Number of simulation replicates per scenario
R = 1000

# Do simulation breaking up R into this many chunks
M_CHUNK = 10

###### Set up the multifactor simulation #######

# chunkNo is a hack to make a bunch of smaller chunks for doing parallel more
# efficiently.
factors = expand_grid( chunkNo = 1:M_CHUNK,
                       N = c( 500, 1000, 2000 ),
                       pi_c = c( 0.05, 0.075, 0.10 ),
                       nt_shift = c( -1, 0, 1 ),
                       pred_comp = c( "yes", "no" ),
                       pred_Y = c( "yes", "no" ),
                       het_tx = c( "yes", "no" ),
                       sd0 = 1
                       )
factors <- factors %>% mutate(
    reps = R / M_CHUNK,
    seed = 16200320 + 1:n()
)
```

This generates a data frame of all our factor combinations.
This is our list of "tasks" (each row of factors).
These tasks have repeats: the "chunks" means we do a portion of each scenario, as specified by our simulation factors, as a process.
This would allow for greater parallelization (e.g., if we had more cores), and also lets us save our work without finishing an entire scenario of, in this case, 1000 iterations.

To set up our simulation we make a little helper method to do one row.
With each row, once we have run it, we save it to disk.
This means if we kill our simulation half-way through, most of the work would be saved.
Our function is then going to either do the simulation (and save the result to disk immediately), or, if it can find the file with the results from a previous run, load those results from disk:

```{r, eval=FALSE}
safe_run_sim = safely( run_sim )
file_saving_sim = function( chunkNo, seed, ... ) {
    fname = paste0( "results/frags/fragment_", chunkNo, "_", seed, ".rds" )
    res = NA
    if ( !file.exists(fname) ) {
        res = safe_run_sim( chunkNo=chunkNo, seed=seed, ... )
        saveRDS(res, file = fname )
    } else {
        res = readRDS( file=fname )
    }
    return( res )
}
```

Note how we wrap our core `run_sim` method (that takes all our simulation factors and runs a simulation for those factors) in `safely`; `run_sim()` was crashing very occasionally, and so to make the code more robust, we wrapped it so we could see any error messages.
Our method cleverly either loads a saved result, or generates it, for a given chunk.
This means from whatever is calling the function, it will look exactly the same whether it is loading a saved result or generating a new one.

We next run the simulation by calling `file_saving_sim()` for all of our simulation scenarios.

```{r demo_run_on_fly_code, eval=FALSE}
# Shuffle the rows so we run in random order to load balance.
factors = sample_n(factors, nrow(factors) )

if ( TRUE ) {
    # Run in parallel
    parallel::detectCores()
    
    library(future)
    library(furrr)
    
    #plan(multiprocess) # choose an appropriate plan from future package
    #plan(multicore)
    plan(multisession, workers = parallel::detectCores() - 2 )
    
    factors$res <- future_pmap(factors, .f = file_saving_sim,
                          .options = furrr_options(seed = NULL),
                          .progress = TRUE )
    
} else {
  # Run not in parallel, used for debugging
  factors$res <- pmap(factors, .f = file_saving_sim )
}

tictoc::toc()
```

Note how we shuffle the rows of our task list so that which process gets what task is randomized.
If some tasks are much longer (e.g., due to larger sample size) then this will get balanced out across our processes.
See \@ref(parallel-processing) for more on parallel processing.

The `if-then` structure allows us to easily switch between parallel and nonparallel code.
This makes debugging easier: when running in parallel, stuff printed to the console does not show until the simulation is over.
Plus it would be all mixed up since multiple processes are working simultaneously.

The above overall structure allows the researcher to delete one of the "fragment" files from the disk, run the simulation code, and have it just do one tiny piece of the simulation.
This means the researcher can insert a `browser()` command somewhere inside the code, and debug the code, in the natural context of how the simulation is being run.

The seed setting ensures reproducibility.
Once we are done, we need to clean up our results:

```{r run_sim_demo_code, eval=FALSE}
sim_results <- 
    factors %>% 
    unnest(cols = res)

# Cut apart the results and error messages
sim_results$sr = rep( c("res","err"), nrow(sim_results)/2)
sim_results = pivot_wider( sim_results, names_from = sr, values_from = res )

saveRDS( sim_results, file="results/simulation_results.rds" )
```

Our final `simulation_results.rds` file will have all the results from our simulation, made by stacking all of the fragments of our simulation together.


### Loading and combining files of simulation results

Once your simulation files are all generated, the following code will stack them all into a giant set of results, assuming all the files are themselves data frames stored in RDS objects.
This function will try and stack all files found in a given directory; for it to work, you should ensure there are no other files stored there.


```{r load_all_sims_function, eval=FALSE}
load.all.sims = function( filehead="raw_results/" ) {
  
  files = list.files( filehead, full.names=TRUE)
  
  res = map_df( files, function( fname ) {
    cat( "Reading results from ", fname, "\n" )
    rs = readRDS( file = fname )
    rs$filename = fname
    rs
  })
  res
}
```

You would use as so:
```{r use_load_all_sims, eval=FALSE}
results <- load.all.sims( filehead="raw_results/" )
results <- bind_rows( results )
```



