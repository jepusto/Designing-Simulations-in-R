# Case study: A simulation with clustered data

Generating data with complex structure can be intimidating, but if you set out a recipe for how the data is generated it is often not to bad to build that recipe up with code.
We will illustrate how to tackle this kind of data with a case study of best practices for analyzing data from a cluster-randomized RCT of students nested in schools.

A lot of the current literature on multisite trials is exploring how variation in the size of impacts across sites can cause bad things can happen.  What does it mean for this particular context?

There are various ways of analyzing such data:

 * Individual Level Analysis
    - Multilevel modeling (MLM): Fit a multilevel model to account for dependencies within cluster.
    - Linear regression (LR): Fit a linear model and use cluster robust standard errors.
 
 * Aggregation
    - Aggregation (Agg): Calculate average outcomes for each cluster and fit a linear model with heteroskedastic robust SEs
  
We might then ask, are any of these strategies biased?  When and how much?
Are any of these strategies more precise (have smaller SEs)?
Are the standard errors for these different strategies valid?
We might think aggregation should be worse since we are losing information, right?
If so, how much is lost?

To make this investigation a bit more rich, we are also going to ask a final question that will influence our data generating process.
We want to investigate what happens when the impact of a site depends on the site size.
This is a common question that has gained some attention in the education world, where we might reasonably think sites of different sizes may respond to treatment differently.
We want to know if our studied methods would end up giving us biased results, should there be such a relationship.


## A design decision: What do we want to manipulate?
Some concerns:
We figure if all the sites are the same size, we are probably safe. But if sites vary...
Also, if site size varies, but has nothing to do with impact, then we are probably good, but if it is associated...

Conclusion:
We need to consider both all-same-size sites and variable size sites.
Our DGP probably should have some impact variation across sites.
We should probably connect impact variation to site size to explore a more malicious context.
For simplicity we will simply include a site size by treatment interaction term to get our heterogeneity.


## A mathematical model for cluster-randomized data

The easiest way to write down a recipe for data generation is with a mathematical model.
This is especially important for more complex DGPs, such as those for hierarchical data.

We know we want a collection of clusters with different sizes and different baseline mean outcomes.
To keep things simple, we might want a common treatment shift within cluster: if we treat a cluster, everyone is raised by some specified amount.

In our case, we could end up with this model to describe this sort of data:
$$
\begin{aligned}
Y_{ij} &= \beta_{0j} + \epsilon_{ij} \\
\epsilon_{ij} &\sim N( 0, \sigma^2_\epsilon ) \\
\beta_{0j} &= \gamma_{0} + \gamma_{1} Z_j + \gamma_2 Z_j \left(\frac{n_j - \bar{n}}{\bar{n}} \right)  + u_j \\
u_j &\sim N( 0, \sigma^2_u )
\end{aligned}
$$
Our parameters are the mean outcome of control unit ($\gamma_0$), the treatment Impact ($\gamma_1$), the amount of cross site variation ($\sigma^2_u$), and residual variation ($\sigma^2_\epsilon$).
Our $\gamma_2$ is our site-size by treatment interaction term: bigger sites will (assuming $\gamma_2$ is positive) have larger treatment impacts.

If you prefer the reduced form, it would be:

$$ Y_{ij} = \gamma_{0} + \gamma_{1} Z_j + \gamma_2 Z_j \left(\frac{n_j - \bar{n}}{\bar{n}} \right)  + u_j + \epsilon_{ij}  $$


To generate data, we would also need several other quantities specified.
First, we need to know the number of clusters ($J$), the sizes of the clusters ($n_j$, for $j = 1, \ldots, J$).
We have to provide a recipe for generating these sizes.  We might try

$$ n_j \sim unif( (1-\alpha)\bar{n}, (1+\alpha)\bar{n} ) $$
with a fixed $\alpha$ to control the amount of variation in cluster size.
If $\bar{n} = 100$ and $\alpha = 0.25$ then we would, for example, have sites ranging from 75 to 125 in size.

Given how we are generating site size, look again at our treatment impact heterogeneity term:

$$ \gamma_2 Z_j \left(\frac{n_j - \bar{n}}{\bar{n}}\right)  $$ 
Note how we are standardizing by average site size to make our covariate not change in terms of its importance as a function of site size, but rather as a function of $\alpha$.
In particular, $\frac{n_j - \bar{n}}{\bar{n}}$ will range from $-\alpha$ to $\alpha$, regardless of average site size.
Carefully setting up a DGP so the "knobs" we use are standardized like this can make interpreting the simulation results much easier.
Consider if we did not divide by $\bar{n}$: then larger sites would also have more severe heterogeniety in treatment impact; this could make interpreting the results very confusing.


We next need to define how we generate our treatment indicator, $Z_j$.
We might specify some proportion $p$ assigned to treatment, and set $Z_j = 1$ or $Z_j = 0$ using a simple random sampling approach on our $J$ units.



## Multilevel data generation is a recipe using a statistical model

Now let's translate our mathematical model to code.
In the real world:
 - We obtain data, we pick a model, we estimate parameters
 - The data comes with covariates and outcomes
 - It also comes with sample size, sizes of the clusters, etc.

In the simulation world, by comparison:
 - We pick a model, we decide how much data, we generate covariates, we pick the parameters, and then we generate outcomes 
 - We need to decide how many clusters, how big the clusters are, etc.
 - We have to specify how the covariates are made.  This piece is very different from real-world analysis.


Step 1: Generate your sites
 - Generate site-level covariates
 - Generate sample size within each site
 - Generate site level random effects

Step 2: Generate your students inside the sites
 - Generate student covariates
 - Generate student residuals
 - Add everything up to generate student outcomes

The mathematical model gives us exactly the details we need to execute on these steps.
In particular, we can translate the math directly to R code, and then finally put it all in a function.

In general, we have several components to our model:

*COVARIATES/INDEPENDENT VARIABLES, and STRUCTURAL COVARIATES*
Covariates are the things that we are usually given when analyzing real data.
This is a broad definition, including things beside baseline information:

 - Conventional: student demographics, school-level characteristics, treatment assignment
 - Structural: number of observations in each school, proportion treated in each school

We don’t often think of these things as “covariates” but in a simulation we have to get them from somewhere.

*MODEL*
This is the parametric relationship between everything: how the outcomes are linked to the covariates.
This includes specification of any additional randomness (residuals, etc.)

*DESIGN PARAMETERS*
These are, e.g., the number of sites or variation in site size.
These control how we generate the structural covariates.
In the real world, we don't tend to think of these things are covariates per se, they are more just consequences of the data.
We rarely model them, but instead coniditon on them, in a statistical analysis.

*PARAMETERS*
These are the specifics: for a given model, parameters describe degree of variability, what the slope is, and so forth.
We usually estimate these FROM data.
Critically, if we know them, we can GENERATE NEW DATA.



### Generating the multisite data

We know we will need to generate and then analyze data.
First lets focus on the "generate" piece.
Borrowing from our DGP skeleton, we specify a function with all the parameters we might want to pass it, including defaults for each:

```{r, eval=FALSE}
gen_dat_model <- function( n_bar = 10,
                           J = 30,
                           p = 0.5,
                           gamma_0 = 0, gamma_1 = 0, gamma_2 = 0,
                           sigma2_u = 0, sigma2_e = 1,
                           site_var = 0 ) {
  # Code (see below) goes here.
}
```                           
                           
Note our parameters are a mix of *model parameters* (gamma_0, gamma_1, sigma2_e, etc., representing coefficients in regressions, variance terms, etc.) and *design parameters* (n_bar, J, p) that directly inform data generation.
We set default arguments (e.g., gamma_0=0) so we can ignore aspects of your DGP that we don’t care about later on.


*Sidebar on default arguments.*
To generate easy-to-use, but easy to configure code, use default arguments.
For example,

```{r}
my_function = function( a = 10, b = 20 ) {
     100 * a + b
}

my_function()
my_function( 5 )
my_function( b = 5 )
my_function( b = 5, a = 1 )
```

We can call it when we don't know what the arguments are, but then when we know more about the function, we can specify things of interest.
Lots of R commands work exactly this way, and for good reason.


*Make the sites.*
We make the sites first:

```{r, eval = FALSE}
  # generate site sizes 
  n_min = round( n_bar * (1 - site_var) )
  n_max = round( n_bar * (1 + site_var) )
  nj <- sample( n_min:n_max, J, replace=TRUE )

  # Generate average control outcome and average ATE for all sites
  # (The random effects)
  u0j = rnorm( J, mean=0, sd=sqrt( sigma2_u ) )
  
  # randomize units within each site (proportion p to treatment)
  Zj = ifelse( sample( 1:J ) <= J * p, 1, 0)
  
  # Calculate site intercept for each site
  beta_0j = gamma_0 + gamma_1 * Zj + gamma_2 * Zj * (nj-n_bar)/n_bar + u0j
```

Note the line with `sample(1:J) <= J*p`; this is a simple trick to generate treatment and control.


*Make the individuals.*
Then the individuals
```{r, eval=FALSE}
  # Make individual site membership
  sid = as.factor( rep( 1:J, nj ) )
  dd = data.frame( sid = sid )
  
  # Make individual level tx variables
  dd$Z = Zj[ dd$sid ]
  
  # Generate the residuals 
  N = sum( nj )
  e = rnorm( N, mean=0, sd=sqrt( sigma2_e ) )
  
  # Bundle and send out
  dd <- mutate( dd, 
                sid=as.factor(sid),
                Yobs = beta_0j[sid] + e, 
                Z = Zj[ sid ] )
```

The `rep` command will repeat each number, 1, 2... J, the corresponding number of times as listed in nj.

```{r, include=FALSE}
gen_dat_model <- function( n_bar = 10,
                           J = 30,
                           p = 0.5,
                           gamma_0 = 0, gamma_1 = 0, gamma_2 = 0,
                           sigma2_u = 0, sigma2_e = 1,
                           site_var = 0 ) {
  # generate site sizes 
  n_min = round( n_bar * (1 - site_var) )
  n_max = round( n_bar * (1 + site_var) )
  nj <- sample( n_min:n_max, J, replace=TRUE )

  # Generate average control outcome and average ATE for all sites
  # (The random effects)
  u0j = rnorm( J, mean=0, sd=sqrt( sigma2_u ) )
  
  # randomize units within each site (proportion p to treatment)
  Zj = ifelse( sample( 1:J ) <= J * p, 1, 0)
  
  # Calculate site intercept for each site
  beta_0j = gamma_0 + gamma_1 * Zj + gamma_2 * Zj * (nj-n_bar)/n_bar + u0j

    # Make individual site membership
  sid = as.factor( rep( 1:J, nj ) )
  dd = data.frame( sid = sid )
  
  # Make individual level tx variables
  dd$Z = Zj[ dd$sid ]
  
  # Generate the residuals 
  N = sum( nj )
  e = rnorm( N, mean=0, sd=sqrt( sigma2_e ) )
  
  # Bundle and send out
  dd <- mutate( dd, 
                sid=as.factor(sid),
                Yobs = beta_0j[sid] + e, 
                Z = Zj[ sid ] )
}
```


We wrap it all in a function, and when we call it we get:
```{r}
dat <- gen_dat_model( n=5, J=3, p=0.5, 
                        gamma_0=0, gamma_1=0.2, gamma_2=0.2,
                        sigma2_u = 0.4, sigma2_e = 1,
                      site_var = 0.5 )

dat
```

## Analyzing our data

To analyze our data, we use two libraries, the `lme4` package (for multilevel modeling), the `arm` package (which gives us nice access to standard errors, with `se.fixef()`), and `lmerTest` (which gives us $p$-values for multilevel modeling).
We also need the `estimatr` package to get robust SEs with `lm_robust`.

```{r, warning=FALSE, message=FALSE}
library( lme4 )
library( arm )
library( lmerTest )
library( estimatr )
```

We have three analysis functions, which we can put in three different methods:

Multilevel Regression (MLM):

```{r}
analysis_MLM <- function( dat ) {
  M1 = lmer( Yobs ~ 1 + Z + (1|sid),
             data=dat )
  est = fixef( M1 )[["Z"]]
  se = se.fixef( M1 )[["Z"]]
  pv = summary(M1)$coefficients["Z",5]
  tibble( ATE_hat = est, SE_hat = se, p_value = pv )
}
```


Linear Regression with Cluster-Robust Standard Errors (LM):
```{r}
analysis_OLS <- function( dat ) {
  M2 <- lm_robust( Yobs ~ 1 + Z, 
            data=dat, clusters=sid )
  est <- M2$coefficients[["Z"]]
  se  <- M2$std.error[["Z"]]
  pv <- M2$p.value[["Z"]]
  tibble( ATE_hat = est, SE_hat = se, p_value = pv )
}
```


Aggregate data (Agg):
```{r}
analysis_agg <- function( dat ) {
  datagg <- 
    dat %>% 
    group_by( sid, Z ) %>%
    summarise( 
      Ybar = mean( Yobs ),
      n = n() 
    )
  
  stopifnot( nrow( datagg ) == length(unique(dat$sid) ) )
  
  M3 <- lm_robust( Ybar ~ 1 + Z, 
                   data=datagg, se_type = "HC2" )
  est <- M3$coefficients[["Z"]]
  se <- M3$std.error[["Z"]]
  pv <- M3$p.value[["Z"]]
  tibble( ATE_hat = est, SE_hat = se, p_value = pv )
}
```

And then a single function that puts all these together:
  
```{r}
analyze_data = function( dat ) {
  MLM = analysis_MLM( dat )
  LR = analysis_OLS( dat )
  Agg = analysis_agg( dat )
  
  bind_rows( MLM = MLM, LR = LR, Agg = Agg,
             .id = "method" )
}

```

When we pass a dataset to it, we get a nice table of results that we can evaluate:

```{r}
analyze_data( dat )
```
Each method for analysis is a single line.  We record estimated impact, estimated standard error, and a nominal p-value.  
Note how the `bind_rows()` method can take naming on the fly, and give us a column of `method`, which will be very useful to keep track of what estimated what.
We intentionally wrap up our results with a data frame to make later processing of data with the tidyverse package much easier.


## The simulation


So we have our two steps, and the next step of our simulation is to rerun them both a bunch of times.
Always start with a single scenario to figure out if your code is working and if your intuition is working.

```{r cluster_rand_sim, cache=TRUE, message=FALSE, warning=FALSE}
set.seed( 40404 )
tictoc::tic()  # Start the clock!

ATE <- 0.30
R <- 1000

runs <- 
  purrr::rerun( R, {
    dat <- gen_dat_model( n_bar = 200, J=30, 
                          gamma_1 = ATE, gamma_2 = 0.5,
                          sigma2_u = 0.20, sigma2_e = 0.80,
                          site_var = 0.75 )
    analyze_data(dat)  
  } ) %>%
  bind_rows( .id="runID" )

tictoc::toc()
```
We have the individual results of all our methods applied to each generated dataset.


## Analysis

For our single scenario, we can now evaluate how well the estimators did.
We know our ATE from our data generating process command.

### Are the estimators biased?

```{r cluster_bias}
runs %>% 
  group_by( method ) %>%
  summarise( 
    mean_ATE_hat = mean( ATE_hat ),
    bias = mean( ATE_hat - ATE ),
    SE_bias = sd( ATE_hat - ATE ) / sqrt(R)
  )
```


Agg and MLM seem to be biased.
There is no evidence of bias for LR.

### Which method has the smallest standard error?

```{r}
true_SE <- runs %>% 
  group_by( method ) %>%
  summarise( 
    SE = sd( ATE_hat )
  )
true_SE %>%
  mutate( per_SE = SE / SE[method=="LR"] )
```

The true Standard Error is simply how variable the point estimates are, i.e., the standard deviation of the point estimates.

The other methods appear to have SEs about 6% smaller than Linear Regression

### Which method has the smallest Root Mean Squared Error?



```{r}
runs %>% 
  group_by( method ) %>%
  summarise( 
    RMSE = sqrt( mean( (ATE_hat - ATE)^2 ) )
  )
```

RMSE is a way of taking both bias and variance into account, all at once. 

Here, LR's bias plus increased variability is giving it a higher RMSE

### Do the methods have correctly estimated standard errors?

To assess this, we can look at the average _estimated_ (squared) standard error and compare it to the true standard error.
Our standard errors are _inflated_ if they are systematically larger than they should be, across the simulation runs.
We can also look at how stable our standard error estimates are, by taking the standard deviation of our standard error estimates.

```{r}
runs %>%  group_by( method ) %>%
  summarise( 
    SE = sd( ATE_hat ),
    mean_SE_hat = sqrt( mean( SE_hat^2 ) ),
    infl = 100 * mean_SE_hat / SE,
    sd_SE_hat = sqrt( sd( SE_hat^2 ) ) )
```

All of the SEs appear to be a bit conservative on average.  (3 or 4 percentage points too big).
The stability of the SE-hats is also of interest.  I.e., are the estimates of the uncertainty generally good?

### Sense-making across our results

Linear Regression is estimating a person-weighted quantity, so is biased for the site-average ATE.
We might next ask, how much does bias change if we change the site-size by impact relationship?

Linear Regression has a higher standard error.  This raises further questions:

 - Is this a general finding?
 - If site size is even more variable, will we see Agg do worse and worse?
 - Are there contexts where linear regression will do better than the others?

The estimated SEs all appear to be good.
We might then ask, is this always the case?  Will the estimated SEs fall apart in different contexts?

But we only investigated a single scenario.
How do our findings generalize?  When are the different methods differently appropriate? 
To answer this, we need to extend to a multi factor simulation.

This will give rise to all sorts of concerns such as how to handle convergence issues in the modeling.
We also need to think about how to deal with nuisance factors and how to summarize complex simulations.
Finally, how do we choose appropriate factors and not become beholden to the parameters in our model?


## Recap
This case study illustrates how multisite simulation (or any simulation) can be constructed by using a model as a recipe for data generation.
We also continue our basic message that making functions to wrap parts of simulation substantially eases code construction.

And finally, we saw several classic measures for the performance of estimators:

 - Is it biased? (bias)
 - Is it precise? (standard error)
 - Does it predict well? (RMSE)
 - Can we estimate uncertainty well?  Is it valid?  (i.e., are our estimated SEs about right?)



## Exercises

1. What is the variance of the outcomes generated by our model if there is no treatment effect?  (Try simulating data to check!) What other quick checks can you make on your DGP to make sure it is working? 

2. Extend the data generating process to include individual level covariates?

3. As foreground to the following chapters, can you explore multiple scenarios to see if the trends are common?  First write a function that takes a set of parameters and runs the entire simulation and returns the results as a small dataframe.
Then use code like this to make a graph of some result measure as a function of a varying parameter (you pick which parameter you wish to vary):

```{r, eval=FALSE}

vals = seq( start, stop, length.out = 5 )
res = map_df( vals, my_simulation_function, 
              par1 = val1, par2 = val2, etc )
```




