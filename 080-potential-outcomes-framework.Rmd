
# Simulation under the Potential Outcomes Framework

```{r echo = FALSE, messages = FALSE, include=FALSE}
library(tidyverse)
knitr::opts_chunk$set(echo = TRUE, 
                      fig.width = 5,
                      fig.height = 3,
                      out.width = "75%", 
                      fig.align = "center")
options(list(dplyr.summarise.inform = FALSE))
theme_set( theme_classic() )

```


If we are in the business of evaluating how various methods such as matching or propensity score weighting work in practice, we would probably turn to the potential outcomes framework for our simulations.
The potential outcomes framework is a framework typically used in the causal inference literature to make very explicit statements regarding the mechanics of causality and the associated estimands one might target when estimating causal effects.
While we recommend reading, for a more thourough overview, either [CITE Raudenbush or Field Experiments textbook], we briefly outline them here.

Take a sample of experimental units, indexed by $i$.
For each unit, we can treat it or not.
Denote treatment as $Z_i = 1$ for treated or $Z_i = 0$ for not treated.
Now we imagine each unit has two potential outcomes being the outcome we would see if we treated it ($Y_i(1)$) or if we did not ($Y_i(0)$).
Finally, our observed outcome is then 
$$ Y_i^{obs} = Z_i Y_i(1) + (1-Z_i)Y_i(0) .$$
For a unit, the treatment effect is $\tau_i = Y_i(1) - Y_i(0)$; it is how much our outcome changes if we treat vs. not treat.
Frustratingly, for each unit we can only see one of its two potential outcomes, so we can never get an estimate of these individual $\tau_i$.
Under this view, causality is a missing data problem: if we only were able to impute the missing potential outcomes, we could have a dataset where we could calculate any estimands we wanted.  E.g., the true average treatment effect _for the sample_ $\mathcal{S}$ would be:

$$ ATE_{\mathcal{S}} = \frac{1}{N} \sum_{i} Y_i(1) - Y_i( 0 ) . $$

## Finite vs. Superpopulation inference

Consider a sample of $n$ units, $\mathcal{S}$, along with their set of potential outcomes.
We can talk about the true ATE of the sample, or, if we thought of the sample as being drawn from some larger population, we could talk about the true ATE of that larger population.

This is a tension that often arises in potential outcomes based simulations: if we are focused on $ATE_{\mathcal{S}}$ then for each sample we generate, our estimand could be (maybe only slightly) different.
If, on the other hand, we are generating all the units from full model, our estimand may well change.

The catch is when we calculate our performance metrics, we have two possible targets to pick from.
Furthermore, if we are targeting the superpopulation ATE, then the error in estimation may be due to the representativeness of the sample, _not_ the estimation or uncertainty due to the random assignment.

We will follow this theme throughout this chapter.


## Data generation processes for potential outcomes

If we want to write a simulation using the potential outcomes framework, it is clear and transparent to first generate a complete set of potential outcomes, then generate a random assignment based on some assignment mechanism, and finally generate the observed outcomes as a function of assignment and original potential outcomes.

For example, we might say that our data generation process is as follows: First generate each unit $i = 1, \ldots, n$, as
$$
\begin{aligned}
X_i &\sim exp( 1 ) - 1 \\
Y_i(0) &= \beta_0 + \beta_1 X_i + \epsilon_i \mbox{ with } \epsilon_i \sim N( 0, \sigma^2 ) \\
\tau_i &= \tau_0 + \tau_1 X_i + \alpha u_i \mbox{ with } u_i \sim t_{df} \\
Y_i(1) &= Y_i(0) + \tau_i 
\end{aligned}
$$
with $exp(1)$ being the standard exponential distribution and $t_{df}$ being a $t$ distribution with $df$ degrees of freedom.
We subtract 1 from $X_i$ to zero-center it (it is often convienent so we can then, e.g., interpret $\tau_0$ as the true ATE of our experiment).

The above model is saying that we first, for each unit, generate a covariate.
We then generate our two potential outcomes.  
I.e., we are generating what the outcome would be for each unit if it were treated and if it were not treated.
We are driving both the level and the treatment effect with $X_i$, assuming $\beta_1$ and $\tau_1$ are non-zero.

We would then generate the _observed outcomes_ by assigning our (synthetic, randomly generated) $n$ units to treatment or control.
For example, say we wanted to simulate an observational context where treatment was a function of our covariate.
We could model each unit as flipping a weighted coin with some probability that was a function of $X_i$ as so:

\begin{align*}
p_i &= logit^{-1}( \xi_0 + \xi_1 X_i ) \\
Z_i &= Bern( p_i ) \\
Y_i &= Z_i Y_i(1) + (1-Z_i) Y_i(0) 
\end{align*}

One advantage of generating all the potential outcomes is we can then calculate the finite-sample estimands such as the true average treatment effect for the generated sample: we just take the average of $Y_i(1) - Y_i(0)$ for our sample.

Here is some code to illustrate the first part of the data generating process (we leave treatment assignment to later):

```{r}
gen_data <- function( n = 100,
                      R2 = 0.5,
                      beta_0 = 0, beta_1 = 1,
                      tau_0 = 1, tau_1 = 1, 
                      alpha = 1, df = 3 ) {
  stopifnot( R2 >= 0 && R2 < 1 )
  X_i = rexp( n, rate = 1 ) - 1
  beta_1 = sqrt( 1 - R2 )
  sigma_e = sqrt( R2 )
  Y0_i = beta_0 + beta_1 * X_i + rnorm( n, sd=sigma_e )
  tau_i = tau_0 + tau_1 * X_i + alpha * rt( n, df = df )
  Y1_i = Y0_i + tau_i
  
  tibble( X = X_i, Y0 = Y0_i, Y1 = Y1_i )
}
```

```{r, include=FALSE, eval=FALSE}
#debugging
dd = gen_data( 100000 )
sd( dd$Y0 )
sd( dd$Y1 )
cor( dd$Y0, dd$Y1 )
dd = gen_data( 100000, tau_1 = -1 )
sd( dd$Y0 )
sd( dd$Y1 )
cor( dd$Y0, dd$Y1 )

```

And now we see our estimand can change:
```{r}
set.seed( 40454 )
d1 <- gen_data( 50 )
mean( d1$Y1 - d1$Y0 )
d2 <- gen_data( 50 )
mean( d2$Y1 - d2$Y0 )
```
In reviewing our code, we know our superpopulation ATE should be `tau`, or 1 exactly.
If our estimate for `d1` is `r round(mean( d1$Y1 - d1$Y0 ), digits=1)` do we say that is close or far from the target?
From a finite sample performance approach, we nailed it.
From superpopulation, less so.


Also in looking at the above, there are a few details to call out:

 * We can store the latent, intermediate quantities (both potential outcomes, in particular) so we can calculate the estimands of interest or learn about our data generating process. When we hand the data to an estimator, we would not provide this "secret" information.
 * We are using a trick to index our DGP by an R2 value rather than coefficients on X so we can have a standardized control-side outcome (the expected variation of $Y_i(0)$ will be 1).  The treatment outcomes will have more variation due to the heterogeniety of the treatment impacts.
 * If we were generating data with a constant treatment impact, then $ATE_{\mathcal{S}} = ATE$ always; this is typical for many similations in the literature. That being said, treatment variation is what causes a lot of methods to fail and so having simulations that have this variation is usually important.

And now for assigning our data to treatment and control:
```{r}
assign_data <- function( dat,
                         xi_0 = -1, xi_1 = 1 ) {
  p_i = arm::invlogit( xi_0 + xi_1 * X_i )
  Z_i = rbinom( n, 1, prob=p_i )
  dat = mutate( dat,
                Z = Z_i,
                Yobs = ifelse( Z == 1, Y1, Y0 ) )
  dat
}
```

Separating our our DGP and our random assignment underscores the potential outcomes framework adage of the data are what they are, and we the experimenters (or nature) is randomly assigning these whole units to various conditions and observing the consequences.



## Calibrated simulations and the potential outcomes framework

The potential outcomes framework provides a natural way of generating calibrated simulations.
In particular, one way forward with a calibrated simulation is to use an existing datasets to generate plausible scenarios.

The way we do this with potential outcomes is to take an existing randomized experiment or observational study and impute all the missing potential outcomes under some specific scheme.
This fully defines the sample of interest and thus any target parameters, such as a measure of heterogeneity, are then known.
We will then synthetically, and repeatedly, randomize and ``observe'' outcomes to be analyzed with the methods we are testing.
We could also resample from our dataset to generate datasets of different size, or to have a superpopulation target as our estimand.

The key feature is the imputation step.
One baseline method one can use is to generate a matched-pairs dataset by, for each unit, finding a close match given all the demographic and other covariate information of the sample.  We then use the matched unit as the imputed potential outcome.  
By doing this (with replacement) for all units we can generate a fully imputed dataset which we then use as our population.
This can preserve complex relationships in the data that are not model dependent.
In particular, if outcomes tend to be coarsely defined (e.g., on an integer scale) or have specific clumps (such as zero-inflation or rounding), this structure will be preserved.

One concern with this approach is the noise in the matching will in general dilute the structure of the treatment effect.  
This is akin to measurement error diluting found relationships in linear models.
We can then sharpen these relationships towards a given model by first imputing missing outcomes using a specified model, and then matching on all units including the imputed potential outcome.
This is not a data analysis strategy, but instead a method of generating synthetic data that both has a given structure of interest and also remains faithful to the idiosyncrasies of an actual dataset.

A second approach that allows for varying the level of a systematic effect is to specify the treatment effect model and impute treatment outcomes for all control units.  
Then the complex structure between covariates and $Y(0)$ would be perfectly preserved.
Unfortunately, this would give 0 idiosyncratic treatment variation (unit-to-unit variation in the $\tau_i$ that is not explained by a model).
To add in idiosyncratic variation we would then need to generate a distribution of perturbations and add these to the imputed outcomes just as an error term in a regression model.  

Regardless, once a fully observed sample has been obtained we can investigate several aspects of our estimators as above.



## Finite sample performance measures

Let's generate a single dataset with our DGP from above, and run a small experiment where we actually randomize units to treatment and control:

```{r}
n = 100
set.seed(442423)
dat = gen_data(n, tau_1 = -1)
dat = mutate( dat,
              Z = 0 + (sample( n ) <= n/2),
              Yobs = ifelse( Z == 1, Y1, Y0 ) )
mod = lm( Yobs ~ Z, data=dat )
coef(mod)[["Z"]]
```
We can compare this to the true finite-sample ATE:
```{r}
mean( dat$Y1 - dat$Y0 )
```

Our finite-population simulation would be:
```{r finite_pop_sim, cache=TRUE}
rps <- rerun( 1000, {
  dat = mutate( dat,
              Z = 0 + (sample( n ) <= n/2),
              Yobs = ifelse( Z == 1, Y1, Y0 ) )
  mod = lm( Yobs ~ Z, data=dat )
  tibble( ATE_hat = coef(mod)[["Z"]],
          SE_hat = arm::se.coef(mod)[["Z"]] )
  }) %>%
  bind_rows()

rps %>% summarise( EATE_hat = mean( ATE_hat ),
                   SE = sd( ATE_hat ),
                   ESE_hat = mean( SE_hat ) )
```
We are simulating on a single dataset.
In particular, our set of potential outcomes is entirely fixed; the only source of randomness (and thus the randomness behind our SE) is the random assignment.
Now this opens up some room for critique: what if our single dataset is non-standard?


Our super-population simulation would be, by contrast:
```{r super_pop_sim, cache=TRUE}
rps_sup <- rerun( 1000, {
  dat = gen_data(n)
  dat = mutate( dat,
              Z = 0 + (sample( n ) <= n/2),
              Yobs = ifelse( Z == 1, Y1, Y0 ) )
  mod = lm( Yobs ~ Z, data=dat )
  tibble( ATE_hat = coef(mod)[["Z"]],
          SE_hat = arm::se.coef(mod)[["Z"]] )
  }) %>%
  bind_rows()

rps_sup %>% summarise( EATE_hat = mean( ATE_hat ),
                   SE = sd( ATE_hat ),
                   ESE_hat = mean( SE_hat ))
```

First, note our superpopulation simulation is not biased for the superpopulation ATE.
Also note the true SE is larger than our finite-sample simulation; this is because part of the uncertainty in our estimator is the uncertainty of whether our sample is representative of the superpopulation.

Finally, this clarifies that our linear regression estimator is estimating standard errors assuming a superpopulation model.
The true finite sample standard error is less than the expected estimated error: from a finite sample perspective, our estimator is giving overly conservative uncertainty estimates.
(This discrepancy is often called the correlation of potential outcomes problem.)


## Nested finite simulation procedure

We just saw a difference between a specific, single, finite-sample dataset and a superpopulation.
What if we wanted to know if this phenomenon was more general across a set of datasets?
This question can be levied more broadly: if we run a simulation on a single dataset, this is even more narrow than running on a single scenario: if we compare methods and find one is superior to another for our single dataset, how do we know this is not an artifact of some specific characteristic of _that data_ and not a general phenomonen at all?

One way forward is to run a nested simulation, where we generate a series of finite sample datasets, and then for each dataset run a small simulation.
We then calculate the expected finite sample performance across the datasets.
One could almost think of the datasets themselves as a "factor" in our multifactor experiment.
This is what we did in [CITE estimands paper]


Borrowing from the simulation appendix of [CITE estimands paper], repeat $R$ times:

1. Generate a dataset using a particular DGP. This data generation is the "sampling step" for a superpopulation (SP) framework. The DGP represents an inﬁnite superpopulation. Each dataset includes, for each observation, the potential outcome under treatment or control.

2. Record the true finite-sample ATE, both person and site weighted. 
3. Then, three times, do a finite simulation as follows:

  a. Randomize units to treatment and control.
  b. Calculate the corresponding observed outcomes.
  c. Analyze the results using the methods of interest, recording both the point estimate and estimated standard error for each.

Having only three trials will give a poor estimate of within-dataset variability for each dataset, but the average across the
  $R$ datasets in a given scenario gives a reasonable estimate of expected variability across datasets of the type we would see given the scenario parameters.
  


To demonstrate we first make a mini-finite sample driver:

```{r}
one_finite_run <- function( R0 = 3, n = 100, ... ) {
  dat = gen_data( n = n, ... )
  rps <- rerun( R0, {
         dat = mutate( dat,
                    Z = 0 + (sample( n ) <= n/2),
                    Yobs = ifelse( Z == 1, Y1, Y0 ) )
        mod = lm( Yobs ~ Z, data=dat )
        tibble( ATE_hat = coef(mod)[["Z"]],
                SE_hat = arm::se.coef(mod)[["Z"]] )
    }) %>%
    bind_rows()
  rps$ATE = mean( dat$Y1 - dat$Y0 )
  rps
}
```

This driver also stores the finite sample ATE for future reference:
```{r}
one_finite_run()
```
We then run a bunch of finite runs.

```{r, cache=TRUE}
runs <- rerun( 500, one_finite_run() ) %>%
  bind_rows( .id = "runID" )
```

We use `.id` because we will need to separate out each finite run and analyze separately, and then aggregate.

Each finite run is a very noisy simulation for a fixed dataset.
This means when we calculate performance measures we have to be careful to avoid bias in the calculations; in particular, we need to focus on estimating $SE^2$ across the finite runs, not $SE$, to avoid th bias caused by having a few observations with every estimate.

```{r}
fruns <- runs %>% group_by( runID ) %>%
  summarise( EATE_hat = mean( ATE_hat ),
             SE2 = var( ATE_hat ),
             ESE_hat = mean( SE_hat ),
             .groups = "drop" )
```

And then we aggregate our finite sample runs:
```{r}
res <- fruns %>%
  summarise( EEATE_hat = mean( EATE_hat ),
             EESE_hat = sqrt( mean( ESE_hat^2 ) ),
             ESE = sqrt( mean( SE2 ) ) ) %>%
  mutate( calib = 100 * EESE_hat / ESE )

res
```
We see our expected standard error estimate is, across the collection of finite sample scenarios all sharing a similar parent superpopulation DGP, 15% too large for the true expected finite-sample standard error.

We need to keep the squaring.  If we look at the SEs themselves, we have further apparent bias due to our _estimated_ `ESE_hat` being so unstable due to too few observations:

```{r}
mean( sqrt( fruns$SE2 ) )
```

We can use our runs to estimate superpopulation quantities as well.
Given that the simulation datasets are i.i.d. draws, we can simply take the expectation across all our simulations.
The only concern is our estimates of MCSE will be off due to the clustering in our simulation runs.

```{r}
runs %>%
  summarise( EATE_hat = mean( ATE_hat ),
             SE_true = sd( ATE_hat ),
             SE_hat = mean( SE_hat ),
             SE2_true = var( ATE_hat ),
             SE2_hat = mean( SE_hat^2 ) ) %>%
  pivot_longer( cols = c(SE_true:SE2_hat ),
                names_to = c( "estimand", ".value" ),
                names_sep ="_" ) %>%
  mutate( inflate = 100 * hat / true )
```

