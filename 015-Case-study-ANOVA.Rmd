
# Case Study: Heteroskedastic ANOVA {#case_ANOVA}


```{r include = FALSE}
library(tidyverse)
```


To illustrate the process of programming a simulation, we reconstruct the simulations from Brown and Forsythe (1974).
We also use this case study as a reoccuring example in some of the following chapters.

Brown and Forsythe wanted to study methods for null hypothesis testing in the following model: Consider a population consisting of $g$ separate groups, with population means $\mu_1,...,\mu_g$ and population variances $\sigma_1^2,...,\sigma_g^2$ for some characteristic $X$.
We obtain samples of size $n_1,...,n_g$ from each of the groups, and take measurements of the characteristic for each sampled unit.
Let $x_{ij}$ denote the measurement from unit $i$ in group $j$, for $i = 1,...,n_j$ for each $j = 1,..., g$.
Our goal is to use the sample data to test the hypothesis that the population means are all equal, i.e.,
$$
H_0: \mu_1 = \mu_2 = \cdots = \mu_g.
$$
Now, if the population _variances_ were all equal (i.e., $\sigma_1^2 = \sigma_2^2 = \cdots = \sigma_g^2$), we could use a conventional one-way analysis of variance (ANOVA) to test.
However, one-way ANOVA might not work well if the variances are not equal.
The question is then what are best practices for testing, when in this heteroskedastic case.

To tackle this question, Brown and Forsythe evaluated two different hypothesis testing procedures, developed by James (1951) and Welch (1951), that had been proposed for testing this hypothesis without assuming equality of variances, along with the conventional one-way ANOVA F-test as a benchmark.
They also proposed and evaluated a new procedure of their own devising.
(This latter pieces makes this paper one of a canonical format for statistical methodology papers: find some problem that current procedures do not perfectly solve, invent something to do a better job, and then do simulation and/or math to build a case that the new procedure is better.)
Overall, the simulation involves comparing the performance of these different hypothesis testing procedures (the methods) under a range of conditions (different data generating processes).

For hypothesis testing, there are two main performance metrics of interest: type-I error rate and power.
The type-I error rate is, when the null hypothesis is true, how often a test falsely rejects the null.
It is a measure of how _valid_ a method is.
Power is how often a test correctly rejects the null when it is indeed false.
It is a measure of how _powerful_ (or sensitive) a method is.
The authors explored error rates and power for nominal $\alpha$-levels of 1%, 5%, and 10%.
Table 1 of their paper reports the simulation results for type-I error (labeled as "size"); ideally, a test should have true type-I error very close to the nominal $\alpha$.

They looked at ten different scenarios:

```{r, echo=FALSE}
table_1 <- tribble(
  ~Groups, ~`Sample Sizes`,           ~`Standard Deviations`,
  "4",     "4,4,4,4",                 "1,1,1,1",
  "4",     "4,8,10,12",               "1,2,2,3",
  "4",     "11,11,11,11",             "1,1,1,1",
  "4",     "11,16,16,21",             "1,2,2,3",
  "6",     "4,4,4,4,4,4",             "1,1,1,1,1,1",
  "6",     "4,6,6,8,10,12",           "1,2,2,3",
  "6",     "6,6,6,6,6,6",             "1,1,1,1,1,1",
  "6",     "11,11,11,11,11,11",       "1,1,1,1,1,1",
  "6",     "16,16,16,16,16,16",       "1,2,2,3",
  "6",     "21,21,21,21,21,21",       "1,2,2,3",
  "10",    "20,20,20,20,20,20,20,20,20,20", "1,1,1.5,1.5,2,2,2.5,2.5,3,3"
)
knitr::kable( table_1, caption = "Scenarios explored by Brown and Forsythe from their Table 1",
              label = "BF-scenarios" )
```

We also provide some of the numbers they reported in their Table 1 for these scenarios on Table @tab:BF-table1.

```{r, echo=FALSE}
library(tibble)

table_5_percent <- tribble(
  ~Groups, ~`F`, ~`F*`, ~`W`, ~`J`,
  "4",     4.9,     3.4,      4.5,     7.9,
  "4",     6.7,     4.1,      4.7,     8.1,
  "4",     5.1,     4.8,      5.7,     6.7,
  "4",     3.0,     5.7,      4.9,     5.6,
  "6",     14.4,    6.2,      6.5,     7.7,
  "6",     5.1,     5.7,      5.0,     5.5,
  "6",     6.3,     5.7,      5.0,     5.4,
  "6",     4.9,     5.1,      5.0,     5.3,
  "6",     10.8,    6.2,      5.5,     5.8,
  "6",     4.0,     6.5,      5.4,     5.6,
  "10",    4.9,     3.4,      6.1,     9.5
)
#nrow( table_5_percent )
table_1_percent <- tribble(
  ~Groups, ~`F`, ~`F*`, ~`W`, ~`J`,
  "4",     0.9,     0.5,      0.8,     2.4,
  "4",     1.7,     0.7,      0.8,     2.7,
  "4",     1.1,     1.0,      1.6,     2.1,
  "4",     0.6,     1.4,      0.9,     1.3,
  "6",     5.6,     1.8,      2.0,     2.9,
  "6",     1.0,     1.5,      0.9,     1.1,
  "6",     1.8,     1.5,      1.1,     1.3,
  "6",     1.1,     1.0,      1.0,     1.2,
  "6",     3.9,     1.8,      1.2,     1.3,
  "6",     1.0,     1.8,      1.1,     1.1,
  "10",    1.0,     0.4,      1.4,     3.8
)


table_10_percent <- tribble(
  ~Groups, ~`F`, ~`F*`, ~`W`, ~`J`,
  "4",     10.2,     7.8,       9.6,      13.3,
  "4",     12.0,     8.9,       10.3,     13.8,
  "4",     9.9,      9.5,       10.8,     12.1,
  "4",     5.9,      10.3,      9.8,      10.8,
  "6",     21.9,     11.0,      11.3,     12.9,
  "6",     10.1,     9.8,       10.0,     10.6,
  "6",     11.4,     10.7,      10.1,     10.6,
  "6",     10.3,     10.3,      10.2,     10.5,
  "6",     17.3,     11.1,      10.5,     10.9,
  "6",     7.3,      11.5,      10.6,     10.9,
  "10",    9.6,      7.3,       11.4,     14.7
)

table_1 = bind_rows( `10%` = table_10_percent,
                     `5%` = table_5_percent, 
                     `1%` = table_1_percent, .id = "level" )
table_1 <- mutate( table_1, 
                   group = rep( 1:11,3 ),
                   level = factor( level, levels=c("10%", "5%", "1%") ) ) %>%
  dplyr::select( -Groups, -J ) %>%
  relocate( group )
table_1 <- table_1 %>%
  pivot_wider( names_from = level, values_from = c( F, `F*`, W ) )

table_1 %>%
  knitr::kable( caption='Portion of "Table 1," reproduced from Brown and Forsythe',
                label = "BF-table1")
```

Table 2 reports results on power; it is desirable to have higher power to reject null hypotheses that are false, so higher rates are better here.

To replicate this simulation we are going to first write code to evaluate our three procedures in a specific scenario with a specific set of core parameters (e.g., sample sizes, number of groups, and so forth), and then scale up to do a range of scenarios where we vary these parameters.


## The data-generating model

In the heteroskedastic one-way ANOVA simulation, there are three sets of parameter values: population means, population variances, and sample sizes.
Rather than attempting to write a general data-generating function immediately, it is often easier to write code for a specific case first and then use that code as a launch point for the rest.
For example, say that we have four groups with means of 1, 2, 5, 6; variances of 3, 2, 5, 1; and sample sizes of 3, 6, 2, 4:

```{r}
mu <- c(1, 2, 5, 6)
sigma_sq <- c(3, 2, 5, 1)
sample_size <- c(3, 6, 2, 4)
```

Following Brown and Forsythe, we'll assume that the measurements are normally distributed within each sub-group of the population. The following code generates a vector of group id's and a vector of simulated measurements:

```{r}
N <- sum(sample_size) # total sample size
g <- length(sample_size) # number of groups

# group id
group <- rep(1:g, times = sample_size)

# mean for each unit of the sample
mu_long <- rep(mu, times = sample_size) 

# sd for each unit of the sample
sigma_long <- rep(sqrt(sigma_sq), times = sample_size) 

# See what we have?
tibble( group=group, mu=mu_long, sigma=sigma_long)

# Now make our data
x <- rnorm(N, mean = mu_long, sd = sigma_long)
tibble(group = group, x = x)
```

We have made a small dataset of group membership and outcome.
We note that there are many different and legitimate ways of doing this in R.
E.g., we could generate each group separately, and then stack our groups, instead of using `rep` to do it all at once.
In general, we advocate the adage that if you can do it at all, then you should feel good about yourself.
Do not worry about writing code the "best" way when you are initially putting a simulation together.

To continue, as we are going to generate data over and over, we wrap this code in a function.
We also make our means, variances and sample sizes be parameters of our function so we can make datasets of different sizes and shapes, like so:

```{r generate_data_block}
generate_data <- function(mu, sigma_sq, sample_size) {

  N <- sum(sample_size)
  g <- length(sample_size)

  group <- rep(1:g, times = sample_size)
  mu_long <- rep(mu, times = sample_size)
  sigma_long <- rep(sqrt(sigma_sq), times = sample_size)

  x <- rnorm(N, mean = mu_long, sd = sigma_long)
  sim_data <- tibble(group = group, x = x)

  return(sim_data)
}
```

The above code is simply the code we built previously, all bundled up.
Our workflow is to scrabble around to get it to work once, the way we want, and then bundle up our final work into a function for later reuse.

Given our method, we can easily call our function to get a new set of simulated data.
For example, to generate a dataset with the same parameters as before, we can do:

```{r}
sim_data <- generate_data(mu = mu, sigma_sq = sigma_sq, 
                          sample_size = sample_size)
```

To generate one with 0s for the averages of each group, but the same group variances and sample sizes as before, we can do:
```{r}
sim_data_null <- generate_data( mu = c( 0, 0, 0, 0 ),
                                sigma_sq = sigma_sq, 
                                sample_size = sample_size)
```

### Coding remark

In the above, we built some sample code, and then bundled it into a function by literally cutting and pasting the initial work we did into a function skeleton.
In the process, we shifted from having variables in our workspace with different names to using those variable names as parameters in our function call.

Developing code in this way is not without hazards.
In particular, after finishing making our function, our workspace has a variable `mu` in it and our function also has a parameter named `mu`.
Inside the function, R will use the parameter `mu` first, but this is potentially confusing.
As are, potentially, lines such as `mu = mu`, which means "set the function's parameter called `mu` to the variable called `mu`."
These are different things (with the same name).

One way to check your code, once a function is built, is to comment out the initial code (or delete it), restart R or at least clear out the workspace, and then re-run the code that uses the function.
If things still work, then you should be somewhat confident you successfully bundled your code into the function.

You can also, once you bundle your code, do a search and replace to change variable names in your function to something more generic, to make the separation more clear.





## The estimation procedures

Brown and Forsythe considered four different hypothesis testing procedures for heteroskedastic ANOVA.
We start with the simplest one, which is just to use a conventional one-way ANOVA (while mistakenly assuming homoskedasticity). R's `oneway.test` function will actually calculate this test automatically:

```{r}
sim_data <- generate_data(mu = mu, sigma_sq = sigma_sq,
                          sample_size = sample_size)
oneway.test(x ~ factor(group), data = sim_data, var.equal = TRUE)
```

The main result we need here is the $p$-value, which will let us assess the test's Type-I error and power for a given nominal $\alpha$-level. The following function takes simulated data as input and returns as output the $p$-value from a one-way ANOVA:

```{r}
ANOVA_F_aov <- function(sim_data) {
  oneway_anova <- oneway.test(x ~ factor(group), data = sim_data,
                              var.equal = TRUE)
  return(oneway_anova$p.value)
}

ANOVA_F_aov(sim_data)
```

We might instead write the code to implement the ANOVA test ourselves.
This has some plusses (e.g., `oneway.test` maybe is doing a lot of other stuff which could take time and slow down our simulation) and minuses (e..g, writing our own code takes _our_ time, and it gives us lots of room to make mistakes and thus make life hard on ourself).
For further discussion of the trade-offs, see \@ref(optimize_code), where we do implement it by hand and see what kind of speed-ups we can obtain by doing that.

We next implement the Welch test, another one of the tests considered by Brown and Forsythe.
Here is a function that calculates the Welch test by hand, again following the notation and formulas from the paper:

```{r, include=FALSE}
old_Welch_F <- function(sim_data) {

  x_bar <- with(sim_data, tapply(x, group, mean))
  s_sq <- with(sim_data, tapply(x, group, var))
  n <- table(sim_data$group)
  g <- length(x_bar)

  w <- n / s_sq
  u <- sum(w)
  x_tilde <- sum(w * x_bar) / u
  msbtw <- sum(w * (x_bar - x_tilde)^2) / (g - 1)

  G <- sum((1 - w / u)^2 / (n - 1))
  denom <- 1 +  G * 2 * (g - 2) / (g^2 - 1)
  W <- msbtw / denom
  f <- (g^2 - 1) / (3 * G)

  pval <- pf(W, df1 = g - 1, df2 = f, lower.tail = FALSE)

  return(pval)
}
```

```{r, Welch_F_function}
Welch_F <- function(sim_data) {

  stats <- sim_data %>% group_by( group ) %>%
    summarise( x_bar = mean( x ),
               s_sq = var( x ),
               n = n() )
  
  g = nrow( stats )

  stats <- mutate( stats, 
                   w = n / s_sq )
  
  res <- stats %>%
    summarise( u = sum(w),
               x_tilde = sum( w * x_bar ) / u,
               msbtw = sum( w * (x_bar - x_tilde)^2 ) / (g - 1),
               G = sum( (1 - w / u)^2 / (n - 1) ),
               denom = 1 +  G * 2 * (g - 2) / (g^2 - 1),
               W = msbtw / denom,
               f = (g^2 - 1) / (3 * G) )

  pval <- pf(res$W, df1 = g - 1, df2 = res$f, lower.tail = FALSE)

  return(pval)
}

Welch_F(sim_data)
```

Our estimation function does not care if the data are simulated or not--we call the parameter `data` not `sim_data` to reflect this.
Eventually, we might imagine using this function on our real data!
Also, we can put a lot of complex stuff in a function--and then forget all about it.
In the above, for example, you can understand everything we say about writing simulations even if you do not understand anything at all about what this function is doing: it is just a black box that takes data and returns a $p$-value.

## Running the simulation

We now have functions that implement steps 2 and 3 of the simulation.
Given some parameters, `generate_data` produces a simulated dataset and `ANOVA_F_aov` and `Welch_F` use the simulated data to calculate $p$-values two different ways.
We now want to know which way is better, and how.
To answer this question, we next need to repeat this chain of calculations a bunch of times.

We first make a function that puts our chain together in a single method.
This method is also responsible for putting the results together in a tidy structure that is easy to aggregate and analyze.

```{r}
one_run = function( mu, sigma_sq, sample_size ) {
  sim_data <- generate_data(mu = mu, sigma_sq = sigma_sq,
                            sample_size = sample_size)
  anova_p <- ANOVA_F_aov(sim_data)
  Welch_p <- Welch_F(sim_data)
  tibble(ANOVA = anova_p, Welch = Welch_p)
}

one_run( mu = mu, sigma_sq = sigma_sq, sample_size = sample_size )
```
A single simulation trial simply does steps 2 and 3, ending with a nice dataframe or tibble that has our results for that single run.

We next call `one_run()` over and over; see @repeating_oneself for some discussion of options.
The following uses `map_df` to run `one_run()` 4 times and then stack the results into a single data frame (the `_df` tells R to do the stacking):

```{r}
sim_data <- map_df(1:4,
                  ~ one_run(mu = mu, sigma_sq = sigma_sq,
                          sample_size = sample_size) )
sim_data
```

Voila! We have simulated $p$-values!

## Analyzing the Simulation

We now have all the pieces in place to reproduce the results from Brown and Forsythe (1974).
We first focus on calculating the actual type-I error rate of these tests---that is, the proportion of the time that they reject the null hypothesis of equal means when that null is actually true---for an $\alpha$-level of .05.
We therefore need to simulate data according to process where the population means are indeed all equal. Arbitrarily, we start with $g = 4$ groups and set all of the means equal to zero:

```{r}
mu <- rep(0, 4)
```

In the fifth row of Table 1, Brown and Forsythe examine performance for the following parameter values for sample size and population variance:

```{r}
sample_size <- c(4, 8, 10, 12)
sigma_sq <- c(3, 2, 2, 1)^2
```

With these parameter values, we can use our `replicate` code to simulate 10,000 $p$-values:

```{r welch_sim_1, cache=TRUE}
p_vals <- rerun(10000, 
  sim_data <- one_run(mu = mu,
                      sigma_sq = sigma_sq,
                      sample_size = sample_size) )
p_vals <- bind_rows(p_vals)
p_vals
```

We next use our replications to calculate the rejection rates.
The rule is that the null is rejected if the $p$-value is less than $\alpha$. To get the rejection rate, calculate the proportion of replications where the null is rejected.
```{r}
sum(p_vals$ANOVA < 0.05) / 10000
```
This is equivalent to taking the mean of the logical conditions:

```{r}
mean(p_vals$ANOVA < 0.05)
```

We get a rejection rate that is much larger than $\alpha = .05$.
We have learned that the ANOVA F-test does not adequately control Type-I error under this set of conditions.

```{r}
mean(p_vals$Welch < 0.05)
```

The Welch test does much better, although it appears to be a little bit in excess of 0.05.

Note that these two numbers are quite close (though not quite identical) to the corresponding entries in Table 1 of Brown and Forsythe (1974). The difference is due to the fact that both Table 1 and are results are actually _estimated_ rejection rates, because we have not actually simulated an infinite number of replications. The estimation error arising from using a finite number of replications is called _simulation error_ (or _Monte Carlo error_).
Later on, we will look more at how to estimate and control the Monte Carlo simulation error in our studies.


## Exercises {#exAnovaExercises}

The following exercises involve exploring and tweaking the above simulation code we have developed to replicate the results of Brown and Forsythe (1974).

1. Table 1 from Brown and Forsythe reported rejection rates for $\alpha = .01$ and $\alpha = .10$ in addition to $\alpha = .05$. Calculate the rejection rates of the ANOVA F and Welch tests for all three $\alpha$-levels.

2. Try simulating the Type-I error rates for the parameter values in the first two rows of Table 1 of the original paper. Use 10,000 replications. How do your results compare to the results reported in Table 1?

3. Try simulating the **power levels** for a couple of sets of parameter values from Table 2. Use 10,000 replications. How do your results compare to the results reported in the Table?

4. One might, instead of having `one_run` return a single row with the columns for the $p$-values, have multiple rows with each row being a test (so one row for ANOVA and one for Welch).  E.g., it might produce results like this:

```{r, include=FALSE}
one_run_long <- function() {
   sim_data <- generate_data(mu = mu, sigma_sq = sigma_sq,
                            sample_size = sample_size)
  anova_p <- ANOVA_F_aov(sim_data)
  Welch_p <- Welch_F(sim_data)
  tibble(method = c( "ANOVA", "Welch" ),
         pvalue = c(anova_p, Welch_p) )
}
```

```{r, echo=TRUE}
one_run_long()
```

Modify `one_run()` to do this, update your simulation code, and then use `group_by()` plus `summarise()` to calculate rejection rates in one go.  The "long" approach is often nicer when evaluating more than two methods, or when each method returns not just a $p$-value but other quantities of interest.


