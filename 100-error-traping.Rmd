---
output: html_document
editor_options: 
  chunk_output_type: console
---
```{r, include =FALSE}
library( purrr )
```


# Error trapping and other headaches

If you have an advanced estimator, or are relying on some package, it is quite possible that every so often your estimate will trigger an error, or give you a NA result, or something similarly bad.
More innocuous, you might have estimators that can generate warnings; if you run 10,000 trials, that can add up to a lot of warnings which can be overwhelming to sort through.
In some cases, these warnings might be coupled with the estimator returning a very off result; it is unclear, in this case, whether we should include that result in our overall performance measures for that estimator.
After all, it tried to warn us!

In this section, we talk about some ways to make your simulations safe and robust, and also discuss some ways to track warnings and include them in peformance measures.
 

## Safe code {#safe_code}


Sometimes if you write a function that does a lot of complex things with
uncertain objects -- i.e. consider a complex estimator using an unstable package not of your own creation -- you can run into trouble where you have a intermittent error.

This can really be annoying; consider the case where your simulation crashes on 1 out of 500 chance: if you run 1000 simulation trials, your program will likely not make it to the end, thus wasting all of your time.
You can write code that can, instead of stopping when it reaches an error, trap the error and move on with the next simulation trial.

To illustrate, consider the following broken function:

```{r, error=TRUE}
my_complex_function = function( param ) {
    
    vals = rnorm( param, mean = 0.5 )
    if ( sum( vals ) > 5 ) {
        broken_code( 4 )
    } else {
        sqrt( sum( vals ) )
    }
}
```

We run it like so:
```{r, error=TRUE}
my_complex_function( 1 )
my_complex_function( 7 )

resu = rerun( 20, my_complex_function( 7 ) )
```

Oh no!  Our function crashes sometimes.
To trap the errors we use the purrr package to make a "safe" function as so:

```{r}
my_safe_function = safely( my_complex_function )
my_safe_function( 7 )
```

There are other function wrappers in this family, such as "possibly":
```{r}
my_possible_function = possibly( my_complex_function, 
                                 otherwise = NA )
my_possible_function( 7 )

as.numeric( rerun( 10, my_possible_function(7) ) )
```

Finally, we can use "quietly" to make warnings and stuff go away:
```{r}
my_quiet_function = quietly( my_complex_function )

my_quiet_function( 1 )
```

This can be especially valuable to control massive amounts of printout in a simulation.  If you have lots of extraneous printout, it can slow down the execution of your code far more than you might think.


### What to do with warnings

Sometimes our analytic strategy might give some sort of warning (or fail altogether).
For example, from the cluster randomized experiment case study we have:

```{r, include=FALSE}
source( "case_study_code/clustered_data_simulation.R" )
```

```{r illustrate_warnings}
set.seed(101012)  # (I picked this to show a warning.)
dat = gen_dat_model( J = 50, n_bar = 100, sigma2_u = 0 )
mod <- lmer( Yobs ~ 1 + Z + (1|sid), data=dat )
```

We have to make a deliberate decision as to what to do about this:
 
 - Keep these "weird" trials?
 - Drop them?

If you decide to drop them, you should drop the entire simulation iteration including the other estimators, even if they worked fine!
If there is something particularly unusual about the dataset, then dropping for one estimator, and keeping for the others that maybe didn't give a warning, but did struggle to estimate the estimand, would be unfair: in the final performance measures the estimators that did not give a warning could be being held to a higher standard, making the comparisons between estimators biased.

If your estimators generate warnings, you should calculate the rate of errors or warning messages as a performance measure.
Especially if you drop some trials, it is important to see how often things are acting pecularly.

The main tool for doing this is the `quietly()` function:
```{r}
quiet_lmer = quietly( lmer )
qmod <- quiet_lmer( Yobs ~ 1 + Z + (1|sid), data=dat )
qmod
```

You then might have, in your analyzing code:
```{r, eval=FALSE}
analyze_data <- function( dat ) {
    
    M1 <- quiet_lmer( Yobs ~ 1 + Z + (1|sid), data=dat )
    message1 = ifelse( length( M1$message ) > 0, 1, 0 )
    warning1 = ifelse( length( M1$warning ) > 0, 1, 0 )

    # Compile our results
    tibble( ATE_hat = coef(M1)["Z"],
            SE_hat = se.coef(M1)["Z"],
            message = message1,
            warning = warning1 )
}
```

Now you have your primary estimates, and also flags for whether there was a convergence issue.
In the analysis section you can then evaluate what proportion of the time there was a warning or message, and then do subset analyses to those simulation trials where there was no such warning.

## Protecting your functions with "stop" {#about_stopifnot}

When writing functions, especially those that take a lot of parameters, it is often wise to include `stopifnot()` statements at the top to verify the function is getting what it expects.
For example, look at this (fake) example of generating data with different means and variances

```{r assert_demo}
make_groups <- function( means, sds ) {
  Y = rnorm( length(means), mean=means, sd = sds )
  round( Y )
}
```

If we call it, but provide different lengths for our means and variances, nothing happens, because R simply recycles the standard deviation parameter:
```{r}
make_groups( c(100,200,300,400), c(1,100,10000) )
```

If this function was used in our data generating code, we would see the warnings but might not know exactly what they were.
We can instead protect our function by putting an *assert statements* in our function like this:
```{r}
make_groups <- function( means, sds ) {
  stopifnot( length(means) == length(sds) )
  Y = rnorm( length(means), mean=means, sd = sds )
  round( Y )
}
```

This ensures your code is getting called as you intended.
What is nasty about this possible error is nothing is telling you something is wrong!
You could build an entire simulation on this, not realizing that your fourth group has the variance of your first, and get results that make no sense to you.
You could even publish something based on a finding that depends on this error, which would eventually be quite embarrasing.

These statements can also serve as a sort of documentation as to what you expect.
Consider, for example:
```{r}
make_xy <- function( N, mu_x, mu_y, rho ) {
  stopifnot( -1 <= rho && rho <= 1 )
  X = mu_x + rnorm( N )
  Y = mu_y + rho * X + sqrt(1-rho^2)*rnorm(N)
  tibble(X = X, Y=Y)
}
```
Here we see that rho should be between -1 and 1 quite clearly.
A good reminder of what the parameter is for.

This also protects you from inadvetently misremembering the order of your parameters when you call the function (although it is good practice to name your parameters as you pass).
Consider:
```{r, error=TRUE}
a <- make_xy( 10, 2, 3, 0.75 )
b <- make_xy( 10, 0.75, 2, 3 )
c <- make_xy( 10, rho = 0.75, mu_x = 2, mu_y = 3 )
```

