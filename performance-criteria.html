<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 9 Performance metrics | Designing Monte Carlo Simulations in R</title>
  <meta name="description" content="A text on designing, implementing, and reporting on Monte Carlo simulation studies" />
  <meta name="generator" content="bookdown 0.44 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 9 Performance metrics | Designing Monte Carlo Simulations in R" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="A text on designing, implementing, and reporting on Monte Carlo simulation studies" />
  <meta name="github-repo" content="jepusto/Designing-Simulations-in-R" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 9 Performance metrics | Designing Monte Carlo Simulations in R" />
  
  <meta name="twitter:description" content="A text on designing, implementing, and reporting on Monte Carlo simulation studies" />
  

<meta name="author" content="Luke W. Miratrix and James E. Pustejovsky (Equal authors)" />


<meta name="date" content="2025-09-06" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="running-the-simulation-process.html"/>
<link rel="next" href="exp-design.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
<span class="math inline">
\(\newcommand{\Prob}{\text{Pr}}\)
\(\newcommand{\E}{\mathbb{E}}\)
\(\newcommand{\M}{\mathbb{M}}\)
\(\newcommand{\Q}{\mathbb{Q}}\)
\(\newcommand{\Var}{\text{Var}}\)
\(\newcommand{\Bias}{\text{Bias}}\)
\(\newcommand{\RMSE}{\text{RMSE}}\)
\(\newcommand{\Cov}{\text{Cov}}\)
\(\newcommand{\cor}{\text{cor}}\)
\(\newcommand{\diag}{\text{diag}}\)
\(\newcommand{\mat}[1]{\mathbf{#1}}\)
\(\newcommand{\bs}{\boldsymbol}\)
\(\newcommand{\trace}{\text{tr}}\)
</span>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Designing Simulations in R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#license"><i class="fa fa-check"></i>License</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#about-the-authors"><i class="fa fa-check"></i>About the authors</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
<li class="part"><span><b>I An Introductory Look</b></span></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#some-of-simulations-many-uses"><i class="fa fa-check"></i><b>1.1</b> Some of simulation’s many uses</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="introduction.html"><a href="introduction.html#comparing-statistical-approaches"><i class="fa fa-check"></i><b>1.1.1</b> Comparing statistical approaches</a></li>
<li class="chapter" data-level="1.1.2" data-path="introduction.html"><a href="introduction.html#assessing-performance-of-complex-pipelines"><i class="fa fa-check"></i><b>1.1.2</b> Assessing performance of complex pipelines</a></li>
<li class="chapter" data-level="1.1.3" data-path="introduction.html"><a href="introduction.html#assessing-performance-under-misspecification"><i class="fa fa-check"></i><b>1.1.3</b> Assessing performance under misspecification</a></li>
<li class="chapter" data-level="1.1.4" data-path="introduction.html"><a href="introduction.html#assessing-the-finite-sample-performance-of-a-statistical-approach"><i class="fa fa-check"></i><b>1.1.4</b> Assessing the finite-sample performance of a statistical approach</a></li>
<li class="chapter" data-level="1.1.5" data-path="introduction.html"><a href="introduction.html#conducting-power-analyses"><i class="fa fa-check"></i><b>1.1.5</b> Conducting Power Analyses</a></li>
<li class="chapter" data-level="1.1.6" data-path="introduction.html"><a href="introduction.html#simulating-processess"><i class="fa fa-check"></i><b>1.1.6</b> Simulating processess</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#the-perils-of-simulation-as-evidence"><i class="fa fa-check"></i><b>1.2</b> The perils of simulation as evidence</a></li>
<li class="chapter" data-level="1.3" data-path="introduction.html"><a href="introduction.html#simulating-to-learn"><i class="fa fa-check"></i><b>1.3</b> Simulating to learn</a></li>
<li class="chapter" data-level="1.4" data-path="introduction.html"><a href="introduction.html#why-r"><i class="fa fa-check"></i><b>1.4</b> Why R?</a></li>
<li class="chapter" data-level="1.5" data-path="introduction.html"><a href="introduction.html#organization-of-the-text"><i class="fa fa-check"></i><b>1.5</b> Organization of the text</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="programming-preliminaries.html"><a href="programming-preliminaries.html"><i class="fa fa-check"></i><b>2</b> Programming Preliminaries</a>
<ul>
<li class="chapter" data-level="2.1" data-path="programming-preliminaries.html"><a href="programming-preliminaries.html#welcome-to-the-tidyverse"><i class="fa fa-check"></i><b>2.1</b> Welcome to the tidyverse</a></li>
<li class="chapter" data-level="2.2" data-path="programming-preliminaries.html"><a href="programming-preliminaries.html#functions"><i class="fa fa-check"></i><b>2.2</b> Functions</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="programming-preliminaries.html"><a href="programming-preliminaries.html#rolling-your-own"><i class="fa fa-check"></i><b>2.2.1</b> Rolling your own</a></li>
<li class="chapter" data-level="2.2.2" data-path="programming-preliminaries.html"><a href="programming-preliminaries.html#a-dangerous-function"><i class="fa fa-check"></i><b>2.2.2</b> A dangerous function</a></li>
<li class="chapter" data-level="2.2.3" data-path="programming-preliminaries.html"><a href="programming-preliminaries.html#using-named-arguments"><i class="fa fa-check"></i><b>2.2.3</b> Using Named Arguments</a></li>
<li class="chapter" data-level="2.2.4" data-path="programming-preliminaries.html"><a href="programming-preliminaries.html#argument-defaults"><i class="fa fa-check"></i><b>2.2.4</b> Argument Defaults</a></li>
<li class="chapter" data-level="2.2.5" data-path="programming-preliminaries.html"><a href="programming-preliminaries.html#function-skeletons"><i class="fa fa-check"></i><b>2.2.5</b> Function skeletons</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="programming-preliminaries.html"><a href="programming-preliminaries.html#pipe-dreams"><i class="fa fa-check"></i><b>2.3</b> <code>\&gt;</code> (Pipe) dreams</a></li>
<li class="chapter" data-level="2.4" data-path="programming-preliminaries.html"><a href="programming-preliminaries.html#recipes-versus-patterns"><i class="fa fa-check"></i><b>2.4</b> Recipes versus Patterns</a></li>
<li class="chapter" data-level="2.5" data-path="programming-preliminaries.html"><a href="programming-preliminaries.html#exercises"><i class="fa fa-check"></i><b>2.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="t-test-simulation.html"><a href="t-test-simulation.html"><i class="fa fa-check"></i><b>3</b> An initial simulation</a>
<ul>
<li class="chapter" data-level="3.1" data-path="t-test-simulation.html"><a href="t-test-simulation.html#simulating-a-single-scenario"><i class="fa fa-check"></i><b>3.1</b> Simulating a single scenario</a></li>
<li class="chapter" data-level="3.2" data-path="t-test-simulation.html"><a href="t-test-simulation.html#a-non-normal-population-distribution"><i class="fa fa-check"></i><b>3.2</b> A non-normal population distribution</a></li>
<li class="chapter" data-level="3.3" data-path="t-test-simulation.html"><a href="t-test-simulation.html#simulating-across-different-scenarios"><i class="fa fa-check"></i><b>3.3</b> Simulating across different scenarios</a></li>
<li class="chapter" data-level="3.4" data-path="t-test-simulation.html"><a href="t-test-simulation.html#extending-the-simulation-design"><i class="fa fa-check"></i><b>3.4</b> Extending the simulation design</a></li>
<li class="chapter" data-level="3.5" data-path="t-test-simulation.html"><a href="t-test-simulation.html#exercises-1"><i class="fa fa-check"></i><b>3.5</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>II Structure and Mechanics of a Simulation Study</b></span></li>
<li class="chapter" data-level="4" data-path="simulation-structure.html"><a href="simulation-structure.html"><i class="fa fa-check"></i><b>4</b> Structure of a simulation study</a>
<ul>
<li class="chapter" data-level="4.1" data-path="simulation-structure.html"><a href="simulation-structure.html#general-structure-of-a-simulation"><i class="fa fa-check"></i><b>4.1</b> General structure of a simulation</a></li>
<li class="chapter" data-level="4.2" data-path="simulation-structure.html"><a href="simulation-structure.html#tidy-modular-simulations"><i class="fa fa-check"></i><b>4.2</b> Tidy, modular simulations</a></li>
<li class="chapter" data-level="4.3" data-path="simulation-structure.html"><a href="simulation-structure.html#skeleton-of-a-simulation-study"><i class="fa fa-check"></i><b>4.3</b> Skeleton of a simulation study</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="simulation-structure.html"><a href="simulation-structure.html#data-generating-process"><i class="fa fa-check"></i><b>4.3.1</b> Data-Generating Process</a></li>
<li class="chapter" data-level="4.3.2" data-path="simulation-structure.html"><a href="simulation-structure.html#data-analysis-procedure"><i class="fa fa-check"></i><b>4.3.2</b> Data Analysis Procedure</a></li>
<li class="chapter" data-level="4.3.3" data-path="simulation-structure.html"><a href="simulation-structure.html#repetition"><i class="fa fa-check"></i><b>4.3.3</b> Repetition</a></li>
<li class="chapter" data-level="4.3.4" data-path="simulation-structure.html"><a href="simulation-structure.html#performance-summaries"><i class="fa fa-check"></i><b>4.3.4</b> Performance summaries</a></li>
<li class="chapter" data-level="4.3.5" data-path="simulation-structure.html"><a href="simulation-structure.html#multifactor-simulations"><i class="fa fa-check"></i><b>4.3.5</b> Multifactor simulations</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="simulation-structure.html"><a href="simulation-structure.html#exercises-2"><i class="fa fa-check"></i><b>4.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="case-ANOVA.html"><a href="case-ANOVA.html"><i class="fa fa-check"></i><b>5</b> Case Study: Heteroskedastic ANOVA and Welch</a>
<ul>
<li class="chapter" data-level="5.1" data-path="case-ANOVA.html"><a href="case-ANOVA.html#case-anova-DGP"><i class="fa fa-check"></i><b>5.1</b> The data-generating model</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="case-ANOVA.html"><a href="case-ANOVA.html#now-make-a-function"><i class="fa fa-check"></i><b>5.1.1</b> Now make a function</a></li>
<li class="chapter" data-level="5.1.2" data-path="case-ANOVA.html"><a href="case-ANOVA.html#cautious-coding"><i class="fa fa-check"></i><b>5.1.2</b> Cautious coding</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="case-ANOVA.html"><a href="case-ANOVA.html#the-hypothesis-testing-procedures"><i class="fa fa-check"></i><b>5.2</b> The hypothesis testing procedures</a></li>
<li class="chapter" data-level="5.3" data-path="case-ANOVA.html"><a href="case-ANOVA.html#running-the-simulation"><i class="fa fa-check"></i><b>5.3</b> Running the simulation</a></li>
<li class="chapter" data-level="5.4" data-path="case-ANOVA.html"><a href="case-ANOVA.html#summarizing-test-performance"><i class="fa fa-check"></i><b>5.4</b> Summarizing test performance</a></li>
<li class="chapter" data-level="5.5" data-path="case-ANOVA.html"><a href="case-ANOVA.html#exAnovaExercises"><i class="fa fa-check"></i><b>5.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="data-generating-processes.html"><a href="data-generating-processes.html"><i class="fa fa-check"></i><b>6</b> Data-generating processes</a>
<ul>
<li class="chapter" data-level="6.1" data-path="data-generating-processes.html"><a href="data-generating-processes.html#DGP-examples"><i class="fa fa-check"></i><b>6.1</b> Examples</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="data-generating-processes.html"><a href="data-generating-processes.html#ANOVA-example"><i class="fa fa-check"></i><b>6.1.1</b> Example 1: One-way analysis of variance</a></li>
<li class="chapter" data-level="6.1.2" data-path="data-generating-processes.html"><a href="data-generating-processes.html#BVPois-example"><i class="fa fa-check"></i><b>6.1.2</b> Example 2: Bivariate Poisson model</a></li>
<li class="chapter" data-level="6.1.3" data-path="data-generating-processes.html"><a href="data-generating-processes.html#CRT-example"><i class="fa fa-check"></i><b>6.1.3</b> Example 3: Hierarchical linear model for a cluster-randomized trial</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="data-generating-processes.html"><a href="data-generating-processes.html#components-of-a-dgp"><i class="fa fa-check"></i><b>6.2</b> Components of a DGP</a></li>
<li class="chapter" data-level="6.3" data-path="data-generating-processes.html"><a href="data-generating-processes.html#DGP-functions"><i class="fa fa-check"></i><b>6.3</b> A statistical model is a recipe for data generation</a></li>
<li class="chapter" data-level="6.4" data-path="data-generating-processes.html"><a href="data-generating-processes.html#DGP-plotting"><i class="fa fa-check"></i><b>6.4</b> Plot the artificial data</a></li>
<li class="chapter" data-level="6.5" data-path="data-generating-processes.html"><a href="data-generating-processes.html#check-the-data-generating-function"><i class="fa fa-check"></i><b>6.5</b> Check the data-generating function</a></li>
<li class="chapter" data-level="6.6" data-path="data-generating-processes.html"><a href="data-generating-processes.html#case-cluster"><i class="fa fa-check"></i><b>6.6</b> Example: Simulating clustered data</a>
<ul>
<li class="chapter" data-level="6.6.1" data-path="data-generating-processes.html"><a href="data-generating-processes.html#a-design-decision-what-do-we-want-to-manipulate"><i class="fa fa-check"></i><b>6.6.1</b> A design decision: What do we want to manipulate?</a></li>
<li class="chapter" data-level="6.6.2" data-path="data-generating-processes.html"><a href="data-generating-processes.html#a-model-for-a-cluster-rct"><i class="fa fa-check"></i><b>6.6.2</b> A model for a cluster RCT</a></li>
<li class="chapter" data-level="6.6.3" data-path="data-generating-processes.html"><a href="data-generating-processes.html#from-equations-to-code"><i class="fa fa-check"></i><b>6.6.3</b> From equations to code</a></li>
<li class="chapter" data-level="6.6.4" data-path="data-generating-processes.html"><a href="data-generating-processes.html#DGP-standardization"><i class="fa fa-check"></i><b>6.6.4</b> Standardization in the DGP</a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="data-generating-processes.html"><a href="data-generating-processes.html#three-parameter-IRT"><i class="fa fa-check"></i><b>6.7</b> Sometimes a DGP is all you need</a></li>
<li class="chapter" data-level="6.8" data-path="data-generating-processes.html"><a href="data-generating-processes.html#more-to-explore"><i class="fa fa-check"></i><b>6.8</b> More to explore</a></li>
<li class="chapter" data-level="6.9" data-path="data-generating-processes.html"><a href="data-generating-processes.html#exercises-3"><i class="fa fa-check"></i><b>6.9</b> Exercises</a>
<ul>
<li class="chapter" data-level="6.9.1" data-path="data-generating-processes.html"><a href="data-generating-processes.html#Welch-t-dgp"><i class="fa fa-check"></i><b>6.9.1</b> The Welch test on a shifted-and-scaled <span class="math inline">\(t\)</span> distribution</a></li>
<li class="chapter" data-level="6.9.2" data-path="data-generating-processes.html"><a href="data-generating-processes.html#plot-the-bivariate-poisson"><i class="fa fa-check"></i><b>6.9.2</b> Plot the bivariate Poisson</a></li>
<li class="chapter" data-level="6.9.3" data-path="data-generating-processes.html"><a href="data-generating-processes.html#BVP-check"><i class="fa fa-check"></i><b>6.9.3</b> Check the bivariate Poisson function</a></li>
<li class="chapter" data-level="6.9.4" data-path="data-generating-processes.html"><a href="data-generating-processes.html#BVP-error"><i class="fa fa-check"></i><b>6.9.4</b> Add error-catching to the bivariate Poisson function</a></li>
<li class="chapter" data-level="6.9.5" data-path="data-generating-processes.html"><a href="data-generating-processes.html#BVNB1"><i class="fa fa-check"></i><b>6.9.5</b> A bivariate negative binomial distribution</a></li>
<li class="chapter" data-level="6.9.6" data-path="data-generating-processes.html"><a href="data-generating-processes.html#BVNB2"><i class="fa fa-check"></i><b>6.9.6</b> Another bivariate negative binomial distribution</a></li>
<li class="chapter" data-level="6.9.7" data-path="data-generating-processes.html"><a href="data-generating-processes.html#cluster-RCT-plot"><i class="fa fa-check"></i><b>6.9.7</b> Plot the data from a cluster-randomized trial</a></li>
<li class="chapter" data-level="6.9.8" data-path="data-generating-processes.html"><a href="data-generating-processes.html#cluster-RCT-checks"><i class="fa fa-check"></i><b>6.9.8</b> Checking the Cluster RCT DGP</a></li>
<li class="chapter" data-level="6.9.9" data-path="data-generating-processes.html"><a href="data-generating-processes.html#cluster-RCT-heterogeneity"><i class="fa fa-check"></i><b>6.9.9</b> More school-level variation</a></li>
<li class="chapter" data-level="6.9.10" data-path="data-generating-processes.html"><a href="data-generating-processes.html#cluster-RCT-baseline"><i class="fa fa-check"></i><b>6.9.10</b> Cluster-randomized trial with baseline predictors</a></li>
<li class="chapter" data-level="6.9.11" data-path="data-generating-processes.html"><a href="data-generating-processes.html#IRT-DGP-parameters"><i class="fa fa-check"></i><b>6.9.11</b> 3-parameter IRT datasets</a></li>
<li class="chapter" data-level="6.9.12" data-path="data-generating-processes.html"><a href="data-generating-processes.html#IRT-DGP-checking"><i class="fa fa-check"></i><b>6.9.12</b> Check the 3-parameter IRT DGP</a></li>
<li class="chapter" data-level="6.9.13" data-path="data-generating-processes.html"><a href="data-generating-processes.html#IRT-DGP-breaking"><i class="fa fa-check"></i><b>6.9.13</b> Explore the 3-parameter IRT model</a></li>
<li class="chapter" data-level="6.9.14" data-path="data-generating-processes.html"><a href="data-generating-processes.html#meta-regression-DGP"><i class="fa fa-check"></i><b>6.9.14</b> Random effects meta-regression</a></li>
<li class="chapter" data-level="6.9.15" data-path="data-generating-processes.html"><a href="data-generating-processes.html#Vevea-Hedges-DGP"><i class="fa fa-check"></i><b>6.9.15</b> Meta-regression with selective reporting</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="data-analysis-procedures.html"><a href="data-analysis-procedures.html"><i class="fa fa-check"></i><b>7</b> Data analysis procedures</a>
<ul>
<li class="chapter" data-level="7.1" data-path="data-analysis-procedures.html"><a href="data-analysis-procedures.html#estimation-functions"><i class="fa fa-check"></i><b>7.1</b> Writing estimation functions</a></li>
<li class="chapter" data-level="7.2" data-path="data-analysis-procedures.html"><a href="data-analysis-procedures.html#multiple-estimation-procedures"><i class="fa fa-check"></i><b>7.2</b> Including Multiple Data Analysis Procedures</a></li>
<li class="chapter" data-level="7.3" data-path="data-analysis-procedures.html"><a href="data-analysis-procedures.html#validating-an-estimation-function"><i class="fa fa-check"></i><b>7.3</b> Validating an Estimation Function</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="data-analysis-procedures.html"><a href="data-analysis-procedures.html#checking-against-existing-implementations"><i class="fa fa-check"></i><b>7.3.1</b> Checking against existing implementations</a></li>
<li class="chapter" data-level="7.3.2" data-path="data-analysis-procedures.html"><a href="data-analysis-procedures.html#checking-novel-procedures"><i class="fa fa-check"></i><b>7.3.2</b> Checking novel procedures</a></li>
<li class="chapter" data-level="7.3.3" data-path="data-analysis-procedures.html"><a href="data-analysis-procedures.html#checking-with-simulations"><i class="fa fa-check"></i><b>7.3.3</b> Checking with simulations</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="data-analysis-procedures.html"><a href="data-analysis-procedures.html#handling-errors-warnings-and-other-hiccups"><i class="fa fa-check"></i><b>7.4</b> Handling errors, warnings, and other hiccups</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="data-analysis-procedures.html"><a href="data-analysis-procedures.html#capturing-errors-and-warnings"><i class="fa fa-check"></i><b>7.4.1</b> Capturing errors and warnings</a></li>
<li class="chapter" data-level="7.4.2" data-path="data-analysis-procedures.html"><a href="data-analysis-procedures.html#adapting-for-errors"><i class="fa fa-check"></i><b>7.4.2</b> Adapting estimation procedures for errors and warnings</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="data-analysis-procedures.html"><a href="data-analysis-procedures.html#exercises-4"><i class="fa fa-check"></i><b>7.5</b> Exercises</a>
<ul>
<li class="chapter" data-level="7.5.1" data-path="data-analysis-procedures.html"><a href="data-analysis-procedures.html#BFFs-forever"><i class="fa fa-check"></i><b>7.5.1</b> More Heteroskedastic ANOVA</a></li>
<li class="chapter" data-level="7.5.2" data-path="data-analysis-procedures.html"><a href="data-analysis-procedures.html#contingent-testing"><i class="fa fa-check"></i><b>7.5.2</b> Contingent testing</a></li>
<li class="chapter" data-level="7.5.3" data-path="data-analysis-procedures.html"><a href="data-analysis-procedures.html#cross-check-CRT-estimators"><i class="fa fa-check"></i><b>7.5.3</b> Check the cluster-RCT functions</a></li>
<li class="chapter" data-level="7.5.4" data-path="data-analysis-procedures.html"><a href="data-analysis-procedures.html#CRT-ANCOVA-estimators"><i class="fa fa-check"></i><b>7.5.4</b> Extending the cluster-RCT functions</a></li>
<li class="chapter" data-level="7.5.5" data-path="data-analysis-procedures.html"><a href="data-analysis-procedures.html#contingent-estimator-processing"><i class="fa fa-check"></i><b>7.5.5</b> Contingent estimator processing</a></li>
<li class="chapter" data-level="7.5.6" data-path="data-analysis-procedures.html"><a href="data-analysis-procedures.html#IRT-3PL-estimation"><i class="fa fa-check"></i><b>7.5.6</b> Estimating 3-parameter item response theory models</a></li>
<li class="chapter" data-level="7.5.7" data-path="data-analysis-procedures.html"><a href="data-analysis-procedures.html#Vevea-Hedges-estimation"><i class="fa fa-check"></i><b>7.5.7</b> Meta-regression with selective reporting</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="running-the-simulation-process.html"><a href="running-the-simulation-process.html"><i class="fa fa-check"></i><b>8</b> Running the Simulation Process</a>
<ul>
<li class="chapter" data-level="8.1" data-path="running-the-simulation-process.html"><a href="running-the-simulation-process.html#repeating-oneself"><i class="fa fa-check"></i><b>8.1</b> Repeating oneself</a></li>
<li class="chapter" data-level="8.2" data-path="running-the-simulation-process.html"><a href="running-the-simulation-process.html#one-run-at-a-time"><i class="fa fa-check"></i><b>8.2</b> One run at a time</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="running-the-simulation-process.html"><a href="running-the-simulation-process.html#one-run-reparameterization"><i class="fa fa-check"></i><b>8.2.1</b> Reparameterizing</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="running-the-simulation-process.html"><a href="running-the-simulation-process.html#bundle-sim-demo"><i class="fa fa-check"></i><b>8.3</b> Bundling simulations with <code>simhelpers</code></a></li>
<li class="chapter" data-level="8.4" data-path="running-the-simulation-process.html"><a href="running-the-simulation-process.html#seeds-and-pseudo-RNGs"><i class="fa fa-check"></i><b>8.4</b> Seeds and pseudo-random number generators</a></li>
<li class="chapter" data-level="8.5" data-path="running-the-simulation-process.html"><a href="running-the-simulation-process.html#exercises-5"><i class="fa fa-check"></i><b>8.5</b> Exercises</a>
<ul>
<li class="chapter" data-level="8.5.1" data-path="running-the-simulation-process.html"><a href="running-the-simulation-process.html#Welch-simulation"><i class="fa fa-check"></i><b>8.5.1</b> Welch simulations</a></li>
<li class="chapter" data-level="8.5.2" data-path="running-the-simulation-process.html"><a href="running-the-simulation-process.html#Pearson-sampling-distributions"><i class="fa fa-check"></i><b>8.5.2</b> Compare sampling distributions of Pearson’s correlation coefficients</a></li>
<li class="chapter" data-level="8.5.3" data-path="running-the-simulation-process.html"><a href="running-the-simulation-process.html#reparameterization-redux"><i class="fa fa-check"></i><b>8.5.3</b> Reparameterization, redux</a></li>
<li class="chapter" data-level="8.5.4" data-path="running-the-simulation-process.html"><a href="running-the-simulation-process.html#fancy-cluster-RCT-sims"><i class="fa fa-check"></i><b>8.5.4</b> Fancy clustered RCT simulations</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="performance-criteria.html"><a href="performance-criteria.html"><i class="fa fa-check"></i><b>9</b> Performance metrics</a>
<ul>
<li class="chapter" data-level="9.1" data-path="performance-criteria.html"><a href="performance-criteria.html#assessing-point-estimators"><i class="fa fa-check"></i><b>9.1</b> Metrics for Point Estimators</a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="performance-criteria.html"><a href="performance-criteria.html#clusterRCTperformance"><i class="fa fa-check"></i><b>9.1.1</b> Comparing the Performances of the Cluster RCT Estimation Procedures</a></li>
<li class="chapter" data-level="9.1.2" data-path="performance-criteria.html"><a href="performance-criteria.html#less-conventional-performance-metrics"><i class="fa fa-check"></i><b>9.1.2</b> Less Conventional Performance metrics</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="performance-criteria.html"><a href="performance-criteria.html#metrics-for-standard-error-estimators"><i class="fa fa-check"></i><b>9.2</b> Metrics for Standard Error Estimators</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="performance-criteria.html"><a href="performance-criteria.html#assessing-ses-for-our-cluster-rct-simulation"><i class="fa fa-check"></i><b>9.2.1</b> Assessing SEs for Our Cluster RCT Simulation</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="performance-criteria.html"><a href="performance-criteria.html#metrics-for-confidence-intervals"><i class="fa fa-check"></i><b>9.3</b> Metrics for Confidence Intervals</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="performance-criteria.html"><a href="performance-criteria.html#confidence-intervals-in-our-cluster-rct-example"><i class="fa fa-check"></i><b>9.3.1</b> Confidence Intervals in our Cluster RCT Example</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="performance-criteria.html"><a href="performance-criteria.html#assessing-inferential-procedures"><i class="fa fa-check"></i><b>9.4</b> Metrics for Inferential Procedures (Hypothesis Tests)</a>
<ul>
<li class="chapter" data-level="9.4.1" data-path="performance-criteria.html"><a href="performance-criteria.html#validity"><i class="fa fa-check"></i><b>9.4.1</b> Validity</a></li>
<li class="chapter" data-level="9.4.2" data-path="performance-criteria.html"><a href="performance-criteria.html#power"><i class="fa fa-check"></i><b>9.4.2</b> Power</a></li>
<li class="chapter" data-level="9.4.3" data-path="performance-criteria.html"><a href="performance-criteria.html#the-rejection-rate"><i class="fa fa-check"></i><b>9.4.3</b> The Rejection Rate</a></li>
<li class="chapter" data-level="9.4.4" data-path="performance-criteria.html"><a href="performance-criteria.html#inference-in-our-cluster-rct-simulation"><i class="fa fa-check"></i><b>9.4.4</b> Inference in our Cluster RCT Simulation</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="performance-criteria.html"><a href="performance-criteria.html#sec-relative-performance"><i class="fa fa-check"></i><b>9.5</b> Selecting Relative vs. Absolute Metrics</a></li>
<li class="chapter" data-level="9.6" data-path="performance-criteria.html"><a href="performance-criteria.html#summary-of-peformance-measures"><i class="fa fa-check"></i><b>9.6</b> Summary of Peformance Measures</a>
<ul>
<li class="chapter" data-level="9.6.1" data-path="performance-criteria.html"><a href="performance-criteria.html#windsorization-to-control-outliers"><i class="fa fa-check"></i><b>9.6.1</b> Windsorization to control outliers</a></li>
<li class="chapter" data-level="9.6.2" data-path="performance-criteria.html"><a href="performance-criteria.html#correlation-measures-vs-absolute-performance"><i class="fa fa-check"></i><b>9.6.2</b> Correlation measures vs absolute performance</a></li>
</ul></li>
<li class="chapter" data-level="9.7" data-path="performance-criteria.html"><a href="performance-criteria.html#summary-of-peformance-measures-1"><i class="fa fa-check"></i><b>9.7</b> Summary of Peformance Measures</a></li>
<li class="chapter" data-level="9.8" data-path="performance-criteria.html"><a href="performance-criteria.html#implicit-estimands"><i class="fa fa-check"></i><b>9.8</b> Estimands Not Represented By a Parameter</a></li>
<li class="chapter" data-level="9.9" data-path="performance-criteria.html"><a href="performance-criteria.html#MCSE"><i class="fa fa-check"></i><b>9.9</b> Uncertainty in Performance Estimates (the Monte Carlo Standard Error)</a>
<ul>
<li class="chapter" data-level="9.9.1" data-path="performance-criteria.html"><a href="performance-criteria.html#mcse-for-relative-variance-estimators"><i class="fa fa-check"></i><b>9.9.1</b> MCSE for Relative Variance Estimators</a></li>
<li class="chapter" data-level="9.9.2" data-path="performance-criteria.html"><a href="performance-criteria.html#calculating-mcses-with-the-simhelpers-package"><i class="fa fa-check"></i><b>9.9.2</b> Calculating MCSEs With the <code>simhelpers</code> Package</a></li>
<li class="chapter" data-level="9.9.3" data-path="performance-criteria.html"><a href="performance-criteria.html#mcse-calculation-in-our-cluster-rct-example"><i class="fa fa-check"></i><b>9.9.3</b> MCSE Calculation in our Cluster RCT Example</a></li>
</ul></li>
<li class="chapter" data-level="9.10" data-path="performance-criteria.html"><a href="performance-criteria.html#concluding-thoughts"><i class="fa fa-check"></i><b>9.10</b> Concluding thoughts</a></li>
<li class="chapter" data-level="9.11" data-path="performance-criteria.html"><a href="performance-criteria.html#exercises-6"><i class="fa fa-check"></i><b>9.11</b> Exercises</a>
<ul>
<li class="chapter" data-level="9.11.1" data-path="performance-criteria.html"><a href="performance-criteria.html#Brown-Forsythe-performance"><i class="fa fa-check"></i><b>9.11.1</b> Brown and Forsythe (1974)</a></li>
<li class="chapter" data-level="9.11.2" data-path="performance-criteria.html"><a href="performance-criteria.html#jackknife-MCSE"><i class="fa fa-check"></i><b>9.11.2</b> Jackknife calculation of MCSEs</a></li>
<li class="chapter" data-level="9.11.3" data-path="performance-criteria.html"><a href="performance-criteria.html#cluster-RCT-SPATE"><i class="fa fa-check"></i><b>9.11.3</b> Distribution theory for person-level average treatment effects</a></li>
<li class="chapter" data-level="9.11.4" data-path="performance-criteria.html"><a href="performance-criteria.html#multiple-scenario-performance"><i class="fa fa-check"></i><b>9.11.4</b> Multiple scenarios</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>III Multifactor Simulations</b></span></li>
<li class="chapter" data-level="10" data-path="exp-design.html"><a href="exp-design.html"><i class="fa fa-check"></i><b>10</b> Designing and executing multifactor simulations</a>
<ul>
<li class="chapter" data-level="10.1" data-path="exp-design.html"><a href="exp-design.html#choosing-parameter-combinations"><i class="fa fa-check"></i><b>10.1</b> Choosing parameter combinations</a></li>
<li class="chapter" data-level="10.2" data-path="exp-design.html"><a href="exp-design.html#using-pmap-to-run-multifactor-simulations"><i class="fa fa-check"></i><b>10.2</b> Using pmap to run multifactor simulations</a></li>
<li class="chapter" data-level="10.3" data-path="exp-design.html"><a href="exp-design.html#when-to-calculate-performance-metrics"><i class="fa fa-check"></i><b>10.3</b> When to calculate performance metrics</a>
<ul>
<li class="chapter" data-level="10.3.1" data-path="exp-design.html"><a href="exp-design.html#aggregate-as-you-simulate-inside"><i class="fa fa-check"></i><b>10.3.1</b> Aggregate as you simulate (inside)</a></li>
<li class="chapter" data-level="10.3.2" data-path="exp-design.html"><a href="exp-design.html#keep-all-simulation-runs-outside"><i class="fa fa-check"></i><b>10.3.2</b> Keep all simulation runs (outside)</a></li>
<li class="chapter" data-level="10.3.3" data-path="exp-design.html"><a href="exp-design.html#getting-raw-results-ready-for-analysis"><i class="fa fa-check"></i><b>10.3.3</b> Getting raw results ready for analysis</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="exp-design.html"><a href="exp-design.html#summary"><i class="fa fa-check"></i><b>10.4</b> Summary</a></li>
<li class="chapter" data-level="10.5" data-path="exp-design.html"><a href="exp-design.html#case-study-a-multifactor-evaluation-of-cluster-rct-estimators"><i class="fa fa-check"></i><b>10.5</b> Case Study: A multifactor evaluation of cluster RCT estimators</a>
<ul>
<li class="chapter" data-level="10.5.1" data-path="exp-design.html"><a href="exp-design.html#choosing-parameters-for-the-clustered-rct"><i class="fa fa-check"></i><b>10.5.1</b> Choosing parameters for the Clustered RCT</a></li>
<li class="chapter" data-level="10.5.2" data-path="exp-design.html"><a href="exp-design.html#redundant-factor-combinations"><i class="fa fa-check"></i><b>10.5.2</b> Redundant factor combinations</a></li>
<li class="chapter" data-level="10.5.3" data-path="exp-design.html"><a href="exp-design.html#running-the-simulations"><i class="fa fa-check"></i><b>10.5.3</b> Running the simulations</a></li>
<li class="chapter" data-level="10.5.4" data-path="exp-design.html"><a href="exp-design.html#calculating-performance-metrics"><i class="fa fa-check"></i><b>10.5.4</b> Calculating performance metrics</a></li>
</ul></li>
<li class="chapter" data-level="10.6" data-path="exp-design.html"><a href="exp-design.html#exercises-7"><i class="fa fa-check"></i><b>10.6</b> Exercises</a>
<ul>
<li class="chapter" data-level="10.6.1" data-path="exp-design.html"><a href="exp-design.html#brown-and-forsythe-redux"><i class="fa fa-check"></i><b>10.6.1</b> Brown and Forsythe redux</a></li>
<li class="chapter" data-level="10.6.2" data-path="exp-design.html"><a href="exp-design.html#meta-regression"><i class="fa fa-check"></i><b>10.6.2</b> Meta-regression</a></li>
<li class="chapter" data-level="10.6.3" data-path="exp-design.html"><a href="exp-design.html#exercise:trimmed-mean"><i class="fa fa-check"></i><b>10.6.3</b> Comparing the trimmed mean, median and mean</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="presentation-of-results.html"><a href="presentation-of-results.html"><i class="fa fa-check"></i><b>11</b> Exploring and presenting simulation results</a>
<ul>
<li class="chapter" data-level="11.1" data-path="presentation-of-results.html"><a href="presentation-of-results.html#tabulation"><i class="fa fa-check"></i><b>11.1</b> Tabulation</a>
<ul>
<li class="chapter" data-level="11.1.1" data-path="presentation-of-results.html"><a href="presentation-of-results.html#example-estimators-of-treatment-variation"><i class="fa fa-check"></i><b>11.1.1</b> Example: estimators of treatment variation</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="presentation-of-results.html"><a href="presentation-of-results.html#visualization"><i class="fa fa-check"></i><b>11.2</b> Visualization</a>
<ul>
<li class="chapter" data-level="11.2.1" data-path="presentation-of-results.html"><a href="presentation-of-results.html#example-0-rmse-in-cluster-rcts"><i class="fa fa-check"></i><b>11.2.1</b> Example 0: RMSE in Cluster RCTs</a></li>
<li class="chapter" data-level="11.2.2" data-path="presentation-of-results.html"><a href="presentation-of-results.html#example-1-biserial-correlation-estimation"><i class="fa fa-check"></i><b>11.2.2</b> Example 1: Biserial correlation estimation</a></li>
<li class="chapter" data-level="11.2.3" data-path="presentation-of-results.html"><a href="presentation-of-results.html#example-2-variance-estimation-and-meta-regression"><i class="fa fa-check"></i><b>11.2.3</b> Example 2: Variance estimation and Meta-regression</a></li>
<li class="chapter" data-level="11.2.4" data-path="presentation-of-results.html"><a href="presentation-of-results.html#example-3-heat-maps-of-coverage"><i class="fa fa-check"></i><b>11.2.4</b> Example 3: Heat maps of coverage</a></li>
<li class="chapter" data-level="11.2.5" data-path="presentation-of-results.html"><a href="presentation-of-results.html#example-4-relative-performance-of-treatment-effect-estimators"><i class="fa fa-check"></i><b>11.2.5</b> Example 4: Relative performance of treatment effect estimators</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="presentation-of-results.html"><a href="presentation-of-results.html#modeling"><i class="fa fa-check"></i><b>11.3</b> Modeling</a>
<ul>
<li class="chapter" data-level="11.3.1" data-path="presentation-of-results.html"><a href="presentation-of-results.html#example-1-biserial-revisited"><i class="fa fa-check"></i><b>11.3.1</b> Example 1: Biserial, revisited</a></li>
<li class="chapter" data-level="11.3.2" data-path="presentation-of-results.html"><a href="presentation-of-results.html#example-2-comparing-methods-for-cross-classified-data"><i class="fa fa-check"></i><b>11.3.2</b> Example 2: Comparing methods for cross-classified data</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="presentation-of-results.html"><a href="presentation-of-results.html#reporting"><i class="fa fa-check"></i><b>11.4</b> Reporting</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="building-good-visualization.html"><a href="building-good-visualization.html"><i class="fa fa-check"></i><b>12</b> Building good visualizations</a>
<ul>
<li class="chapter" data-level="12.1" data-path="building-good-visualization.html"><a href="building-good-visualization.html#subsetting-and-many-small-multiples"><i class="fa fa-check"></i><b>12.1</b> Subsetting and Many Small Multiples</a></li>
<li class="chapter" data-level="12.2" data-path="building-good-visualization.html"><a href="building-good-visualization.html#bundling"><i class="fa fa-check"></i><b>12.2</b> Bundling</a></li>
<li class="chapter" data-level="12.3" data-path="building-good-visualization.html"><a href="building-good-visualization.html#aggregation"><i class="fa fa-check"></i><b>12.3</b> Aggregation</a></li>
<li class="chapter" data-level="12.4" data-path="building-good-visualization.html"><a href="building-good-visualization.html#assessing-true-ses"><i class="fa fa-check"></i><b>12.4</b> Assessing true SEs</a></li>
<li class="chapter" data-level="12.5" data-path="building-good-visualization.html"><a href="building-good-visualization.html#the-bias-se-rmse-plot"><i class="fa fa-check"></i><b>12.5</b> The Bias-SE-RMSE plot</a></li>
<li class="chapter" data-level="12.6" data-path="building-good-visualization.html"><a href="building-good-visualization.html#assessing-estimated-ses"><i class="fa fa-check"></i><b>12.6</b> Assessing estimated SEs</a></li>
<li class="chapter" data-level="12.7" data-path="building-good-visualization.html"><a href="building-good-visualization.html#assessing-confidence-intervals"><i class="fa fa-check"></i><b>12.7</b> Assessing confidence intervals</a></li>
<li class="chapter" data-level="12.8" data-path="building-good-visualization.html"><a href="building-good-visualization.html#exercises-8"><i class="fa fa-check"></i><b>12.8</b> Exercises</a>
<ul>
<li class="chapter" data-level="12.8.1" data-path="building-good-visualization.html"><a href="building-good-visualization.html#assessing-uncertainty"><i class="fa fa-check"></i><b>12.8.1</b> Assessing uncertainty</a></li>
<li class="chapter" data-level="12.8.2" data-path="building-good-visualization.html"><a href="building-good-visualization.html#assessing-power"><i class="fa fa-check"></i><b>12.8.2</b> Assessing power</a></li>
<li class="chapter" data-level="12.8.3" data-path="building-good-visualization.html"><a href="building-good-visualization.html#going-deeper-with-coverage"><i class="fa fa-check"></i><b>12.8.3</b> Going deeper with coverage</a></li>
<li class="chapter" data-level="12.8.4" data-path="building-good-visualization.html"><a href="building-good-visualization.html#pearson-correlations-with-a-bivariate-poisson-distribution"><i class="fa fa-check"></i><b>12.8.4</b> Pearson correlations with a bivariate Poisson distribution</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="special-topics-on-reporting-simulation-results.html"><a href="special-topics-on-reporting-simulation-results.html"><i class="fa fa-check"></i><b>13</b> Special Topics on Reporting Simulation Results</a>
<ul>
<li class="chapter" data-level="13.1" data-path="special-topics-on-reporting-simulation-results.html"><a href="special-topics-on-reporting-simulation-results.html#using-regression-to-analyze-simulation-results"><i class="fa fa-check"></i><b>13.1</b> Using regression to analyze simulation results</a>
<ul>
<li class="chapter" data-level="13.1.1" data-path="special-topics-on-reporting-simulation-results.html"><a href="special-topics-on-reporting-simulation-results.html#example-1-biserial-revisited-1"><i class="fa fa-check"></i><b>13.1.1</b> Example 1: Biserial, revisited</a></li>
<li class="chapter" data-level="13.1.2" data-path="special-topics-on-reporting-simulation-results.html"><a href="special-topics-on-reporting-simulation-results.html#example-2-cluster-rct-example-revisited"><i class="fa fa-check"></i><b>13.1.2</b> Example 2: Cluster RCT example, revisited</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="special-topics-on-reporting-simulation-results.html"><a href="special-topics-on-reporting-simulation-results.html#using-regression-trees-to-find-important-factors"><i class="fa fa-check"></i><b>13.2</b> Using regression trees to find important factors</a></li>
<li class="chapter" data-level="13.3" data-path="special-topics-on-reporting-simulation-results.html"><a href="special-topics-on-reporting-simulation-results.html#analyzing-results-with-few-iterations-per-scenario"><i class="fa fa-check"></i><b>13.3</b> Analyzing results with few iterations per scenario</a>
<ul>
<li class="chapter" data-level="13.3.1" data-path="special-topics-on-reporting-simulation-results.html"><a href="special-topics-on-reporting-simulation-results.html#example-clusterrct-with-only-100-replicates-per-scenario"><i class="fa fa-check"></i><b>13.3.1</b> Example: ClusterRCT with only 100 replicates per scenario</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="special-topics-on-reporting-simulation-results.html"><a href="special-topics-on-reporting-simulation-results.html#what-to-do-with-warnings-in-simulations"><i class="fa fa-check"></i><b>13.4</b> What to do with warnings in simulations</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="case-study-comparing-different-estimators.html"><a href="case-study-comparing-different-estimators.html"><i class="fa fa-check"></i><b>14</b> Case study: Comparing different estimators</a>
<ul>
<li class="chapter" data-level="14.1" data-path="case-study-comparing-different-estimators.html"><a href="case-study-comparing-different-estimators.html#bias-variance-tradeoffs"><i class="fa fa-check"></i><b>14.1</b> Bias-variance tradeoffs</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="simulations-as-evidence.html"><a href="simulations-as-evidence.html"><i class="fa fa-check"></i><b>15</b> Simulations as evidence</a>
<ul>
<li class="chapter" data-level="15.1" data-path="simulations-as-evidence.html"><a href="simulations-as-evidence.html#strategies-for-making-relevant-simulations"><i class="fa fa-check"></i><b>15.1</b> Strategies for making relevant simulations</a>
<ul>
<li class="chapter" data-level="15.1.1" data-path="simulations-as-evidence.html"><a href="simulations-as-evidence.html#break-symmetries-and-regularities"><i class="fa fa-check"></i><b>15.1.1</b> Break symmetries and regularities</a></li>
<li class="chapter" data-level="15.1.2" data-path="simulations-as-evidence.html"><a href="simulations-as-evidence.html#make-your-simulation-general-with-an-extensive-multi-factor-experiment"><i class="fa fa-check"></i><b>15.1.2</b> Make your simulation general with an extensive multi-factor experiment</a></li>
<li class="chapter" data-level="15.1.3" data-path="simulations-as-evidence.html"><a href="simulations-as-evidence.html#use-previously-published-simulations-to-beat-them-at-their-own-game"><i class="fa fa-check"></i><b>15.1.3</b> Use previously published simulations to beat them at their own game</a></li>
<li class="chapter" data-level="15.1.4" data-path="simulations-as-evidence.html"><a href="simulations-as-evidence.html#calibrate-simulation-factors-to-real-data"><i class="fa fa-check"></i><b>15.1.4</b> Calibrate simulation factors to real data</a></li>
<li class="chapter" data-level="15.1.5" data-path="simulations-as-evidence.html"><a href="simulations-as-evidence.html#use-real-data-to-obtain-directly"><i class="fa fa-check"></i><b>15.1.5</b> Use real data to obtain directly</a></li>
<li class="chapter" data-level="15.1.6" data-path="simulations-as-evidence.html"><a href="simulations-as-evidence.html#fully-calibrated-simulations"><i class="fa fa-check"></i><b>15.1.6</b> Fully calibrated simulations</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>IV Computational Considerations</b></span></li>
<li class="chapter" data-level="16" data-path="organizing-a-simulation-project.html"><a href="organizing-a-simulation-project.html"><i class="fa fa-check"></i><b>16</b> Organizing a simulation project</a>
<ul>
<li class="chapter" data-level="16.1" data-path="organizing-a-simulation-project.html"><a href="organizing-a-simulation-project.html#well-structured-r-scripts"><i class="fa fa-check"></i><b>16.1</b> Well structured R scripts</a>
<ul>
<li class="chapter" data-level="16.1.1" data-path="organizing-a-simulation-project.html"><a href="organizing-a-simulation-project.html#about-source-command"><i class="fa fa-check"></i><b>16.1.1</b> The source command</a></li>
<li class="chapter" data-level="16.1.2" data-path="organizing-a-simulation-project.html"><a href="organizing-a-simulation-project.html#putting-headers-in-your-.r-file"><i class="fa fa-check"></i><b>16.1.2</b> Putting headers in your .R file</a></li>
<li class="chapter" data-level="16.1.3" data-path="organizing-a-simulation-project.html"><a href="organizing-a-simulation-project.html#about-keeping-tests-with-FALSE"><i class="fa fa-check"></i><b>16.1.3</b> Storing testing code in your scripts</a></li>
</ul></li>
<li class="chapter" data-level="16.2" data-path="organizing-a-simulation-project.html"><a href="organizing-a-simulation-project.html#principled-directory-structures"><i class="fa fa-check"></i><b>16.2</b> Principled directory structures</a></li>
<li class="chapter" data-level="16.3" data-path="organizing-a-simulation-project.html"><a href="organizing-a-simulation-project.html#saving-files"><i class="fa fa-check"></i><b>16.3</b> Saving simulation results</a>
<ul>
<li class="chapter" data-level="16.3.1" data-path="organizing-a-simulation-project.html"><a href="organizing-a-simulation-project.html#saving-simulations-in-general"><i class="fa fa-check"></i><b>16.3.1</b> Saving simulations in general</a></li>
<li class="chapter" data-level="16.3.2" data-path="organizing-a-simulation-project.html"><a href="organizing-a-simulation-project.html#saving-simulations-as-you-go"><i class="fa fa-check"></i><b>16.3.2</b> Saving simulations as you go</a></li>
<li class="chapter" data-level="16.3.3" data-path="organizing-a-simulation-project.html"><a href="organizing-a-simulation-project.html#dynamically-making-directories"><i class="fa fa-check"></i><b>16.3.3</b> Dynamically making directories</a></li>
<li class="chapter" data-level="16.3.4" data-path="organizing-a-simulation-project.html"><a href="organizing-a-simulation-project.html#loading-and-combining-files-of-simulation-results"><i class="fa fa-check"></i><b>16.3.4</b> Loading and combining files of simulation results</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="17" data-path="parallel-processing.html"><a href="parallel-processing.html"><i class="fa fa-check"></i><b>17</b> Parallel Processing</a>
<ul>
<li class="chapter" data-level="17.1" data-path="parallel-processing.html"><a href="parallel-processing.html#parallel-on-your-computer"><i class="fa fa-check"></i><b>17.1</b> Parallel on your computer</a></li>
<li class="chapter" data-level="17.2" data-path="parallel-processing.html"><a href="parallel-processing.html#parallel-on-a-virtual-machine"><i class="fa fa-check"></i><b>17.2</b> Parallel on a virtual machine</a></li>
<li class="chapter" data-level="17.3" data-path="parallel-processing.html"><a href="parallel-processing.html#parallel-on-a-cluster"><i class="fa fa-check"></i><b>17.3</b> Parallel on a cluster</a>
<ul>
<li class="chapter" data-level="17.3.1" data-path="parallel-processing.html"><a href="parallel-processing.html#what-is-a-command-line-interface"><i class="fa fa-check"></i><b>17.3.1</b> What is a command-line interface?</a></li>
<li class="chapter" data-level="17.3.2" data-path="parallel-processing.html"><a href="parallel-processing.html#running-a-job-on-a-cluster"><i class="fa fa-check"></i><b>17.3.2</b> Running a job on a cluster</a></li>
<li class="chapter" data-level="17.3.3" data-path="parallel-processing.html"><a href="parallel-processing.html#checking-on-a-job"><i class="fa fa-check"></i><b>17.3.3</b> Checking on a job</a></li>
<li class="chapter" data-level="17.3.4" data-path="parallel-processing.html"><a href="parallel-processing.html#running-lots-of-jobs-on-a-cluster"><i class="fa fa-check"></i><b>17.3.4</b> Running lots of jobs on a cluster</a></li>
<li class="chapter" data-level="17.3.5" data-path="parallel-processing.html"><a href="parallel-processing.html#resources-for-harvards-odyssey"><i class="fa fa-check"></i><b>17.3.5</b> Resources for Harvard’s Odyssey</a></li>
<li class="chapter" data-level="17.3.6" data-path="parallel-processing.html"><a href="parallel-processing.html#acknowledgements-1"><i class="fa fa-check"></i><b>17.3.6</b> Acknowledgements</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="18" data-path="debugging-and-testing.html"><a href="debugging-and-testing.html"><i class="fa fa-check"></i><b>18</b> Debugging and Testing</a>
<ul>
<li class="chapter" data-level="18.1" data-path="debugging-and-testing.html"><a href="debugging-and-testing.html#about-print"><i class="fa fa-check"></i><b>18.1</b> Debugging with <code>print()</code></a></li>
<li class="chapter" data-level="18.2" data-path="debugging-and-testing.html"><a href="debugging-and-testing.html#about-browser-debugging"><i class="fa fa-check"></i><b>18.2</b> Debugging with <code>browser()</code></a></li>
<li class="chapter" data-level="18.3" data-path="debugging-and-testing.html"><a href="debugging-and-testing.html#debugging-with-debug"><i class="fa fa-check"></i><b>18.3</b> Debugging with <code>debug()</code></a></li>
<li class="chapter" data-level="18.4" data-path="debugging-and-testing.html"><a href="debugging-and-testing.html#about-stopifnot"><i class="fa fa-check"></i><b>18.4</b> Protecting functions with <code>stop()</code></a></li>
<li class="chapter" data-level="18.5" data-path="debugging-and-testing.html"><a href="debugging-and-testing.html#testing-code"><i class="fa fa-check"></i><b>18.5</b> Testing code</a></li>
</ul></li>
<li class="part"><span><b>V Complex Data Structures</b></span></li>
<li class="chapter" data-level="19" data-path="sec:power.html"><a href="sec:power.html"><i class="fa fa-check"></i><b>19</b> Using simulation as a power calculator</a>
<ul>
<li class="chapter" data-level="19.1" data-path="sec:power.html"><a href="sec:power.html#getting-design-parameters-from-pilot-data"><i class="fa fa-check"></i><b>19.1</b> Getting design parameters from pilot data</a></li>
<li class="chapter" data-level="19.2" data-path="sec:power.html"><a href="sec:power.html#the-data-generating-process"><i class="fa fa-check"></i><b>19.2</b> The data generating process</a></li>
<li class="chapter" data-level="19.3" data-path="sec:power.html"><a href="sec:power.html#running-the-simulation-1"><i class="fa fa-check"></i><b>19.3</b> Running the simulation</a></li>
<li class="chapter" data-level="19.4" data-path="sec:power.html"><a href="sec:power.html#evaluating-power"><i class="fa fa-check"></i><b>19.4</b> Evaluating power</a>
<ul>
<li class="chapter" data-level="19.4.1" data-path="sec:power.html"><a href="sec:power.html#checking-validity-of-our-models"><i class="fa fa-check"></i><b>19.4.1</b> Checking validity of our models</a></li>
<li class="chapter" data-level="19.4.2" data-path="sec:power.html"><a href="sec:power.html#assessing-precision-se"><i class="fa fa-check"></i><b>19.4.2</b> Assessing Precision (SE)</a></li>
<li class="chapter" data-level="19.4.3" data-path="sec:power.html"><a href="sec:power.html#assessing-power-1"><i class="fa fa-check"></i><b>19.4.3</b> Assessing power</a></li>
<li class="chapter" data-level="19.4.4" data-path="sec:power.html"><a href="sec:power.html#assessing-minimum-detectable-effects"><i class="fa fa-check"></i><b>19.4.4</b> Assessing Minimum Detectable Effects</a></li>
</ul></li>
<li class="chapter" data-level="19.5" data-path="sec:power.html"><a href="sec:power.html#power-for-multilevel-data"><i class="fa fa-check"></i><b>19.5</b> Power for Multilevel Data</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="potential-outcomes.html"><a href="potential-outcomes.html"><i class="fa fa-check"></i><b>20</b> Simulation under the Potential Outcomes Framework</a>
<ul>
<li class="chapter" data-level="20.1" data-path="potential-outcomes.html"><a href="potential-outcomes.html#finite-vs.-superpopulation-inference"><i class="fa fa-check"></i><b>20.1</b> Finite vs. Superpopulation inference</a></li>
<li class="chapter" data-level="20.2" data-path="potential-outcomes.html"><a href="potential-outcomes.html#data-generation-processes-for-potential-outcomes"><i class="fa fa-check"></i><b>20.2</b> Data generation processes for potential outcomes</a></li>
<li class="chapter" data-level="20.3" data-path="potential-outcomes.html"><a href="potential-outcomes.html#finite-sample-performance-measures"><i class="fa fa-check"></i><b>20.3</b> Finite sample performance measures</a></li>
<li class="chapter" data-level="20.4" data-path="potential-outcomes.html"><a href="potential-outcomes.html#nested-finite-simulation-procedure"><i class="fa fa-check"></i><b>20.4</b> Nested finite simulation procedure</a></li>
</ul></li>
<li class="chapter" data-level="21" data-path="the-parametric-bootstrap.html"><a href="the-parametric-bootstrap.html"><i class="fa fa-check"></i><b>21</b> The Parametric bootstrap</a>
<ul>
<li class="chapter" data-level="21.1" data-path="the-parametric-bootstrap.html"><a href="the-parametric-bootstrap.html#air-conditioners-a-stolen-case-study"><i class="fa fa-check"></i><b>21.1</b> Air conditioners: a stolen case study</a></li>
</ul></li>
<li class="appendix"><span><b>Appendices</b></span></li>
<li class="chapter" data-level="A" data-path="coding-tidbits.html"><a href="coding-tidbits.html"><i class="fa fa-check"></i><b>A</b> Coding Reference</a>
<ul>
<li class="chapter" data-level="A.1" data-path="coding-tidbits.html"><a href="coding-tidbits.html#more-repeating-oneself"><i class="fa fa-check"></i><b>A.1</b> How to repeat yourself</a>
<ul>
<li class="chapter" data-level="A.1.1" data-path="coding-tidbits.html"><a href="coding-tidbits.html#using-replicate"><i class="fa fa-check"></i><b>A.1.1</b> Using <code>replicate()</code></a></li>
<li class="chapter" data-level="A.1.2" data-path="coding-tidbits.html"><a href="coding-tidbits.html#using-map"><i class="fa fa-check"></i><b>A.1.2</b> Using <code>map()</code></a></li>
<li class="chapter" data-level="A.1.3" data-path="coding-tidbits.html"><a href="coding-tidbits.html#map-with-no-inputs"><i class="fa fa-check"></i><b>A.1.3</b> map with no inputs</a></li>
<li class="chapter" data-level="A.1.4" data-path="coding-tidbits.html"><a href="coding-tidbits.html#other-approaches-for-repetition"><i class="fa fa-check"></i><b>A.1.4</b> Other approaches for repetition</a></li>
</ul></li>
<li class="chapter" data-level="A.2" data-path="coding-tidbits.html"><a href="coding-tidbits.html#default-arguments"><i class="fa fa-check"></i><b>A.2</b> Default arguments for functions</a></li>
<li class="chapter" data-level="A.3" data-path="coding-tidbits.html"><a href="coding-tidbits.html#profiling-code"><i class="fa fa-check"></i><b>A.3</b> Profiling Code</a>
<ul>
<li class="chapter" data-level="A.3.1" data-path="coding-tidbits.html"><a href="coding-tidbits.html#using-sys.time-and-system.time"><i class="fa fa-check"></i><b>A.3.1</b> Using <code>Sys.time()</code> and <code>system.time()</code></a></li>
<li class="chapter" data-level="A.3.2" data-path="coding-tidbits.html"><a href="coding-tidbits.html#the-tictoc-package"><i class="fa fa-check"></i><b>A.3.2</b> The <code>tictoc</code> package</a></li>
<li class="chapter" data-level="A.3.3" data-path="coding-tidbits.html"><a href="coding-tidbits.html#the-bench-package"><i class="fa fa-check"></i><b>A.3.3</b> The <code>bench</code> package</a></li>
<li class="chapter" data-level="A.3.4" data-path="coding-tidbits.html"><a href="coding-tidbits.html#profiling-with-profvis"><i class="fa fa-check"></i><b>A.3.4</b> Profiling with <code>profvis</code></a></li>
</ul></li>
<li class="chapter" data-level="A.4" data-path="coding-tidbits.html"><a href="coding-tidbits.html#optimize-code"><i class="fa fa-check"></i><b>A.4</b> Optimizing code (and why you often shouldn’t)</a>
<ul>
<li class="chapter" data-level="A.4.1" data-path="coding-tidbits.html"><a href="coding-tidbits.html#hand-building-functions"><i class="fa fa-check"></i><b>A.4.1</b> Hand-building functions</a></li>
<li class="chapter" data-level="A.4.2" data-path="coding-tidbits.html"><a href="coding-tidbits.html#sec_comp_efficiency"><i class="fa fa-check"></i><b>A.4.2</b> Computational efficiency versus simplicity</a></li>
<li class="chapter" data-level="A.4.3" data-path="coding-tidbits.html"><a href="coding-tidbits.html#reusing-code-to-speed-up-computation"><i class="fa fa-check"></i><b>A.4.3</b> Reusing code to speed up computation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="B" data-path="further-readings-and-resources.html"><a href="further-readings-and-resources.html"><i class="fa fa-check"></i><b>B</b> Further readings and resources</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Designing Monte Carlo Simulations in R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="performance-criteria" class="section level1 hasAnchor" number="9">
<h1><span class="header-section-number">Chapter 9</span> Performance metrics<a href="performance-criteria.html#performance-criteria" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Once we run a simulation, we end up with a pile of results to sort through.
For example, Figure <a href="performance-criteria.html#fig:CRT-ATE-hist">9.1</a> depicts the distribution of average treatment effect estimates from the cluster-randomized experiment simulation, which we generated in Chapter <a href="running-the-simulation-process.html#running-the-simulation-process">8</a>.
There are three different estimators, each with 1000 replications.
Each histogram is an approximation of the <em>sampling distribution</em> of the estimator, meaning its distribution across repetitions of the data-generating process.
With results such as these, the question before us is now how to evaluate how well these procedures worked. And, if we are comparing several different estimators, how do we determine which ones work better or worse than others? In this chapter, we look at a variety of <strong>performance metrics</strong> that can answer these questions.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:CRT-ATE-hist"></span>
<img src="Designing-Simulations-in-R_files/figure-html/CRT-ATE-hist-1.png" alt="Sampling distribution of average treatment effect estimates from a cluster-randomized trial with a true average treatment effect of 0.3." width="75%" />
<p class="caption">
Figure 9.1: Sampling distribution of average treatment effect estimates from a cluster-randomized trial with a true average treatment effect of 0.3.
</p>
</div>
<p>Performance metrics are summaries of a sampling distribution—formulas for boiling down the thousands of values from the simulation into a smaller set of numbers. Performance metrics describe how an estimator or data analysis procedure behaves on average if we could repeat the data-generating process an infinite number of times.
For example, the bias of an estimator is the difference between the average value of the estimator and the corresponding target parameter. Bias measures the central tendency of the sampling distribution, capturing how far off, on average, the estimator would be from the true parameter value if we repeated the data-generating process an infinite number of times.
In Figure <a href="performance-criteria.html#fig:CRT-ATE-hist">9.1</a>, black dashed lines mark the true average treatment effect of 0.3 and the colored vertical lines with circles at the end mark the means of the estimators.
Bias is therefore the distance between the colored lines and the black dashed lines.
This distance is nearly zero for the aggregation estimator and the multilevel model estimator, but larger for the linear regression estimator.</p>
<p>Different types of data-analysis results produce different types of information, and so the relevant set of performance metrics depends on the type of data analysis result under evaluation.
For procedures that produce point estimates or point predictions, conventional performance metrics include bias, variance, and root mean squared error.
If the point estimates come with corresponding standard errors, then we may also want to evaluate how accurately the standard errors represent the true uncertainty of the point estimators; conventional performance metrics for capturing this include the relative bias and relative root mean squared error of the variance estimator.
For procedures that produce confidence intervals or other types of interval estimates, conventional performance metrics include the coverage rate and average interval width.
Finally, for inferential procedures that involve hypothesis tests (or more generally, classification tasks), conventional performance metrics include Type I error rates and power.
We describe each of these metrics in Sections <a href="performance-criteria.html#assessing-point-estimators">9.1</a> through <a href="performance-criteria.html#assessing-inferential-procedures">9.4</a>.</p>
<p>Performance metrics are defined with respect to sampling distributions, or the results of applying a data analysis procedure to data generated according to a particular process across an infinite number of replications.
In defining specific metrics, we will use conventional statistical notation for the means, variances, and other moments of the sampling distribution.
Specifically, we will use the expectation operator <span class="math inline">\(\E()\)</span> to denote the mean of a sampling distribution, <span class="math inline">\(\M()\)</span> to denote the median of a sampling distribution, <span class="math inline">\(\Var()\)</span> to denote the variance of a sampling distribution, and <span class="math inline">\(\Prob()\)</span> to denote probabilities of specific outcomes with respect to the sampling distribution.
We will use <span class="math inline">\(\Q_p()\)</span> to denote the <span class="math inline">\(p^{th}\)</span> quantile of a distribution, which is the value <span class="math inline">\(x\)</span> such that <span class="math inline">\(\Prob(T \leq x) = p\)</span> (so <span class="math inline">\(\M() = \Q_{0.5}()\)</span>).</p>
<p>For some simple combinations of data-generating processes and data analysis procedures, it may be possible to derive exact mathematical formulas for calculating some performance metrics (such as exact mathematical expressions for the bias and variance of the linear regression estimator).<br />
But for many problems, the math is difficult or intractable—that’s why we do simulations in the first place.
Simulations do not produce the <em>exact</em> sampling distribution or give us <em>exact</em> values of performance metrics.
Instead, simulations yield <em>samples</em>—usually large samples—from the the sampling distribution, and we can use these to compute <em>estimates</em> of the performance metrics of interest.
In Figure <a href="performance-criteria.html#fig:CRT-ATE-hist">9.1</a>, we calculated the bias of each estimator by taking the mean of 1000 observations from its sampling distribution; if we were to repeat the whole set of calculations (with a different seed), then our bias results would shift slightly.</p>
<p>In working with simulation results, it is important to keep track of the degree of uncertainty in performance metric estimates.
We call such uncertainty <em>Monte Carlo error</em> because it is the error arising from using a finite number of replications of the Monte Carlo simulation process.
One way to quantify it is with the <em>Monte Carlo standard error (MCSE)</em>, or the standard error of a performance estimate based on a finite number of replications.
Just as when we analyze real, empirical data, we can apply statistical techniques to estimate the MCSE and even to generate confidence intervals for performance metrics.</p>
<p>The size of an MCSE is driven by how many replications we use: if we only use a few, we will have noisy estimates of performance with large MCSEs; if we use millions of replications, the MCSE will be tiny.
It is important to keep in mind that the MCSE is not measuring anything about how a data analysis procedure performs in general.
It only describes how precisely we have approximated a performance criterion, an artifact of how we conducted the simulation.
Moreover, MCSEs are under our control.
Given a desired MCSE, we can determine how many replications we would need to ensure our performance estimates have the specified level of precision.
Section <a href="performance-criteria.html#MCSE">9.9</a> provides details about how to compute MCSEs for conventional performance measures, along with some discussion of general techniques for computing MCSE for less conventional measures.</p>
<div id="assessing-point-estimators" class="section level2 hasAnchor" number="9.1">
<h2><span class="header-section-number">9.1</span> Metrics for Point Estimators<a href="performance-criteria.html#assessing-point-estimators" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The most common performance measures used to assess a point estimator are bias, variance, mean squared error, and root mean squared error).
Bias compares the mean of the sampling distribution to the target parameter.
Positive bias implies that the estimator tends to systematically over-state the quantity of interest, while negative bias implies that it systematically under-shoots the quantity of interest.
If bias is zero (or nearly zero), we say that the estimator is unbiased (or approximately unbiased).
Variance (or its square root, the true standard error) describes the spread of the sampling distribution, or the extent to which it varies around its central tendency.
All else equal, we would like estimators to have low variance (or to be more precise).
Root mean squared error (RMSE) is a conventional measure of the overall accuracy of an estimator, or its average degree of error with respect to the target parameter.
For absolute assessments of performance, an estimator with low bias, low variance, and thus low RMSE is desired.
In making comparisons of several different estimators, one with lower RMSE is usually preferable to one with higher RMSE.
If two estimators have comparable RMSE, then the estimator with lower bias would usually be preferable.</p>
<p>To define these quantities more precisely, let’s consider a generic estimator <span class="math inline">\(T\)</span> that is targeting a parameter <span class="math inline">\(\theta\)</span>.
We call the target parameter the <em>estimand</em>.
In most cases, in running our simulation we set the estimand <span class="math inline">\(\theta\)</span> and then generate a (typically large) series of <span class="math inline">\(R\)</span> datasets, for each of which <span class="math inline">\(\theta\)</span> is the true target parameter.
We then analyze each dataset, obtaining a sample of estimates <span class="math inline">\(T_1,...,T_R\)</span>.
Formally, the bias, variance, and RMSE of <span class="math inline">\(T\)</span> are defined as
<span class="math display" id="eq:bias-variance-RMSE">\[
\begin{aligned}
\Bias(T) &amp;= \E(T) - \theta, \\
\Var(T) &amp;= \E\left[\left(T - \E (T)\right)^2 \right], \\
\RMSE(T) &amp;= \sqrt{\E\left[\left(T - \theta\right)^2 \right]}.
\end{aligned}
\tag{9.1}
\]</span>
These three measures are inter-connected.
In particular, RMSE is the combination of (squared) bias and variance, as in
<span class="math display" id="eq:RMSE-decomposition">\[
\left[\RMSE(T)\right]^2 = \left[\Bias(T)\right]^2 + \Var(T). \tag{9.2}
\]</span></p>
<p>When conducting a simulation, we do not compute these performance measures directly but rather must estimate them using the replicates <span class="math inline">\(T_1,...,T_R\)</span> generated from the sampling distribution.
There’s nothing very surprising about how we construct estimates of the performance measures.
It is just a matter of substituting sample quantities in place of the expectations and variances.
Specifically, we estimate bias by taking
<span class="math display" id="eq:bias-estimator">\[
\widehat{\Bias}(T) = \bar{T} - \theta, \tag{9.3}
\]</span>
where <span class="math inline">\(\bar{T}\)</span> is the arithmetic mean of the replicates,
<span class="math display">\[
\bar{T} = \frac{1}{R}\sum_{r=1}^R T_r.
\]</span>
We estimate variance by taking the sample variance of the replicates, as
<span class="math display" id="eq:var-estimator">\[
S_T^2 = \frac{1}{R - 1}\sum_{r=1}^R \left(T_r - \bar{T}\right)^2. \tag{9.4}
\]</span>
The square root of <span class="math inline">\(S^2_T\)</span>, <span class="math inline">\(S_T\)</span> is an estimate of the true standard error of <span class="math inline">\(T\)</span>, or the standard deviation of the estimator across an infinite set of replications of the data-generating process.<a href="#fn12" class="footnote-ref" id="fnref12"><sup>12</sup></a>
We usually prefer to work with the true SE <span class="math inline">\(S_T\)</span> rather than the sampling variance <span class="math inline">\(S_T^2\)</span> because the former quantity has the same units as the target parameter.</p>
<p>Finally, the RMSE estimate can be calculated as
<span class="math display" id="eq:rmse-estimator">\[
\widehat{\RMSE}(T) = \sqrt{\frac{1}{R} \sum_{r = 1}^R \left( T_r - \theta\right)^2 }.  \tag{9.5}
\]</span>
Often, people talk about the MSE (Mean Squared Error), which is just the square of RMSE.
Just like the true SE is usually easier to interpret than the sampling variance, we find the units of RMSE easier to interpret than the units of MSE.</p>
<p>It is important to recognize that the above performance measures depend on the scale of the parameter.
For example, if our estimators are measuring a treatment impact in dollars, then the bias, SE, and RMSE of the estimators are all in dollars.
The variance and MSE would be in dollars squared, which is why we take their square roots to put them back on the more intepretable scale of dollars.</p>
<p>In many simulations, the scale of the outcome is an arbitrary feature of the data-generating process, making the absolute magnitude of performance metrics less meaningful.
To ease interpretation of performance metrics, it is useful to consider their magnitude relative to the baseline level of variation in the outcome.
One way to achieve this is to generate data so the outcome has unit variance (i.e., we generate outcomes in <em>standardized units</em>).
Doing so puts the bias, true standard error, and root mean-squared error on the scale of standard deviation units, which can facilitate interpretation about what constitutes a meaningfully large bias or a meaningful difference in RMSE.</p>
<p>In addition to understanding the scale of these performance metrics, it is also important to recognize that their magnitude depends on the scale of the parameter.
A non-linear transformation of a parameter will generally lead to changes in the magnitude of the performance metrics.
For instance, suppose that <span class="math inline">\(\theta\)</span> measures the proportion of time that something occurs.
One natural way to transform this parameter would be to put it on the log-odds (logit) scale.
However, because of the non-linear aspect of the logit,
<span class="math display">\[\text{Bias}\left[\text{logit}(T)\right] \neq \text{logit}\left(\text{Bias}[T]\right), \qquad \text{RMSE}\left[\text{logit}(T)\right] \neq \text{logit}\left(\text{RMSE}[T]\right),\]</span>
and so on.
This is a consequence of how the performance metrics are defined.
One might see this property as a limitation on the utility of using bias and RMSE to measure the performance of an estimator, because these metrics can be quite sensitive to the scale of the parameter.</p>
<div id="clusterRCTperformance" class="section level3 hasAnchor" number="9.1.1">
<h3><span class="header-section-number">9.1.1</span> Comparing the Performances of the Cluster RCT Estimation Procedures<a href="performance-criteria.html#clusterRCTperformance" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We demonstrate the calculation of performance metrics for the point estimators of average treatment effects in our cluster-RCT example.
In Chapter <a href="running-the-simulation-process.html#running-the-simulation-process">8</a>, we generated a large set of replications of several different treatment effect estimators.
Using these results, we can assess the bias, standard error, and RMSE of our three different estimators of the ATE.
These performance metrics address the following questions:</p>
<ul>
<li>Is the estimator systematically off? (bias)</li>
<li>Is it precise? (standard error)</li>
<li>Does it predict well? (RMSE)</li>
</ul>
<p>Let us see how the three estimators compare on these metrics.</p>
<div id="are-the-estimators-biased" class="section level4 unnumbered hasAnchor">
<h4>Are the estimators biased?<a href="performance-criteria.html#are-the-estimators-biased" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Bias is defined with respect to a target estimand.
Here we assess whether our estimates are systematically different from the <span class="math inline">\(\gamma_1\)</span> parameter, which we defined in standardized units by setting the standard deviation of the student-level distribution of the outcome equal to one.
For these data, we generated data based on a school-level ATE parameter of 0.30 SDs.</p>
<div class="sourceCode" id="cb294"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb294-1"><a href="performance-criteria.html#cb294-1" tabindex="-1"></a>ATE <span class="ot">&lt;-</span> <span class="fl">0.30</span></span>
<span id="cb294-2"><a href="performance-criteria.html#cb294-2" tabindex="-1"></a></span>
<span id="cb294-3"><a href="performance-criteria.html#cb294-3" tabindex="-1"></a>runs <span class="sc">%&gt;%</span> </span>
<span id="cb294-4"><a href="performance-criteria.html#cb294-4" tabindex="-1"></a>  <span class="fu">group_by</span>( method ) <span class="sc">%&gt;%</span></span>
<span id="cb294-5"><a href="performance-criteria.html#cb294-5" tabindex="-1"></a>  <span class="fu">summarise</span>( </span>
<span id="cb294-6"><a href="performance-criteria.html#cb294-6" tabindex="-1"></a>    <span class="at">mean_ATE_hat =</span> <span class="fu">mean</span>( ATE_hat ),</span>
<span id="cb294-7"><a href="performance-criteria.html#cb294-7" tabindex="-1"></a>    <span class="at">bias =</span> <span class="fu">mean</span>( ATE_hat ) <span class="sc">-</span> ATE</span>
<span id="cb294-8"><a href="performance-criteria.html#cb294-8" tabindex="-1"></a>  )</span></code></pre></div>
<pre><code>## # A tibble: 3 × 3
##   method mean_ATE_hat    bias
##   &lt;chr&gt;         &lt;dbl&gt;   &lt;dbl&gt;
## 1 Agg           0.306 0.00561
## 2 LR            0.390 0.0899 
## 3 MLM           0.308 0.00788</code></pre>
<p>There is no indication of major bias for aggregation or multi-level modeling.
Linear regression, with a bias of about 0.09 SDs, appears about ten times as biased as the other estimators.
This is because the linear regression is targeting the person-level average average treatment effect.
The data-generating process of this simulation makes larger sites have larger effects, so the person-level average effect is going to be higher because those larger sites will count more.
In contrast, our estimand is the school-level average treatment effect, i.e., the simple average of each school’s true impact, which we have set to 0.30.
The aggregation and multi-level modeling methods target this school-level average effect, putting them in line with our DGP.
If we had instead decided that the target estimand should be the person-level average effect, then we would find that linear regression is unbiased whereas aggregation and multi-level modeling are biased.
This illustrates how crucial it is to think carefully about the appropriate target parameter and to assess performance with respect to a well-justified and clearly articulated target.</p>
</div>
<div id="which-method-has-the-smallest-standard-error" class="section level4 unnumbered hasAnchor">
<h4>Which method has the smallest standard error?<a href="performance-criteria.html#which-method-has-the-smallest-standard-error" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The true standard error measures the degree of variability in a point estimator.
It reflects how stable our estimates are across replications of the data-generating process.
We calculate the standard error by taking the standard deviation of the replications of each estimator.
For purposes of interpretation, it is useful to compare the true standard errors to the variation in a benchmark estimator.
Here, we treat the linear regression estimator as the benchmark and compute the magnitude of the true SEs of each method <em>relative</em> to the SE of the linear regression estimator:</p>
<div class="sourceCode" id="cb296"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb296-1"><a href="performance-criteria.html#cb296-1" tabindex="-1"></a>true_SE <span class="ot">&lt;-</span> </span>
<span id="cb296-2"><a href="performance-criteria.html#cb296-2" tabindex="-1"></a>  runs <span class="sc">%&gt;%</span> </span>
<span id="cb296-3"><a href="performance-criteria.html#cb296-3" tabindex="-1"></a>  <span class="fu">group_by</span>( method ) <span class="sc">%&gt;%</span></span>
<span id="cb296-4"><a href="performance-criteria.html#cb296-4" tabindex="-1"></a>  <span class="fu">summarise</span>( </span>
<span id="cb296-5"><a href="performance-criteria.html#cb296-5" tabindex="-1"></a>    <span class="at">SE =</span> <span class="fu">sd</span>( ATE_hat )</span>
<span id="cb296-6"><a href="performance-criteria.html#cb296-6" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb296-7"><a href="performance-criteria.html#cb296-7" tabindex="-1"></a>  <span class="fu">mutate</span>( </span>
<span id="cb296-8"><a href="performance-criteria.html#cb296-8" tabindex="-1"></a>    <span class="at">per_SE =</span> SE <span class="sc">/</span> SE[method<span class="sc">==</span><span class="st">&quot;LR&quot;</span>] </span>
<span id="cb296-9"><a href="performance-criteria.html#cb296-9" tabindex="-1"></a>  )</span>
<span id="cb296-10"><a href="performance-criteria.html#cb296-10" tabindex="-1"></a></span>
<span id="cb296-11"><a href="performance-criteria.html#cb296-11" tabindex="-1"></a>true_SE</span></code></pre></div>
<pre><code>## # A tibble: 3 × 3
##   method    SE per_SE
##   &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;
## 1 Agg    0.168  0.916
## 2 LR     0.183  1    
## 3 MLM    0.168  0.916</code></pre>
<p>In a real data analysis, these standard errors are what we would be trying to approximate with a standard error estimator.
Aggregation and multi-level modeling have SEs about 8% smaller than Linear Regression.
For these data-generating conditions, aggregation and multi-level modeling are preferable to linear regression because they are more precise.</p>
</div>
<div id="which-method-has-the-smallest-root-mean-squared-error" class="section level4 unnumbered hasAnchor">
<h4>Which method has the smallest Root Mean Squared Error?<a href="performance-criteria.html#which-method-has-the-smallest-root-mean-squared-error" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>So far linear regression is not doing well: it has more bias and a larger standard error than the other two estimators.
We can assess overall accuracy by combining these two quantities with the RMSE:</p>
<div class="sourceCode" id="cb298"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb298-1"><a href="performance-criteria.html#cb298-1" tabindex="-1"></a>runs <span class="sc">%&gt;%</span> </span>
<span id="cb298-2"><a href="performance-criteria.html#cb298-2" tabindex="-1"></a>  <span class="fu">group_by</span>( method ) <span class="sc">%&gt;%</span></span>
<span id="cb298-3"><a href="performance-criteria.html#cb298-3" tabindex="-1"></a>  <span class="fu">summarise</span>( </span>
<span id="cb298-4"><a href="performance-criteria.html#cb298-4" tabindex="-1"></a>    <span class="at">bias =</span> <span class="fu">mean</span>( ATE_hat <span class="sc">-</span> ATE ),</span>
<span id="cb298-5"><a href="performance-criteria.html#cb298-5" tabindex="-1"></a>    <span class="at">SE =</span> <span class="fu">sd</span>( ATE_hat ),</span>
<span id="cb298-6"><a href="performance-criteria.html#cb298-6" tabindex="-1"></a>    <span class="at">RMSE =</span> <span class="fu">sqrt</span>( <span class="fu">mean</span>( (ATE_hat <span class="sc">-</span> ATE)<span class="sc">^</span><span class="dv">2</span> ) )</span>
<span id="cb298-7"><a href="performance-criteria.html#cb298-7" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb298-8"><a href="performance-criteria.html#cb298-8" tabindex="-1"></a>  <span class="fu">mutate</span>( </span>
<span id="cb298-9"><a href="performance-criteria.html#cb298-9" tabindex="-1"></a>    <span class="at">per_RMSE =</span> RMSE <span class="sc">/</span> RMSE[method<span class="sc">==</span><span class="st">&quot;LR&quot;</span>]</span>
<span id="cb298-10"><a href="performance-criteria.html#cb298-10" tabindex="-1"></a>  )</span></code></pre></div>
<pre><code>## # A tibble: 3 × 5
##   method    bias    SE  RMSE per_RMSE
##   &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;
## 1 Agg    0.00561 0.168 0.168    0.823
## 2 LR     0.0899  0.183 0.204    1    
## 3 MLM    0.00788 0.168 0.168    0.823</code></pre>
<p>We also include SE and bias as points of reference.</p>
<p>RMSE takes into account both bias and variance.
For aggregation and multi-level modeling, the RMSE is the same as the standard error, which makes sense because these estimators are not biased.
For linear regression, the combination of bias plus increased variability yields a higher RMSE, with the standard error dominating the bias term (note how RMSE and SE are more similar than RMSE and bias).
This is especially the case because RMSE is the square root of the bias and standard errors <em>squared</em>, which makes the difference between them even more extreme.
Overall, aggregation and multi-level modeling have RMSEs around 17% smaller than linear regression—a consequential difference in accuracy.</p>
</div>
</div>
<div id="less-conventional-performance-metrics" class="section level3 hasAnchor" number="9.1.2">
<h3><span class="header-section-number">9.1.2</span> Less Conventional Performance metrics<a href="performance-criteria.html#less-conventional-performance-metrics" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Depending on the model and estimation procedures being examined, a range of different metrics might be used to assess estimator performance.
For point estimation, we have introduced bias, variance and MSE as the three core measures of performance.
If an estimator generally does well, except for an occasional large mistake, these conventional measures will indicate very poor overall performance because they are based on arithmetic averages.
Other metrics exist, such as the median bias and the median absolute deviation of <span class="math inline">\(T\)</span>, which are less sensitive to outliers in the sampling distribution compared to the conventional metrics.</p>
<p>Median bias is an alternative measure of the central tendency of a sampling distribution.
Positive median bias implies that more than 50% of the sampling distribution exceeds the quantity of interest, while negative median bias implies that more than 50% of the sampling distribution fall below the quantity of interest.
Formally,
<span class="math display" id="eq:median-bias">\[
\text{Median-Bias}(T) = \M(T) - \theta \tag{9.6}.
\]</span>
An estimator of median bias is computed using the sample median of <span class="math inline">\(T_1,...,T_R\)</span>.</p>
<p>Another robust measure of central tendency is based on winsorizing the sampling distribution, or truncating all errors larger than a certain maximum size.
Using a winsorized distribution amounts to arguing that you don’t care about errors beyond a certain size, so anything beyond a certain threshold will be treated the same as if it were exactly on the threshold.
The threshold for truncation is usually defined relative to the first and third quartiles of the sampling distribution, along with a given span of the inter-quartile range.
The thresholds for truncation are taken as
<span class="math display">\[
L_w = \Q_{0.25}(T) - w \times (\Q_{0.75}(T) - \Q_{0.25}(T)) \quad \text{and} \quad U_w = \Q_{0.75}(T) + w \times (\Q_{0.75}(T) - \Q_{0.25}(T)),
\]</span>
where <span class="math inline">\(\Q_{0.25}(T)\)</span> and <span class="math inline">\(\Q_{0.75}(T)\)</span> are the first and third quartiles of the distribution of <span class="math inline">\(T\)</span>, respectively, and <span class="math inline">\(w\)</span> is the number of inter-quartile ranges below which an observation will be treated as an outlier.<a href="#fn13" class="footnote-ref" id="fnref13"><sup>13</sup></a>
Let <span class="math inline">\(T^{(w)} = \min\{\max\{T, L_w\}, U_w\}\)</span>.
The winsorized bias is then defined as
<span class="math display" id="eq:winsorized-bias">\[
\text{Winz-Bias}(T; w) = \E\left(T^{(w)}\right) - \theta. \tag{9.7}
\]</span>
Alternative measures of spread and overall accuracy can be defined along similar lines, using winsorized values in place of the raw values of <span class="math inline">\(T\)</span>.
Specifically,
<span class="math display" id="eq:winsorized-variance-RMSE">\[
\begin{aligned}
\Var(T) &amp;= \E\left[\left(T^{(w)} - \E (T^{(w)})\right)^2 \right], \\
\RMSE(T) &amp;= \sqrt{\E\left[\left(T^{(w)} - \theta\right)^2 \right]}.
\end{aligned}
\tag{9.8}
\]</span></p>
<p>To compute estimates of the winsorized performance criteria, we substitute sample quantiles in place of <span class="math inline">\(\Q_{0.25}(T)\)</span> and <span class="math inline">\(\Q_{0.25}(T)\)</span> to get estimated thresholds, <span class="math inline">\(\hat{L}_w\)</span> and <span class="math inline">\(\hat{U}_w\)</span>, find <span class="math inline">\(\hat{T}_r^{(w)} = \min\{\max\{T_r, \hat{L}_w\}, \hat{U}_w\}\)</span>, and compute the sample performance metrics using Equations <a href="performance-criteria.html#eq:bias-estimator">(9.3)</a>, <a href="performance-criteria.html#eq:var-estimator">(9.4)</a>, and <a href="performance-criteria.html#eq:rmse-estimator">(9.5)</a>, but with <span class="math inline">\(\hat{T}_r^{(w)}\)</span> in place of <span class="math inline">\(T_r\)</span>.</p>
<p>A further robust measure of central tendency used the <span class="math inline">\(p \times 100\%\)</span>-trimmed mean, which ignores the estimates in the lowest and highest <span class="math inline">\(p\)</span>-quantiles of the sampling distribution.
Formally, the trimmed-mean bias is
<span class="math display" id="eq:trimmed-bias">\[
\text{Trimmed-Bias}(T; p) = \E\left[ T \left| \Q_{p}(T) &lt; T &lt; \Q_{(1 - p)}(T) \right.\right] - \theta. \tag{9.9}
\]</span>
Median bias is thus a special case of trimmed mean bias, with <span class="math inline">\(p = 0.5\)</span>.
To estimate the trimmed bias, we use sample quantiles <span class="math inline">\(\hat{Q}_p\)</span> and <span class="math inline">\(\hat{Q}_{(1 - p)}\)</span> and take the mean of the middle <span class="math inline">\(1 - 2p\)</span> fraction of the distribution
<span class="math display" id="eq:sample-trimmed-bias">\[
\widehat{\text{Trimmed-Bias}}(T; p) = \frac{1}{(1 - 2p)R} \sum_{r=1}^R T_r \times I\left(\hat{Q}_{p} &lt; T &lt; \hat{Q}_{(1 - p)}\right) - \theta. \tag{9.10}
\]</span></p>
<p>For a symmetric sampling distribution, winsorized bias and trimmed-mean bias will be the same as the conventional (mean) bias, but will be less affected by outlying values (i.e., values of <span class="math inline">\(T\)</span> very far from the center of the distribution).
However, if a sampling distribution is not symmetric, winsorized bias and trimmed-mean bias become distinct performance measures, which put less emphasis on large errors compared to the conventional bias metric.</p>
<p>Alternative measures of the overall accuracy of an estimator can also be defined along similar lines.
For instance, an alternative to RMSE is to use the median absolute error (MAE), defined as
<span class="math display">\[
\text{MAE} = \M\left(\left|T - \theta\right|\right),
\]</span>
which can be estimated by using the sample median in place of <span class="math inline">\(\M()\)</span>.
Many robust measures of the spread of the sampling distribution are also available, including the Rosseeuw-Croux scale estimator <span class="math inline">\(Q_n\)</span> <span class="citation">(<a href="#ref-Rousseeuw1993alternatives">Rousseeuw and Croux 1993</a>)</span> and the biweight midvariance <span class="citation">(<a href="#ref-Wilcox2022introduction">Wilcox 2022</a>)</span>.
<span class="citation">Maronna, Martin, and Yohai (<a href="#ref-Maronna2006robust">2006</a>)</span> provide a useful introduction to these metrics and robust statistics more broadly.
The <code>robustbase</code> package <span class="citation">(<a href="#ref-robustbase">Maechler et al. 2024</a>)</span> provides a convenient interface for calculating many robust statistics.</p>
</div>
</div>
<div id="metrics-for-standard-error-estimators" class="section level2 hasAnchor" number="9.2">
<h2><span class="header-section-number">9.2</span> Metrics for Standard Error Estimators<a href="performance-criteria.html#metrics-for-standard-error-estimators" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Statistics is concerned not only with how to estimate things, but also with assessing how good an estimate is—that is, understanding the extent of uncertainty in estimates of target parameters.
These concerns apply for Monte Carlo simulation studies as well.
In a simulation, we can simply compute an estimator’s actual properties.
When we use an estimator with real data, we need to estimate its associated standard error and generate confidence intervals and other assessments of uncertainty.
To understand if these uncertainty assessments would work in practice, we need to evaluate not only the behavior of the estimator itself, but also the behavior of these associated quantities.
In other words, we generally want to know not only whether a point estimator is doing a good job, but also whether we can obtain a good standard error for that point estimator.</p>
<p>Commonly used metrics for quantifying the performance of estimated standard errors include relative bias, relative variance, and relative root mean squared error.
These metrics are defined in relative termss (rather than absolute ones) by comparing their magnitude to the <em>true</em> degree of uncertainty.
In addition, it is also useful to examine the correlation between standard error estimators and actual error (i.e., <span class="math inline">\(\left|T - \theta \right|\)</span>).
Good estimates of uncertainty should predict error in a given context <span class="citation">(especially if calculating conditional estimates; see <a href="#ref-sundberg2003conditional">Sundberg 2003</a>)</span>.</p>
<p>Typically, performance metrics are computed for <em>variance</em> estimators rather than standard error estimators.
There are a few reasons for working with variance rather than standard error.
First, in practice, so-called unbiased standard errors usually are not in fact actually unbiased (see the delightfully titled section 11.5, “The Joke Is on Us: The Standard Deviation Estimator is Biased after All,” in <span class="citation">Westfall and Henning (<a href="#ref-westfall2013understanding">2013</a>)</span> for further discussion).
For linear regression, for example, the classic standard error estimator is an unbiased <em>variance</em> estimator, but
<span class="math display">\[
\E[ \sqrt{ V } ] \neq \sqrt{ \E[ V ] },
\]</span>
so the standard error estimator is not exactly unbiased.
Variance is also the metric that gives us the bias-variance decomposition of <span class="math inline">\(MSE = Variance + Bias^2\)</span>. Thus, if we are trying to determine whether MSE is due to instability or systematic bias, operating in this squared space may be preferable.</p>
<p>To make this concrete, let us consider a generic standard error estimator <span class="math inline">\(\widehat{SE}\)</span> to go along with our generic estimator <span class="math inline">\(T\)</span> of target parameter <span class="math inline">\(\theta\)</span>, and let <span class="math inline">\(V = \widehat{SE}^2\)</span>.
The simulation yields a large sample of standard errors, <span class="math inline">\(\widehat{SE}_1,...,\widehat{SE}_R\)</span> and variance estimators <span class="math inline">\(V_r = \widehat{SE}_r^2\)</span> for <span class="math inline">\(r = 1,...,R\)</span>.
Formally, the relative bias, variance, and RMSE of <span class="math inline">\(V\)</span> are defined as
<span class="math display" id="eq:relative-bias-variance-RMSE">\[
\begin{aligned}
\text{Relative Bias}(V) &amp;= \frac{\E(V)}{\Var(T)} \\
\text{Relative Var}(V) &amp;= \frac{\Var(V)}{\Var(T)} \\
\text{Relative RMSE}(V) &amp;= \frac{\sqrt{\E\left[\left(V - \Var(T)\right)^2 \right]}}{\Var(T)}.
\end{aligned}
\tag{9.11}
\]</span>
In contrast to performance metrics for <span class="math inline">\(T\)</span>, we define these metrics in relative terms because the raw magnitude of <span class="math inline">\(V\)</span> is not a stable or interpretable parameter.
Instead, it will generally depend on many of the parameters of the data-generating process, including the sample size and any other design parameters.
Defining bias in relative terms makes for a more interpretable metric: a value of 1 corresponds to exact unbiasedness of the variance estimator.
Relative bias measures <em>proportionate</em> under- or over-estimation.
For example, a relative bias of 1.12 would mean the standard error was, on average, 12% too large.
We discuss relative performance measures further in Section <a href="performance-criteria.html#sec-relative-performance">9.5</a>.</p>
<p>To estimate these relative performance measures, we proceed by substituting sample quantities in place of the expectations and variances.
In contrast to the performance metrics for <span class="math inline">\(T\)</span>, we will not generally be able to compute the true degree of uncertainty exactly.
Instead, we must estimate the target quantity <span class="math inline">\(\Var(T)\)</span> using <span class="math inline">\(S_T^2\)</span>, the sample variance of <span class="math inline">\(T\)</span> across replications.
Denoting the arithmetic mean of the variance estimates as
<span class="math display">\[
\bar{V} = \frac{1}{R} \sum_{r=1}^R V_r
\]</span>
and the sample variance as
<span class="math display">\[
S_V^2 = \frac{1}{R - 1}\sum_{r=1}^R \left(V_r - \bar{V}\right)^2,
\]</span>
we estimate the relative bias, variance, and RMSE of <span class="math inline">\(V\)</span> using
<span class="math display" id="eq:relative-bias-variance-RMSE-estimators">\[
\begin{aligned}
\widehat{\text{Relative Bias}}(V) &amp;= \frac{\bar{V}}{S_T^2} \\
\widehat{\text{Relative Var}}(V) &amp;= \frac{S_V^2}{S_T^2} \\
\widehat{\text{Relative RMSE}}(V) &amp;= \frac{\sqrt{\frac{1}{R}\sum_{r=1}^R\left(V_r - S_T^2\right)^2}}{S_T^2}.
\end{aligned}
\tag{9.12}
\]</span></p>
<div id="assessing-ses-for-our-cluster-rct-simulation" class="section level3 hasAnchor" number="9.2.1">
<h3><span class="header-section-number">9.2.1</span> Assessing SEs for Our Cluster RCT Simulation<a href="performance-criteria.html#assessing-ses-for-our-cluster-rct-simulation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>To assess whether our estimated SEs are about right, we can look at the average <em>estimated</em> (squared) standard error and compare it to the true standard error.
Our standard errors are <em>inflated</em> if they are systematically larger than they should be, across the simulation runs.
We can also look at how stable our standard error estimates are, by taking the standard deviation of our standard error estimates.
We interpret this quantity relative to the actual standard error to get how far off, as a percent of the actual standard error, we tend to be.</p>
<div class="sourceCode" id="cb300"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb300-1"><a href="performance-criteria.html#cb300-1" tabindex="-1"></a>runs <span class="sc">%&gt;%</span>  <span class="fu">group_by</span>( method ) <span class="sc">%&gt;%</span></span>
<span id="cb300-2"><a href="performance-criteria.html#cb300-2" tabindex="-1"></a>  <span class="fu">summarise</span>( </span>
<span id="cb300-3"><a href="performance-criteria.html#cb300-3" tabindex="-1"></a>    <span class="at">SE =</span> <span class="fu">sd</span>( ATE_hat ),</span>
<span id="cb300-4"><a href="performance-criteria.html#cb300-4" tabindex="-1"></a>    <span class="at">mean_SEhat =</span> <span class="fu">sqrt</span>( <span class="fu">mean</span>( SE_hat<span class="sc">^</span><span class="dv">2</span> ) ),</span>
<span id="cb300-5"><a href="performance-criteria.html#cb300-5" tabindex="-1"></a>    <span class="at">infl =</span> <span class="dv">100</span> <span class="sc">*</span> mean_SEhat <span class="sc">/</span> SE,</span>
<span id="cb300-6"><a href="performance-criteria.html#cb300-6" tabindex="-1"></a>    <span class="at">sd_SEhat =</span> <span class="fu">sd</span>( SE_hat ),</span>
<span id="cb300-7"><a href="performance-criteria.html#cb300-7" tabindex="-1"></a>    <span class="at">stability =</span> <span class="dv">100</span> <span class="sc">*</span> sd_SEhat <span class="sc">/</span> SE )</span></code></pre></div>
<pre><code>## # A tibble: 3 × 6
##   method    SE mean_SEhat  infl sd_SEhat stability
##   &lt;chr&gt;  &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;
## 1 Agg    0.168      0.174  104.   0.0232      13.8
## 2 LR     0.183      0.185  101.   0.0309      16.8
## 3 MLM    0.168      0.174  104.   0.0232      13.8</code></pre>
<p>The SEs for Agg and MLM appear to be a bit conservative on average. (3 or 4 percentage points too big).</p>
<p>The last column (<code>stability</code>) shows how variable the standard error estimates are relative to the true standard error.
50% would mean the standard error estimates can easily be off by 50% of the truth, which would not be particularly good.
Here we see the linear regression is more unstable than the other methods (cluster-robust standard errors are generally known to be a bit unstable, so this is not too surprising).
It is a bad day for linear regression.</p>
</div>
</div>
<div id="metrics-for-confidence-intervals" class="section level2 hasAnchor" number="9.3">
<h2><span class="header-section-number">9.3</span> Metrics for Confidence Intervals<a href="performance-criteria.html#metrics-for-confidence-intervals" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Some estimation procedures result in confidence intervals (or sets) which are ranges of values that should contain the true answer with some specified degree of confidence.
For example, a normal-based confidence interval is a combination of an estimator and its estimated uncertainty.</p>
<p>We typically score a confidence interval along two dimensions, <strong>coverage rate</strong> and <strong>average length</strong>.
To calculate coverage rate, we score whether each interval “captured” the true parameter.
A success is if the true parameter is inside the interval.
To calculate average length, we record each confidence interval’s length, and then average across simulation runs.
We say an estimator has good properties if it has good coverage, i.e. it is capturing the true value at least <span class="math inline">\(1-\alpha\)</span> of the time, and if it is generally short (i.e., the average length of the interval is less than the average length for other methods).</p>
<p>Confidence interval coverage is simultaneously evaluating the estimators in terms of how well they estimate (precision) and their inferential properties.
We have combined inference and estimation.</p>
<p>Suppose that the confidence intervals are for the target parameter <span class="math inline">\(\theta\)</span> and have coverage level <span class="math inline">\(\beta\)</span>.
Let <span class="math inline">\(A_r\)</span> and <span class="math inline">\(B_r\)</span> denote the lower and upper end-points of the confidence interval from simulation replication <span class="math inline">\(r\)</span>, and let <span class="math inline">\(W_r = B_r - A_r\)</span>, all for <span class="math inline">\(r = 1,...,R\)</span>.
The coverage rate <span class="math inline">\(\omega_\beta\)</span> and average length <span class="math inline">\(\text{E}(W)\)</span> metrics are then as defined in the table below.</p>
<table>
<colgroup>
<col width="25%" />
<col width="42%" />
<col width="32%" />
</colgroup>
<thead>
<tr class="header">
<th>Criterion</th>
<th>Definition</th>
<th>Estimate</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Coverage</td>
<td><span class="math inline">\(\omega_\beta = \text{Pr}(A \leq \theta \leq B)\)</span></td>
<td><span class="math inline">\(\frac{1}{R}\sum_{r=1}^R I(A_r \leq \theta \leq B_r)\)</span></td>
</tr>
<tr class="even">
<td>Expected length</td>
<td><span class="math inline">\(\text{E}(W) = \text{E}(B - A)\)</span></td>
<td><span class="math inline">\(\bar{W} = \bar{B} - \bar{A}\)</span></td>
</tr>
</tbody>
</table>
<p>Just as with hypothesis testing, a strict statistical interpretation would deem a hypothesis testing procedure acceptable if it has actual coverage rate greater than or equal to <span class="math inline">\(\beta\)</span>.
If multiple tests satisfy this criterion, then the test with the lowest expected length would be preferable. Some analysts prefer to look at lower and upper coverage separately, where lower coverage is <span class="math inline">\(\text{Pr}(A \leq \theta)\)</span> and upper coverage is <span class="math inline">\(\text{Pr}(\theta \leq B)\)</span>.</p>
<div id="confidence-intervals-in-our-cluster-rct-example" class="section level3 hasAnchor" number="9.3.1">
<h3><span class="header-section-number">9.3.1</span> Confidence Intervals in our Cluster RCT Example<a href="performance-criteria.html#confidence-intervals-in-our-cluster-rct-example" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>For our CRT simulation, we first have to calculate confidence intervals, and then assess coverage.
We could have used methods such as <code>confint()</code> in the estimation approaches; this would be preferred if we wanted more accurately calculated confidence intervals that used <span class="math inline">\(t\)</span>-distributions and so forth to account for the moderate number of clusters.</p>
<p>But if we want to use normal assumption confidence intervals we can calculate them post-hoc:</p>
<div class="sourceCode" id="cb302"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb302-1"><a href="performance-criteria.html#cb302-1" tabindex="-1"></a>runs <span class="sc">%&gt;%</span> <span class="fu">mutate</span>( <span class="at">CI_l =</span> ATE_hat <span class="sc">-</span> <span class="fl">1.96</span><span class="sc">*</span>SE_hat,</span>
<span id="cb302-2"><a href="performance-criteria.html#cb302-2" tabindex="-1"></a>                 <span class="at">CI_h =</span> ATE_hat <span class="sc">+</span> <span class="fl">1.96</span><span class="sc">*</span>SE_hat,</span>
<span id="cb302-3"><a href="performance-criteria.html#cb302-3" tabindex="-1"></a>                 <span class="at">covered =</span> CI_l <span class="sc">&lt;=</span> ATE <span class="sc">&amp;</span> ATE <span class="sc">&lt;=</span> CI_h,</span>
<span id="cb302-4"><a href="performance-criteria.html#cb302-4" tabindex="-1"></a>                 <span class="at">width =</span> CI_h <span class="sc">-</span> CI_l ) <span class="sc">%&gt;%</span></span>
<span id="cb302-5"><a href="performance-criteria.html#cb302-5" tabindex="-1"></a>  <span class="fu">group_by</span>( method ) <span class="sc">%&gt;%</span></span>
<span id="cb302-6"><a href="performance-criteria.html#cb302-6" tabindex="-1"></a>  <span class="fu">summarise</span>( <span class="at">coverage =</span> <span class="fu">mean</span>( covered ),</span>
<span id="cb302-7"><a href="performance-criteria.html#cb302-7" tabindex="-1"></a>             <span class="at">width =</span> <span class="fu">mean</span>( width ))</span></code></pre></div>
<pre><code>## # A tibble: 3 × 3
##   method coverage width
##   &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;
## 1 Agg       0.942 0.677
## 2 LR        0.908 0.717
## 3 MLM       0.943 0.677</code></pre>
<p>Our coverage is about right for Agg and MLM, and around 5 percentage points too low for LR.
Linear regression is taking a hit from the bias term.
The CIs of LR are a bit wider than the other methods due to the estimated SEs being slightly larger.</p>
</div>
</div>
<div id="assessing-inferential-procedures" class="section level2 hasAnchor" number="9.4">
<h2><span class="header-section-number">9.4</span> Metrics for Inferential Procedures (Hypothesis Tests)<a href="performance-criteria.html#assessing-inferential-procedures" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>When hypothesis tests are used in practice, the researcher specifies a null (e.g., no treatment effect), collects data, and generates a <span class="math inline">\(p\)</span>-value, which is a measure of how extreme the observed data are from what we would expect to naturally occur, if the null were true.
When we assess a method for hypothesis testing, we are therefore typically concerned with two aspects: <em>validity</em> and <em>power</em>.</p>
<div id="validity" class="section level3 hasAnchor" number="9.4.1">
<h3><span class="header-section-number">9.4.1</span> Validity<a href="performance-criteria.html#validity" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Validity revolves around whether we erroneously reject a true null more than we should.
Put another way, we say an inference method is valid if it has no more than an <span class="math inline">\(\alpha\)</span> chance of rejecting the null, when it is true, when we are testing at the <span class="math inline">\(\alpha\)</span> level.
This means if we used this method 1000 times, where the null was true for all of those 1000 times, we should not see more than about <span class="math inline">\(1000 \alpha\)</span> rejections (so, 50, if we were using the classic <span class="math inline">\(\alpha = 0.05\)</span> rule).</p>
<p>To assess validity we would therefore specify a data generating process where the null is in fact true.
We then, for a series of such data sets with a true null, conduct our inferential processes on the data, record the <span class="math inline">\(p\)</span>-value, and score whether we reject the null hypothesis or not.</p>
<p>We might then test our methods by exploring more extreme data generation processes, where the null is true but other aspects of the data (such as outliers or heavy skew) make estimation difficult.
This allows us to understand if our methods are robust to strange data patterns in finite sample contexts.</p>
<p>The key concept for validity is that the date we generate, no matter how we do it, must be data with a true null.
The check is always then to see if we reject the null more than we should.</p>
</div>
<div id="power" class="section level3 hasAnchor" number="9.4.2">
<h3><span class="header-section-number">9.4.2</span> Power<a href="performance-criteria.html#power" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Power is, loosely speaking, how often we notice an effect when one is there.
Power is a much more nebulous concept than validity, because some effects (e.g. large effects) are clearly easier to notice than others.
If we are comparing estimators to each other, the overall chance of noticing is less of a concern, because we are typically interested in relative performance.
That being said, in order to generate data for a power evaluation, we have to generate data where there is something to detect.
In other words, we need to commit to what the alternative is, and this can be a tricky business.</p>
<p>Typically, we think of power as a function of sample size or effect size. Therefore, we will typically examine a sequence of scenarios with steadily increasing sample size or effect size, estimating the power for each scenario in the sequence.</p>
<p>We then, for each sample in our series, estimate the power by the same process as for validity, above.
When assessing validity, we want rejection rates to be low, below <span class="math inline">\(\alpha\)</span>, and when assessing power we want them to be as high as possible. But the simulation process itself, other than the data generating process, is exactly the same.</p>
</div>
<div id="the-rejection-rate" class="section level3 hasAnchor" number="9.4.3">
<h3><span class="header-section-number">9.4.3</span> The Rejection Rate<a href="performance-criteria.html#the-rejection-rate" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>To put some technical terms to this framing, for both validity and power assessment the main performance criterion is the <strong>rejection rate</strong> of the hypothesis test. When the data are simulated from a model in which the null hypothesis being tested is true, then the rejection rate is equivalent to the <strong>Type-I error rate</strong> of the test. When the data are simulated from a model in which the null hypothesis is false, then the rejection rate is equivalent to the <strong>power</strong> of the test (for the given alternate hypothesis represented by the DGP).
Ideally, a testing procedure should have actual Type-I error equal to the nominal level <span class="math inline">\(\alpha\)</span> (this is the definition of validity), but such exact tests are rare.</p>
<p>There are some different perspectives on how close the actual Type-I error rate should be in order to qualify as suitable for use in practice. Following a strict statistical definition, a hypothesis testing procedure is said to be <strong>level-<span class="math inline">\(\alpha\)</span></strong> if its actual Type-I error rate is <em>always</em> less than or equal to <span class="math inline">\(\alpha\)</span>.
Among a set of level-<span class="math inline">\(\alpha\)</span> tests, the test with highest power would be preferred.
If looking only at null rejection rates, then the test with Type-I error closest to <span class="math inline">\(\alpha\)</span> would usually be preferred.
A less stringent criterion is sometimes used instead, where type I error would be considered acceptable if it is within 50% of the desired <span class="math inline">\(\alpha\)</span>.</p>
<p>Often, it is of interest to evaluate the performance of the test at several different <span class="math inline">\(\alpha\)</span> levels.
A convenient way to calculate a set of different rejection rates is to record the simulated <span class="math inline">\(p\)</span>-values and then calculate from those.
To illustrate, suppose that <span class="math inline">\(P_r\)</span> is the <span class="math inline">\(p\)</span>-value from simulation replication <span class="math inline">\(k\)</span>, for <span class="math inline">\(k = 1,...,R\)</span>.
Then the rejection rate for a level-<span class="math inline">\(\alpha\)</span> test is defined as <span class="math inline">\(\rho_\alpha = \text{Pr}\left(P_r &lt; \alpha\right)\)</span> and estimated as, using the recorded <span class="math inline">\(p\)</span>-values,
<span class="math display">\[r_\alpha = \frac{1}{R} \sum_{r=1}^R I(P_r &lt; \alpha).\]</span></p>
<p>For a null DGP, one can also plot the emperical cumulative density function of the <span class="math inline">\(p\)</span>-values; a valid test should give a <span class="math inline">\(45^\circ\)</span> line as the <span class="math inline">\(p\)</span>-values should be standard uniform in distribution.</p>
</div>
<div id="inference-in-our-cluster-rct-simulation" class="section level3 hasAnchor" number="9.4.4">
<h3><span class="header-section-number">9.4.4</span> Inference in our Cluster RCT Simulation<a href="performance-criteria.html#inference-in-our-cluster-rct-simulation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>For our scenario, we generated data with an actual treatment effect.
Without further simulation, we therefore could only assess power, not validity.
This is easily solved!
We simply rerun our simulation code that we made last chapter with <code>simhelpers</code>, but with setting <code>ATE = 0</code>.</p>
<div class="sourceCode" id="cb304"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb304-1"><a href="performance-criteria.html#cb304-1" tabindex="-1"></a><span class="fu">set.seed</span>( <span class="dv">404044</span> )</span>
<span id="cb304-2"><a href="performance-criteria.html#cb304-2" tabindex="-1"></a>runs_val <span class="ot">&lt;-</span> <span class="fu">sim_function</span>( R, <span class="at">n_bar =</span> <span class="dv">30</span>, <span class="at">J =</span> <span class="dv">20</span>, <span class="at">gamma_1 =</span> <span class="dv">0</span> )</span>
<span id="cb304-3"><a href="performance-criteria.html#cb304-3" tabindex="-1"></a><span class="fu">saveRDS</span>( runs_val, <span class="at">file =</span> <span class="st">&quot;results/cluster_RCT_simulation_validity.rds&quot;</span> )</span></code></pre></div>
<p>Assessing power and validity is exactly the same calculation: we see how often we have a <span class="math inline">\(p\)</span>-value less than 0.05.
For power we have:</p>
<div class="sourceCode" id="cb305"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb305-1"><a href="performance-criteria.html#cb305-1" tabindex="-1"></a>runs <span class="sc">%&gt;%</span> <span class="fu">group_by</span>( method ) <span class="sc">%&gt;%</span></span>
<span id="cb305-2"><a href="performance-criteria.html#cb305-2" tabindex="-1"></a>  <span class="fu">summarise</span>( <span class="at">power =</span> <span class="fu">mean</span>( p_value <span class="sc">&lt;=</span> <span class="fl">0.05</span> ) )</span></code></pre></div>
<pre><code>## # A tibble: 3 × 2
##   method power
##   &lt;chr&gt;  &lt;dbl&gt;
## 1 Agg    0.376
## 2 LR     0.503
## 3 MLM    0.383</code></pre>
<p>For validity:</p>
<div class="sourceCode" id="cb307"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb307-1"><a href="performance-criteria.html#cb307-1" tabindex="-1"></a>runs_val <span class="sc">%&gt;%</span> <span class="fu">group_by</span>( method ) <span class="sc">%&gt;%</span></span>
<span id="cb307-2"><a href="performance-criteria.html#cb307-2" tabindex="-1"></a>  <span class="fu">summarise</span>( <span class="at">power =</span> <span class="fu">mean</span>( p_value <span class="sc">&lt;=</span> <span class="fl">0.05</span> ) )</span></code></pre></div>
<pre><code>## # A tibble: 3 × 2
##   method power
##   &lt;chr&gt;  &lt;dbl&gt;
## 1 Agg    0.051
## 2 LR     0.059
## 3 MLM    0.048</code></pre>
<p>The power when there is an effect (for this specific scenario) is not particularly high, and the validity is around 0.05, as desired.</p>
<p>Linear regression has notabily higher power… but this may be in part due to the invalidity of the test (note the rejection rate is around 6%, rather than the target of 5%).
The elevated power is also likely due to the upward bias in estimation.
As discussed above, LR is targeting the person-average impact which, in this case, is not 0 even under our null because we have kept our impact heterogeniety parameter to its default of <span class="math inline">\(\gamma_2=0.2\)</span>, meaning we have treatment variation around 0.
We could run our simulation with truly null effects to see if the false rejection rate goes down.</p>
</div>
</div>
<div id="sec-relative-performance" class="section level2 hasAnchor" number="9.5">
<h2><span class="header-section-number">9.5</span> Selecting Relative vs. Absolute Metrics<a href="performance-criteria.html#sec-relative-performance" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We have primarily examined performance estimators for point estimators using absolute metrics, focusing on measures like bias directly on the scale of the outcome.
In contrast, for evaluation things such as estimated standard errors, which are always positive and scale-dependent, it often makes sense to use relative metrics, i.e., metrics calculated as proportions of the target parameter (<span class="math inline">\(T/\theta\)</span>) rather than as differences (<span class="math inline">\(T - \theta\)</span>).
We typically apply absolute metrics to point estimators and relative metrics to standard error estimators (we are setting aside, for the moment, the relative metrics of a measure from one estimation procedure to another, as we saw earlier when we compared the SEs to a baseline SE of linear regression for the cluster randomized trial simulation.
So how do we select when to use what?</p>
<p>As a first piece of guidance, establish whether we expect the performance (e.g., bias, standard error, or RMSE) of a point estimate to depend on the magnitude of the estimand.
For example, if we are estimating some mean <span class="math inline">\(\theta\)</span>, and we generate data where <span class="math inline">\(\theta = 100\)</span> vs where <span class="math inline">\(\theta = 1000\)</span> (or any other arbitrary number), we would not generally expect the value of <span class="math inline">\(\theta\)</span> to change the magnitude of bias, variance, or MSE.
On the other hand, these different <span class="math inline">\(\theta\)</span>s would have a large impact on the <em>relative</em> bias and <em>relative</em> MSE.
(Want smaller relative bias? Just add a million to the parameter!)
For these sorts of “location parameters” we generally use absolute measures of performance.</p>
<p>That being said, a more principled approach for determining whether to use absolute or relative performance metrics depends on assessing performance for <em>multiple</em> values of the parameter.
In many simulation studies, replications are generated and performance metrics are calculated for several different values of a parameter, say <span class="math inline">\(\theta = \theta_1,...,\theta_p\)</span>.
Let’s focus on bias for now, and say that we’ve estimated (from a large number of replications) the bias at each parameter value.
We present two hypothetical scenarios, A and B, in the figures below.</p>
<p><img src="Designing-Simulations-in-R_files/figure-html/unnamed-chunk-170-1.png" width="75%" style="display: block; margin: auto;" /></p>
<p>If the absolute bias is roughly the same for all values of <span class="math inline">\(\theta\)</span> (as in Scenario A), then it makes sense to report absolute bias as the summary performance criterion.
On the other hand, if the bias grows roughly in proportion to <span class="math inline">\(\theta\)</span> (as in Scenario B), then relative bias might be a better summary criterion.</p>
<p><strong>Performance relative to a baseline estimator.</strong></p>
<p>Another relative measure, as we saw earlier, is to calculate performance relative to some baseline.
For example, if one of the estimators is the “generic method,” we could calculate ratios of the RMSE of our estimators to the baseline RMSE.
This can provide a way of standardizing across simulation scenarios where the overall scale of the RMSE changes radically.
This could be critical to, for example, examining trends across simulations that have different sample sizes, where we would expect all estimators’ performance measures to improve as sample size grows.
This kind of relative standardization allows us to make statements such as “Aggregation has standard errors around 8% smaller than linear regression”–which is very interpretable, more interpretable than saying “Aggregation has standard errors around 0.01 smaller than linear regression.”
In the latter case, we do not know if that is big or small.</p>
<p>While a powerful tool, standardization is not without risks: if you scale relative to something, then higher or lower ratios can either be due to the primary method of interest (the numerator) or due to the behavior of the reference method in the denominator.
These relative ratios can end up being confusing to interpret due to this tension.</p>
<p>They can also break when everything is on a constrained scale, like power.
If we have a power of 0.05, and we improve it to 0.10, we have doubled our power, but if it is 0.10 and we increase to 0.15, we have only increased by 50%.
Ratios when near zero can be very deceiving.</p>
</div>
<div id="summary-of-peformance-measures" class="section level2 hasAnchor" number="9.6">
<h2><span class="header-section-number">9.6</span> Summary of Peformance Measures<a href="performance-criteria.html#summary-of-peformance-measures" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Depending on the model and estimation procedures being examined, a range of different criteria might be used to assess estimator performance.
For point estimation, we have seen bias, variance and MSE as the three core measures of performance.
Other criteria exist, such as the median bias and the median absolute deviation of <span class="math inline">\(T\)</span>, where we use the median <span class="math inline">\(\tilde{T}\)</span> of our estimates rather than the mean <span class="math inline">\(\bar{T}\)</span>.</p>
<p>The usual bias, variance and MSE measures can be sensitive to outliers.
If an estimator generally does well, except for an occasional large mistake, these classic measures can return very poor overall performance.
Instead, we might turn to quantities such as the median bias (sort all the estimation errors across the simulation scenarios, and take the middle), or the Median Absolute Distance (MAD, where you take the median of the absolute values of the errors, which is an alternative to RMSE) as a measure of performance.</p>
<p>As an example that really does require some handling of outliers, we next discuss Instrumental Variables.
Instrumental variables (IV) estimation is used when a regressor (e.g., a treatment <span class="math inline">\(D\)</span>) is correlated with some unobserved variable (e.g., <span class="math inline">\(\theta\)</span>), making a vanilla OLS regression of <span class="math inline">\(Y\)</span> on <span class="math inline">\(D\)</span> biased.
An instrument <span class="math inline">\(Z\)</span>–a variable correlated with <span class="math inline">\(D\)</span> but uncorrelated with the error term–can isolate exogenous variation in <span class="math inline">\(D\)</span> so we can recover a consistent estimate of its causal effect on <span class="math inline">\(Y\)</span>.
Classic IV estimation uses Two Stage Least Squares (2SLS), where we first regress <span class="math inline">\(D\)</span> on <span class="math inline">\(Z\)</span> and any controls (e.g., <span class="math inline">\(X\)</span>), then use the predicted values of <span class="math inline">\(D\)</span> to estimate the effect on <span class="math inline">\(Y\)</span>.
The key idea is that our instrument <span class="math inline">\(Z\)</span> “moves” <span class="math inline">\(D\)</span>, and thus we know that if <span class="math inline">\(Y\)</span> changes when <span class="math inline">\(Z\)</span> changes, that must have happened due to <span class="math inline">\(Z\)</span>’s influence on <span class="math inline">\(D\)</span>, allowing us to recover the impact <span class="math inline">\(D\)</span> has on <span class="math inline">\(Y\)</span>.
Unfortunately, if the effect of <span class="math inline">\(Z\)</span> on <span class="math inline">\(D\)</span> is small, we can end up with a weak instrument, which leads to large standard errors in our IV estimates.
We might hope that if we have some auxillary covariate <span class="math inline">\(X\)</span>, we can control for it in our IV regression, which can help stabilize the estimation.</p>
<p>To test this out we might put together a simple simulation as follows.
We simulate data where <span class="math inline">\(Y\)</span> is a function of <span class="math inline">\(D\)</span>, <span class="math inline">\(X\)</span>, and an unobserved confounder <span class="math inline">\(\theta\)</span>.
We generate <span class="math inline">\(D\)</span> as a function of <span class="math inline">\(Z\)</span> and <span class="math inline">\(\theta\)</span>, and then <span class="math inline">\(Y\)</span> as a function of <span class="math inline">\(D\)</span>, <span class="math inline">\(X\)</span>, and <span class="math inline">\(\theta\)</span>.
We are not making individual components here, to keep things brief, and we are using a package from <code>AER</code> to do the IV estimation.</p>
<div class="sourceCode" id="cb309"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb309-1"><a href="performance-criteria.html#cb309-1" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb309-2"><a href="performance-criteria.html#cb309-2" tabindex="-1"></a><span class="fu">library</span>(AER)</span></code></pre></div>
<pre><code>## Loading required package: car</code></pre>
<pre><code>## Loading required package: carData</code></pre>
<pre><code>## 
## Attaching package: &#39;car&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:metafor&#39;:
## 
##     vif</code></pre>
<pre><code>## The following object is masked from &#39;package:arm&#39;:
## 
##     logit</code></pre>
<pre><code>## The following object is masked from &#39;package:psych&#39;:
## 
##     logit</code></pre>
<pre><code>## The following object is masked from &#39;package:dplyr&#39;:
## 
##     recode</code></pre>
<pre><code>## The following object is masked from &#39;package:purrr&#39;:
## 
##     some</code></pre>
<pre><code>## Loading required package: lmtest</code></pre>
<pre><code>## Loading required package: zoo</code></pre>
<pre><code>## 
## Attaching package: &#39;zoo&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:base&#39;:
## 
##     as.Date, as.Date.numeric</code></pre>
<pre><code>## 
## Attaching package: &#39;lmtest&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:future&#39;:
## 
##     reset</code></pre>
<pre><code>## Loading required package: sandwich</code></pre>
<pre><code>## Loading required package: survival</code></pre>
<pre><code>## 
## Attaching package: &#39;survival&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:future&#39;:
## 
##     cluster</code></pre>
<div class="sourceCode" id="cb328"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb328-1"><a href="performance-criteria.html#cb328-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb328-2"><a href="performance-criteria.html#cb328-2" tabindex="-1"></a></span>
<span id="cb328-3"><a href="performance-criteria.html#cb328-3" tabindex="-1"></a>sim_iv <span class="ot">&lt;-</span> <span class="cf">function</span>(<span class="at">n =</span> <span class="dv">1000</span>, <span class="at">pi =</span> <span class="fl">0.1</span>, <span class="at">reps =</span> <span class="dv">100</span>) {</span>
<span id="cb328-4"><a href="performance-criteria.html#cb328-4" tabindex="-1"></a>  <span class="fu">map_dfr</span>(<span class="dv">1</span><span class="sc">:</span>reps, <span class="sc">~</span>{</span>
<span id="cb328-5"><a href="performance-criteria.html#cb328-5" tabindex="-1"></a>    dat <span class="ot">=</span> <span class="fu">tibble</span>( </span>
<span id="cb328-6"><a href="performance-criteria.html#cb328-6" tabindex="-1"></a>      <span class="at">Z =</span> <span class="fu">rnorm</span>(n),</span>
<span id="cb328-7"><a href="performance-criteria.html#cb328-7" tabindex="-1"></a>      <span class="at">theta =</span> <span class="fu">rnorm</span>(n),</span>
<span id="cb328-8"><a href="performance-criteria.html#cb328-8" tabindex="-1"></a>      <span class="at">X =</span> <span class="fu">rnorm</span>(n),</span>
<span id="cb328-9"><a href="performance-criteria.html#cb328-9" tabindex="-1"></a>      <span class="at">D =</span> pi<span class="sc">*</span>Z <span class="sc">+</span> <span class="fl">0.5</span><span class="sc">*</span>theta <span class="sc">+</span> <span class="fu">rnorm</span>(n),</span>
<span id="cb328-10"><a href="performance-criteria.html#cb328-10" tabindex="-1"></a>      <span class="at">Y =</span> <span class="dv">1</span><span class="sc">*</span>D <span class="sc">+</span> <span class="dv">2</span><span class="sc">*</span>X <span class="sc">+</span> <span class="dv">1</span><span class="sc">*</span>theta <span class="sc">+</span> <span class="fu">rnorm</span>(n) )</span>
<span id="cb328-11"><a href="performance-criteria.html#cb328-11" tabindex="-1"></a>  </span>
<span id="cb328-12"><a href="performance-criteria.html#cb328-12" tabindex="-1"></a>    <span class="co"># Don&#39;t control for X</span></span>
<span id="cb328-13"><a href="performance-criteria.html#cb328-13" tabindex="-1"></a>    iv_fit1 <span class="ot">&lt;-</span> <span class="fu">ivreg</span>(Y <span class="sc">~</span> D <span class="sc">|</span> Z, <span class="at">data=</span>dat)</span>
<span id="cb328-14"><a href="performance-criteria.html#cb328-14" tabindex="-1"></a>    sum1 <span class="ot">&lt;-</span> <span class="fu">summary</span>(iv_fit1)</span>
<span id="cb328-15"><a href="performance-criteria.html#cb328-15" tabindex="-1"></a>    pe1 <span class="ot">&lt;-</span> sum1<span class="sc">$</span>coefficients[<span class="st">&quot;D&quot;</span>, <span class="st">&quot;Estimate&quot;</span>]</span>
<span id="cb328-16"><a href="performance-criteria.html#cb328-16" tabindex="-1"></a>    se1 <span class="ot">&lt;-</span> sum1<span class="sc">$</span>coefficients[<span class="st">&quot;D&quot;</span>, <span class="st">&quot;Std. Error&quot;</span>]</span>
<span id="cb328-17"><a href="performance-criteria.html#cb328-17" tabindex="-1"></a>  </span>
<span id="cb328-18"><a href="performance-criteria.html#cb328-18" tabindex="-1"></a>    <span class="co"># Control for X</span></span>
<span id="cb328-19"><a href="performance-criteria.html#cb328-19" tabindex="-1"></a>    iv_fit2 <span class="ot">&lt;-</span> <span class="fu">ivreg</span>(Y <span class="sc">~</span> X <span class="sc">+</span> D <span class="sc">|</span> X <span class="sc">+</span> Z, <span class="at">data=</span>dat)</span>
<span id="cb328-20"><a href="performance-criteria.html#cb328-20" tabindex="-1"></a>    sum2 <span class="ot">&lt;-</span> <span class="fu">summary</span>(iv_fit2)</span>
<span id="cb328-21"><a href="performance-criteria.html#cb328-21" tabindex="-1"></a>    pe2 <span class="ot">&lt;-</span> sum2<span class="sc">$</span>coefficients[<span class="st">&quot;D&quot;</span>, <span class="st">&quot;Estimate&quot;</span>]</span>
<span id="cb328-22"><a href="performance-criteria.html#cb328-22" tabindex="-1"></a>    se2 <span class="ot">&lt;-</span> sum2<span class="sc">$</span>coefficients[<span class="st">&quot;D&quot;</span>, <span class="st">&quot;Std. Error&quot;</span>]</span>
<span id="cb328-23"><a href="performance-criteria.html#cb328-23" tabindex="-1"></a>    </span>
<span id="cb328-24"><a href="performance-criteria.html#cb328-24" tabindex="-1"></a>    <span class="co"># OLS as baseline</span></span>
<span id="cb328-25"><a href="performance-criteria.html#cb328-25" tabindex="-1"></a>    ols_fit <span class="ot">&lt;-</span> <span class="fu">lm</span>(Y <span class="sc">~</span> X <span class="sc">+</span> D, <span class="at">data=</span>dat)</span>
<span id="cb328-26"><a href="performance-criteria.html#cb328-26" tabindex="-1"></a>    ols_sum <span class="ot">&lt;-</span> <span class="fu">summary</span>(ols_fit)</span>
<span id="cb328-27"><a href="performance-criteria.html#cb328-27" tabindex="-1"></a>    pe_ols <span class="ot">&lt;-</span> ols_sum<span class="sc">$</span>coefficients[<span class="st">&quot;D&quot;</span>, <span class="st">&quot;Estimate&quot;</span>]</span>
<span id="cb328-28"><a href="performance-criteria.html#cb328-28" tabindex="-1"></a>    se_ols <span class="ot">&lt;-</span> ols_sum<span class="sc">$</span>coefficients[<span class="st">&quot;D&quot;</span>, <span class="st">&quot;Std. Error&quot;</span>]</span>
<span id="cb328-29"><a href="performance-criteria.html#cb328-29" tabindex="-1"></a>    </span>
<span id="cb328-30"><a href="performance-criteria.html#cb328-30" tabindex="-1"></a>    <span class="fu">tibble</span>( <span class="at">method =</span> <span class="fu">c</span>( <span class="st">&quot;simple&quot;</span>, <span class="st">&quot;control&quot;</span>, <span class="st">&quot;ols&quot;</span>),</span>
<span id="cb328-31"><a href="performance-criteria.html#cb328-31" tabindex="-1"></a>            <span class="at">pe =</span> <span class="fu">c</span>(pe1, pe2, pe_ols),</span>
<span id="cb328-32"><a href="performance-criteria.html#cb328-32" tabindex="-1"></a>            <span class="at">sehat =</span> <span class="fu">c</span>(se1, se2, se_ols) )</span>
<span id="cb328-33"><a href="performance-criteria.html#cb328-33" tabindex="-1"></a>  })</span>
<span id="cb328-34"><a href="performance-criteria.html#cb328-34" tabindex="-1"></a>}</span></code></pre></div>
<p>Our function runs a single scenario, and gives us back three estimates for each iteration: the controlled IV regression, the simple IV regression, and the OLS regression.
We can plot our estimates to see what they look like:</p>
<div class="sourceCode" id="cb329"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb329-1"><a href="performance-criteria.html#cb329-1" tabindex="-1"></a>results <span class="ot">&lt;-</span> <span class="fu">sim_iv</span>(<span class="at">pi =</span> <span class="fl">0.50</span>)</span>
<span id="cb329-2"><a href="performance-criteria.html#cb329-2" tabindex="-1"></a><span class="fu">ggplot</span>( results, <span class="fu">aes</span>( pe )) <span class="sc">+</span></span>
<span id="cb329-3"><a href="performance-criteria.html#cb329-3" tabindex="-1"></a>  <span class="fu">facet_wrap</span>( <span class="sc">~</span> method, <span class="at">nrow=</span><span class="dv">1</span> ) <span class="sc">+</span></span>
<span id="cb329-4"><a href="performance-criteria.html#cb329-4" tabindex="-1"></a>  <span class="fu">geom_histogram</span>( <span class="at">bins =</span> <span class="dv">30</span>, <span class="at">fill =</span> <span class="st">&quot;lightblue&quot;</span>, <span class="at">color =</span> <span class="st">&quot;black&quot;</span> ) <span class="sc">+</span></span>
<span id="cb329-5"><a href="performance-criteria.html#cb329-5" tabindex="-1"></a>  <span class="fu">geom_vline</span>( <span class="at">xintercept =</span> <span class="dv">1</span> ) <span class="sc">+</span></span>
<span id="cb329-6"><a href="performance-criteria.html#cb329-6" tabindex="-1"></a>  <span class="fu">labs</span>( <span class="at">title =</span> <span class="st">&quot;IV Estimates with Strong Instrument (pi = 0.5)&quot;</span> )</span></code></pre></div>
<p><img src="Designing-Simulations-in-R_files/figure-html/unnamed-chunk-172-1.png" width="75%" style="display: block; margin: auto;" />
With a stong instrument, our IV estimators are close to the true value of 1, while the OLS estimator is biased due to the unobserved confounder <span class="math inline">\(\theta\)</span>.
It is also clear that controlling for our confounder does in fact reduce our standard error.</p>
<p>Now let’s run our simulation for a series of weaker instrument values:</p>
<div class="sourceCode" id="cb330"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb330-1"><a href="performance-criteria.html#cb330-1" tabindex="-1"></a>pis <span class="ot">=</span> <span class="fu">c</span>( <span class="fl">0.01</span>, <span class="fl">0.025</span>, <span class="fl">0.05</span>, <span class="fl">0.075</span>, <span class="fl">0.1</span>, <span class="fl">0.2</span>, <span class="fl">0.3</span> )</span>
<span id="cb330-2"><a href="performance-criteria.html#cb330-2" tabindex="-1"></a>results <span class="ot">&lt;-</span> <span class="fu">map_dfr</span>(pis, \(pi) {</span>
<span id="cb330-3"><a href="performance-criteria.html#cb330-3" tabindex="-1"></a>  <span class="fu">sim_iv</span>(<span class="at">n =</span> <span class="dv">1000</span>, <span class="at">pi =</span> pi, <span class="at">reps =</span> <span class="dv">1000</span>) <span class="sc">%&gt;%</span></span>
<span id="cb330-4"><a href="performance-criteria.html#cb330-4" tabindex="-1"></a>    <span class="fu">mutate</span>(<span class="at">pi =</span> pi)</span>
<span id="cb330-5"><a href="performance-criteria.html#cb330-5" tabindex="-1"></a>} )</span></code></pre></div>
<p>We compute some performance metrics, and look at the bias and standard error of our estimates.</p>
<div class="sourceCode" id="cb331"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb331-1"><a href="performance-criteria.html#cb331-1" tabindex="-1"></a>sres <span class="ot">&lt;-</span> results <span class="sc">%&gt;%</span></span>
<span id="cb331-2"><a href="performance-criteria.html#cb331-2" tabindex="-1"></a>  <span class="fu">group_by</span>( pi, method ) <span class="sc">%&gt;%</span></span>
<span id="cb331-3"><a href="performance-criteria.html#cb331-3" tabindex="-1"></a>  <span class="fu">summarise</span>( </span>
<span id="cb331-4"><a href="performance-criteria.html#cb331-4" tabindex="-1"></a>    <span class="at">bias =</span> <span class="fu">mean</span>(pe) <span class="sc">-</span> <span class="dv">1</span>,</span>
<span id="cb331-5"><a href="performance-criteria.html#cb331-5" tabindex="-1"></a>    <span class="at">SE =</span> <span class="fu">sd</span>(pe), <span class="at">.groups =</span> <span class="st">&quot;drop&quot;</span> )</span>
<span id="cb331-6"><a href="performance-criteria.html#cb331-6" tabindex="-1"></a></span>
<span id="cb331-7"><a href="performance-criteria.html#cb331-7" tabindex="-1"></a>sresL <span class="ot">&lt;-</span> sres <span class="sc">%&gt;%</span></span>
<span id="cb331-8"><a href="performance-criteria.html#cb331-8" tabindex="-1"></a>  <span class="fu">pivot_longer</span>(</span>
<span id="cb331-9"><a href="performance-criteria.html#cb331-9" tabindex="-1"></a>    <span class="at">cols =</span> <span class="fu">c</span>(bias, SE ),</span>
<span id="cb331-10"><a href="performance-criteria.html#cb331-10" tabindex="-1"></a>    <span class="at">names_to=</span><span class="st">&quot;metric&quot;</span>,</span>
<span id="cb331-11"><a href="performance-criteria.html#cb331-11" tabindex="-1"></a>    <span class="at">values_to=</span><span class="st">&quot;value&quot;</span> )</span>
<span id="cb331-12"><a href="performance-criteria.html#cb331-12" tabindex="-1"></a></span>
<span id="cb331-13"><a href="performance-criteria.html#cb331-13" tabindex="-1"></a><span class="fu">ggplot</span>( sresL, <span class="fu">aes</span>(<span class="at">x =</span> pi, <span class="at">y =</span> value, <span class="at">color =</span> method) ) <span class="sc">+</span></span>
<span id="cb331-14"><a href="performance-criteria.html#cb331-14" tabindex="-1"></a>  <span class="fu">facet_wrap</span>( <span class="sc">~</span> metric, <span class="at">scales=</span><span class="st">&quot;free&quot;</span> ) <span class="sc">+</span></span>
<span id="cb331-15"><a href="performance-criteria.html#cb331-15" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span> <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb331-16"><a href="performance-criteria.html#cb331-16" tabindex="-1"></a>  <span class="fu">labs</span>( <span class="at">y =</span> <span class="st">&quot;bias&quot;</span>, <span class="at">title =</span> <span class="st">&quot;Bias of IV estimators&quot;</span> ) <span class="sc">+</span></span>
<span id="cb331-17"><a href="performance-criteria.html#cb331-17" tabindex="-1"></a>  <span class="fu">scale_x_log10</span>()</span></code></pre></div>
<p><img src="Designing-Simulations-in-R_files/figure-html/unnamed-chunk-174-1.png" width="75%" style="display: block; margin: auto;" /></p>
<p>Something is not right.
We can look at our raw estimates to get a better idea of what is going on.</p>
<div class="sourceCode" id="cb332"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb332-1"><a href="performance-criteria.html#cb332-1" tabindex="-1"></a><span class="fu">ggplot</span>( results,</span>
<span id="cb332-2"><a href="performance-criteria.html#cb332-2" tabindex="-1"></a>        <span class="fu">aes</span>(<span class="at">x =</span> <span class="fu">as.factor</span>(pi), <span class="at">y =</span> pe, </span>
<span id="cb332-3"><a href="performance-criteria.html#cb332-3" tabindex="-1"></a>            <span class="at">col =</span> method, <span class="at">fill =</span> method) ) <span class="sc">+</span></span>
<span id="cb332-4"><a href="performance-criteria.html#cb332-4" tabindex="-1"></a>  <span class="fu">geom_boxplot</span>(<span class="at">outlier.size=</span><span class="fl">0.5</span>) <span class="sc">+</span></span>
<span id="cb332-5"><a href="performance-criteria.html#cb332-5" tabindex="-1"></a>  <span class="fu">coord_flip</span>() <span class="sc">+</span></span>
<span id="cb332-6"><a href="performance-criteria.html#cb332-6" tabindex="-1"></a>  <span class="fu">labs</span>( <span class="at">title =</span> <span class="st">&quot;Individual IV estimates for pi=0.05&quot;</span> )</span></code></pre></div>
<p><img src="Designing-Simulations-in-R_files/figure-html/unnamed-chunk-175-1.png" width="75%" style="display: block; margin: auto;" />
We have massive outliers, upwards of 50,000 in size.
These outliers are completely distorting our results.</p>
<p>Sometimes we have to tweak our performance metrics to account for this kind of instability.
One approach is to look at median, rather than mean, performance:</p>
<div class="sourceCode" id="cb333"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb333-1"><a href="performance-criteria.html#cb333-1" tabindex="-1"></a>sresL <span class="ot">&lt;-</span> results <span class="sc">%&gt;%</span></span>
<span id="cb333-2"><a href="performance-criteria.html#cb333-2" tabindex="-1"></a>  <span class="fu">group_by</span>( pi, method ) <span class="sc">%&gt;%</span></span>
<span id="cb333-3"><a href="performance-criteria.html#cb333-3" tabindex="-1"></a>  <span class="fu">summarise</span>( </span>
<span id="cb333-4"><a href="performance-criteria.html#cb333-4" tabindex="-1"></a>    <span class="at">bias =</span> <span class="fu">median</span>(pe) <span class="sc">-</span> <span class="dv">1</span>,</span>
<span id="cb333-5"><a href="performance-criteria.html#cb333-5" tabindex="-1"></a>    <span class="at">MAD =</span> <span class="fu">median</span>(<span class="fu">abs</span>(pe <span class="sc">-</span> <span class="fu">median</span>(pe))), <span class="at">.groups =</span> <span class="st">&quot;drop&quot;</span> ) <span class="sc">%&gt;%</span></span>
<span id="cb333-6"><a href="performance-criteria.html#cb333-6" tabindex="-1"></a>  <span class="fu">pivot_longer</span>(</span>
<span id="cb333-7"><a href="performance-criteria.html#cb333-7" tabindex="-1"></a>    <span class="at">cols =</span> <span class="fu">c</span>(bias, MAD ),</span>
<span id="cb333-8"><a href="performance-criteria.html#cb333-8" tabindex="-1"></a>    <span class="at">names_to=</span><span class="st">&quot;metric&quot;</span>,</span>
<span id="cb333-9"><a href="performance-criteria.html#cb333-9" tabindex="-1"></a>    <span class="at">values_to=</span><span class="st">&quot;value&quot;</span> )</span>
<span id="cb333-10"><a href="performance-criteria.html#cb333-10" tabindex="-1"></a></span>
<span id="cb333-11"><a href="performance-criteria.html#cb333-11" tabindex="-1"></a><span class="fu">ggplot</span>( sresL, <span class="fu">aes</span>(<span class="at">x =</span> pi, <span class="at">y =</span> value, <span class="at">color =</span> method) ) <span class="sc">+</span></span>
<span id="cb333-12"><a href="performance-criteria.html#cb333-12" tabindex="-1"></a>  <span class="fu">facet_wrap</span>( <span class="sc">~</span> metric, <span class="at">scales=</span><span class="st">&quot;free&quot;</span> ) <span class="sc">+</span></span>
<span id="cb333-13"><a href="performance-criteria.html#cb333-13" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span> <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb333-14"><a href="performance-criteria.html#cb333-14" tabindex="-1"></a>  <span class="fu">labs</span>( <span class="at">y =</span> <span class="st">&quot;bias&quot;</span>, <span class="at">title =</span> <span class="st">&quot;Bias of IV estimators&quot;</span> ) <span class="sc">+</span></span>
<span id="cb333-15"><a href="performance-criteria.html#cb333-15" tabindex="-1"></a>  <span class="fu">scale_x_log10</span>()</span></code></pre></div>
<p><img src="Designing-Simulations-in-R_files/figure-html/unnamed-chunk-176-1.png" width="75%" style="display: block; margin: auto;" />
We see much more clearly that as pi increases, the IV estimators get much more stable (lower MAD) and that the controlled IV estimator has a smaller MAD than the simple IV estimator.
We also see that there is still some median bias for low IV values, but that the median bias quickly goes to 0 as pi increases.</p>
<div id="windsorization-to-control-outliers" class="section level3 hasAnchor" number="9.6.1">
<h3><span class="header-section-number">9.6.1</span> Windsorization to control outliers<a href="performance-criteria.html#windsorization-to-control-outliers" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Median performance is often workable, but it does not take into account the impact of outliers at all.
Other robust measures are also possible, such as simply truncating all errors to a maximum size (this is called Windsorizing).
This is a way of saying “I don’t care if you are off by 1000, I am only going to count it as 10.”
If we do this to our raw estimates, we can get more stable estimates of performance.
We can top-code at a large value that we would consider an outlier, so that estimators that have extreme estimates do get penalized, but just do not get penalized so much it distorts our picture.</p>
<div class="sourceCode" id="cb334"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb334-1"><a href="performance-criteria.html#cb334-1" tabindex="-1"></a>results <span class="ot">&lt;-</span> results <span class="sc">%&gt;%</span></span>
<span id="cb334-2"><a href="performance-criteria.html#cb334-2" tabindex="-1"></a>  <span class="fu">mutate</span>( <span class="at">pe_wind =</span> <span class="fu">pmax</span>( <span class="sc">-</span><span class="dv">20</span>, <span class="fu">pmin</span>(pe, <span class="dv">20</span>) ) )</span></code></pre></div>
<p>Here are our boxplots of the raw estimates, with the top-coded values.</p>
<div class="sourceCode" id="cb335"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb335-1"><a href="performance-criteria.html#cb335-1" tabindex="-1"></a><span class="fu">ggplot</span>( results,</span>
<span id="cb335-2"><a href="performance-criteria.html#cb335-2" tabindex="-1"></a>        <span class="fu">aes</span>(<span class="at">x =</span> <span class="fu">as.factor</span>(pi), <span class="at">y =</span> pe_wind, </span>
<span id="cb335-3"><a href="performance-criteria.html#cb335-3" tabindex="-1"></a>             <span class="at">fill =</span> method) ) <span class="sc">+</span></span>
<span id="cb335-4"><a href="performance-criteria.html#cb335-4" tabindex="-1"></a>  <span class="fu">geom_boxplot</span>(<span class="at">outlier.size=</span><span class="fl">0.5</span>) <span class="sc">+</span></span>
<span id="cb335-5"><a href="performance-criteria.html#cb335-5" tabindex="-1"></a>  <span class="fu">coord_flip</span>() <span class="sc">+</span></span>
<span id="cb335-6"><a href="performance-criteria.html#cb335-6" tabindex="-1"></a>  <span class="fu">labs</span>( <span class="at">title =</span> <span class="st">&quot;Individual IV estimates for pi=0.05&quot;</span> )</span></code></pre></div>
<p><img src="Designing-Simulations-in-R_files/figure-html/unnamed-chunk-178-1.png" width="75%" style="display: block; margin: auto;" />
We can see the inner distribution much better now.
We can also assess how much Windorization there is:</p>
<div class="sourceCode" id="cb336"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb336-1"><a href="performance-criteria.html#cb336-1" tabindex="-1"></a>sres <span class="ot">&lt;-</span> results <span class="sc">%&gt;%</span></span>
<span id="cb336-2"><a href="performance-criteria.html#cb336-2" tabindex="-1"></a>  <span class="fu">group_by</span>( pi, method ) <span class="sc">%&gt;%</span></span>
<span id="cb336-3"><a href="performance-criteria.html#cb336-3" tabindex="-1"></a>  <span class="fu">summarise</span>( </span>
<span id="cb336-4"><a href="performance-criteria.html#cb336-4" tabindex="-1"></a>    <span class="at">wind =</span> <span class="fu">mean</span>(pe_wind <span class="sc">!=</span> pe), <span class="at">.groups =</span> <span class="st">&quot;drop&quot;</span></span>
<span id="cb336-5"><a href="performance-criteria.html#cb336-5" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb336-6"><a href="performance-criteria.html#cb336-6" tabindex="-1"></a>  <span class="fu">pivot_wider</span>( <span class="at">names_from =</span> <span class="st">&quot;method&quot;</span>,</span>
<span id="cb336-7"><a href="performance-criteria.html#cb336-7" tabindex="-1"></a>               <span class="at">values_from=</span><span class="st">&quot;wind&quot;</span> )</span>
<span id="cb336-8"><a href="performance-criteria.html#cb336-8" tabindex="-1"></a>sres <span class="sc">%&gt;%</span></span>
<span id="cb336-9"><a href="performance-criteria.html#cb336-9" tabindex="-1"></a>  knitr<span class="sc">::</span><span class="fu">kable</span>( <span class="at">digits =</span> <span class="dv">2</span> )</span></code></pre></div>
<table>
<thead>
<tr class="header">
<th align="right">pi</th>
<th align="right">control</th>
<th align="right">ols</th>
<th align="right">simple</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">0.01</td>
<td align="right">0.04</td>
<td align="right">0</td>
<td align="right">0.08</td>
</tr>
<tr class="even">
<td align="right">0.03</td>
<td align="right">0.04</td>
<td align="right">0</td>
<td align="right">0.06</td>
</tr>
<tr class="odd">
<td align="right">0.05</td>
<td align="right">0.02</td>
<td align="right">0</td>
<td align="right">0.03</td>
</tr>
<tr class="even">
<td align="right">0.07</td>
<td align="right">0.01</td>
<td align="right">0</td>
<td align="right">0.01</td>
</tr>
<tr class="odd">
<td align="right">0.10</td>
<td align="right">0.00</td>
<td align="right">0</td>
<td align="right">0.00</td>
</tr>
<tr class="even">
<td align="right">0.20</td>
<td align="right">0.00</td>
<td align="right">0</td>
<td align="right">0.00</td>
</tr>
<tr class="odd">
<td align="right">0.30</td>
<td align="right">0.00</td>
<td align="right">0</td>
<td align="right">0.00</td>
</tr>
</tbody>
</table>
<p>We could even decide 8% is too much data to windorize, and we could use this to inform our choice of threshold.</p>
<p>We then use our windorized values to calculate our final performance:</p>
<div class="sourceCode" id="cb337"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb337-1"><a href="performance-criteria.html#cb337-1" tabindex="-1"></a>sresL <span class="ot">&lt;-</span> results <span class="sc">%&gt;%</span></span>
<span id="cb337-2"><a href="performance-criteria.html#cb337-2" tabindex="-1"></a>  <span class="fu">group_by</span>( pi, method ) <span class="sc">%&gt;%</span></span>
<span id="cb337-3"><a href="performance-criteria.html#cb337-3" tabindex="-1"></a>  <span class="fu">summarise</span>( </span>
<span id="cb337-4"><a href="performance-criteria.html#cb337-4" tabindex="-1"></a>    <span class="at">bias =</span> <span class="fu">mean</span>(pe_wind) <span class="sc">-</span> <span class="dv">1</span>,</span>
<span id="cb337-5"><a href="performance-criteria.html#cb337-5" tabindex="-1"></a>    <span class="at">SE =</span> <span class="fu">sd</span>(pe_wind), <span class="at">.groups =</span> <span class="st">&quot;drop&quot;</span> ) <span class="sc">%&gt;%</span></span>
<span id="cb337-6"><a href="performance-criteria.html#cb337-6" tabindex="-1"></a>  <span class="fu">pivot_longer</span>(</span>
<span id="cb337-7"><a href="performance-criteria.html#cb337-7" tabindex="-1"></a>    <span class="at">cols =</span> <span class="fu">c</span>(bias, SE ),</span>
<span id="cb337-8"><a href="performance-criteria.html#cb337-8" tabindex="-1"></a>    <span class="at">names_to=</span><span class="st">&quot;metric&quot;</span>,</span>
<span id="cb337-9"><a href="performance-criteria.html#cb337-9" tabindex="-1"></a>    <span class="at">values_to=</span><span class="st">&quot;value&quot;</span> )</span>
<span id="cb337-10"><a href="performance-criteria.html#cb337-10" tabindex="-1"></a></span>
<span id="cb337-11"><a href="performance-criteria.html#cb337-11" tabindex="-1"></a><span class="fu">ggplot</span>( sresL, <span class="fu">aes</span>(<span class="at">x =</span> pi, <span class="at">y =</span> value, <span class="at">color =</span> method) ) <span class="sc">+</span></span>
<span id="cb337-12"><a href="performance-criteria.html#cb337-12" tabindex="-1"></a>  <span class="fu">facet_wrap</span>( <span class="sc">~</span> metric, <span class="at">scales=</span><span class="st">&quot;free&quot;</span> ) <span class="sc">+</span></span>
<span id="cb337-13"><a href="performance-criteria.html#cb337-13" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span> <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb337-14"><a href="performance-criteria.html#cb337-14" tabindex="-1"></a>  <span class="fu">labs</span>( <span class="at">y =</span> <span class="st">&quot;bias&quot;</span>, <span class="at">title =</span> <span class="st">&quot;Bias of IV estimators&quot;</span> ) <span class="sc">+</span></span>
<span id="cb337-15"><a href="performance-criteria.html#cb337-15" tabindex="-1"></a>  <span class="fu">scale_x_log10</span>()</span></code></pre></div>
<p><img src="Designing-Simulations-in-R_files/figure-html/unnamed-chunk-180-1.png" width="75%" style="display: block; margin: auto;" /></p>
<p>Overall, when your estimators have strange tail behavior (e.g., extreme outliers) you may have to depart from the usual performance metrics or tweak your estimates to get a clearer picture of how your estimators are performing.
But it is important to caveat your reporting to include the fact that you are using a modified performance metric, and why you chose to do so.
When windorizing, you should always report how much data you windorized, and you do not want it to be too high a percent of your data.</p>
</div>
<div id="correlation-measures-vs-absolute-performance" class="section level3 hasAnchor" number="9.6.2">
<h3><span class="header-section-number">9.6.2</span> Correlation measures vs absolute performance<a href="performance-criteria.html#correlation-measures-vs-absolute-performance" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Say you have two methods that predict individual outcomes for each individual in a given evaluation set as a function of some set of covariates.
One method might heavily use regularization, while the other might use a more complex model allowing for interactions.
Regularization, where a method shrinks its estimates towards some overall center, would potentially bias the individual estimates, but that might not matter if the relative ordering of the estimates is preserved.</p>
<p>For example, in a project evaluating how well machine learning methods predict individual outcomes, we might have a simulation run such as the following:</p>
<div class="sourceCode" id="cb338"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb338-1"><a href="performance-criteria.html#cb338-1" tabindex="-1"></a>tt <span class="ot">&lt;-</span> <span class="fu">tibble</span>( <span class="at">ID =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">1000</span>,</span>
<span id="cb338-2"><a href="performance-criteria.html#cb338-2" tabindex="-1"></a>              <span class="at">theta =</span> <span class="fu">rnorm</span>( <span class="dv">1000</span>, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="dv">1</span> ),</span>
<span id="cb338-3"><a href="performance-criteria.html#cb338-3" tabindex="-1"></a>              <span class="at">pred_LASSO =</span> <span class="fl">0.2</span> <span class="sc">*</span> ( theta <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="dv">1000</span>, <span class="at">sd =</span> <span class="fl">0.3</span>) ),</span>
<span id="cb338-4"><a href="performance-criteria.html#cb338-4" tabindex="-1"></a>              <span class="at">pred_BART =</span> theta <span class="sc">+</span> <span class="fu">rnorm</span>( <span class="dv">1000</span>, <span class="at">sd =</span> <span class="fl">0.6</span> ) ) </span>
<span id="cb338-5"><a href="performance-criteria.html#cb338-5" tabindex="-1"></a>tt<span class="sc">$</span>theta <span class="ot">=</span> tt<span class="sc">$</span>theta <span class="sc">*</span> <span class="dv">10</span></span>
<span id="cb338-6"><a href="performance-criteria.html#cb338-6" tabindex="-1"></a>tt<span class="sc">$</span>pred_LASSO <span class="ot">=</span> tt<span class="sc">$</span>pred_LASSO <span class="sc">*</span> <span class="dv">10</span></span>
<span id="cb338-7"><a href="performance-criteria.html#cb338-7" tabindex="-1"></a>tt<span class="sc">$</span>pred_BART <span class="ot">=</span> tt<span class="sc">$</span>pred_BART <span class="sc">*</span> <span class="dv">10</span></span>
<span id="cb338-8"><a href="performance-criteria.html#cb338-8" tabindex="-1"></a></span>
<span id="cb338-9"><a href="performance-criteria.html#cb338-9" tabindex="-1"></a>ttL <span class="ot">&lt;-</span> tt <span class="sc">%&gt;%</span></span>
<span id="cb338-10"><a href="performance-criteria.html#cb338-10" tabindex="-1"></a>  <span class="fu">pivot_longer</span>( <span class="at">cols =</span> <span class="fu">c</span>(pred_LASSO, pred_BART),</span>
<span id="cb338-11"><a href="performance-criteria.html#cb338-11" tabindex="-1"></a>                <span class="at">names_to =</span> <span class="st">&quot;method&quot;</span>,</span>
<span id="cb338-12"><a href="performance-criteria.html#cb338-12" tabindex="-1"></a>                <span class="at">names_prefix=</span><span class="st">&quot;pred_&quot;</span>,</span>
<span id="cb338-13"><a href="performance-criteria.html#cb338-13" tabindex="-1"></a>                <span class="at">values_to =</span> <span class="st">&quot;pred&quot;</span> ) <span class="sc">%&gt;%</span></span>
<span id="cb338-14"><a href="performance-criteria.html#cb338-14" tabindex="-1"></a>  <span class="fu">relocate</span>( method )</span>
<span id="cb338-15"><a href="performance-criteria.html#cb338-15" tabindex="-1"></a>ttL</span></code></pre></div>
<pre><code>## # A tibble: 2,000 × 4
##    method    ID   theta    pred
##    &lt;chr&gt;  &lt;int&gt;   &lt;dbl&gt;   &lt;dbl&gt;
##  1 LASSO      1   0.871   0.261
##  2 BART       1   0.871  -6.49 
##  3 LASSO      2  -4.68   -1.20 
##  4 BART       2  -4.68    2.13 
##  5 LASSO      3  -5.74   -1.66 
##  6 BART       3  -5.74  -13.2  
##  7 LASSO      4  -6.21   -0.607
##  8 BART       4  -6.21   -1.70 
##  9 LASSO      5 -12.7    -2.92 
## 10 BART       5 -12.7   -13.8  
## # ℹ 1,990 more rows</code></pre>
<p>If we calculate average RMSE performances for this single simulation run we would have:</p>
<div class="sourceCode" id="cb340"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb340-1"><a href="performance-criteria.html#cb340-1" tabindex="-1"></a>ttL <span class="sc">%&gt;%</span></span>
<span id="cb340-2"><a href="performance-criteria.html#cb340-2" tabindex="-1"></a>  <span class="fu">group_by</span>( method ) <span class="sc">%&gt;%</span></span>
<span id="cb340-3"><a href="performance-criteria.html#cb340-3" tabindex="-1"></a>  <span class="fu">summarise</span>( <span class="at">RMSE =</span> <span class="fu">sqrt</span>( <span class="fu">mean</span>( (theta <span class="sc">-</span> pred)<span class="sc">^</span><span class="dv">2</span> ) ) )</span></code></pre></div>
<pre><code>## # A tibble: 2 × 2
##   method  RMSE
##   &lt;chr&gt;  &lt;dbl&gt;
## 1 BART    5.72
## 2 LASSO   8.35</code></pre>
<p>But if you calculated how correlated the predictions were, you would get</p>
<div class="sourceCode" id="cb342"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb342-1"><a href="performance-criteria.html#cb342-1" tabindex="-1"></a>ttL <span class="sc">%&gt;%</span></span>
<span id="cb342-2"><a href="performance-criteria.html#cb342-2" tabindex="-1"></a>  <span class="fu">group_by</span>( method ) <span class="sc">%&gt;%</span></span>
<span id="cb342-3"><a href="performance-criteria.html#cb342-3" tabindex="-1"></a>  <span class="fu">summarise</span>( <span class="at">cor =</span> <span class="fu">cor</span>( theta, pred ),</span>
<span id="cb342-4"><a href="performance-criteria.html#cb342-4" tabindex="-1"></a>             <span class="at">spear =</span> <span class="fu">cor</span>( theta, pred, <span class="at">method =</span> <span class="st">&quot;spearman&quot;</span> ) )</span></code></pre></div>
<pre><code>## # A tibble: 2 × 3
##   method   cor spear
##   &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt;
## 1 BART   0.880 0.868
## 2 LASSO  0.961 0.957</code></pre>
<p>Now we see the LASSO is better at giving predictions that are ordered in the same order as the true values, while the BART method is not.</p>
<p>Different measures of performance target different concepts, and it is important to track which ones are most appropriate to your given circumstance.</p>
<p>The above example is a stylized version of a project investigating how well different machine learning tools predict individual treatment effects (e.g., if we were using covariates to predict how responsive someone would be to a given treatment).
If we are interested in which methods identify those most responsive to treatment, average spearman’s correlation might be a better measure of performance than average RMSE of the individual predictions.
In our paper, we report RMSE, SE, Bias, and Spearman’s correlation, so readers can better understand the details of why some methods are better or worse than others.</p>
</div>
</div>
<div id="summary-of-peformance-measures-1" class="section level2 hasAnchor" number="9.7">
<h2><span class="header-section-number">9.7</span> Summary of Peformance Measures<a href="performance-criteria.html#summary-of-peformance-measures-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We list most of the performance criteria we saw in this chapter in the table below, for reference:</p>
<table>
<colgroup>
<col width="25%" />
<col width="38%" />
<col width="36%" />
</colgroup>
<thead>
<tr class="header">
<th>Criterion</th>
<th>Definition</th>
<th>Estimate</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Bias</td>
<td><span class="math inline">\(\text{E}(T) - \theta\)</span></td>
<td><span class="math inline">\(\bar{T} - \theta\)</span></td>
</tr>
<tr class="even">
<td>Median bias</td>
<td><span class="math inline">\(\text{M}(T) - \theta\)</span></td>
<td><span class="math inline">\(\tilde{T} - \theta\)</span></td>
</tr>
<tr class="odd">
<td>Variance</td>
<td><span class="math inline">\(\text{E}\left[\left(T - \text{E}(T)\right)^2\right]\)</span></td>
<td><span class="math inline">\(S_T^2\)</span></td>
</tr>
<tr class="even">
<td>MSE</td>
<td><span class="math inline">\(\text{E}\left[\left(T - \theta\right)^2\right]\)</span></td>
<td><span class="math inline">\(\left(\bar{T} - \theta\right)^2 + S_T^2\)</span></td>
</tr>
<tr class="odd">
<td>MAE</td>
<td><span class="math inline">\(\text{M}\left[\left|T - \theta\right|\right]\)</span></td>
<td><span class="math inline">\(\left[\left|T - \theta\right|\right]_{R/2}\)</span></td>
</tr>
<tr class="even">
<td>Relative bias</td>
<td><span class="math inline">\(\text{E}(T) / \theta\)</span></td>
<td><span class="math inline">\(\bar{T} / \theta\)</span></td>
</tr>
<tr class="odd">
<td>Relative median bias</td>
<td><span class="math inline">\(\text{M}(T) / \theta\)</span></td>
<td><span class="math inline">\(\tilde{T} / \theta\)</span></td>
</tr>
<tr class="even">
<td>Relative MSE</td>
<td><span class="math inline">\(\text{E}\left[\left(T - \theta\right)^2\right] / \theta^2\)</span></td>
<td><span class="math inline">\(\frac{\left(\bar{T} - \theta\right)^2 + S_T^2}{\theta^2}\)</span></td>
</tr>
</tbody>
</table>
<ul>
<li>Bias and median bias are measures of whether the estimator is systematically higher or lower than the target parameter.</li>
<li>Variance is a measure of the <strong>precision</strong> of the estimator—that is, how far it deviates <em>from its average</em>. We might look at the square root of this, to assess the precision in the units of the original measure. This is the true SE of the estimator.</li>
<li>Mean-squared error is a measure of <strong>overall accuracy</strong>, i.e. is a measure how far we typically are from the truth. We more frequently use the root mean-squared error, or RMSE, which is just the square root of the MSE.</li>
<li>The median absolute deviation (MAD) is another measure of overall accuracy that is less sensitive to outlier estimates. The RMSE can be driven up by a single bad egg. The MAD is less sensitive to this.</li>
</ul>
</div>
<div id="implicit-estimands" class="section level2 hasAnchor" number="9.8">
<h2><span class="header-section-number">9.8</span> Estimands Not Represented By a Parameter<a href="performance-criteria.html#implicit-estimands" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In our Cluster RCT example, we focused on the estimand of the school-level ATE, represented by the model parameter <span class="math inline">\(\gamma_1\)</span>.
What if we were instead interested in the person-level average effect?
This estimand does not correspond to any input parameter in our data generating process.
Instead, it is defined <em>implicitly</em> by a combination of other parameters.
In order to compute performance characteristics such as bias and RMSE, we would need to calculate the parameter based on the inputs of the data-generating processes.
There are at least three possible ways to accomplish this.</p>
<p>One way is to use mathematical distribution theory to compute an implied parameter.
Our target parameter will be some function of the parameters and random variables in the data-generating process, and it may be possible to evaluate that function algebraically or numerically (i.e., using numerical integration functions such as <code>integrate()</code>).
Such an exercise can be very worthwhile if it provides insights into the relationship between the target parameter and the inputs of the data-generating process.
However, this approach requires knowledge of distribution theory, and it can get quite complicated and technical.<a href="#fn14" class="footnote-ref" id="fnref14"><sup>14</sup></a>
Other approaches are often feasible and more closely aligned with our focus on Monte Carlo simulation.</p>
<p>Another alternative approach is to simply generate a massive dataset—so large that can stand in for the entire data-generating model—and then simply calculate the target parameter of interest in this massive dataset. In the cluster-RCT example, we can apply this strategy by generating data from a very large number of clusters and then simply calculating the true person-average effect across all generated clusters.
If the dataset is big enough, then the uncertainty in this estimate will be negligible compared to the uncertainty in our simulation.</p>
<p>We implement this approach as follows, generating a dataset with 100,000 clusters:</p>
<div class="sourceCode" id="cb344"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb344-1"><a href="performance-criteria.html#cb344-1" tabindex="-1"></a>dat <span class="ot">&lt;-</span> <span class="fu">gen_cluster_RCT</span>( </span>
<span id="cb344-2"><a href="performance-criteria.html#cb344-2" tabindex="-1"></a>  <span class="at">n_bar =</span> <span class="dv">30</span>, <span class="at">J =</span> <span class="dv">100000</span>, </span>
<span id="cb344-3"><a href="performance-criteria.html#cb344-3" tabindex="-1"></a>  <span class="at">gamma_1 =</span> <span class="fl">0.3</span>, <span class="at">gamma_2 =</span> <span class="fl">0.5</span>,</span>
<span id="cb344-4"><a href="performance-criteria.html#cb344-4" tabindex="-1"></a>  <span class="at">sigma2_u =</span> <span class="fl">0.20</span>, <span class="at">sigma2_e =</span> <span class="fl">0.80</span>,</span>
<span id="cb344-5"><a href="performance-criteria.html#cb344-5" tabindex="-1"></a>  <span class="at">alpha =</span> <span class="fl">0.75</span>  </span>
<span id="cb344-6"><a href="performance-criteria.html#cb344-6" tabindex="-1"></a>)</span>
<span id="cb344-7"><a href="performance-criteria.html#cb344-7" tabindex="-1"></a>ATE_person <span class="ot">&lt;-</span> <span class="fu">mean</span>( dat<span class="sc">$</span>Yobs[dat<span class="sc">$</span>Z<span class="sc">==</span><span class="dv">1</span>] ) <span class="sc">-</span> <span class="fu">mean</span>( dat<span class="sc">$</span>Yobs[dat<span class="sc">$</span>Z<span class="sc">==</span><span class="dv">0</span>] )</span>
<span id="cb344-8"><a href="performance-criteria.html#cb344-8" tabindex="-1"></a>ATE_person</span></code></pre></div>
<pre><code>## [1] 0.3992419</code></pre>
<p>Note our estimate of the person-average effect of 0.4 is about what we would expect given the bias we saw earlier for the linear model.</p>
<p>With respect to the <code>ATE_person</code> estimand, the bias and RMSE of our estimators will shift, although SE will stay the same as in our performance calculations for the school-level average effect:</p>
<div class="sourceCode" id="cb346"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb346-1"><a href="performance-criteria.html#cb346-1" tabindex="-1"></a>runs <span class="sc">%&gt;%</span> </span>
<span id="cb346-2"><a href="performance-criteria.html#cb346-2" tabindex="-1"></a>  <span class="fu">group_by</span>( method ) <span class="sc">%&gt;%</span></span>
<span id="cb346-3"><a href="performance-criteria.html#cb346-3" tabindex="-1"></a>  <span class="fu">summarise</span>( </span>
<span id="cb346-4"><a href="performance-criteria.html#cb346-4" tabindex="-1"></a>    <span class="at">bias =</span> <span class="fu">mean</span>( ATE_hat <span class="sc">-</span> ATE_person ),</span>
<span id="cb346-5"><a href="performance-criteria.html#cb346-5" tabindex="-1"></a>    <span class="at">SE =</span> <span class="fu">sd</span>( ATE_hat ),</span>
<span id="cb346-6"><a href="performance-criteria.html#cb346-6" tabindex="-1"></a>    <span class="at">RMSE =</span> <span class="fu">sqrt</span>( <span class="fu">mean</span>( (ATE_hat <span class="sc">-</span> ATE_person)<span class="sc">^</span><span class="dv">2</span> ) )</span>
<span id="cb346-7"><a href="performance-criteria.html#cb346-7" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb346-8"><a href="performance-criteria.html#cb346-8" tabindex="-1"></a>  <span class="fu">mutate</span>( <span class="at">per_RMSE =</span> RMSE <span class="sc">/</span> RMSE[method<span class="sc">==</span><span class="st">&quot;LR&quot;</span>] )</span></code></pre></div>
<pre><code>## # A tibble: 3 × 5
##   method     bias    SE  RMSE per_RMSE
##   &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;
## 1 Agg    -0.0936  0.168 0.192     1.05
## 2 LR     -0.00937 0.183 0.184     1   
## 3 MLM    -0.0914  0.168 0.191     1.04</code></pre>
<p>For the person-weighted estimand, Agg and MLM are biased but LR is unbiased.
RMSE is now a tension between bias and reduced variance.
Overall, Agg and MLM are 4% worse than LR in terms of RMSE, because they have lower SEs but higher bias.</p>
<p>A further approach for calculating <code>ATE_person</code> would be to record the true person average effect of the dataset with each simulation iteration, and then average the sample-specific parameters at the end.
The overall average of the dataset-specific <code>ATE_person</code>s corresponds to the population person-level ATE.
This approach is effectively equivalent to generating a massive dataset—we just generate it in piece.</p>
<p>To implement this approach, we would need to modify the data-generating function <code>gen_cluster_RCT()</code> to track the additional information.
We might have, for example</p>
<div class="sourceCode" id="cb348"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb348-1"><a href="performance-criteria.html#cb348-1" tabindex="-1"></a>tx_effect <span class="ot">&lt;-</span> gamma_1 <span class="sc">+</span> gamma_2 <span class="sc">*</span> (nj<span class="sc">-</span>n_bar)<span class="sc">/</span>n_bar</span>
<span id="cb348-2"><a href="performance-criteria.html#cb348-2" tabindex="-1"></a>beta_0j <span class="ot">&lt;-</span> gamma_0 <span class="sc">+</span> Zj <span class="sc">*</span> tx_effect <span class="sc">+</span> u0j</span></code></pre></div>
<p>and then we would return <code>tx_effect</code> as well as <code>Yobs</code> and <code>Z</code> as a column in our dataset.
This approach is quite similar to directly calculating <em>potential outcomes</em>, as discussed in Chapter <a href="potential-outcomes.html#potential-outcomes">20</a>.</p>
<p>After modifying the data-generating function, we will also need to modify the analysis function(s) to record the sample-specific treatment effect parameter.
We might have, for example:</p>
<div class="sourceCode" id="cb349"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb349-1"><a href="performance-criteria.html#cb349-1" tabindex="-1"></a>analyze_data <span class="ot">=</span> <span class="cf">function</span>( dat ) {</span>
<span id="cb349-2"><a href="performance-criteria.html#cb349-2" tabindex="-1"></a>  MLM <span class="ot">&lt;-</span> <span class="fu">analysis_MLM</span>( dat )</span>
<span id="cb349-3"><a href="performance-criteria.html#cb349-3" tabindex="-1"></a>  LR <span class="ot">&lt;-</span> <span class="fu">analysis_OLS</span>( dat )</span>
<span id="cb349-4"><a href="performance-criteria.html#cb349-4" tabindex="-1"></a>  Agg <span class="ot">&lt;-</span> <span class="fu">analysis_agg</span>( dat )</span>
<span id="cb349-5"><a href="performance-criteria.html#cb349-5" tabindex="-1"></a>  res <span class="ot">&lt;-</span> <span class="fu">bind_rows</span>( </span>
<span id="cb349-6"><a href="performance-criteria.html#cb349-6" tabindex="-1"></a>    <span class="at">MLM =</span> MLM, <span class="at">LR =</span> LR, <span class="at">Agg =</span> Agg,</span>
<span id="cb349-7"><a href="performance-criteria.html#cb349-7" tabindex="-1"></a>    <span class="at">.id =</span> <span class="st">&quot;method&quot;</span> </span>
<span id="cb349-8"><a href="performance-criteria.html#cb349-8" tabindex="-1"></a>  )</span>
<span id="cb349-9"><a href="performance-criteria.html#cb349-9" tabindex="-1"></a>  res<span class="sc">$</span>ATE_person <span class="ot">&lt;-</span> <span class="fu">mean</span>( dat<span class="sc">$</span>tx_effect )</span>
<span id="cb349-10"><a href="performance-criteria.html#cb349-10" tabindex="-1"></a>  <span class="fu">return</span>( res )</span>
<span id="cb349-11"><a href="performance-criteria.html#cb349-11" tabindex="-1"></a>}</span></code></pre></div>
<p>Now when we run our simulation, we will have a column which is the true person-level average treatment effect for each dataset.
We could then take the average of these value across replications to estimate the true person average treatment effect in the population, and then use this as the target parameter for performance calculations.</p>
<p>Clearly, an estimand not represented by any single input parameter is more difficult to work with, but it is not impossible.
The key is to be clear about what you are trying to estimate, since the performance of an estimator depends critically on the estimand to which the estimator is compared.</p>
</div>
<div id="MCSE" class="section level2 hasAnchor" number="9.9">
<h2><span class="header-section-number">9.9</span> Uncertainty in Performance Estimates (the Monte Carlo Standard Error)<a href="performance-criteria.html#MCSE" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Our performance metrics are defined as average performance across an infinite number of trials.
Of course, in our simulations we only run a finite number of trials, and estimate the performance metrics with the sample of trials we generate.
For example, if we are assessing coverage across 100 trials, we can calculate what fraction rejected the null for that 100.
This is an <em>estimate</em> of the true coverage rate.
Due to random chance, we might see a higher, or lower, proportion rejected than what we would see if we ran the simulation forever.</p>
<p>To account for estimation uncertainty we want associated uncertainty estimates to go with our point estimates of performance.
We want to, in other words, treat our simulation results as a dataset in its own right.
(And yes, this is quite meta!)</p>
<p>Once we frame the problem in these terms, it is relatively straightforward to calculate standard errors for most of the performance critera because we have an independent and identically distributed set of measurements.
We call these standard errors Monte Carlo Simulation Errors, or MCSEs.
For some of the performance metrics we have to be a bit more clever, as we will discuss below.</p>
<p>We list MCSE expressions for many of our straightforward performance measures on the following table.
In reading the table, recall that, for an estimator <span class="math inline">\(T\)</span>, we have <span class="math inline">\(S_T\)</span> being the standard deviation of <span class="math inline">\(T\)</span> across our simulation runs (i.e., our estimated true Standard Error).
We also have</p>
<ul>
<li>Sample skewness (standardized): <span class="math inline">\(\displaystyle{g_T = \frac{1}{R S_T^3}\sum_{r=1}^R \left(T_r - \bar{T}\right)^3}\)</span></li>
<li>Sample kurtosis (standardized): <span class="math inline">\(\displaystyle{k_T = \frac{1}{R S_T^4} \sum_{r=1}^R \left(T_r - \bar{T}\right)^4}\)</span></li>
</ul>
<table>
<colgroup>
<col width="66%" />
<col width="33%" />
</colgroup>
<thead>
<tr class="header">
<th>Criterion for T</th>
<th>MCSE</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Bias (<span class="math inline">\(T-\theta\)</span>)</td>
<td><span class="math inline">\(\sqrt{S_T^2/ R}\)</span></td>
</tr>
<tr class="even">
<td>Variance (<span class="math inline">\(S_T^2\)</span>)</td>
<td><span class="math inline">\(\displaystyle{S_T^2 \sqrt{\frac{k_T - 1}{R}}}\)</span></td>
</tr>
<tr class="odd">
<td>MSE</td>
<td>see below</td>
</tr>
<tr class="even">
<td>MAD</td>
<td>-</td>
</tr>
<tr class="odd">
<td>Power &amp; Validity (<span class="math inline">\(r_\alpha\)</span>)</td>
<td><span class="math inline">\(\sqrt{ r_\alpha \left(1 - r_\alpha\right) / R}\)</span></td>
</tr>
<tr class="even">
<td>Coverage (<span class="math inline">\(\omega_\beta\)</span>)</td>
<td><span class="math inline">\(\sqrt{\omega_\beta \left(1 - \omega_\beta\right) / R}\)</span></td>
</tr>
<tr class="odd">
<td>Average length (<span class="math inline">\(\text{E}(W)\)</span>)</td>
<td><span class="math inline">\(\sqrt{S_W^2 / R}\)</span></td>
</tr>
</tbody>
</table>
<p>The MCSE for the MSE is a bit more complicated, and does not quite fit on our table:
<span class="math display">\[ \widehat{MCSE}( \widehat{MSE} ) = \displaystyle{\sqrt{\frac{1}{R}\left[S_T^4 (k_T - 1) + 4 S_T^3 g_T\left(\bar{T} - \theta\right) + 4 S_T^2 \left(\bar{T} - \theta\right)^2\right]}} .\]</span></p>
<p>For relative quantities with respect to an estimand, simply divide the criterion by the target estimand.
E.g., for relative bias <span class="math inline">\(T / \theta\)</span>, the standard error would be
<span class="math display">\[ SE\left( \frac{T}{\theta} \right) = \frac{1}{\theta} SE(T) = \sqrt{\frac{S_T^2}{R\theta^2}} .\]</span></p>
<p>For square rooted quantities, such as the SE for the true SE (square root of the Variance) or the RMSE (square root of MSE) we can use the Delta method.
The Delta method says (with some conditions), that if we assume <span class="math inline">\(X \sim N( \phi, V )\)</span>, then we can approximate the distribution of <span class="math inline">\(g(X)\)</span> for some continuous function <span class="math inline">\(g(\cdot)\)</span> as
<span class="math display">\[ g(X) \sim N\left( g(\phi), \;\; g&#39;(\phi)^2\cdot V \right) , \]</span>
where <span class="math inline">\(g&#39;(\phi)\)</span> is the derivative of <span class="math inline">\(g(\cdot)\)</span> evaluated at <span class="math inline">\(\phi\)</span>.
In other words,
<span class="math display">\[ SE( g(\hat{X}) ) \approx g&#39;(\theta)  \times SE(\hat{X}) .\]</span>
For estimation, we plug in <span class="math inline">\(\hat{\theta}\)</span> and our estimate of <span class="math inline">\(SE(\hat{X})\)</span> into the above.
Back to the square root, we have <span class="math inline">\(g(x) = \sqrt(x)\)</span> and <span class="math inline">\(g&#39;(x) = 1/2\sqrt(x)\)</span>.
This gives, for example, the estimated MCSE of the SE as
<span class="math display">\[ \widehat{SE}( \widehat{SE} ) = \widehat{SE}( S^2_T ) = \frac{1}{2S^2_T} \widehat{SE}( S^2_T ) = \frac{1}{2S^2_T} S_T^2 \sqrt{\frac{k_T - 1}{R}} = \frac{1}{2} \sqrt{\frac{k_T - 1}{R}} .\]</span></p>
<div id="mcse-for-relative-variance-estimators" class="section level3 hasAnchor" number="9.9.1">
<h3><span class="header-section-number">9.9.1</span> MCSE for Relative Variance Estimators<a href="performance-criteria.html#mcse-for-relative-variance-estimators" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Estimating the MCSE of the relative bias or relative MSE of a (squared) standard error estimator, i.e., of <span class="math inline">\(E( \widehat{SE^2} - SE^2 ) / SE^2 )\)</span> or <span class="math inline">\(\widehat{MSE} / MSE\)</span>, is complicated by the appearance of an estimated quantity, <span class="math inline">\(SE^2\)</span> or <span class="math inline">\(MSE\)</span>, in the denominator of the ratio.
This renders the simple division approach from above unusable, technically speaking.
The problem is we cannot use our clean expressions for MCSEs of relative performance measures since we are not taking the uncertainty of our denominator into account.</p>
<p>To properly assess the overall MCSE, we need to do something else.
One approach is to use the <em>jackknife</em> technique.
Let <span class="math inline">\(\bar{V}_{(j)}\)</span> and <span class="math inline">\(S_{T(j)}^2\)</span> be the average squared standard error estimate and the true variance estimate calculated from the set of replicates <strong><em>that excludes replicate <span class="math inline">\(j\)</span></em></strong>, for <span class="math inline">\(j = 1,...,R\)</span>.
The relative bias estimate, excluding replicate <span class="math inline">\(j\)</span> would then be <span class="math inline">\(\bar{V}_{(j)} / S_{T(j)}^2\)</span>.
Calculating all <span class="math inline">\(R\)</span> versions of this relative bias estimate and taking the variance of these <span class="math inline">\(R\)</span> versions yields the jackknife variance estimator:</p>
<p><span class="math display">\[
MCSE\left( \frac{ \widehat{SE}^2 }{SE^2} \right) = \frac{1}{R} \sum_{j=1}^R \left(\frac{\bar{V}_{(j)}}{S_{T(j)}^2} - \frac{\bar{V}}{S_T^2}\right)^2.
\]</span></p>
<p>This would be quite time-consuming to compute if we did it by brute force. However, a few algebra tricks provide a much quicker way. The tricks come from observing that</p>
<p><span class="math display">\[
\begin{aligned}
\bar{V}_{(j)} &amp;= \frac{1}{R - 1}\left(R \bar{V} - V_j\right) \\
S_{T(j)}^2 &amp;= \frac{1}{R - 2} \left[(R - 1) S_T^2 - \frac{R}{R - 1}\left(T_j - \bar{T}\right)^2\right]
\end{aligned}
\]</span>
These formulas can be used to avoid re-computing the mean and sample variance from every subsample.
Instead, you calculate the overall mean and overall variance, and then do a small adjustment with each jackknife iteration.
You can even implement this with vector processing in R!</p>
</div>
<div id="calculating-mcses-with-the-simhelpers-package" class="section level3 hasAnchor" number="9.9.2">
<h3><span class="header-section-number">9.9.2</span> Calculating MCSEs With the <code>simhelpers</code> Package<a href="performance-criteria.html#calculating-mcses-with-the-simhelpers-package" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The <code>simhelper</code> package is designed to calculate MCSEs (and the performance metrics themselves) for you.
It is easy to use: take this set of simulation runs on the Welch dataset:</p>
<div class="sourceCode" id="cb350"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb350-1"><a href="performance-criteria.html#cb350-1" tabindex="-1"></a><span class="fu">library</span>( simhelpers )</span>
<span id="cb350-2"><a href="performance-criteria.html#cb350-2" tabindex="-1"></a><span class="fu">data</span>( welch_res )</span>
<span id="cb350-3"><a href="performance-criteria.html#cb350-3" tabindex="-1"></a>welch <span class="ot">&lt;-</span> welch_res <span class="sc">%&gt;%</span></span>
<span id="cb350-4"><a href="performance-criteria.html#cb350-4" tabindex="-1"></a>  <span class="fu">filter</span>( method <span class="sc">==</span> <span class="st">&quot;t-test&quot;</span> ) <span class="sc">%&gt;%</span></span>
<span id="cb350-5"><a href="performance-criteria.html#cb350-5" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">select</span>( <span class="sc">-</span>method, <span class="sc">-</span>seed, <span class="sc">-</span>iterations )</span>
<span id="cb350-6"><a href="performance-criteria.html#cb350-6" tabindex="-1"></a></span>
<span id="cb350-7"><a href="performance-criteria.html#cb350-7" tabindex="-1"></a>welch</span></code></pre></div>
<pre><code>## # A tibble: 8,000 × 8
##       n1    n2 mean_diff      est    var p_val
##    &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;
##  1    50    50         0  0.0258  0.0954 0.934
##  2    50    50         0  0.00516 0.0848 0.986
##  3    50    50         0 -0.0798  0.0818 0.781
##  4    50    50         0 -0.0589  0.102  0.854
##  5    50    50         0  0.0251  0.118  0.942
##  6    50    50         0 -0.115   0.106  0.725
##  7    50    50         0  0.157   0.115  0.645
##  8    50    50         0 -0.213   0.121  0.543
##  9    50    50         0  0.509   0.117  0.139
## 10    50    50         0 -0.354   0.0774 0.206
## # ℹ 7,990 more rows
## # ℹ 2 more variables: lower_bound &lt;dbl&gt;,
## #   upper_bound &lt;dbl&gt;</code></pre>
<p>We can calculate performance metrics across all the range of scenarios.
Here is the rejection rate:</p>
<div class="sourceCode" id="cb352"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb352-1"><a href="performance-criteria.html#cb352-1" tabindex="-1"></a>welch_sub <span class="ot">=</span> <span class="fu">filter</span>( welch, n1 <span class="sc">==</span> <span class="dv">50</span>, n2 <span class="sc">==</span> <span class="dv">50</span>, mean_diff<span class="sc">==</span><span class="dv">0</span> )</span>
<span id="cb352-2"><a href="performance-criteria.html#cb352-2" tabindex="-1"></a><span class="fu">calc_rejection</span>(welch_sub, p_val)</span></code></pre></div>
<pre><code>##   K_rejection rej_rate rej_rate_mcse
## 1        1000    0.048   0.006759882</code></pre>
<p>And coverage:</p>
<div class="sourceCode" id="cb354"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb354-1"><a href="performance-criteria.html#cb354-1" tabindex="-1"></a><span class="fu">calc_coverage</span>(welch_sub, lower_bound, upper_bound, mean_diff)</span></code></pre></div>
<pre><code>## # A tibble: 1 × 5
##   K_coverage coverage coverage_mcse width
##        &lt;int&gt;    &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;
## 1       1000    0.952       0.00676  1.25
## # ℹ 1 more variable: width_mcse &lt;dbl&gt;</code></pre>
<p>Using <code>tidyverse</code> it is easy to process across scenarios (more on experimental design and multiple scenarios later):</p>
<div class="sourceCode" id="cb356"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb356-1"><a href="performance-criteria.html#cb356-1" tabindex="-1"></a>welch <span class="sc">%&gt;%</span> <span class="fu">group_by</span>(n1,n2,mean_diff) <span class="sc">%&gt;%</span></span>
<span id="cb356-2"><a href="performance-criteria.html#cb356-2" tabindex="-1"></a>  <span class="fu">summarise</span>( <span class="fu">calc_rejection</span>( <span class="at">p_values =</span> p_val ) )</span></code></pre></div>
<pre><code>## # A tibble: 8 × 6
## # Groups:   n1, n2 [2]
##      n1    n2 mean_diff K_rejection rej_rate
##   &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;       &lt;int&gt;    &lt;dbl&gt;
## 1    50    50       0          1000    0.048
## 2    50    50       0.5        1000    0.34 
## 3    50    50       1          1000    0.876
## 4    50    50       2          1000    1    
## 5    50    70       0          1000    0.027
## 6    50    70       0.5        1000    0.341
## 7    50    70       1          1000    0.904
## 8    50    70       2          1000    1    
## # ℹ 1 more variable: rej_rate_mcse &lt;dbl&gt;</code></pre>
</div>
<div id="mcse-calculation-in-our-cluster-rct-example" class="section level3 hasAnchor" number="9.9.3">
<h3><span class="header-section-number">9.9.3</span> MCSE Calculation in our Cluster RCT Example<a href="performance-criteria.html#mcse-calculation-in-our-cluster-rct-example" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We can check our MCSEs for our performance measures to see if we have enough simulation trials to give us precise enough estimates to believe the differences we reported earlier.
In particular, we have:</p>
<div class="sourceCode" id="cb358"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb358-1"><a href="performance-criteria.html#cb358-1" tabindex="-1"></a><span class="fu">library</span>( simhelpers )</span>
<span id="cb358-2"><a href="performance-criteria.html#cb358-2" tabindex="-1"></a>runs<span class="sc">$</span>ATE <span class="ot">=</span> ATE</span>
<span id="cb358-3"><a href="performance-criteria.html#cb358-3" tabindex="-1"></a>runs <span class="sc">%&gt;%</span> </span>
<span id="cb358-4"><a href="performance-criteria.html#cb358-4" tabindex="-1"></a>  <span class="fu">summarise</span>( <span class="fu">calc_absolute</span>( <span class="at">estimates =</span> ATE_hat,</span>
<span id="cb358-5"><a href="performance-criteria.html#cb358-5" tabindex="-1"></a>                            <span class="at">true_param =</span> ATE,</span>
<span id="cb358-6"><a href="performance-criteria.html#cb358-6" tabindex="-1"></a>                            <span class="at">criteria =</span> <span class="fu">c</span>(<span class="st">&quot;bias&quot;</span>,<span class="st">&quot;stddev&quot;</span>, <span class="st">&quot;rmse&quot;</span>)) ) <span class="sc">%&gt;%</span></span>
<span id="cb358-7"><a href="performance-criteria.html#cb358-7" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">select</span>( <span class="sc">-</span>K_absolute ) <span class="sc">%&gt;%</span></span>
<span id="cb358-8"><a href="performance-criteria.html#cb358-8" tabindex="-1"></a>  knitr<span class="sc">::</span><span class="fu">kable</span>(<span class="at">digits=</span><span class="dv">3</span>)</span></code></pre></div>
<table>
<thead>
<tr class="header">
<th align="right">bias</th>
<th align="right">bias_mcse</th>
<th align="right">stddev</th>
<th align="right">stddev_mcse</th>
<th align="right">rmse</th>
<th align="right">rmse_mcse</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">0.034</td>
<td align="right">0.003</td>
<td align="right">0.178</td>
<td align="right">0.002</td>
<td align="right">0.181</td>
<td align="right">0.003</td>
</tr>
</tbody>
</table>
<p>We see the MCSEs are quite small relative to the linear regression bias term and all the SEs (<code>stddev</code>) and RMSEs: we have simulated enough runs to see the gross trends identified.
We have <em>not</em> simulated enough to for sure know if MLM and Agg are not slightly biased. Given our MCSEs, they could have true bias of around 0.01 (two MCSEs).</p>
</div>
</div>
<div id="concluding-thoughts" class="section level2 hasAnchor" number="9.10">
<h2><span class="header-section-number">9.10</span> Concluding thoughts<a href="performance-criteria.html#concluding-thoughts" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In practice, many data analysis procedures produce multiple pieces of information—not just point estimates, but also standard errors and confidence intervals and p-values from null hypothesis tests—and those pieces are inter-related.
For instance, a confidence interval is usually computed from a point estimate and its standard error.
Consequently, the performance of that confidence interval will be strongly affected by whether the point estimator is biased and whether the standard error tends to understates or over-states the true uncertainty.
Likewise, the performance of a hypothesis testing procedure will often strongly depend on the properties of the point estimator and standard error used to compute the test.<br />
Thus, most simulations will involve evaluating a data analysis procedure on several metrics to arrive at a holistic understanding of its performance.</p>
<p>Moreover, the main aim of many simulations is to compare the performance of several different estimators or to determine which of several data analysis procedures is preferable.
For such aims, we will need to use the performance metrics to understand whether a set of procedures work differently, when and how one is superior to the other, and what factors influence differences in performance.
To fully understand the advantages and trade-offs among a set of estimators, we will generally need to compare them using several performance metrics.</p>
</div>
<div id="exercises-6" class="section level2 hasAnchor" number="9.11">
<h2><span class="header-section-number">9.11</span> Exercises<a href="performance-criteria.html#exercises-6" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="Brown-Forsythe-performance" class="section level3 hasAnchor" number="9.11.1">
<h3><span class="header-section-number">9.11.1</span> Brown and Forsythe (1974)<a href="performance-criteria.html#Brown-Forsythe-performance" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Continuing the exercises from the prior chapters, estimate rejection rates of the BFF* test for the parameter values in the fifth line of Table 1 of Brown and Forsythe (1974).</p>
</div>
<div id="jackknife-MCSE" class="section level3 hasAnchor" number="9.11.2">
<h3><span class="header-section-number">9.11.2</span> Jackknife calculation of MCSEs<a href="performance-criteria.html#jackknife-MCSE" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Implement the jackknife as described above in code. Check your answers against the <code>simhelpers</code> package for the built-in <code>t_res</code> dataset:</p>
<div class="sourceCode" id="cb359"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb359-1"><a href="performance-criteria.html#cb359-1" tabindex="-1"></a><span class="fu">library</span>( simhelpers )</span>
<span id="cb359-2"><a href="performance-criteria.html#cb359-2" tabindex="-1"></a><span class="fu">calc_relative</span>(<span class="at">data =</span> t_res, <span class="at">estimates =</span> est, <span class="at">true_param =</span> true_param)</span></code></pre></div>
<pre><code>## # A tibble: 1 × 7
##   K_relative rel_bias rel_bias_mcse rel_mse
##        &lt;int&gt;    &lt;dbl&gt;         &lt;dbl&gt;   &lt;dbl&gt;
## 1       1000     1.00        0.0128   0.163
## # ℹ 3 more variables: rel_mse_mcse &lt;dbl&gt;,
## #   rel_rmse &lt;dbl&gt;, rel_rmse_mcse &lt;dbl&gt;</code></pre>
</div>
<div id="cluster-RCT-SPATE" class="section level3 hasAnchor" number="9.11.3">
<h3><span class="header-section-number">9.11.3</span> Distribution theory for person-level average treatment effects<a href="performance-criteria.html#cluster-RCT-SPATE" class="anchor-section" aria-label="Anchor link to header"></a></h3>
</div>
<div id="multiple-scenario-performance" class="section level3 hasAnchor" number="9.11.4">
<h3><span class="header-section-number">9.11.4</span> Multiple scenarios<a href="performance-criteria.html#multiple-scenario-performance" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>As foreground to the following chapters, can you explore multiple scenarios for the cluster RCT example to see if the trends are common? First write a function that takes a parameter, runs the entire simulation, and returns the results as a small table. You pick which parameter, e.g., average treatment effect, <code>alpha</code>, or whatever you like), that you wish to vary. Here is a skeleton for the function:</p>
<div class="sourceCode" id="cb361"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb361-1"><a href="performance-criteria.html#cb361-1" tabindex="-1"></a>my_simulation <span class="ot">&lt;-</span> <span class="cf">function</span>( my_param ) {</span>
<span id="cb361-2"><a href="performance-criteria.html#cb361-2" tabindex="-1"></a>  <span class="co"># call the sim_function() simulation function from the end of last</span></span>
<span id="cb361-3"><a href="performance-criteria.html#cb361-3" tabindex="-1"></a>  <span class="co"># chapter, setting the parameter you want to vary to my_param</span></span>
<span id="cb361-4"><a href="performance-criteria.html#cb361-4" tabindex="-1"></a>  </span>
<span id="cb361-5"><a href="performance-criteria.html#cb361-5" tabindex="-1"></a>  <span class="co"># Analyze the results, generating a table of performance metrics,</span></span>
<span id="cb361-6"><a href="performance-criteria.html#cb361-6" tabindex="-1"></a>  <span class="co"># e.g., bias or coverage. Make sure your analysis is a data frame,</span></span>
<span id="cb361-7"><a href="performance-criteria.html#cb361-7" tabindex="-1"></a>  <span class="co"># like we saw earlier this chapter.</span></span>
<span id="cb361-8"><a href="performance-criteria.html#cb361-8" tabindex="-1"></a>  </span>
<span id="cb361-9"><a href="performance-criteria.html#cb361-9" tabindex="-1"></a>  <span class="co"># Return results</span></span>
<span id="cb361-10"><a href="performance-criteria.html#cb361-10" tabindex="-1"></a>}</span></code></pre></div>
<p>Then use code like the following to generate a set of results measured as a function of a varying parameter:</p>
<div class="sourceCode" id="cb362"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb362-1"><a href="performance-criteria.html#cb362-1" tabindex="-1"></a>vals <span class="ot">=</span> <span class="fu">seq</span>( start, stop, <span class="at">length.out =</span> <span class="dv">5</span> )</span>
<span id="cb362-2"><a href="performance-criteria.html#cb362-2" tabindex="-1"></a>res <span class="ot">=</span> <span class="fu">map_df</span>( vals, my_simulation ) </span></code></pre></div>
<p>The above code will give you a data frame of results, one column for each performance measure.
Finally, you can use this table and plot the performance measure as a function of the varying parameter.</p>

</div>
</div>
</div>



<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0">
<div id="ref-robustbase" class="csl-entry">
Maechler, Martin, Peter Rousseeuw, Christophe Croux, Valentin Todorov, Andreas Ruckstuhl, Matias Salibian-Barrera, Tobias Verbeke, Manuel Koller, Eduardo L. T. Conceicao, and Maria Anna di Palma. 2024. <em>Robustbase: Basic Robust Statistics</em>. <a href="http://robustbase.r-forge.r-project.org/">http://robustbase.r-forge.r-project.org/</a>.
</div>
<div id="ref-Maronna2006robust" class="csl-entry">
Maronna, Ricardo A., R. Douglas Martin, and Víctor J. Yohai. 2006. <em>Robust Statistics: Theory and Methods</em>. Wiley Series in Probability and Statistics. Chichester (GB): J. Wiley.
</div>
<div id="ref-Rousseeuw1993alternatives" class="csl-entry">
Rousseeuw, Peter J., and Christophe Croux. 1993. <span>“Alternatives to the <span>Median Absolute Deviation</span>.”</span> <em>Journal of the American Statistical Association</em> 88 (424): 1273–83. <a href="https://doi.org/10.1080/01621459.1993.10476408">https://doi.org/10.1080/01621459.1993.10476408</a>.
</div>
<div id="ref-sundberg2003conditional" class="csl-entry">
Sundberg, Rolf. 2003. <span>“<span class="nocase">Conditional statistical inference and quantification of relevance</span>.”</span> <em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em> 65 (1): 299–315.
</div>
<div id="ref-westfall2013understanding" class="csl-entry">
Westfall, Peter H, and Kevin SS Henning. 2013. <em>Understanding Advanced Statistical Methods</em>. Vol. 543. CRC Press Boca Raton, FL.
</div>
<div id="ref-Wilcox2022introduction" class="csl-entry">
Wilcox, Rand R. 2022. <em>Introduction to Robust Estimation and Hypothesis Testing</em>. Fifth edition. London, United Kingdom San Diego, United States Cambridge, MA Oxford, United Kingdom: Academic Press, an imprint of Elsevier.
</div>
</div>
<div class="footnotes">
<hr />
<ol start="12">
<li id="fn12"><p>Generally, when people say “Standard Error” they actually mean <em>estimated</em> Standard Error, (<span class="math inline">\(\widehat{SE}\)</span>), as we would calculate in a real data analysis (where we have only a single realization of the data-generating process). It is easy to forget that this standard error is itself an estimate of a true parameter, and thus has its own uncertainty.<a href="performance-criteria.html#fnref12" class="footnote-back">↩︎</a></p></li>
<li id="fn13"><p>For a normally distributed sampling distribution, the interquartile range is 1.35 SD; with <span class="math inline">\(w = 2\)</span>, the lower and upper thresholds would then fall at <span class="math inline">\(\pm 3.37\)</span> SD, or the <span class="math inline">\(0.04^{th}\)</span> and <span class="math inline">\(99.96^{th}\)</span> percentiles.
Still assuming a normal sampling distribution), taking <span class="math inline">\(w = 2.5\)</span> will mean that the thresholds fall at the <span class="math inline">\(0.003^{th}\)</span> and <span class="math inline">\(99.997^{th}\)</span> percentiles.<a href="performance-criteria.html#fnref13" class="footnote-back">↩︎</a></p></li>
<li id="fn14"><p>In the cluster-RCT example, the distribution theory is tractable. See Exercise <a href="performance-criteria.html#cluster-RCT-SPATE">9.11.3</a><a href="performance-criteria.html#fnref14" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="running-the-simulation-process.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="exp-design.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": false,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": "https://github.com/jepusto/Designing-Simulations-in-R/edit/master/040-Performance-criteria.Rmd",
    "text": "Edit"
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": ["Designing-Simulations-in-R.pdf", "Designing-Simulations-in-R.epub"],
  "search": {
    "engine": "lunr",
    "options": null
  },
  "toc": {
    "collapse": "section",
    "scroll_highlight": true
  },
  "toolbar": {
    "position": "fixed"
  },
  "info": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
