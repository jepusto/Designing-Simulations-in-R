[["data-generating-processes.html", "Chapter 6 Data-Generating Processes 6.1 Examples 6.2 Components of a DGP 6.3 A statistical model is a recipe for data generation 6.4 Checking the data-generating function 6.5 Example: Simulating clustered data 6.6 Extension: Standardization in a data generating process 6.7 Exercises", " Chapter 6 Data-Generating Processes As we saw in Chapter 4, the first step of a simulation is creating artificial data based on some process where we know (and can control) the truth. This step is what we call the data generating process (DGP). Think of it as a recipe for cooking up artificial data, which can be applied over and over, any time we’re hungry for a new dataset. Like a good recipe, a good DGP needs to be complete—it cannot be missing ingredients and it cannot omit any steps. Unlike cooking or baking, however, DGPs are usually specified in terms of a statistical model, or a set of equations involving constants, parameter values, and random variables, which we will instantiate as an R function (or perhaps a set of functions). More complex DGPs, such as those for hierarchical data or other latent variable models, will often involve a series of several equations that describe different dimensions or levels of the model, which need to be followed in sequence to produce an artificial dataset. Designing DGPs and implementing them in R code involves making choices about what aspects of the model we want to be able to control and how to set up the parameters of the model. In this chapter, we give a high-level overview of DGPs, discuss some of the choices and challenges involved in designing them, and demonstrate how to write R functions for the DGP. We then present a detailed example involving a hierarchical DGP for generating data on students nested within schools. 6.1 Examples Before diving in, it is helpful to consider a few examples that we will return to throughout this and subsequent chapters. 6.1.1 Bivariate Poisson model Suppose we want to understand how the usual Pearson sample correlation estimator behaves with non-normal data or to investigate how the Pearson correlation relates to Spearman’s rank correlation coefficient. To look into such questions, one DGP we might entertain is a bivariate Poisson model, which is a distribution for a pair of counts, \\(C_1,C_2\\), where each count follows a Poisson distribution and where the pair of counts may be correlated. We will denote the expected values of the counts as \\(\\mu_1\\) and \\(\\mu_2\\) and the correlation between the counts as \\(\\rho\\). To simulate a dataset based on this model, we would first need to choose how many observations to generate. Call this sample size \\(N\\). One way to generate data following a bivariate Poisson model is to generate three independent Poisson random variables for each of the \\(N\\) observations: \\[ \\begin{aligned} Z_0 &amp;\\sim Pois\\left( \\rho \\sqrt{\\mu_1 \\mu_2}\\right) \\\\ Z_1 &amp;\\sim Pois\\left(\\mu_1 - \\rho \\sqrt{\\mu_1 \\mu_2}\\right) \\\\ Z_2 &amp;\\sim Pois\\left(\\mu_2 - \\rho \\sqrt{\\mu_1 \\mu_2}\\right) \\end{aligned} \\] and then combine the pieces to create two dependent observations: \\[ \\begin{aligned} C_1 &amp;= Z_0 + Z_1 \\\\ C_2 &amp;= Z_0 + Z_2. \\end{aligned} \\] An interesting feature of this model is that the range of possible correlations is constrained: only positive correlations are possible and, because each of the independent pieces must have a non-negative mean, the maximum possible correlation is \\(\\sqrt{\\frac{\\min\\{\\mu_1,\\mu_2\\}}{\\max\\{\\mu_1,\\mu_2\\}}}\\). 6.1.2 One-way analysis of variance As a second example, consider the one-way ANOVA example from Chapter 5. Here, we consider observations on some variable \\(X\\) drawn from a population consisting of \\(G\\) groups, where group \\(g\\) has population mean \\(\\mu_g\\) and population variance \\(\\sigma_g^2\\) for \\(g = 1,...,G\\). A simulated dataset consists of \\(n_g\\) observations from each group \\(g = 1,...,G\\), where \\(X_{ig}\\) is the measurement for observation \\(i\\) in group \\(g\\). The statistical model for these data can be written as follows: \\[ X_{ig} = \\mu_g + \\epsilon_{ig}, \\quad \\mbox{with} \\quad \\epsilon_{ig} \\sim N( 0, \\sigma^2_g ) \\] Alternately, we could write the model as \\[ X_{ig} \\sim N( \\mu_g, \\sigma_g^2 ) \\] for \\(i = 1,...,n_g\\) and \\(g = 1,...,G\\). 6.1.3 Hierarchical model for students nested within schools Suppose we are interested in understanding the distribution of student academic performance, and specifically how current performance relates to past performance. We collect data on these characteristic for many students from multiple schools, where we have data for multiple students in most or all of the schools. We will index the schools using \\(j = 1,...,J\\) and let \\(n_j\\) denote the number of students observed in scool \\(j\\). Say that \\(X_{ij}\\) and \\(Y_{ij}\\) are the measure of past and current academic performance for student \\(i\\) in school \\(j\\), for \\(1 = 1,...,n_j\\) and \\(j = 1,...,J\\). A widely used approach for describing such data is a heirarchical linear model (HLM). One way to write this model is in two parts. First, we consider a regression model that relates the current and past academic performance of students within school \\(j\\): \\[ Y_{ij} = \\beta_{0j} + \\beta_{1j} X_{ij} + \\epsilon_{ij}, \\qquad \\epsilon_{ij} \\sim N(\\mu_j, \\sigma^2). \\] Second, we might imagine that the school-specific intercepts follow a normal distribution around an overall population average \\(\\gamma_{00}\\), \\[ \\beta_{0j} \\sim N(\\gamma_{00}, \\tau^2), \\] while the slope of the regression is constant across schools, so \\(\\beta_{1j} = \\gamma_{10}\\) for \\(j = 1,...,J\\). If we only consider the first stage of this model, it looks a bit like the one-way ANOVA model from the previous example: in both cases, we have multiple observations from each of several groups. The main distinction is that the ANOVA model treats the \\(G\\) groups as a fixed set, whereas the HLM treats the set of \\(J\\) schools as sampled from a larger population of schools. 6.2 Components of a DGP A DGP involves a statistical model with parameters and random variables, but it also often includes further details as well, beyond those that we would consider to be part of the model as we would use it for analyzing real data. In statistical analysis of real data, we often use models that describe only part of the distribution of the data, rather than its full, multivariate distribution. For instance, when conducting a regression analysis, we are analyzing the distribution of an outcome or response variable, conditional on a set of predictor variables. When using an item response theory (IRT) model, we use responses to a set of items to estimate individual ability levels given the items on the test. In contrast, if we are going to generate data for simulating a regression model or IRT model, we need to specify distributions for these additional features (the predictors in a regression model, the items in an IRT model); we can no longer just take them as given. In designing and discussing DGPs, it is helpful to draw distinctions between the components of the focal statistical model and the remaining components of the DGP that are taken as given when analyzing real data. A first relevant distinction is between structural features, covariates, and outcomes (or more generally, endogenous quantities): Structural features are quantities that describe the structure of a dataset but do not enter directly into the focal statistical model, such as the per-group sample sizes in the one-way ANOVA example. When analyzing real data, we usually take the structural features as they come, but when simulating data, we will need to make choices about the structural features. For instance, in the HLM example involving students nested within schools, the number of students in each school is a structural feature. To simulate data based on HLM, we will need to make choices about the number of schools and the distribution of the number of students in each school (e.g., we might specify that school sizes are uniformly distributed between specified minimum and maximum sizes), even though we do not have to consider these quantities when estimating a hierarchical model on real data. Covariates are variables in a dataset that we typically take as given when analyzing real data. For instance, in the one-way ANOVA example, the group assignments of each observation is a covariate. In the HLM example, covariates would include the measures of past academic performance. In a more elaborate version of the HLM, they might also include variables such as student demographic information or school-level characteristics such as the school’s geographic region or treatment assignment. When analyzing real data, we condition on these quantities, but when specifying a DGP, we will need to make choices about how they are distributed (e.g., we might specify that students’ past academic performance is normally distributed). Outcomes and endogenous quantities are the variables whose distribution is described by the focal statistical model. In the bivariate Poisson model, the outcomes consist of the component variables \\(Z_1,Z_2,Z_3\\) and the observed counts \\(C_1,C_2\\) because all of these quantities follow distributions that are specified as part of the focal model. In the one-way ANOVA example, the outcome variable consists of the measurements \\(X_{ig}\\) for \\(i = 1,...,n_g\\) and \\(g = 1,...,G\\). The focal statistical model specifies the distribution of these variables, and we will be interested in estimating the parameters controlling their distribution. Note that the focal statistical model only determines this third component of the DGP. The focal model consists of the equations describing what we would aim to estimate when analyzing real data. In contrast, the full statistical model also includes additional elements specify how to generate the structural features and covariates—the pieces that are taken as given when analyzing real data. Table 6.1 contrasts the role of structural features, covariates, and outcomes in real data analysis versus in simulations. Table 6.1: Real Data Analysis versus Simulation Component Real world Simulation world Structural features We obtain data of a given sample size, sizes of clusters, etc. We specify sample sizes, we specify how to generate cluster sizes Covariates Data come with covariates We specify how to generate covariates Outcomes Data come with outcome variables We generate outcome data based on a focal model Parameter estimation We estimate a statistical model to learn about the unknown parameters We estimate a statistical model and compare the results to the true parameters For a given DGP, the full statistical model might involve distributions for structural features, distributions for covariates, and distributions for outcomes given the covariates. Each of these distributions will involve parameters that control the properties of the distribution (such as the average and degree of variation in a variable). We think of these parameters as falling into one of three categories: focal, auxiliary, or design. Focal parameters are the quantities that we care about and seek to estimate in real data analysis. These are typically parts of the focal statistical model, such as the correlation between counts \\(\\rho\\) in the bivariate Poisson model, the population means \\(\\mu_1,...,\\mu_G\\) in the one-way ANOVA model, or the slope coefficient \\(\\gamma_{10}\\) in the HLM example. Auxiliary parameters are the other quantities that go into the focal statistical model or some other part of the DGP, which we might not be substantively interested in when analyzing real data but which nonetheless affect the analysis. For instance, in the bivariate Poisson model we might consider the average counts \\(\\mu_1\\) and \\(\\mu_2\\) to be auxiliary parameters. In the one-way ANOVA model, we would consider the population variances \\(\\sigma_1^2,...,\\sigma_G^2\\) to be auxiliary if we are not interested in investigating how they vary from group to group. In the HLM example, we might have parameters that describe the distribution of past academic performance, that are part of a model for this covariate that is entirely separate from the focal HLM. Design parameters are the quantities that control how we generate structural features of the data. For instance, in a randomized experiment, the fraction of observations assigned to treatment is a design parameter that can be directly controlled by the researchers. In a hierarchical model, the design parameters might include the minimum and maximum number of students per school. Typically, we do not directly estimate such parameters because we take the distribution of structural features as given. It is evident from this discussion that DGPs can involve many moving parts. One of the central challenges in specifying DGPs is that the performance of estimation methods will generally be affected by the full statistical model—including the design parameters and distribution of structural features and covariates—even though they are not part of the focal model. 6.3 A statistical model is a recipe for data generation Once we have decided on a full statistical model and written it down in mathematical terms, we need to translate it into code. A function that implements a data-generating model should have the following form: generate_data &lt;- function( focal_parameters, auxiliary_parameters, design_parameters ) { # generate pseudo-random numbers and use those to make some data return(sim_data) } The function takes a set of parameter values as input, simulates random numbers and does calculations, and produces as output a set of simulated data. Typically, the inputs will consist of multiple parameters, and these will include not only the focal model parameters, but also the auxiliary parameters, sample sizes, and other design parameters. The output will typically be a dataset, mimicking what one would see in an analysis of real data. In some cases, the output data might be augmented with some other latent quantities (normally unobserved in the real world) that can be used later to assess whether an estimation procedure produces results that are close to the truth. We have already seen an example of a complete DGP function in the case study on one-way ANOVA (see Section 5.1). In this case study, we developed the following function to generate data for a single outcome from a set of \\(G\\) groups: generate_data &lt;- function(mu, sigma_sq, sample_size) { N &lt;- sum(sample_size) g &lt;- length(sample_size) group &lt;- factor( rep(1:g, times = sample_size) ) mu_long &lt;- rep( mu, times = sample_size ) sigma_long &lt;- rep( sqrt(sigma_sq), times = sample_size ) x &lt;- rnorm( N, mean = mu_long, sd = sigma_long ) sim_data &lt;- tibble( group = group, x = x ) return(sim_data) } This function takes both the focal model parameters (mu, sigma_sq) and other design parameters that one might not think of as parameters per-se (sample_size). When simulating, we have to specify quantities that we take for granted when analyzing real data. How would we write a DGP function for the bivariate Poisson model? The equations in Section 6.1 give us the recipe, so it just a matter of re-expressing them in code. For this model, the only design parameter is the sample size, \\(N\\); the sole focal parameter is the correlation between the variates, \\(\\rho\\); and the auxiliary parameters are the expected counts \\(\\mu_1\\) and \\(\\mu_2\\). Our function should have all four of these quantities as inputs and should produce as output a dataset with two variables, \\(C_1\\) and \\(C_2\\). Here is one way to implement the model: r_bivariate_Poisson &lt;- function(N, rho, mu1, mu2) { # covariance term, equal to E(Z_3) EZ3 &lt;- rho * sqrt(mu1 * mu2) # Generate independent components Z1 &lt;- rpois(N, lambda = mu1 - EZ3) Z2 &lt;- rpois(N, lambda = mu2 - EZ3) Z3 &lt;- rpois(N, lambda = EZ3) # Assemble components dat &lt;- data.frame( C1 = Z1 + Z3, C2 = Z2 + Z3 ) return(dat) } Here we generate 5 observations from the bivariate Poisson with \\(\\rho = 0.5\\) and \\(\\mu_1 = \\mu_2 = 4\\): r_bivariate_Poisson(5, rho = 0.5, mu1 = 4, mu2 = 4) ## C1 C2 ## 1 3 3 ## 2 2 1 ## 3 6 9 ## 4 5 4 ## 5 4 2 6.4 Checking the data-generating function An important part of programming in R—and especially when writing custom functions—is finding ways to test and check the correctness of your code. Just writing a data-generating function is not enough. It is also critical to test whether the output it produces is correct. How best to do this will depend on the particulars of the DGP being implemented. For many DGPs, a broadly useful strategy is to generate a very large sample of data, so large that the sample distribution should very closely resemble the population distribution. One can then test whether features of the sample distribution closely align with corresponding parameters of the population model. For the heteroskedastic ANOVA problem, one basic thing we could do is check that the simulated data from each group follows a normal distribution. In the following code, we simulate very large samples from each of the four groups, and check that the means and variances agree with the input parameters: mu &lt;- c(1, 2, 5, 6) sigma_sq &lt;- c(3, 2, 5, 1) check_data &lt;- generate_data( mu = mu, sigma_sq = sigma_sq, sample_size = rep(10000, 4) ) chk &lt;- check_data %&gt;% group_by( group ) %&gt;% dplyr::summarise( n = n(), mean = mean(x), var = var(x) ) %&gt;% mutate(mu = mu, sigma2 = sigma_sq) %&gt;% relocate( group, n, mean, mu, var, sigma2 ) chk ## # A tibble: 4 × 6 ## group n mean mu var sigma2 ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 10000 0.991 1 3.03 3 ## 2 2 10000 1.98 2 1.98 2 ## 3 3 10000 5.07 5 5.04 5 ## 4 4 10000 6.00 6 1.01 1 It seems we are recovering our parameters. We can also make some diagnostic plots to assess whether we have normal data (using QQ plots, where we expect a straight line if the data are normal): ggplot( check_data ) + aes( sample = x, color = group ) + facet_wrap( ~ group ) + stat_qq() + stat_qq_line() This diagnostic looks good too. Here, these checks may seem a bit silly, but most bugs are silly—at least once you find them! In models that are even a little bit more complex, it is quite easy for small things such as a sign error to slip into your code. Even simple checks such as these can be quite helpful in catching such bugs. 6.5 Example: Simulating clustered data Generating data with complex structure can be intimidating, but if you set out a recipe for how the data is generated it is often not to bad to build that recipe up with code. We will next provide a more complex example with a case study of using simulation to determine best practices for analyzing data from a cluster-randomized RCT of students nested in schools. Recent literature on multisite trials (where, for example, students are randomized to treatment or control within each of a series of sites) has explored how variation in the size of impacts across sites can affect how estimators behave, and what models we should use when there is impact variation (e.g., Miratrix, Weiss, and Henderson 2021; Bloom et al. 2016). We are going to extend this work to explore best practices for estimating treatment effects in cluster randomized trials. Cluster randomized trials are randomized experiments where the unit of randomization is a group of individuals, rather than the individuals themselves. For example, if we have a collection of schools, with students in schools, a cluster randomized trial would randomize the schools into treatment or control, and then measure our outcome on the students inside the schools. We might be trying to estimate, for example, whether the average score of the treatment schools is different from the average score of the control schools. In particular, we want to investigate what happens when the average treatment impact of a school is related to the size of the school. Often we will design a data generating process to allow us to answer a specific question. For our Welch example, we wanted to know how different amounts of variation in different groups impacted estimation. We therefore needed a data generation process that allowed us to control that variation. To figure out what we need for our clustered data example, we need to think about how we are going to use those data in our simulation study. 6.5.1 A design decision: What do we want to manipulate? There are a lot of ways we might generate cluster randomized trial data. To pick between the many options, we need to think about the goals of the simulation. Overall, our final data should be a collection of clusters with different sizes and different baseline mean outcomes. Some of the clusters will be treated, and some not. We can imagine our final data being individual students in schools, with each student having a school id, a treatment assignment (shared for all in the school) and an outcome. A good starting point for building a DGP is to first write down a sketch of what the eventual data might look like on a piece of scratch paper. In our case, for example, we might write down: schoolID Z size studentID Y 1 1 24 1 3.6 1 1 24 3 1.0 1 etc etc etc etc 1 1 24 24 2.0 2 0 32 1 0.5 2 0 32 2 1.5 2 0 32 3 1.2 etc etc etc etc etc We know we are planning on comparing multiple estimators, seeing how they behave differently under different conditions. We also know that we are interested in what happens when the size of the treatment impact varies across sites, and in particular what happens when it is associated with site size. Given these goals and beliefs, we might think: We figure if all the sites are the same size, then all the estimators will probably be ok. But if sites vary, then maybe we could have issues with our estimators. Also, if site size varies, but has nothing to do with impact, then all the estimators might still be ok, at least for bias, but if size is associated with treatment impact, then maybe how our estimators end up averaging across sites is going to matter. Usually, when running a simulation, it is good practice to test the simple option along with the complex one. We want to both check that something does not matter as well as verify that it does. Given this, we land on the following points: We need a DGP that has the option to make all-same-size sites or variable size sites. Our DGP should have some impact variation across sites. Our DGP should allow for different sites to have different treatment impacts. We should have the option to connect impact variation to site size. 6.5.2 A model for a cluster RCT It is usually easiest to start a recipe for data generating by writing down the mathematical model. Write down something, and then, if you do not yet know how to generate some part of what you wrote down, specify how to generate those parts that you are using, in an iterative process. For our model, we start with a model for our student outcome: \\[ Y_{ij} = \\beta_{0j} + \\epsilon_{ij} \\mbox{ with } \\epsilon_{ij} \\sim N( 0, \\sigma^2_\\epsilon ) \\] where \\(Y_{ij}\\) is the outcome for student \\(i\\) in site \\(j\\), \\(\\beta_{0j}\\) is the average outcome in site \\(j\\), and \\(\\epsilon_{ij}\\) is the residual error for student \\(i\\) in site \\(j\\). We then need to figure out how to make the average outcome in site \\(j\\). In looking at our goals, we want \\(\\beta_{0j}\\) to depend on treatment assignment. We might then write down: \\[ \\beta_{0j} = \\gamma_0 + \\gamma_1 Z_j + u_j \\mbox{ with } u_j \\sim N( 0, \\sigma^2_u )\\] saying the average outcome in site \\(j\\) is the average outcome in the control group (\\(\\gamma_0\\)) plus some treatment impact (\\(\\gamma_1\\)) if the site is treated. We added a \\(u_j\\) so that our different sites can be different from each other in terms of their average outcome, even if they are not treated. To keep things simple, we are having a common treatment impact within cluster: if we treat a cluster, everyone in the cluster is raised by some specified amount. But we also want the size of impact to possibly vary by site size. This suggests we also want a treatment by site size interaction term. Instead of just using the site size, however, we are going to standardize our site sizes so they are more interpretable. This makes it so if we double the sizes of all the sites, it does not change our size covariate: we want the size covariate to be relative size, not absolute. To do this, we create a covariate which is the percent of the average site size that a site is: \\[ S_j = \\frac{n_j - \\bar{n}}{ \\bar{n} } \\] where \\(\\bar{n}\\) is the average site size. Using this covariate, we then revise our equation for our site \\(j\\) to: \\[ \\beta_{0j} = \\gamma_{0} + \\gamma_{1} Z_j + \\gamma_2 Z_j S_j + u_j \\] A nice thing about \\(S_j\\) is that it is centered at 0, meaning the average site has an impact of just \\(\\gamma_1\\). If \\(S_j\\) was not centered at zero, then our overall average impact in our data would be a mix of the \\(\\gamma_1\\) and the \\(\\gamma_2\\). By centering, we make it so the average impact is just \\(\\gamma_1\\)–\\(\\gamma_1\\) is our target site average treatment impact. If we put all the above together, we see we have specified a multilevel model to describe our data: \\[ \\begin{aligned} Y_{ij} &amp;= \\beta_{0j} + \\epsilon_{ij} \\\\ \\epsilon_{ij} &amp;\\sim N( 0, \\sigma^2_\\epsilon ) \\\\ \\beta_{0j} &amp;= \\gamma_{0} + \\gamma_{1} Z_j + \\gamma_2 Z_j S_j + u_j \\\\ u_j &amp;\\sim N( 0, \\sigma^2_u ) \\end{aligned} \\] Our parameters are the mean outcome of control unit (\\(\\gamma_0\\)), the average treatment impact (\\(\\gamma_1\\)), the amount of cross site variation (\\(\\sigma^2_u\\)), and residual variation (\\(\\sigma^2_\\epsilon\\)). Our \\(\\gamma_2\\) is our site-size by treatment interaction term: bigger sites will (assuming \\(\\gamma_2\\) is positive) have larger treatment impacts. If you prefer the reduced form, it would be: \\[ Y_{ij} = \\gamma_{0} + \\gamma_{1} Z_j + \\gamma_2 Z_j S_j + u_j + \\epsilon_{ij} \\] We might also include a main effect for \\(S_j\\). A main effect would make larger sites systematically different than smaller sites at baseline, rather than having it only be part of our treatment variation term. For simplicity we drop it here. In reviewing the above, we might notice that we do not have any variation in treatment impact that is not explained by site size. We could once again revise our model to include a term for this, but we will leave it out for now. See the exercises at the end of the chapter. So far we have a mathematical model analogous to what we would write if we were analyzing the data. To generate data, we also need several other quantities specified. First, we need to know the number of clusters (\\(J\\)) and the sizes of the clusters (\\(n_j\\), for \\(j = 1, \\ldots, J\\)). We have to provide a recipe for generating these sizes. We might try \\[ n_j \\sim unif( (1-\\alpha)\\bar{n}, (1+\\alpha)\\bar{n} ) = \\bar{n} + \\bar{n}\\alpha \\cdot unif(-1, 1) ,\\] with a fixed \\(\\alpha\\) to control the amount of variation in cluster size. If \\(\\bar{n} = 100\\) and \\(\\alpha = 0.25\\) then we would, for example, have sites ranging from 75 to 125 in size. This specification is nice in that we can determine two parameters, \\(\\bar{n}\\) and \\(\\alpha\\), to get our site sizes, and both parameters are easy to comprehend: average site size and amount of site size variation. Given how we are generating site size, look again at our treatment impact heterogeneity term: \\[ \\gamma_2 Z_j S_j = \\gamma_2 Z_j \\left(\\frac{n_j - \\bar{n}}{\\bar{n}}\\right) = \\gamma_2 Z_j \\alpha U_j, \\] where \\(U_j\\) is the \\(U_j \\sim unif(-1,1)\\) uniform variable used to generate \\(n_j\\). Due to our standardizing by average site size, we make our covariate not change in terms of its importance as a function of site size, but rather as a function of site variation \\(\\alpha\\). In particular, \\(\\frac{n_j - \\bar{n}}{\\bar{n}}\\) will range from \\(-\\alpha\\) to \\(\\alpha\\), regardless of average site size. Carefully setting up a DGP so the “knobs” we use are standardized like this can make interpreting the simulation results much easier. Consider if we did not standardize and just had \\(\\gamma_2 n_j\\) in our equation: in this case, for a set \\(\\gamma_2\\), the overall average impact would grow if we changed the average site size, which could make interpreting the results across scenarios very confusing. We generally want the parameters in our DGP to change only one aspect of our simulation, if possible, to make isolating effects of different DGP characteristics easier. We next need to define how we generate our treatment indicator, \\(Z_j\\). We might specify the proportion \\(p\\) of clusters we will assign to treatment, and then generate \\(Z_j = 1\\) or \\(Z_j = 0\\) using a simple random sampling approach on our \\(J\\) clusters. We will see code for this below. 6.5.3 Converting our model to code When sketching out our DGP mathematically we worked from the students to the schools. For actual data generation, we will now follow our final model, but go by layers in the other direction. First, we generate the sites: Generate site sizes Generate site-level covariates Generate site level random effects Then we generate the students inside the sites: Generate student covariates Generate student residuals Add everything up to generate student outcomes The mathematical model gives us exactly the details we need to execute on these steps. We start by specifying a function with all the parameters we might want to pass it, including defaults for each (see A.2 for more on function defaults): gen_dat_model &lt;- function( n_bar = 10, J = 30, p = 0.5, gamma_0 = 0, gamma_1 = 0, gamma_2 = 0, sigma2_u = 0, sigma2_e = 1, alpha = 0 ) { # Code (see below) goes here. } Note our parameters are a mix of model parameters (gamma_0, gamma_1, sigma2_e, etc., representing coefficients in regressions, variance terms, etc.) and design parameters (n_bar, J, p) that directly inform data generation. We set default arguments (e.g., gamma_0=0) so we can ignore aspects of the DGP that we do not care about later on. Inside the model, we will have a block of code to generate the sites, and then another to generate the students. Make the sites. We make the sites first: # generate site sizes n_min = round( n_bar * (1 - alpha) ) n_max = round( n_bar * (1 + alpha) ) nj &lt;- sample( n_min:n_max, J, replace=TRUE ) # Generate average control outcome and average ATE for all sites # (The random effects) u0j = rnorm( J, mean=0, sd=sqrt( sigma2_u ) ) # randomize units within each site (proportion p to treatment) Zj = ifelse( sample( 1:J ) &lt;= J * p, 1, 0) # Calculate site intercept for each site beta_0j = gamma_0 + gamma_1 * Zj + gamma_2 * Zj * (nj-n_bar)/n_bar + u0j The code is a literal translation of the math we did before. Note the line with sample(1:J) &lt;= J*p; this is a simple trick to generate a treatment and control 0/1 indicator. There is also a serious error in the above code (serious in that the code will run and look fine in many cases, but not always do what we want); we leave it as an exercise (see below) to find and fix it. Make the individuals. We next use the site characteristics to then generate the individuals. # Make individual site membership sid = as.factor( rep( 1:J, nj ) ) dd = data.frame( sid = sid ) # Make individual level tx variables dd$Z = Zj[ dd$sid ] # Generate the residuals N = sum( nj ) e = rnorm( N, mean=0, sd=sqrt( sigma2_e ) ) # Bundle and send out dd &lt;- mutate( dd, sid=as.factor(sid), Yobs = beta_0j[sid] + e, Z = Zj[ sid ] ) A key piece here is the rep() function that takes a list and repeats each element of the list a specified number of times. In particular, rep() repeats each number (\\(1, 2, /ldots,J\\)), the corresponding number of times as listed in nj. Once we put the above code in our function skeleton, we can our function as so: dat &lt;- gen_dat_model( n_bar = 5, J=3, p = 0.5, gamma_0 = 0, gamma_1 = 0.2, gamma_2 = 0.2, sigma2_u = 0.4, sigma2_e = 1, alpha = 0.5 ) dat ## sid Z Yobs ## 1 1 0 -0.0173473 ## 2 1 0 -1.1309479 ## 3 1 0 -0.4795700 ## 4 1 0 -0.8201985 ## 5 1 0 -0.3662931 ## 6 1 0 -1.6604096 ## 7 1 0 -2.1672191 ## 8 1 0 -1.5574128 ## 9 2 0 -1.6324900 ## 10 2 0 -0.7166888 ## 11 2 0 -0.2953189 ## 12 2 0 -0.8861241 ## 13 2 0 -0.2657047 ## 14 2 0 -0.4760574 ## 15 3 1 2.1221495 ## 16 3 1 -0.2187809 ## 17 3 1 -0.5194328 ## 18 3 1 0.1294759 ## 19 3 1 -0.3914622 ## 20 3 1 1.6752613 Our data generation code is complete. We can control the average size of the clusters (n), the number of clusters (J), the proportion treated (p), the average outcome in the control group (gamma_0), the average treatment effect (gamma_1), the site size by treatment interaction (gamma_2), the amount of cross site variation (sigma2_u), the residual variation (sigma2_e), and the amount of site size variation (alpha). The next step is to test the code, making sure it is doing what we think it is. In fact, it is not–there is a subtle bug that only appears under some specifications of the parameters; see the exercises for more on testing ones code, and for diagnosing and repairing this error in particular. 6.6 Extension: Standardization in a data generating process In this chapter, we made a model to generate data for a cluster randomized trial. Given our model, we can generate data by specifying our parameters and variables of \\(\\gamma_{0}, \\gamma_{1}, \\gamma_{2}, \\sigma^2_\\epsilon, \\sigma^2_u, \\bar{n}, \\alpha, J, p\\). One factor that tends to show up when working with multisite data is how much variation there is within sites vs between sites. For example, the Intra-Class Correlation (ICC), a measure of how much of the variation in the outcome is due to differences between sites, is a major component for power calculations. Because of this, we will likely want to manipulate the amount of within vs. between variation in our simulations. An easy way to do this would be to simply raise or lower the amount of variation within sites (\\(\\sigma^2_u\\)). This unfortunately has a side effect: if we increase \\(\\sigma^2_u\\), our overall variation of \\(Y\\) will also increase. This will make it hard to think about, e.g., power, since we have confounded within vs. between variation with overall variation (which is itself bad for power). It also impacts interpretation of coefficients. A treatment effect of 0.2 on our outcome scale is “smaller” if there is more overall variation. Right now, our model is \\[ Y_{ij} = \\gamma_{0} + \\gamma_{1} Z_j + \\gamma_2 Z_j \\left(\\frac{n_j - \\bar{n}}{\\bar{n}} \\right) + u_j + \\epsilon_{ij} \\] Given our model, the variance of our control-side outcomes is \\[ \\begin{aligned} var( Y_{ij}(0) ) &amp;= var( \\beta_{0j} + \\epsilon_{ij} ) \\\\ &amp;= var( \\gamma_{0} + \\gamma_{1} Z_j + \\gamma_{2}Z_j \\tilde{n}_j + u_j + \\epsilon_{ij} ) \\\\ &amp;= var( \\gamma_{0} + u_j + \\epsilon_{ij} ) \\\\ &amp;= \\sigma^2_u + \\sigma^2_\\epsilon \\end{aligned} \\] (We drop the terms with the \\(Z_j\\) because we are looking at control-side variation, where nothing is treated.) We see that as we increase either within or between variation, overall variation increases. We can improve our data generating process to allow for directly controlling the amount of within vs. between variation without it being confounded with overall variation. To do this we first (1) Standardize our data and then (2) reparameterize, so we have human-selected parameters that we can interpret that we then translate to our list of data generation parameters. In particular, we will index our DGP with the more interpretable parameter of the Intra-Class Correlation (ICC), and standardize our DGP so it is all in effect size units. The effect size of an impact is defined as the impact over the control-side standard deviation. (Sometimes people use the pooled standard deviation, but this is usually a bad choice if one suspects treatment variation. More treatment variation should not reduce the effect size for the same absolute average impact.) \\[ ES = \\frac{\\gamma_1}{SD( Y | Z_j = 0 )} = \\frac{\\gamma_1}{\\sqrt{ \\sigma^2_u + \\sigma^2_\\epsilon } } \\] The way we think about how “big” \\(\\gamma_1\\) is depends on how much site variation and residual variation there is. But it is also easier to detect effects when the residual variation is small. Effect sizes “standardize out” these sorts of tensions. We can use that. In particular, we will use the Intraclass Correlation Coeffiicent (ICC), defined as \\[ ICC = \\frac{ \\sigma^2_u }{ \\sigma^2_\\epsilon + \\sigma^2_u } . \\] The ICC is a measure of within vs. between variation. What we then do is first standardized our data, meaning we ensure the control side variance equals 1. Using the above, this means \\(\\sigma^2_u + \\sigma^2_\\epsilon = 1\\). It also gives us \\(ICC = \\sigma^2_u\\), and \\(\\sigma^2_\\epsilon = 1 - ICC\\). Our two model parameters are now tied together by our single ICC tuning parameter. The core idea is we can now manipulate the aspects of the DGP we want while holding other aspects of the DGP constant. Given our standardized scale, we have dropped a parameter from our set of parameters we might want to vary, and ensured that varying the other parameter (now the ICC) is varying only one aspect of the DGP, not both. Before, increasing \\(\\sigma^2_u\\) had two consequences: total variation and relative amount of variation at the school level. Now, manipulating ICC only does the latter. E.g., we can call our DGP function as follows: ICC = 0.3 dat &lt;- gen_dat_model( n_bar = 20, J = 30, p = 0.5, gamma_0 = 0, gamma_1 = 0.3, gamma_2 = 0.2, sigma2_u = ICC, sigma2_e = 1 - ICC, alpha = 0.5 ) 6.7 Exercises 6.7.1 Check the bivariate Poisson function In Section 6.3, we provided an example of a DGP function for the bivariate Poisson model. However, we have not demonstrated how to check that the function is correct—we’re leaving that to you! Write some code to verify that the function r_bivariate_Poisson() is working properly. Do this by generating a very large sample (say \\(N = 10^4\\) or \\(10^5\\)) and verifying the following: The sample means of \\(C_1\\) and \\(C_2\\) align with the specified population means. The sample variances of \\(C_1\\) and \\(C_2\\) are close to the specified population means (because for a Poisson distribution \\(\\mathbb{E}(C_p) = \\mathbb{V}(C_p)\\) for \\(p = 1,2\\)). The sample correlation aligns with the specified population correlation. The observed counts \\(C_1\\) and \\(C_2\\) follow Poisson distributions. 6.7.2 Add error-catching to the bivariate Poisson function In Section 6.1, we noted that the bivariate Poisson function as we described it can only produce a constrained range of correlations, which a maximum value that depends on the ratio of \\(\\mu_1\\) to \\(\\mu_2\\). Our current implementation of the model does not handle this aspect of the model very well: r_bivariate_Poisson(5, rho = 0.6, mu1 = 4, mu2 = 12) ## Warning in rpois(N, lambda = mu1 - EZ3): NAs produced ## C1 C2 ## 1 NA 9 ## 2 NA 12 ## 3 NA 12 ## 4 NA 10 ## 5 NA 11 For this combination of parameter values, \\(\\rho \\times \\sqrt{\\mu_1 \\mu_2}\\) is larger than \\(\\mu_1\\), which leads to simulated values for \\(C_1\\) that are all missing. That makes it pretty hard to compute the correlation between \\(C_1\\) and \\(C_2\\). Please help us fix this issue! Revise r_bivariate_Poisson() so that it checks for allowable values of \\(\\rho\\). If the user specifies a combination of parameters that does not make sense, make the function throw an error (using R’s stop() function). 6.7.3 A bivariate negative binomial distribution One potential limitation of the bivariate Poisson distribution described above is that the variances of the counts are necessarily equal to the means (i.e., unit dispersion). This limitation is inherited from the univariate Poisson distributions that each variate follows. Just as with the corresponding univariate distributions, one way to relax this limitation is to consider distributions with marginals that are negative binomial rather than Poisson, thereby allowing for overdispersion. Cho et al. (2023) describes one type of bivariate negative binomial distribution. They provide a method for constructing a bivariate negative binomial distribution by using latent, gamma-distributed components. Their algorithm involves first generating components from gamma distributions with specified shape and scale parameters: \\[ \\begin{aligned} Z_0 &amp;\\sim \\Gamma\\left( \\alpha_0, \\beta\\right) \\\\ Z_1 &amp;\\sim \\Gamma\\left( \\alpha_1, \\beta\\right) \\\\ Z_2 &amp;\\sim \\Gamma\\left( \\alpha_2, \\beta\\right) \\end{aligned} \\] for \\(\\alpha_0,\\alpha_1,\\alpha_2 &gt; 0\\) and \\(\\beta &gt; 0\\). Then simulate independent Poisson random variables as \\[ \\begin{aligned} C_1 &amp;\\sim Pois\\left( Z_0 + Z_1 \\right) \\\\ C_2 &amp;\\sim Pois\\left( \\delta(Z_0 + Z_2) \\right). \\end{aligned} \\] The resulting count variables follow marginal negative binomial distributions with moments \\[ \\begin{aligned} \\mathbb{E}(C_1) &amp;= (\\alpha_0 + \\alpha_1) \\beta &amp; \\mathbb{V}(C_1) &amp;= (\\alpha_0 + \\alpha_1) \\beta (\\beta + 1) \\\\ \\mathbb{E}(C_2) &amp;= (\\alpha_0 + \\alpha_2) \\beta \\delta &amp; \\mathbb{V}(C_2) &amp;= (\\alpha_0 + \\alpha_2) \\beta \\delta (\\beta \\delta + 1) \\\\ &amp; &amp; \\text{Cov}(C_1, C_2) &amp;= \\alpha_0 \\beta^2 \\delta. \\end{aligned} \\] The correlation between \\(C_1\\) and \\(C_2\\) is thus \\[ \\text{cor}(C_1, C_2) = \\frac{\\alpha_0}{\\sqrt{(\\alpha_0 + \\alpha_1)(\\alpha_0 + \\alpha_2)}} \\frac{\\beta \\sqrt{\\delta}}{\\sqrt{(\\beta + 1)(\\beta \\delta + 1)}}. \\] Write a DGP function that implements this distribution. Write some code to check that the function produces data where each variate follows a negative binomial distribution and where the correlation agrees with the formula given above. Consider parameter values that produce \\(\\mathbb{E}(C_1) = \\mathbb{E}(C_2) = 10\\) and \\(\\mathbb{V}(C_1) = \\mathbb{V}(C_2) = 15\\). What are the minimum and maximum possible correlations between \\(C_1\\) and \\(C_2\\)? 6.7.4 Another bivariate negative binomial distribution Another model for generating bivariate counts with negative binomial marginal distributions is by using Gaussian copulas. Here is a mathematical recipe for this distribution, which will produce counts with marginal means \\(\\mu_1\\) and \\(\\mu_2\\) and marginal variances \\(\\mu_1 + \\mu_1^2 / p_1\\) and \\(\\mu_2 + \\mu_2^2 / p_2\\). Start by generating variates from a bivariate standard normal distribution with correlation \\(\\rho\\): \\[ \\left(\\begin{array}{c}Z_1 \\\\ Z_2 \\end{array}\\right) \\sim N\\left(\\left[\\begin{array}{c}0 \\\\ 0\\end{array}\\right], \\ \\left[\\begin{array}{cc}1 &amp; \\rho \\\\ \\rho &amp; 1\\end{array}\\right]\\right) \\] Now find \\(U_1 = \\Phi(Z_1)\\) and \\(U_1 = \\Phi(Z_1)\\), where \\(\\Phi()\\) is the standard normal cumulative distribution function (called pnorm() in R). Then generate the counts by evaluating \\(U_1\\) and \\(U_2\\) with the negative binomial quantile function, \\(F_{NB}^{-1}(x | \\mu, p)\\) with mean parameters \\(\\mu\\) and size parameter \\(p\\) (this function is called qnbinom() in R): \\[ C_1 = F_{NB}^{-1}(U_1 | \\mu_1, p_1) \\qquad C_2 = F_{NB}^{-1}(U_2 | \\mu_2, p_2). \\] The resulting counts will be correlated, but the correlation will not be equal to \\(\\rho\\). Write a DGP function that implements this distribution. Write some code to check that the function produces data where each variate follows a negative binomial distribution Use the function to create a graph showing the population correlation between the observed counts as a function of \\(\\rho\\). Use \\(\\mu_1 = \\mu_2 = 10\\) and \\(p_1 = p_2 = 20\\). How does the range of correlations compare to the range from Exercise 6.7.3? 6.7.5 The Welch test on a shifted-and-scaled \\(t\\) distribution The shifted-and-scaled \\(t\\)-distribution has parameters \\(\\mu\\) (mean), \\(\\sigma\\) (scale), and \\(\\nu\\) (degrees of freedom). If \\(T\\) follows a student’s \\(t\\)-distribution with \\(\\nu\\) degrees of freedom, then \\(S = \\mu + \\sigma T\\) follows a shifted-and-scaled \\(t\\)-distribution. The following function will generate random draws from this distribution (the scaling of \\((\\nu-2)/\\nu\\) is to account for a non-scaled \\(t\\)-distribution having a variance of \\(\\nu/(\\nu-2)\\)). r_tss &lt;- function(n, mean, sd, df) { mean + sd * sqrt( (df-2)/df ) * rt(n = n, df = df) } r_tss(n = 8, mean = 3, sd = 2, df = 5) ## [1] -0.7733983 3.1741862 7.8009102 2.9239147 3.9685458 2.9286611 4.3718197 ## [8] 2.8627233 Modify the Welch simulation’s simulate_data() function to generate data from shifted-and-scaled \\(t\\)-distributions rather than from normal distributions. Include the degrees of freedom as an input argument. Simulate a dataset with low degrees of freedom and plot it to see if you see a few outliers. Now generate more data and calculate the means and standard deviations to see if they are correctly calibrated (i.e., generate a big dataset to ensure you get reliable mean and standard deviation estimates). Check df equal to 500, 5, 3, and 2. Once you are satisfied you have a correct DGP function, re-run the Type-I error rate calculations from the prior exercises in Section 5.5 using a \\(t\\)-distribution with 5 degrees of freedom. Do the results change substantially? 6.7.6 Checking and extending the Cluster RCT DGP What is the variance of the outcomes generated by our model if there is no treatment effect? (Try simulating data to check!) What other quick checks can you make on your DGP to make sure it is working? In gen_dat_model() we have the following line of code to generate the number of individuals per site. nj &lt;- sample( n_min:n_max, J, replace=TRUE ) This code has an error. Generate a variety of datasets where you vary n_min, n_max and J to discover the error. Then repair the code. Checking your data generating process across a range of scenarios is extremely important. The DGP allows for site-level treatment impact variation–but only if it is related to site size. How could you modify your simulation to allow for site-level treatment impact variation that is not related to site size? Implement this change and generate some data to show how it works. Extend the data generating process to include an individual level covariate \\(X\\) that is predictive of outcome. In particular, you will want to adjust your level one equation to \\[ Y_{ij} = \\beta_{0j} + \\beta_{1} X_{ij} + \\epsilon_{ij} . \\] Keep the same \\(\\beta_1\\) for all sites. You will have to specify how to generate your \\(X_{ij}\\). For starters, just generate it as a standard normal, and do not worry about having the mean of \\(X_{ij}\\) vary by sites unless you are excited to try to get that to work. References Bloom, Howard S., Stephen W. Raudenbush, Michael J. Weiss, and Kristin Porter. 2016. “Using Multisite Experiments to Study Cross-Site Variation in Treatment Effects: A Hybrid Approach With Fixed Intercepts and a Random Treatment Coefficient.” Journal of Research on Educational Effectiveness 10 (4): 0–0. https://doi.org/10.1080/19345747.2016.1264518. Cho, Hunyong, Chuwen Liu, John S Preisser, and Di Wu. 2023. “A Bivariate Zero-Inflated Negative Binomial Model and Its Applications to Biomedical Settings.” Statistical Methods in Medical Research 32 (7): 1300–1317. https://doi.org/10.1177/09622802231172028. Miratrix, Luke W., Michael J. Weiss, and Brit Henderson. 2021. “An Applied Researcher’s Guide to Estimating Effects from Multisite Individually Randomized Trials: Estimands, Estimators, and Estimates.” Journal of Research on Educational Effectiveness 14 (1): 270–308. https://doi.org/10.1080/19345747.2020.1831115. "]]
