[["data-analysis-procedures.html", "Chapter 7 Data analysis procedures 7.1 Writing estimation functions 7.2 Including Multiple estimation procedures 7.3 Validating Estimation Procedures 7.4 Handling errors, warnings, and other hiccups 7.5 Exercises", " Chapter 7 Data analysis procedures The overall aims of many simulation studies have to do with understand how a particular data-analysis procedure works or comparing the performance of multiple, competing procedures. Thus, the data-analysis procedure or procedures are the central object of study. Depending on the research question, the data-analysis procedure might be very simple—as simple as just computing a sample correlation–or it might involve a combination of several components. For example, the procedure might entail first computing a diagnostic test for heteroskedasticity and then, depending on the outcome of the test, applying either a conventional formula or a heteroskedasticity-robust formula for standard errors. As another example, a data-analysis procedure might involve using multiple imputation for missingness on key variables, then fitting a statistical model, and then generating predicted values based on the model. Also depending on the research question, we might need to create several functions that implement different estimation procedures to be compared. In this chapter, we demonstrate how to implement data-analysis procedures in the form of R functions, so that their performance can be evaluated by repeatedly applying them to artificial data. We start by describing the high-level design of estimation functions and examining some simple but illustrative examples. We then discuss approaches for writing simulations that involve multiple estimation procedure. Next, we describe strategies for validating estimation functions. Finally, we examine methods for handling common computational problems with estimation procedures, such as non-convergence of maximum likelihood estimators. 7.1 Writing estimation functions In the abstract, a function that implements an estimation procedure should have the following form: estimate &lt;- function(data) { # calculations/model-fitting/estimation procedures return(estimates) } The function takes a dataset as input, fits a model or otherwise calculates an estimate, possibly with associated standard errors and so forth, and returns these quantities as output. The estimates could be point estimates of parameters, standard errors, confidence intervals, p-values, predictions, or other quantities. The calculations in the body of the function should be set up to use datasets that have the same structure (i.e., same dimensions, same variable names) as the output of the corresponding function for generating simulated data. However, in principle, we should also be able to run the estimation function on real data as well. In Chapter 5 we wrote a function called ANOVA_Welch_F() for computing \\(p\\)-values from two different procedures for testing equality of means in a heteroskedastic ANOVA: ANOVA_Welch_F &lt;- function(data) { anova_F &lt;- oneway.test(x ~ group, data = data, var.equal = TRUE) Welch_F &lt;- oneway.test(x ~ group, data = data, var.equal = FALSE) result &lt;- tibble( ANOVA = anova_F$p.value, Welch = Welch_F$p.value ) return(result) } Apply this function to a simulated dataset returns two p-values, one for the usual ANOVA \\(F\\) test (which assumes homoskedasticity) and one for Welch’s heteroskedastic \\(F\\) test: sim_data &lt;- generate_ANOVA_data( mu = c(1, 2, 5, 6), sigma_sq = c(3, 2, 5, 1), sample_size = c(3, 6, 2, 4) ) ANOVA_Welch_F(sim_data) ## # A tibble: 1 × 2 ## ANOVA Welch ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.000293 0.0179 Our ANOVA_Welch_F() function is designed to work with the output of generate_ANOVA_data() in that it assumes that the grouping variable is called group and the outcome is called x. Relying on this assumption would be a poor choice if we were designing a function as part of an R package or for general-purpose use. However, because the primary use of the function is for simulation, it is reasonable to assume that the input data will always have appropriate variable names. In Chapter 6, we looked at a data-generating function for a bivariate Poisson distribution, an example of a non-normal bivariate distribution. We might use such a distribution to understand the behavior of Pearson’s sample correlation coefficient and its normalizing transformation, known as Fisher’s \\(z\\)-transformation, which is equivalent to the hyperbolic arc-tangent function (atanh() in R). When the sample measurements follow a bivariate normal distribution, Fisher’s \\(z\\)-transformed correlation is very close to normally distributed and its standard error is simply \\(1 / \\sqrt{N - 3}\\), and thus independent of the correlation. This makes \\(z\\)-transformation very useful for computing confidence intervals, which can then be back-transformed to the Pearson-\\(r\\) scale. In this problem, a simple estimation function would take a dataset with two variables as input and compute the sample correlation and its \\(z\\)-transformation, compute confidence intervals for \\(z\\), and then back-transform the confidence interval end-points. Here is an implementation of these calculations: r_and_z &lt;- function(data) { r &lt;- cor(data$C1, data$C2) z &lt;- atanh(r) se_z &lt;- 1 / sqrt(nrow(data) - 3) ci_z &lt;- z + c(-1, 1) * qnorm(.975) * se_z ci_r &lt;- tanh(ci_z) tibble( r = r, z = z, CI_lo = ci_r[1], CI_hi = ci_r[2] ) } To check that the function returns a result of the expected form, we generate a small dataset using the r_bivariate_Poisson() function developed in the last chapter, then apply our estimation function to the result: Pois_dat &lt;- r_bivariate_Poisson(40, rho = 0.5, mu1 = 4, mu2 = 4) r_and_z(Pois_dat) ## # A tibble: 1 × 4 ## r z CI_lo CI_hi ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.557 0.628 0.296 0.740 Although it is a little cumbersome to do so, we could also apply the estimation function to a real dataset. Here is an example, which calculates the correlation between ratings of judicial integrity and familiarity with the law from the USJudgeRatings dataset (which is included in base R). For the function to work on this dataset, we first need to rename the relevant variables. data(USJudgeRatings) USJudgeRatings %&gt;% dplyr::select(C1 = INTG, C2 = FAMI) %&gt;% r_and_z() ## # A tibble: 1 × 4 ## r z CI_lo CI_hi ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.869 1.33 0.769 0.927 The function returns a valid result—a quite strong correlation! It is a good practice to test out a newly-developed estimation function on real data as a check that it is working as intended. This type of test ensures that the estimation function is not using information outside of the dataset, such as by using known parameter values to construct an estimate. Applying the function to a real dataset demonstrates that the function implements a procedure that could actually be applied in real data analysis contexts. 7.2 Including Multiple estimation procedures Many simulations involve head-to-head comparisons between more than one data-analysis procedure. As a design principle, we generally recommend writing different functions for each estimation method one is planning on evaluating. Doing so makes it easier to add in additional methods as desired or to focus on just a subset of methods. Writing separate function also leads to a code base that is flexible and useful for other purposes (such as analyzing real data). Finally (repeating one of our favorite mantras), separating functions makes debugging easier because it lets you focus attention on one thing at a time, without worrying about how errors in one area might propagate to others. To see how this works in practice, we will return to the case study from Section 6.6, where we developed a data-generating function for simulating a cluster-randomized trial with student-level outcomes but school-level treatment assignment. Our data-generating process allowed for varying school sizes and heterogeneous treatment effects, which might be correlated with school size. Several different procedures might be used to estimate an overall average effect from a clustered experiment, including: Estimating a multi-level regression model (also known as a hierarchical linear model), Estimating an ordinary least squares (OLS) regression model and applying cluster-robust standard errors, or Averaging the outcomes by school, then estimating a linear regression model on the mean outcomes. All three of these methods are are widely used and have some theoretical guarantees supporting their use. Education researchers tend to be more comfortable using multi-level regression models, whereas economists tend to use OLS with clustered standard errors. Let’s develop estimation functions for each of these procedures. For now we won’t worry about incorporating the school size covariate, but will instead focus only on getting a point estimate, standard error, and \\(p\\)-value for the average treatment effect. For starters, we generate a sample dataset using a revised version of gen_cluster_RCT(), which corrects the bug discussed in Exercise 6.8.8: dat &lt;- gen_cluster_RCT( J=16, n_bar = 30, alpha = 0.8, p = 0.5, gamma_0 = 0, gamma_1 = 0.2, gamma_2 = 0.2, sigma2_u = 0.4, sigma2_e = 0.6 ) For the multi-level modeling strategy, there are several different existing packages that we could use. We will implement an estimator using the popular lme4 package, along with the lmerTest function for computing a \\(p\\)-value for the average effect. Here is a basic implementation: analysis_MLM &lt;- function( dat ) { M1 &lt;- lme4::lmer( Yobs ~ 1 + Z + (1 | sid), data = dat ) M1_test &lt;- lmerTest::as_lmerModLmerTest(M1) M1_summary &lt;- summary(M1_test)$coefficients tibble( ATE_hat = M1_summary[&quot;Z&quot;,&quot;Estimate&quot;], SE_hat = M1_summary[&quot;Z&quot;,&quot;Std. Error&quot;], p_value = M1_summary[&quot;Z&quot;, &quot;Pr(&gt;|t|)&quot;] ) } The function fits a multi-level model with a fixed coefficient for the treatment indicator and random intercepts for each school. To get a p-value for the treatment coefficient, we have to convert the model into an lmerModLmerTest object and then pass it through summary(). The function outputs only the statistics in which we are interested. Our function makes use of the lme4 and lmerTest packages. Rather than assuming that these packages will be loaded, we call relevant functions using the package name as a prefix, as in lme4::lmer(). This way, we can run the function even if we have not loaded the packages in the global environment. This approach is also preferable to loading packages inside the function itself (e.g., with require(lme4)) because calling the function does not change which packages are loaded in the global environment. Here is a function implementing OLS regression with cluster-robust standard errors: analysis_OLS &lt;- function( dat, se_type = &quot;CR2&quot; ) { M2 &lt;- estimatr::lm_robust( Yobs ~ 1 + Z, data = dat, clusters = sid, se_type = se_type ) tibble( ATE_hat = M2$coefficients[[&quot;Z&quot;]], SE_hat = M2$std.error[[&quot;Z&quot;]], p_value = M2$p.value[[&quot;Z&quot;]] ) } To get cluster-robust standard errors, we use the lm_robust() function from the estimatr() package, again calling only the relevant function using the package prefix rather than loading the whole package. A novel aspect of this estimation function is that it includes an additional intput argument, se_type, which allows us to control the type of standard error calculated by lm_robust(). Adding this option would let us use the same function to compute (and compare) different types of clustered standard errors for the average treatment effect estimate. We set a default option of \"CR2\", just like the default of lm_robust(). Finally, here is a function implementing the aggregate-then-analyze approach: analysis_agg &lt;- function( dat, se_type = &quot;HC2&quot; ) { datagg &lt;- dplyr::summarise( dat, Ybar = mean( Yobs ), n = n(), .by = c(sid, Z) ) stopifnot( nrow( datagg ) == length(unique(dat$sid) ) ) M3 &lt;- estimatr::lm_robust( Ybar ~ 1 + Z, data = datagg, se_type = se_type ) tibble( ATE_hat = M3$coefficients[[&quot;Z&quot;]], SE_hat = M3$std.error[[&quot;Z&quot;]], p_value = M3$p.value[[&quot;Z&quot;]] ) } Note the stopifnot command. Putting assert statements in your code like this is a good way to guarantee you are not introducing weird and hard-to-track errors in your code. For example, R likes to recycle vectors to make them the right length; if you gave it a wrong length in error, this can be a brutal error to discover. The stopifnot statements halt your code as soon as something goes wrong, rather than letting that initial wrongness flow on to further work, showing up in odd results that you don’t understand later on. See Section 17.2 for more. All of our functions produce output in the same format: analysis_MLM( dat ) ## Warning in check_dep_version(): ABI version mismatch: ## lme4 was built with Matrix ABI version 2 ## Current Matrix ABI version is 1 ## Please re-install lme4 from source or restore original &#39;Matrix&#39; package ## # A tibble: 1 × 3 ## ATE_hat SE_hat p_value ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -0.111 0.323 0.737 analysis_OLS( dat ) ## # A tibble: 1 × 3 ## ATE_hat SE_hat p_value ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -0.177 0.307 0.576 analysis_agg( dat ) ## # A tibble: 1 × 3 ## ATE_hat SE_hat p_value ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -0.0818 0.339 0.813 Ensuring that the output of all the functions is structured in the same way will make it easy to keep the results organized once we start running multiple iterations of the simulation. If each estimation method returns a dataset with the same variables, we can simply stack the results on top of each other. Here is a function that bundles all the estimation procedures together: estimate_Tx_Fx &lt;- function( data, CR_se_type = &quot;CR2&quot;, agg_se_type = &quot;HC2&quot; ) { dplyr::bind_rows( MLM = analysis_MLM( dat ), OLS = analysis_OLS( dat, se_type = CR_se_type), agg = analysis_agg( dat, se_type = agg_se_type), .id = &quot;estimator&quot; ) } estimate_Tx_Fx(dat) ## # A tibble: 3 × 4 ## estimator ATE_hat SE_hat p_value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 MLM -0.111 0.323 0.737 ## 2 OLS -0.177 0.307 0.576 ## 3 agg -0.0818 0.339 0.813 This is a common coding pattern for in simulations that involve multiple estimation procedures. Each procedure is expressed in its own function, then these are assembled together in a single function so that they can all easily be applied to the same dataset. Stacking the results row-wise will make it easy to compute performance measures on the results. This will become more evident once we are working across multiple replications of the simulation process, as we will in Chapter 8. 7.3 Validating Estimation Procedures Just as with data-generating functions, it is critical to verify the accuracy of the estimation functions. If an estimation function involves a known procedure that has been implemented in R or one of its contributed packages, then a straightforward way to do this is to compare your implementation to another existing implementation. For estimation functions that involve multi-step procedures or novel methods, other approaches to verification may be needed, which rely more on statistical theory. 7.3.1 Checking against existing implementations For our Welch test function, we can check the output of ANOVA_Welch_F() against the built-in oneway.test function. Let’s do that with a fresh set of data: sim_data &lt;- generate_ANOVA_data( mu = c(1, 2, 5, 6), sigma_sq = c(3, 2, 5, 1), sample_size = c(3, 6, 2, 4) ) aov_results &lt;- oneway.test(x ~ factor(group), data = sim_data, var.equal = FALSE) aov_results ## ## One-way analysis of means (not assuming equal variances) ## ## data: x and factor(group) ## F = 28.367, num df = 3.0000, denom df = 3.3253, p-value = 0.007427 Welch_results &lt;- ANOVA_Welch_F(sim_data) all.equal(aov_results$p.value, Welch_results$Welch) ## [1] TRUE We use all.equal() because it will check equality up to a tolerance in R, which can avoid some perplexing errors due to rounding. For the bivariate correlation example, we can check the output of r_and_z() against R’s built-in cor.test() function: Pois_dat &lt;- r_bivariate_Poisson(15, rho = 0.6, mu1 = 14, mu2 = 8) my_result &lt;- r_and_z(Pois_dat) my_result ## # A tibble: 1 × 4 ## r z CI_lo CI_hi ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.503 0.553 -0.0125 0.807 R_result &lt;- cor.test(~ C1 + C2, data = Pois_dat) R_result ## ## Pearson&#39;s product-moment correlation ## ## data: C1 and C2 ## t = 2.0983, df = 13, p-value = 0.05599 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.01250535 0.80724858 ## sample estimates: ## cor ## 0.5029796 all.equal( tibble(r = R_result$estimate[[&quot;cor&quot;]], CI_lo = R_result$conf.int[1], CI_hi = R_result$conf.int[2]), select(my_result, -z) ) ## [1] TRUE This type of test is even more useful here because r_and_z() uses our own implementation of the confidence interval calculations, rather than relying on R’s built-in functions as we did with ANOVA_Welch_F(). 7.3.2 Checking novel procedures Simulations are usually an integral part of projects to develop novel statistical methods. Checking estimation functions in such projects presents a challenge: if an estimation procedure truly is new, how do you check that your code is correct? Effective methods for doing so will vary from problem to problem, but an over-arching strategy is to use theoretical results about the performance of the estimator to check that your implementation works as expected. For instance, we might work out the algebraic properties of an estimator for a special case and then check that the result of the estimation function agrees with our algebra. For some estimation problems, we might be able to identify theoretical properties of an estimator when applied to a very large sample of data and when the model is correctly specified. If we can find results about large-sample behavior, then we can test an estimation function by applying it to a very large sample and checking whether the resulting estimates and very close to specified parameter values. We illustrate each of these approaches using our functions for estimating treatment effects from cluster-randomized trials. We start by testing an algebraic property. With each of the three methods we have implemented, the treatment effect estimator is a difference between the weighted average of the outcomes from students in each treatment condition; the only difference between the estimators is in what weights are used. In the special case where all schools have the same number of students, the weights used by all three methods end up being the same: all three methods allocate equal weight to each school. Therefore, we know that there should be no difference between the three point estimates. Furthermore, a bit of algebra will show that the cluster-robust standard error from the OLS approach will end up being identical to the robust standard error from the aggregation approach. If there are also equal numbers of schools assigned to both conditions, then the standard error from the multilevel model will also be identical to the other standard errors. Let’s verify that our estimation functions produce results that are consistent with these theoretical properties. To do so, we will need to generate a dataset with equal cluster sizes, setting \\(\\alpha = 0\\): dat &lt;- gen_cluster_RCT( J=12, n_bar = 30, alpha = 0, p = 0.5, gamma_0 = 0, gamma_1 = 0.2, gamma_2 = 0.2, sigma2_u = 0.4, sigma2_e = 0.6 ) table(dat$sid) # verify equal-sized clusters ## ## 1 2 3 4 5 6 7 8 9 10 11 12 ## 30 30 30 30 30 30 30 30 30 30 30 30 estimate_Tx_Fx(dat) ## # A tibble: 3 × 4 ## estimator ATE_hat SE_hat p_value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 MLM -0.525 0.366 0.183 ## 2 OLS -0.525 0.366 0.183 ## 3 agg -0.525 0.366 0.183 All three methods yield identical results. Now let’s try equal school sizes but unequal allocation to treatment: dat &lt;- gen_cluster_RCT( J=12, n_bar = 30, alpha = 0, p = 2 / 3, gamma_0 = 0, gamma_1 = 0.2, gamma_2 = 0.2, sigma2_u = 0.4, sigma2_e = 0.6 ) estimate_Tx_Fx(dat) ## # A tibble: 3 × 4 ## estimator ATE_hat SE_hat p_value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 MLM 0.189 0.547 0.737 ## 2 OLS 0.189 0.413 0.663 ## 3 agg 0.189 0.413 0.657 As expected, all three point estimators match, but the SE from the multilevel model is a little bit discrepant from the others. We can also use large-sample theory to check the multilevel modeling estimator. If the model is correctly specified, then all the parameters of the model should be accurately estimated if the model is fit to a very large sample of data. To check this property, we will need access to the full model output, not just the selected results returned by analysis_MLM(). One way to handle this is to make a small tweak to the estimation function, adding an option to control whether to return the entire model or just selected results. Here is the tweaked function: analysis_MLM &lt;- function( dat, all_results = FALSE) { M1 &lt;- lme4::lmer( Yobs ~ 1 + Z + (1 | sid), data = dat ) M1_test &lt;- lmerTest::as_lmerModLmerTest(M1) if (all_results) { return(summary(M1_test)) } M1_summary &lt;- summary(M1_test)$coefficients tibble( ATE_hat = M1_summary[&quot;Z&quot;,&quot;Estimate&quot;], SE_hat = M1_summary[&quot;Z&quot;,&quot;Std. Error&quot;], p_value = M1_summary[&quot;Z&quot;, &quot;Pr(&gt;|t|)&quot;] ) } Setting all_results to TRUE will return the entire function; keeping it at the default value of FALSE will return the same output as the other functions. Now let’s apply the estimation function to a very large dataset, with variation in cluster sizes. We set gamma_2 = 0 so that the estimation model is correctly specified: dat &lt;- gen_cluster_RCT( J=5000, n_bar = 20, alpha = 0.9, p = 2 / 3, gamma_0 = 2, gamma_1 = 0.30, gamma_2 = 0, sigma2_u = 0.4, sigma2_e = 0.6 ) analysis_MLM(dat, all_results = TRUE) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: Yobs ~ 1 + Z + (1 | sid) ## Data: dat ## ## REML criterion at convergence: 242678 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -4.3080 -0.6581 0.0007 0.6594 4.2134 ## ## Random effects: ## Groups Name Variance Std.Dev. ## sid (Intercept) 0.3972 0.6302 ## Residual 0.6030 0.7765 ## Number of obs: 98772, groups: sid, 5000 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 2.017e+00 1.636e-02 4.969e+03 123.29 &lt;2e-16 *** ## Z 2.838e-01 2.003e-02 4.964e+03 14.17 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) ## Z -0.817 The intercept and treatment effect coefficient estimates are very close to their true parameter values, as are the estimated school-level variance and student-level residual variance. This all gives some assurance that the analysis_MLM() function is working properly. Of course, it is important to bear in mind that these tests are only partial verifications. With the algebraic test, we’ve checked that the functions seem to be working properly for scenarios with equal school sizes, but they still might have errors that only appear when school sizes vary. Likewise, analysis_MLM() seems to be working properly for very large datasets, but our test does not rule out the possibility of bugs that only crawl out when \\(J\\) is small. Our large-sample test also relies on the correctness of the gen_cluster_RCT() function; if we had seen a discrepancy between parameters and estimates from the multilevel model, it could have been because of a problem with the data-generation function rather than with the estimation function. These limitations are typical of what can be accomplished through tests based on theoretical results, because theoretical results typically only hold under specific conditions. After all, if we had comprehensive theoretical results, we would not need to simulate anything in the first place! Nonetheless, it is good to work through such tests to the extent that relevant theory is available for the problem you are studying. 7.3.3 Checking with simulations Checking, debugging, and revising should not be limited to when you are initially developing estimation functions. It often happens that later steps in the process of conducting a simulation will reveal problems with the code for earlier steps. For instance, once you have run the data-generating and estimation steps repeatedly, calculated performance summaries, and created some graphs of the results, you might find an unusual or anomolous pattern in the performance of an estimator. This might be a legitimate result—it might be that the estimator really does behave weirdly or not work well—or it might be due to a problem in how you implemented the estimator or data-generating process. When faced with an unusual pattern, we recommend revisiting the estimation code to double check for bugs and also thinking further about what might lead to the anomoly. Further exploration might lead you to a deeper understanding of how a method works and perhaps even an idea for how to improve the estimator or refine the data-generating process. A good illustration of this process comes from one of Luke’s past research projects, in which he and other co-authors were working on a way to improve Instrumental Variable (IV) estimation using post-stratification. The method they studied involved grouping units based on a covariate that predicts compliance status, then calculating estimates within each group, then summarizing the estimates across groups. They used simulations to see whether this method would improve the accuracy of the overall summary effect estimate. In the first simulation, the estimates were full of NAs and odd results because the estimation function failed to account for what happens in groups of observations where the number of compliers was estimated to be zero. After repairing that problem and re-running everything, the simulation results still indicated serious and unexpected bias, which turned out to be due to an error in how the estimation function implemented the step of summarizing estimates across groups. After again correcting and re-running, the simulation results showed that the gains in accuracy from this new method were minimal, even when the groups were formed based on a variable that was almost perfectly predictive of compliance status. Eventually, we understood that the groups with very few compliers produced such unstable estimates that they spoiled the overall average estimate. This inspired us to revise our estimation strategy and introduce a method that dropped or down-weighted strata with few compliers, which ultimately helped us to strengthen the contribution of our work. As this experience highlights, simulations seldom follow a single, well-defined trajectory. The point of conducting simulations is to help us, as researchers, learn about estimation methods so that we can make progress with analysis of real data. What we learn from simulation gives us a better understanding of the methods (potentially including a better understanding of theoretical results), leading to ideas about better methods or new scenarios to explore in further simulations. Of course, at some point one needs to step off this merry-go-round, write up the findings, cook dinner, and clean the bathroom. But, just like many other research endeavors, simulations follow a highly iterative process. 7.4 Handling errors, warnings, and other hiccups 7.5 Exercises 7.5.1 More Heteroskedastic ANOVA In the classic simulation by Brown and Forsythe (1974), they not only looked at the performance of the homoskedastic ANOVA-F test and Welch’s heteroskedastic-F test, they also proposed their own new hypothesis testing procedure. Write a function that implements the Brown-Forsythe F* test (the BFF* test!) as described on p. 130 of Brown and Forsythe (1974), using the following code skeleton: BF_F &lt;- function( data ) { # fill in the guts here return(pval) } Run the following code to check that your function produces a result: sim_data &lt;- generate_ANOVA_data( mu = c(1, 2, 5, 6), sigma_sq = c(3, 2, 5, 1), sample_size = c(3, 6, 2, 4) ) BF_F( sim_data ) Try calling your BF_F function on a variety of datasets of different sizes and shapes, to make sure it works. What kinds of datasets should you test out? Add the BFF* test into the output of Welch_ANOVA_F() by calling your BF_F() function inside the body of Welch_ANOVA_F(). The onewaytests package implements a variety of different hypothesis testing procedures for one-way ANOVA. Validate your Welch_ANOVA_F() function by comparing the results to the output of the relevant functions from onewaytests. 7.5.2 Contingent testing In the one-way ANOVA problem, one approach that an analyst might think to take is to conduct a preliminary significance test for heterogeneity of variances (such as Levene’s test or Bartlett’s test), and then report the \\(p\\)-value from the homoskedastic ANOVA F test if variance heterogeneity is not detected but the \\(p\\)-value from the BFF* test if variance heteogeneity is detected. Modify the Welch_ANOVA_F() function to return the \\(p\\)-value from this contingent BFF* test in addition to the \\(p\\)-values from the (non-contingent) ANOVA-F, Welch, and BFF* tests. Include an input option that allows the user to control the \\(\\alpha\\) level of the preliminary test for heterogeneity of variances, as in the following skeleton. Welch_ANOVA_F &lt;- function( data , pre_alpha = .05) { # preliminary test for variance heterogeneity # compute non-contingent F tests for group differences # compute contingent test # compile results return(result) } 7.5.3 Check the cluster-RCT functions Section 7.2 presented functions implementing several different strategies for estimating an average treatment effect from a cluster-randomized trial. Write some code to validate these functions by comparing their output to the results of other tools for doing the same calculation. Use one or more datasets simulated with gen_cluster_RCT(). For each of these tests, you will need to figure out the appropriate syntax by reading the package documentation. For analysis_MLM(), check the output by fitting the same model using lme() from the nlme() package or glmmTMB() from the package of the same name. For analysis_OLS(), check the output by fitting the linear model using the base R function lm(), then computing standard errors using vcovCL() from the sandwich package. Also compare the output to the results of feeding the fitted model through coef_test() from the clubSandwich package. For analysis_agg(), check the output by aggregating the data to the school-level, fitting the linear model using lm(), and computing standard errors using vcovHC() from the sandwich package. 7.5.4 Extending the cluster-RCT functions Exercise 6.8.10 from Chapter 6 asked you to extend the data-generating function for the cluster-randomized trial to include generating a student-level covariate, \\(X\\), that is predictive of the outcome. Use your modified function to generate a dataset. Modify the estimation functions from Section 7.2 to use models that include the covariate as a predictor. Further extend the functions to include an input argument for the set of predictors to be included in the model, as in the following skeleton for the multi-level model estimator: analysis_MLM &lt;- function(dat, predictors = &quot;Z&quot;) { } analysis_MLM( dat ) analysis_MLM( dat, predictors = c(&quot;Z&quot;,&quot;X&quot;)) analysis_MLM( dat, predictors = c(&quot;Z&quot;,&quot;X&quot;, &quot;X:Z&quot;)) Hint: Check out the reformulate() function, which makes it easy to build formulas for different sets of predictors. "]]
