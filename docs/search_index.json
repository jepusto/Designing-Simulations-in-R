[["data-generating-processes.html", "Chapter 6 Data-Generating Processes 6.1 Examples 6.2 Components of a DGP 6.3 A statistical model is a recipe for data generation 6.4 Plot the artificial data 6.5 Check the data-generating function 6.6 Example: Simulating clustered data 6.7 Extension: Standardization in a data generating process 6.8 Exercises", " Chapter 6 Data-Generating Processes As we saw in Chapter 4, the first step of a simulation is creating artificial data based on some process where we know (and can control) the truth. This step is what we call the data generating process (DGP). Think of it as a recipe for cooking up artificial data, which can be applied over and over, any time we’re hungry for a new dataset. Like a good recipe, a good DGP needs to be complete—it cannot be missing ingredients and it cannot omit any steps. Unlike cooking or baking, however, DGPs are usually specified in terms of a statistical model, or a set of equations involving constants, parameter values, and random variables, which we will instantiate as an R function (or perhaps a set of functions). More complex DGPs, such as those for hierarchical data or other latent variable models, will often involve a series of several equations that describe different dimensions or levels of the model, which need to be followed in sequence to produce an artificial dataset. Designing DGPs and implementing them in R code involves making choices about what aspects of the model we want to be able to control and how to set up the parameters of the model. In this chapter, we give a high-level overview of DGPs, discuss some of the choices and challenges involved in designing them, and demonstrate how to write R functions for the DGP. We then present a detailed example involving a hierarchical DGP for generating data on students nested within schools. 6.1 Examples Before diving in, it is helpful to consider a few examples that we will return to throughout this and subsequent chapters. 6.1.1 One-way analysis of variance We have already seen one example of a DGP in the ANOVA example from Chapter 5. Here, we consider observations on some variable \\(X\\) drawn from a population consisting of \\(G\\) groups, where group \\(g\\) has population mean \\(\\mu_g\\) and population variance \\(\\sigma_g^2\\) for \\(g = 1,...,G\\). A simulated dataset consists of \\(n_g\\) observations from each group \\(g = 1,...,G\\), where \\(X_{ig}\\) is the measurement for observation \\(i\\) in group \\(g\\). The statistical model for these data can be written as follows: \\[ X_{ig} = \\mu_g + \\epsilon_{ig}, \\quad \\mbox{with} \\quad \\epsilon_{ig} \\sim N( 0, \\sigma^2_g ) \\] Alternately, we could write the model as \\[ X_{ig} \\sim N( \\mu_g, \\sigma_g^2 ) \\] for \\(i = 1,...,n_g\\) and \\(g = 1,...,G\\). 6.1.2 Bivariate Poisson model As a second example, suppose that we want to understand how the usual Pearson sample correlation coefficient behaves with non-normal data or to investigate how the Pearson correlation relates to Spearman’s rank correlation coefficient. To look into such questions, one DGP we might entertain is a bivariate Poisson model, which is a distribution for a pair of counts, \\(C_1,C_2\\), where each count follows a Poisson distribution and where the pair of counts may be correlated. We will denote the expected values of the counts as \\(\\mu_1\\) and \\(\\mu_2\\) and the correlation between the counts as \\(\\rho\\). To simulate a dataset based on this model, we would first need to choose how many observations to generate. Call this sample size \\(N\\). One way to generate data following a bivariate Poisson model is to generate three independent Poisson random variables for each of the \\(N\\) observations: \\[ \\begin{aligned} Z_0 &amp;\\sim Pois\\left( \\rho \\sqrt{\\mu_1 \\mu_2}\\right) \\\\ Z_1 &amp;\\sim Pois\\left(\\mu_1 - \\rho \\sqrt{\\mu_1 \\mu_2}\\right) \\\\ Z_2 &amp;\\sim Pois\\left(\\mu_2 - \\rho \\sqrt{\\mu_1 \\mu_2}\\right) \\end{aligned} \\] and then combine the pieces to create two dependent observations: \\[ \\begin{aligned} C_1 &amp;= Z_0 + Z_1 \\\\ C_2 &amp;= Z_0 + Z_2. \\end{aligned} \\] An interesting feature of this model is that the range of possible correlations is constrained: only positive correlations are possible and, because each of the independent pieces must have a non-negative mean, the maximum possible correlation is \\(\\sqrt{\\frac{\\min\\{\\mu_1,\\mu_2\\}}{\\max\\{\\mu_1,\\mu_2\\}}}\\). 6.1.3 Hierarchical linear model for a cluster-randomized trial Cluster-randomized trials are randomized experiments where the unit of randomization is a group of individuals, rather than the individuals themselves. For example, suppose we have a collection of schools and the students within them. A cluster-randomized trial involves randomizing the schools into treatment or control conditions and then measuring an outcome such as academic performance on the multiple students within the schools. Typically, researchers will be interested in the extent to which average outcomes differ across schools assigned to different conditions, which captures the impact of the treatment relative to the control condition. We will index the schools using \\(j = 1,...,J\\) and let \\(n_j\\) denote the number of students observed in school \\(j\\). Say that \\(Y_{ij}\\) is the outcome measure for student \\(i\\) in school \\(j\\), for \\(1 = 1,...,n_j\\) and \\(j = 1,...,J\\), and let \\(Z_j\\) be an indicator equal to 1 if school \\(j\\) is assigned to the treatment condition and otherwise equal to 0. A widely used approach for estimating impacts from cluster-randomized trials is heirarchical linear modeling (HLM). One way to write an HLM is in two parts. First, we consider a regression model that describes the distribution of the outcomes across students within school \\(j\\): \\[ Y_{ij} = \\beta_{0j} + \\epsilon_{ij}, \\qquad \\epsilon_{ij} \\sim N(0, \\sigma_{\\epsilon}^2), \\] where \\(\\beta_{0j}\\) is the average outcome across students in school \\(j\\). Second, we allow that the school-level average outcomes differ by a treatment effect \\(\\gamma_{01}\\) and that, for schools within each condition, the average outcomes follow a normal distribution with variance \\(\\sigma_u^2\\). We can write these relationships as a regression equation for the school-specific average outcome: \\[ \\beta_{0j} = \\gamma_{00} + \\gamma_{10} Z_j + u_{0j}, \\quad u_{0j} \\sim N(0, \\tau^2), \\] where \\(\\gamma_{00}\\) is the average outcome among schools in the control condition. If we only consider the first stage of this model, it looks a bit like the one-way ANOVA model from the previous example: in both cases, we have multiple observations from each of several groups. The main distinction is that the ANOVA model treats the \\(G\\) groups as a fixed set, whereas the HLM treats the set of \\(J\\) schools as sampled from a larger population of schools and includes a regression model describing variation in the school-level average outcomes. 6.2 Components of a DGP A DGP involves a statistical model with parameters and random variables, but it also often includes further details as well, beyond those that we would consider to be part of the model as we would use it for analyzing real data. In statistical analysis of real data, we often use models that describe only part of the distribution of the data, rather than its full, multivariate distribution. For instance, when conducting a regression analysis, we are analyzing the distribution of an outcome or response variable, conditional on a set of predictor variables. When using an item response theory (IRT) model, we use responses to a set of items to estimate individual ability levels given the items on the test. In contrast, if we are going to generate data for simulating a regression model or IRT model, we need to specify distributions for these additional features (the predictors in a regression model, the items in an IRT model); we can no longer just take them as given. In designing and discussing DGPs, it is helpful to draw distinctions between the components of the focal statistical model and the remaining components of the DGP that are taken as given when analyzing real data. A first relevant distinction is between structural features, covariates, and outcomes (or more generally, endogenous quantities): Structural features are quantities that describe the structure of a dataset but do not enter directly into the focal statistical model, such as the per-group sample sizes in the one-way ANOVA example. When analyzing real data, we usually take the structural features as they come, but when simulating data, we will need to make choices about the structural features. For instance, in the HLM example involving students nested within schools, the number of students in each school is a structural feature. To simulate data based on HLM, we will need to make choices about the number of schools and the distribution of the number of students in each school (e.g., we might specify that school sizes are uniformly distributed between specified minimum and maximum sizes), even though we do not have to consider these quantities when estimating a hierarchical model on real data. Covariates are variables in a dataset that we typically take as given when analyzing real data. For instance, in the one-way ANOVA example, the group assignments of each observation is a covariate. In the HLM example, covariates would include the treatment indicators \\(Z_1,...,Z_J\\). In a more elaborate version of the HLM, they might also include variables such as student demographic information, measures of past academic performance, or school-level characteristics such as the school’s geographic region or treatment assignment. When analyzing real data, we condition on these quantities, but when specifying a DGP, we will need to make choices about how they are distributed (e.g., we might specify that students’ past academic performance is normally distributed). Outcomes and endogenous quantities are the variables whose distribution is described by the focal statistical model. In the one-way ANOVA example, the outcome variable consists of the measurements \\(X_{ig}\\) for \\(i = 1,...,n_g\\) and \\(g = 1,...,G\\). In the bivariate Poisson model, the outcomes consist of the component variables \\(Z_1,Z_2,Z_3\\) and the observed counts \\(C_1,C_2\\) because all of these quantities follow distributions that are specified as part of the focal model. The focal statistical model specifies the distribution of these variables, and we will be interested in estimating the parameters controlling their distribution. Note that the focal statistical model only determines this third component of the DGP. The focal model consists of the equations describing what we would aim to estimate when analyzing real data. In contrast, the full statistical model also includes additional elements specify how to generate the structural features and covariates—the pieces that are taken as given when analyzing real data. Table 6.1 contrasts the role of structural features, covariates, and outcomes in real data analysis versus in simulations. Table 6.1: Real Data Analysis versus Simulation Component Real world Simulation world Structural features We obtain data of a given sample size, sizes of clusters, etc. We specify sample sizes, we specify how to generate cluster sizes Covariates Data come with covariates We specify how to generate covariates Outcomes Data come with outcome variables We generate outcome data based on a focal model Parameter estimation We estimate a statistical model to learn about the unknown parameters We estimate a statistical model and compare the results to the true parameters For a given DGP, the full statistical model might involve distributions for structural features, distributions for covariates, and distributions for outcomes given the covariates. Each of these distributions will involve parameters that control the properties of the distribution (such as the average and degree of variation in a variable). We think of these parameters as falling into one of three categories: focal, auxiliary, or design. Focal parameters are the quantities that we care about and seek to estimate in real data analysis. These are typically parts of the focal statistical model, such as the population means \\(\\mu_1,...,\\mu_G\\) in the one-way ANOVA model, the correlation between counts \\(\\rho\\) in the bivariate Poisson model, or the treatment effect \\(\\gamma_{01}\\) in the HLM example. Auxiliary parameters are the other quantities that go into the focal statistical model or some other part of the DGP, which we might not be substantively interested in when analyzing real data but which nonetheless affect the analysis. For instance, in the one-way ANOVA model, we would consider the population variances \\(\\sigma_1^2,...,\\sigma_G^2\\) to be auxiliary if we are not interested in investigating how they vary from group to group. In the bivariate Poisson model we might consider the average counts \\(\\mu_1\\) and \\(\\mu_2\\) to be auxiliary parameters. Design parameters are the quantities that control how we generate structural features of the data. For instance, in a cluster-randomized trial, the fraction of schools assigned to treatment is a design parameter that can be directly controlled by the researchers. Additional design parameters might include the minimum and maximum number of students per school. Typically, we do not directly estimate such parameters because we take the distribution of structural features as given. It is evident from this discussion that DGPs can involve many moving parts. One of the central challenges in specifying DGPs is that the performance of estimation methods will generally be affected by the full statistical model—including the design parameters and distribution of structural features and covariates—even though they are not part of the focal model. 6.3 A statistical model is a recipe for data generation Once we have decided on a full statistical model and written it down in mathematical terms, we need to translate it into code. A function that implements a data-generating model should have the following form: generate_data &lt;- function( focal_parameters, auxiliary_parameters, design_parameters ) { # generate pseudo-random numbers and use those to make some data return(sim_data) } The function takes a set of parameter values as input, simulates random numbers and does calculations, and produces as output a set of simulated data. Typically, the inputs will consist of multiple parameters, and these will include not only the focal model parameters, but also the auxiliary parameters, sample sizes, and other design parameters. The output will typically be a dataset, mimicking what one would see in an analysis of real data. In some cases, the output data might be augmented with some other latent quantities (normally unobserved in the real world) that can be used later to assess whether an estimation procedure produces results that are close to the truth. We have already seen an example of a complete DGP function in the case study on one-way ANOVA (see Section 5.1). In this case study, we developed the following function to generate data for a single outcome from a set of \\(G\\) groups: generate_data &lt;- function(mu, sigma_sq, sample_size) { N &lt;- sum(sample_size) g &lt;- length(sample_size) group &lt;- factor( rep(1:g, times = sample_size) ) mu_long &lt;- rep( mu, times = sample_size ) sigma_long &lt;- rep( sqrt(sigma_sq), times = sample_size ) x &lt;- rnorm( N, mean = mu_long, sd = sigma_long ) sim_data &lt;- tibble( group = group, x = x ) return(sim_data) } This function takes both the focal model parameters (mu, sigma_sq) and other design parameters that one might not think of as parameters per-se (sample_size). When simulating, we have to specify quantities that we take for granted when analyzing real data. How would we write a DGP function for the bivariate Poisson model? The equations in Section 6.1 give us the recipe, so it just a matter of re-expressing them in code. For this model, the only design parameter is the sample size, \\(N\\); the sole focal parameter is the correlation between the variates, \\(\\rho\\); and the auxiliary parameters are the expected counts \\(\\mu_1\\) and \\(\\mu_2\\). Our function should have all four of these quantities as inputs and should produce as output a dataset with two variables, \\(C_1\\) and \\(C_2\\). Here is one way to implement the model: r_bivariate_Poisson &lt;- function(N, rho, mu1, mu2) { # covariance term, equal to E(Z_3) EZ3 &lt;- rho * sqrt(mu1 * mu2) # Generate independent components Z1 &lt;- rpois(N, lambda = mu1 - EZ3) Z2 &lt;- rpois(N, lambda = mu2 - EZ3) Z3 &lt;- rpois(N, lambda = EZ3) # Assemble components dat &lt;- data.frame( C1 = Z1 + Z3, C2 = Z2 + Z3 ) return(dat) } Here we generate 5 observations from the bivariate Poisson with \\(\\rho = 0.5\\) and \\(\\mu_1 = \\mu_2 = 4\\): r_bivariate_Poisson(5, rho = 0.5, mu1 = 4, mu2 = 4) ## C1 C2 ## 1 6 7 ## 2 4 5 ## 3 7 5 ## 4 1 5 ## 5 6 6 6.4 Plot the artificial data The whole purpose of writing a DGP is to produce something that can be treated just as if it were real data. Considering that is our goal, we should act like it and engage in data analysis processes that we would apply whenever we analyze real data. In particular, we often find it worthwhile to create one or more plots of the data generated by a DGP, just as we would if we were exploring a new real dataset for the first time. This exercise can be very helpful for catching problems in the DGP function (about which more below). Beyond just debugging, constructing graphic visualizations can be a very effective way to study a model and strengthen your understanding of how to interpret its parameters. In the one-way ANOVA example, it would be conventional to visualize the data with box plots or some other summary statistics for the data from each group. For exploratory graphics, we prefer plots that include representations of the raw data points, not just summary statistics. The figure below uses a density ridge-plot, filled in with points for each observation in each group. The plot is based on a simulated dataset with 50 observations in each of five groups. Here is a plot of 30 observations from the bivariate Poisson distribution with means \\(\\mu_1 = 10, \\mu_2 = 7\\) and correlation \\(\\rho = .65\\) (points are jittered slightly to avoid over-plotting): Figure 6.1: \\(N = 30\\) observations from the bivariate Poisson distribution with \\(\\mu_1 = 10, \\mu_2 = 7, ho = .65\\). Plots like these are useful for building intuitions about a model. For instance, we can inspect 6.1 to get a sense of the order of magnitude and range of the observations, as well as the likelihood of obtaining multiple observations with identical counts. Depending on the analysis procedures we will apply to the dataset, we might even create plots of transformations of the dataset, such as a histogram of the differences \\(C_2 - C_1\\) or a scatterplot of the rank transformations of \\(C_1\\) and \\(C_2\\). 6.5 Check the data-generating function An important part of programming in R—and especially when writing custom functions—is finding ways to test and check the correctness of your code. Just writing a data-generating function is not enough. It is also critical to test whether the output it produces is correct. How best to do this will depend on the particulars of the DGP being implemented. For many DGPs, a broadly useful strategy is to generate a very large sample of data, so large that the sample distribution should very closely resemble the population distribution. One can then test whether features of the sample distribution closely align with corresponding parameters of the population model. For the heteroskedastic ANOVA problem, one basic thing we could do is check that the simulated data from each group follows a normal distribution. In the following code, we simulate very large samples from each of the four groups, and check that the means and variances agree with the input parameters: mu &lt;- c(1, 2, 5, 6) sigma_sq &lt;- c(3, 2, 5, 1) check_data &lt;- generate_data( mu = mu, sigma_sq = sigma_sq, sample_size = rep(10000, 4) ) chk &lt;- check_data %&gt;% group_by( group ) %&gt;% dplyr::summarise( n = n(), mean = mean(x), var = var(x) ) %&gt;% mutate(mu = mu, sigma2 = sigma_sq) %&gt;% relocate( group, n, mean, mu, var, sigma2 ) chk ## # A tibble: 4 × 6 ## group n mean mu var sigma2 ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 10000 1.01 1 2.97 3 ## 2 2 10000 1.99 2 2.07 2 ## 3 3 10000 5.00 5 4.97 5 ## 4 4 10000 6.00 6 1.03 1 It seems we are recovering our parameters. We can also make some diagnostic plots to assess whether we have normal data (using QQ plots, where we expect a straight line if the data are normal): ggplot( check_data ) + aes( sample = x, color = group ) + facet_wrap( ~ group ) + stat_qq() + stat_qq_line() This diagnostic looks good too. Here, these checks may seem a bit silly, but most bugs are silly—at least once you find them! In models that are even a little bit more complex, it is quite easy for small things such as a sign error to slip into your code. Even simple checks such as these can be quite helpful in catching such bugs. 6.6 Example: Simulating clustered data Generating data with complex structure can be intimidating, but if you set out a recipe for how the data is generated it is often not too bad to then convert that recipe into code. We now illustrate this process with a detailed case study involving a more complex data-generating practice. Recent literature on multisite trials (where, for example, students are randomized to treatment or control within each of a series of sites) has explored how variation in the strength of effects can affect how different data-analysis procedures behave (e.g., Miratrix, Weiss, and Henderson 2021; Bloom et al. 2016). In this example, we are going to extend this work to explore best practices for estimating treatment effects in cluster randomized trials. In particular, we will investigate what happens when the treatment impact for each school is related to the size of the school. 6.6.1 A design decision: What do we want to manipulate? In designing a simulation study, we need to use a DGP that will allow us to address the specific questions we are interested in investigating. For instance, in the one-way ANOVA example, we wanted to see how different degrees of within-group variation impacted the performance. We therefore needed a data generation process that allowed us to control the within-group variation. To figure out what DGP we need for our clustered data example, we need to consider how we are going to use those data in our simulation study. Because we are interested in understanding what happens when school-specific effects are related to school size, we will need data with the following features: observations for students in each of several schools; schools are different sizes and have different mean outcomes; school-specific treatment effects correlate with school size; and schools are assigned to different treatment conditions. A given dataset will consist of observations for individual students in schools, with each student having a school id, a treatment assignment (shared for all in the school), and an outcome. A good starting point for building a DGP is to first sketch out what a simulated dataset should look like. For this example, we need data like the following: schoolID Z size studentID Y 1 1 24 1 3.6 1 1 24 3 1.0 1 etc etc etc etc 1 1 24 24 2.0 2 0 32 1 0.5 2 0 32 2 1.5 2 0 32 3 1.2 etc etc etc etc etc When running simulations, it is good practice to look at simple scenarios along with complex ones. This lets us check that some feature of the DGP does not matter as well as to identify conditions where the feature is important. Given this, we land on the following points: We need a DGP that lets us generate schools that are all the same size or that are all different sizes. Our DGP should allow for variation in the school-specific treatment effects. We should have the option to generate school-specific effects that are related to school size. 6.6.2 A model for a cluster RCT DGPs are expressed and communicated using mathematical models. In developing a DGP, we often start by considering the model for the outcomes (along with its focal parameters), which covers some of the steps in the full DGP but it may not be the complete recipe. It is helpful to write down the equations for the outcome model and then consider what further quantities need to be generated (such as structural features and covariates). Then we can consider how to generate these quantities with auxiliary models. Section 6.1.3 introduced a basic HLM for a cluster-randomized trial. This model had two parts, starting with a model for our student outcome: \\[ Y_{ij} = \\beta_{0j} + \\epsilon_{ij} \\mbox{ with } \\epsilon_{ij} \\sim N( 0, \\sigma^2_\\epsilon ) \\] where \\(Y_{ij}\\) is the outcome for student \\(i\\) in site \\(j\\), \\(\\beta_{0j}\\) is the average outcome in site \\(j\\), and \\(\\epsilon_{ij}\\) is the residual error for student \\(i\\) in site \\(j\\). The model was completed by specifying how the site-specific outcomes vary as a function of treatment assignment: \\[ \\beta_{0j} = \\gamma_{00} + \\gamma_{01} Z_j + u_j, \\quad u_j \\sim N( 0, \\sigma^2_u ).\\] This model has a constant treatment effect: if a school is assigned to treatment, then all outcomes in the cluster are raised by the amount \\(\\gamma_{01}\\). But we also want to allow the size of impact to vary by school size. This suggests we will need to elaborate the model to include a treatment-by-size interaction term. One approach for allowing the school-specific impacts to depend on school size is to introduce school size as a predictor, as in \\[ \\beta_{0j} = \\gamma_{00} + \\gamma_{01} Z_j + \\gamma_{02} \\left(Z_j \\times n_j\\right) + u_j. \\] A drawback of this specification is that changing the average size of the schools will change the average treatment impact. A more interpretable specification is to allow treatment effects to depend on the relative school sizes. To do this, we can define a covariate that describes the deviation in the school size relative to the average size. Thus, let \\[ S_j = \\frac{n_j - \\bar{n}}{ \\bar{n} }, \\] where \\(\\bar{n}\\) is the overall average school size. Using this covariate, we then revise our equation for our site \\(j\\) to: \\[ \\beta_{0j} = \\gamma_{00} + \\gamma_{01} Z_j + \\gamma_{02} \\left( Z_j \\times S_j\\right) + u_j. \\] If \\(\\gamma_{02}\\) is positive, then bigger schools will have larger treatment impacts. Because \\(S_j\\) is centered at 0, the overall average impact across schools will be simply \\(\\gamma_{01}\\). (If \\(S_j\\) was not centered at zero, then our overall average impact in our data would be some function of \\(\\gamma_{01}\\) and \\(\\gamma_{02}\\).) Putting all of the above together, we now have an HLM to describe the distribution of outcomes conditional on the covariates and structural features: \\[ \\begin{aligned} Y_{ij} &amp;= \\beta_{0j} + \\epsilon_{ij} \\quad &amp;\\epsilon_{ij} &amp;\\sim N( 0, \\sigma^2_\\epsilon ) \\\\ \\beta_{0j} &amp;= \\gamma_{00} + \\gamma_{01} Z_j + \\gamma_{02} Z_j S_j + u_j \\quad &amp; u_j &amp;\\sim N( 0, \\sigma^2_u ) \\end{aligned} \\] Substituting the second equation into the first leads to a single equation for generating the student-level outcomes (or what is called the reduced form of the HLM): \\[ Y_{ij} = \\gamma_{00} + \\gamma_{01} Z_j + \\gamma_{02} Z_j S_j + u_j + \\epsilon_{ij}\\] The parameters of this focal model are the mean outcome among control schools (\\(\\gamma_{00}\\)), the average treatment impact (\\(\\gamma_{01}\\)), the site-size by treatment interaction term (\\(\\gamma_{02}\\)), the amount of school-level variation (\\(\\sigma^2_u\\)), and the amount of within-school variation (\\(\\sigma^2_\\epsilon\\)). There are several ways that we could elaborate this model further. For one, we might want to include a main effect for \\(S_j\\), so that average outcomes in the absence of treatment are also dependent on school size. For another, we might revise the model to allow for school-to-school variation in treatment impacts that is not explained by school size. For simplicity, we do not build in these further features, but see the exercises at the end of the chapter. So far we have a mathematical model analogous to what we would write if we were analyzing the data. To generate data, we also need a way to generate the structural features and covariates involved in the model. First, we need to know the number of clusters (\\(J\\)) and the sizes of the clusters (\\(n_j\\), for \\(j = 1, ..., J\\)). For illustrative purposes, we will generate size sizes from a uniform distribution with average school size \\(\\bar{n}\\) and a fixed parameter \\(\\alpha\\) that controls the degree of variation in school size. Mathematically, \\[ n_j \\sim unif\\left[ (1-\\alpha)\\bar{n}, (1+\\alpha)\\bar{n} \\right].\\] Equivalently, we could generate site sizes by taking \\[n_j = \\bar{n}(1 + \\alpha U_j), \\quad U_j \\sim unif(-1, 1).\\] For instance, if \\(\\bar{n} = 100\\) and \\(\\alpha = 0.25\\) then schools would range in size from 75 to 125. This specification is nice because it is simple, with just two parameters,both of which are easy to interpret: \\(\\bar{n}\\) is the average school size and \\(\\alpha\\) is the amount of variation in school size. To round out the model, we also need to define how to generate the treatment indicator, \\(Z_j\\). To allow for different treatment allocations, we will specify a proportion \\(p\\) of clusters assigned to treatment. Because we are simulating a cluster-randomized trial, we then generate \\(Z_j = 1\\) or \\(Z_j = 0\\) by drawing a simple random sample (without replacement) of \\(p \\times J\\) schools out of the total sample of \\(J\\) schools. We will denote this process as \\(Z_1,...,Z_J \\sim SRS(p, J)\\), where SRS stands for simple random sample. Now that we have an auxiliary model for school sizes, let us look again at our treatment impact heterogeneity term: \\[ \\gamma_2 Z_j S_j = \\gamma_2 Z_j \\left(\\frac{n_j - \\bar{n}}{\\bar{n}}\\right) = \\gamma_2 Z_j \\alpha U_j, \\] where \\(U_j \\sim unif(-1,1)\\) is the uniform variable used to generate \\(n_j\\). Because we have standardized by average school size, the importance of the covariate does not change as a function of average school size, but rather as a function of the relative variation parameter \\(\\alpha\\). Setting up a DGP with standardize quantity will make it easier to interpret simulation results, especially if we are looking at results from multiple scenarios with different parameter values. To the extent feasible, we want the parameters of the DGP to change only one feature of the data, so that it is easier to isolate the influence of each parameter. 6.6.3 From equations to code When sketching out our DGP mathematically we worked from the students to the schools. For actual data generation, we will now follow our final model, but go by layers in the other direction. First, we generate the sites: Generate site sizes Generate site-level covariates Generate site level random effects Then we generate the students inside the sites: Generate student covariates Generate student residuals Add everything up to generate student outcomes The mathematical model gives us exactly the details we need to execute on these steps. We start by specifying a function with all the parameters we might want to pass it, including defaults for each (see A.2 for more on function defaults): gen_dat_model &lt;- function( n_bar = 10, J = 30, p = 0.5, gamma_0 = 0, gamma_1 = 0, gamma_2 = 0, sigma2_u = 0, sigma2_e = 1, alpha = 0 ) { # Code (see below) goes here. } Note our parameters are a mix of model parameters (gamma_0, gamma_1, sigma2_e, etc., representing coefficients in regressions, variance terms, etc.) and design parameters (n_bar, J, p) that directly inform data generation. We set default arguments (e.g., gamma_0=0) so we can ignore aspects of the DGP that we do not care about later on. Inside the model, we will have a block of code to generate the sites, and then another to generate the students. Make the sites. We make the sites first: # generate site sizes n_min = round( n_bar * (1 - alpha) ) n_max = round( n_bar * (1 + alpha) ) nj &lt;- sample( n_min:n_max, J, replace=TRUE ) # Generate average control outcome and average ATE for all sites # (The random effects) u0j = rnorm( J, mean=0, sd=sqrt( sigma2_u ) ) # randomize units within each site (proportion p to treatment) Zj = ifelse( sample( 1:J ) &lt;= J * p, 1, 0) # Calculate site intercept for each site beta_0j = gamma_0 + gamma_1 * Zj + gamma_2 * Zj * (nj-n_bar)/n_bar + u0j The code is a literal translation of the math we did before. Note the line with sample(1:J) &lt;= J*p; this is a simple trick to generate a treatment and control 0/1 indicator. There is also a serious error in the above code (serious in that the code will run and look fine in many cases, but not always do what we want); we leave it as an exercise (see below) to find and fix it. Make the individuals. We next use the site characteristics to then generate the individuals. # Make individual site membership sid = as.factor( rep( 1:J, nj ) ) dd = data.frame( sid = sid ) # Make individual level tx variables dd$Z = Zj[ dd$sid ] # Generate the residuals N = sum( nj ) e = rnorm( N, mean=0, sd=sqrt( sigma2_e ) ) # Bundle and send out dd &lt;- mutate( dd, sid=as.factor(sid), Yobs = beta_0j[sid] + e, Z = Zj[ sid ] ) A key piece here is the rep() function that takes a list and repeats each element of the list a specified number of times. In particular, rep() repeats each number (\\(1, 2, /ldots,J\\)), the corresponding number of times as listed in nj. Once we put the above code in our function skeleton, we can our function as so: dat &lt;- gen_dat_model( n_bar = 5, J=3, p = 0.5, gamma_0 = 0, gamma_1 = 0.2, gamma_2 = 0.2, sigma2_u = 0.4, sigma2_e = 1, alpha = 0.5 ) dat ## sid Z Yobs ## 1 1 1 0.2687859 ## 2 1 1 -1.2065572 ## 3 1 1 -0.9641666 ## 4 1 1 -2.3129148 ## 5 1 1 1.5152454 ## 6 1 1 -1.1710916 ## 7 2 0 1.8776766 ## 8 2 0 0.4619042 ## 9 2 0 -0.1163104 ## 10 2 0 1.5650643 ## 11 2 0 0.6035344 ## 12 2 0 2.0486276 ## 13 2 0 1.7362065 ## 14 3 0 0.5466768 ## 15 3 0 -0.9304395 ## 16 3 0 -0.5485582 ## 17 3 0 0.4810946 Our data generation code is complete. We can control the average size of the clusters (n), the number of clusters (J), the proportion treated (p), the average outcome in the control group (gamma_0), the average treatment effect (gamma_1), the site size by treatment interaction (gamma_2), the amount of cross site variation (sigma2_u), the residual variation (sigma2_e), and the amount of site size variation (alpha). The next step is to test the code, making sure it is doing what we think it is. In fact, it is not–there is a subtle bug that only appears under some specifications of the parameters; see the exercises for more on testing ones code, and for diagnosing and repairing this error in particular. 6.7 Extension: Standardization in a data generating process In this chapter, we made a model to generate data for a cluster randomized trial. Given our model, we can generate data by specifying our parameters and variables of \\(\\gamma_{0}, \\gamma_{1}, \\gamma_{2}, \\sigma^2_\\epsilon, \\sigma^2_u, \\bar{n}, \\alpha, J, p\\). One factor that tends to show up when working with multisite data is how much variation there is within sites vs between sites. For example, the Intra-Class Correlation (ICC), a measure of how much of the variation in the outcome is due to differences between sites, is a major component for power calculations. Because of this, we will likely want to manipulate the amount of within vs. between variation in our simulations. An easy way to do this would be to simply raise or lower the amount of variation within sites (\\(\\sigma^2_u\\)). This unfortunately has a side effect: if we increase \\(\\sigma^2_u\\), our overall variation of \\(Y\\) will also increase. This will make it hard to think about, e.g., power, since we have confounded within vs. between variation with overall variation (which is itself bad for power). It also impacts interpretation of coefficients. A treatment effect of 0.2 on our outcome scale is “smaller” if there is more overall variation. Right now, our model is \\[ Y_{ij} = \\gamma_{0} + \\gamma_{1} Z_j + \\gamma_2 Z_j \\left(\\frac{n_j - \\bar{n}}{\\bar{n}} \\right) + u_j + \\epsilon_{ij} \\] Given our model, the variance of our control-side outcomes is \\[ \\begin{aligned} var( Y_{ij}(0) ) &amp;= var( \\beta_{0j} + \\epsilon_{ij} ) \\\\ &amp;= var( \\gamma_{0} + \\gamma_{1} Z_j + \\gamma_{2}Z_j \\tilde{n}_j + u_j + \\epsilon_{ij} ) \\\\ &amp;= var( \\gamma_{0} + u_j + \\epsilon_{ij} ) \\\\ &amp;= \\sigma^2_u + \\sigma^2_\\epsilon \\end{aligned} \\] (We drop the terms with the \\(Z_j\\) because we are looking at control-side variation, where nothing is treated.) We see that as we increase either within or between variation, overall variation increases. We can improve our data generating process to allow for directly controlling the amount of within vs. between variation without it being confounded with overall variation. To do this we first (1) Standardize our data and then (2) reparameterize, so we have human-selected parameters that we can interpret that we then translate to our list of data generation parameters. In particular, we will index our DGP with the more interpretable parameter of the Intra-Class Correlation (ICC), and standardize our DGP so it is all in effect size units. The effect size of an impact is defined as the impact over the control-side standard deviation. (Sometimes people use the pooled standard deviation, but this is usually a bad choice if one suspects treatment variation. More treatment variation should not reduce the effect size for the same absolute average impact.) \\[ ES = \\frac{\\gamma_1}{SD( Y | Z_j = 0 )} = \\frac{\\gamma_1}{\\sqrt{ \\sigma^2_u + \\sigma^2_\\epsilon } } \\] The way we think about how “big” \\(\\gamma_1\\) is depends on how much site variation and residual variation there is. But it is also easier to detect effects when the residual variation is small. Effect sizes “standardize out” these sorts of tensions. We can use that. In particular, we will use the Intraclass Correlation Coeffiicent (ICC), defined as \\[ ICC = \\frac{ \\sigma^2_u }{ \\sigma^2_\\epsilon + \\sigma^2_u } . \\] The ICC is a measure of within vs. between variation. What we then do is first standardized our data, meaning we ensure the control side variance equals 1. Using the above, this means \\(\\sigma^2_u + \\sigma^2_\\epsilon = 1\\). It also gives us \\(ICC = \\sigma^2_u\\), and \\(\\sigma^2_\\epsilon = 1 - ICC\\). Our two model parameters are now tied together by our single ICC tuning parameter. The core idea is we can now manipulate the aspects of the DGP we want while holding other aspects of the DGP constant. Given our standardized scale, we have dropped a parameter from our set of parameters we might want to vary, and ensured that varying the other parameter (now the ICC) is varying only one aspect of the DGP, not both. Before, increasing \\(\\sigma^2_u\\) had two consequences: total variation and relative amount of variation at the school level. Now, manipulating ICC only does the latter. E.g., we can call our DGP function as follows: ICC = 0.3 dat &lt;- gen_dat_model( n_bar = 20, J = 30, p = 0.5, gamma_0 = 0, gamma_1 = 0.3, gamma_2 = 0.2, sigma2_u = ICC, sigma2_e = 1 - ICC, alpha = 0.5 ) 6.8 Exercises 6.8.1 The Welch test on a shifted-and-scaled \\(t\\) distribution The shifted-and-scaled \\(t\\)-distribution has parameters \\(\\mu\\) (mean), \\(\\sigma\\) (scale), and \\(\\nu\\) (degrees of freedom). If \\(T\\) follows a student’s \\(t\\)-distribution with \\(\\nu\\) degrees of freedom, then \\(S = \\mu + \\sigma T\\) follows a shifted-and-scaled \\(t\\)-distribution. The following function will generate random draws from this distribution (the scaling of \\((\\nu-2)/\\nu\\) is to account for a non-scaled \\(t\\)-distribution having a variance of \\(\\nu/(\\nu-2)\\)). r_tss &lt;- function(n, mean, sd, df) { mean + sd * sqrt( (df-2)/df ) * rt(n = n, df = df) } r_tss(n = 8, mean = 3, sd = 2, df = 5) ## [1] 1.7742761 0.8768227 7.0415207 0.3654655 1.3298592 2.4276308 3.9402337 ## [8] 2.2716459 Modify the Welch simulation’s simulate_data() function to generate data from shifted-and-scaled \\(t\\)-distributions rather than from normal distributions. Include the degrees of freedom as an input argument. Simulate a dataset with low degrees of freedom and plot it to see if you see a few outliers. Now generate more data and calculate the means and standard deviations to see if they are correctly calibrated (i.e., generate a big dataset to ensure you get reliable mean and standard deviation estimates). Check df equal to 500, 5, 3, and 2. Once you are satisfied you have a correct DGP function, re-run the Type-I error rate calculations from the prior exercises in Section 5.5 using a \\(t\\)-distribution with 5 degrees of freedom. Do the results change substantially? 6.8.2 Plot the bivariate Poisson In Section 6.3, we provided an example of a DGP function for the bivariate Poisson model. We demonstrated a plot of data simulated from this function in 6.4. Create a similar plot but for a much larger sample size of \\(N = 1000\\). With such a large dataset, it will likely be hard to distinguish individual observations because of over-plotting. Create a better visual representation of the same simulated dataset, such as a heatmap or a contour plot. 6.8.3 Check the bivariate Poisson function Although we presented a DGP function for the bivariate Poisson model, we have not demonstrated how to check that the function is correct—we’re leaving that to you! Write some code to verify that the function r_bivariate_Poisson() is working properly. Do this by generating a very large sample (say \\(N = 10^4\\) or \\(10^5\\)) and verifying the following: The sample means of \\(C_1\\) and \\(C_2\\) align with the specified population means. The sample variances of \\(C_1\\) and \\(C_2\\) are close to the specified population means (because for a Poisson distribution \\(\\mathbb{E}(C_p) = \\mathbb{V}(C_p)\\) for \\(p = 1,2\\)). The sample correlation aligns with the specified population correlation. The observed counts \\(C_1\\) and \\(C_2\\) follow Poisson distributions. 6.8.4 Add error-catching to the bivariate Poisson function In Section 6.1, we noted that the bivariate Poisson function as we described it can only produce a constrained range of correlations, which a maximum value that depends on the ratio of \\(\\mu_1\\) to \\(\\mu_2\\). Our current implementation of the model does not handle this aspect of the model very well: r_bivariate_Poisson(5, rho = 0.6, mu1 = 4, mu2 = 12) ## Warning in rpois(N, lambda = mu1 - EZ3): NAs produced ## C1 C2 ## 1 NA 9 ## 2 NA 8 ## 3 NA 15 ## 4 NA 14 ## 5 NA 13 For this combination of parameter values, \\(\\rho \\times \\sqrt{\\mu_1 \\mu_2}\\) is larger than \\(\\mu_1\\), which leads to simulated values for \\(C_1\\) that are all missing. That makes it pretty hard to compute the correlation between \\(C_1\\) and \\(C_2\\). Please help us fix this issue! Revise r_bivariate_Poisson() so that it checks for allowable values of \\(\\rho\\). If the user specifies a combination of parameters that does not make sense, make the function throw an error (using R’s stop() function). 6.8.5 A bivariate negative binomial distribution One potential limitation of the bivariate Poisson distribution described above is that the variances of the counts are necessarily equal to the means (i.e., unit dispersion). This limitation is inherited from the univariate Poisson distributions that each variate follows. Just as with the corresponding univariate distributions, one way to relax this limitation is to consider distributions with marginals that are negative binomial rather than Poisson, thereby allowing for overdispersion. Cho et al. (2023) describes one type of bivariate negative binomial distribution. They provide a method for constructing a bivariate negative binomial distribution by using latent, gamma-distributed components. Their algorithm involves first generating components from gamma distributions with specified shape and scale parameters: \\[ \\begin{aligned} Z_0 &amp;\\sim \\Gamma\\left( \\alpha_0, \\beta\\right) \\\\ Z_1 &amp;\\sim \\Gamma\\left( \\alpha_1, \\beta\\right) \\\\ Z_2 &amp;\\sim \\Gamma\\left( \\alpha_2, \\beta\\right) \\end{aligned} \\] for \\(\\alpha_0,\\alpha_1,\\alpha_2 &gt; 0\\) and \\(\\beta &gt; 0\\). Then simulate independent Poisson random variables as \\[ \\begin{aligned} C_1 &amp;\\sim Pois\\left( Z_0 + Z_1 \\right) \\\\ C_2 &amp;\\sim Pois\\left( \\delta(Z_0 + Z_2) \\right). \\end{aligned} \\] The resulting count variables follow marginal negative binomial distributions with moments \\[ \\begin{aligned} \\mathbb{E}(C_1) &amp;= (\\alpha_0 + \\alpha_1) \\beta &amp; \\mathbb{V}(C_1) &amp;= (\\alpha_0 + \\alpha_1) \\beta (\\beta + 1) \\\\ \\mathbb{E}(C_2) &amp;= (\\alpha_0 + \\alpha_2) \\beta \\delta &amp; \\mathbb{V}(C_2) &amp;= (\\alpha_0 + \\alpha_2) \\beta \\delta (\\beta \\delta + 1) \\\\ &amp; &amp; \\text{Cov}(C_1, C_2) &amp;= \\alpha_0 \\beta^2 \\delta. \\end{aligned} \\] The correlation between \\(C_1\\) and \\(C_2\\) is thus \\[ \\text{cor}(C_1, C_2) = \\frac{\\alpha_0}{\\sqrt{(\\alpha_0 + \\alpha_1)(\\alpha_0 + \\alpha_2)}} \\frac{\\beta \\sqrt{\\delta}}{\\sqrt{(\\beta + 1)(\\beta \\delta + 1)}}. \\] Write a DGP function that implements this distribution. Write some code to check that the function produces data where each variate follows a negative binomial distribution and where the correlation agrees with the formula given above. Consider parameter values that produce \\(\\mathbb{E}(C_1) = \\mathbb{E}(C_2) = 10\\) and \\(\\mathbb{V}(C_1) = \\mathbb{V}(C_2) = 15\\). What are the minimum and maximum possible correlations between \\(C_1\\) and \\(C_2\\)? 6.8.6 Another bivariate negative binomial distribution Another model for generating bivariate counts with negative binomial marginal distributions is by using Gaussian copulas. Here is a mathematical recipe for this distribution, which will produce counts with marginal means \\(\\mu_1\\) and \\(\\mu_2\\) and marginal variances \\(\\mu_1 + \\mu_1^2 / p_1\\) and \\(\\mu_2 + \\mu_2^2 / p_2\\). Start by generating variates from a bivariate standard normal distribution with correlation \\(\\rho\\): \\[ \\left(\\begin{array}{c}Z_1 \\\\ Z_2 \\end{array}\\right) \\sim N\\left(\\left[\\begin{array}{c}0 \\\\ 0\\end{array}\\right], \\ \\left[\\begin{array}{cc}1 &amp; \\rho \\\\ \\rho &amp; 1\\end{array}\\right]\\right) \\] Now find \\(U_1 = \\Phi(Z_1)\\) and \\(U_1 = \\Phi(Z_1)\\), where \\(\\Phi()\\) is the standard normal cumulative distribution function (called pnorm() in R). Then generate the counts by evaluating \\(U_1\\) and \\(U_2\\) with the negative binomial quantile function, \\(F_{NB}^{-1}(x | \\mu, p)\\) with mean parameters \\(\\mu\\) and size parameter \\(p\\) (this function is called qnbinom() in R): \\[ C_1 = F_{NB}^{-1}(U_1 | \\mu_1, p_1) \\qquad C_2 = F_{NB}^{-1}(U_2 | \\mu_2, p_2). \\] The resulting counts will be correlated, but the correlation will not be equal to \\(\\rho\\). Write a DGP function that implements this distribution. Write some code to check that the function produces data where each variate follows a negative binomial distribution Use the function to create a graph showing the population correlation between the observed counts as a function of \\(\\rho\\). Use \\(\\mu_1 = \\mu_2 = 10\\) and \\(p_1 = p_2 = 20\\). How does the range of correlations compare to the range from Exercise 6.8.5? 6.8.7 Checking and extending the Cluster RCT DGP What is the variance of the outcomes generated by our model if there is no treatment effect? (Try simulating data to check!) What other quick checks can you make on your DGP to make sure it is working? In gen_dat_model() we have the following line of code to generate the number of individuals per site. nj &lt;- sample( n_min:n_max, J, replace=TRUE ) This code has an error. Generate a variety of datasets where you vary n_min, n_max and J to discover the error. Then repair the code. Checking your data generating process across a range of scenarios is extremely important. The DGP allows for site-level treatment impact variation–but only if it is related to site size. How could you modify your simulation to allow for site-level treatment impact variation that is not related to site size? Implement this change and generate some data to show how it works. Extend the data generating process to include an individual level covariate \\(X\\) that is predictive of outcome. In particular, you will want to adjust your level one equation to \\[ Y_{ij} = \\beta_{0j} + \\beta_{1} X_{ij} + \\epsilon_{ij} . \\] Keep the same \\(\\beta_1\\) for all sites. You will have to specify how to generate your \\(X_{ij}\\). For starters, just generate it as a standard normal, and do not worry about having the mean of \\(X_{ij}\\) vary by sites unless you are excited to try to get that to work. References Bloom, Howard S., Stephen W. Raudenbush, Michael J. Weiss, and Kristin Porter. 2016. “Using Multisite Experiments to Study Cross-Site Variation in Treatment Effects: A Hybrid Approach With Fixed Intercepts and a Random Treatment Coefficient.” Journal of Research on Educational Effectiveness 10 (4): 0–0. https://doi.org/10.1080/19345747.2016.1264518. Cho, Hunyong, Chuwen Liu, John S Preisser, and Di Wu. 2023. “A Bivariate Zero-Inflated Negative Binomial Model and Its Applications to Biomedical Settings.” Statistical Methods in Medical Research 32 (7): 1300–1317. https://doi.org/10.1177/09622802231172028. Miratrix, Luke W., Michael J. Weiss, and Brit Henderson. 2021. “An Applied Researcher’s Guide to Estimating Effects from Multisite Individually Randomized Trials: Estimands, Estimators, and Estimates.” Journal of Research on Educational Effectiveness 14 (1): 270–308. https://doi.org/10.1080/19345747.2020.1831115. "]]
