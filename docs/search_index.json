[["special-topics-on-reporting-simulation-results.html", "Chapter 14 Special Topics on Reporting Simulation Results 14.1 Analyzing results with few iterations per scenario 14.2 Analyzing results when some trials have failed 14.3 More on modeling", " Chapter 14 Special Topics on Reporting Simulation Results In this chapter we will cover some special topics on reporting simulation results. In particular, we dive more deeply into having only a few iterations per scenario, and then discuss what to do when you are evaluating methods that sometimes fail to converge or give an answer. 14.1 Analyzing results with few iterations per scenario When your simulation iterations are expensive to run (i.e., when each model fitting takes several minutes, then running thousands of iterations for many scenarios may no longer be computationally feasible) then you can run into serious issues with noisy estimates of performance. First, this is why understanding how large your Monte Carlo Standard Errors (MCSEs) are is so important. If the methods being evaluated are substantially different, then you may be able to demonstrate what you need to, even with few iterations. If, however, your MCSEs are too large, then you can use the visualization methods that average across scenarios, giving you more precise estimates of aggregated performance. Do not, by contrast, trust the boxplot approaches–the MCSEs will make your boxes wider, and give the impression that there is more variation than there really is. Regression approaches can be particularly useful: the regressions are effectively again averaging performance across scenario, looking for overall trends. You can even fit random effects regression, specifially accounting for the noise in the scenario-specific performance measures. For more on this approach see Gilbert and Miratrix (2024). 14.1.1 Example: ClusterRCT with only 100 replicates per scenario In the prior chapter we analyzed the results of our cluster RCT simulation with 1000 replicates per scenario. But say we only had 100. In this case, some of our trends would be less apparent. Using the prior chapter as a guide, we recreate some of the plots to show how MCSE can distort the picture of what is going on. First, we look at our single plot of the raw results. Before we plot, however, we calculate MCSEs and add them to the plot as error bars. sres_sub &lt;- sres %&gt;% filter( n_bar == 320, J == 20 ) %&gt;% mutate( bias.mcse = SE / sqrt( R ) ) ggplot( sres_sub, aes( as.factor(alpha), bias, col=method, pch=method, group=method ) ) + facet_grid( size_coef ~ ICC, labeller = label_both ) + geom_point() + geom_errorbar( aes( ymin = bias - 2*bias.mcse, ymax = bias + 2*bias.mcse ), width = 0 ) + geom_line() + geom_hline( yintercept = 0 ) + theme_minimal() Aggregation should smooth out some of our uncertainty. When we aggregate across 9 scenarios, our number of replicates goes from 100 to 900; our MCSEs should be about a third the size. Here is our aggregated bias plot: TODO Even with the addtional replicates per point, we are seeing noise. Note how our three methods track each other, giving a sense of a shared bias in some cases. This is because all methods are analyzing the same set of datasets; they have shared uncertainty. This uncertainty can be deceptive. It can also be a boon: if we are explicitly comparing the performance of one method vs another, the shared uncertainty can be subtracted out, similar to having a blocked experiment. For more on this, see (CITE GILBERT META REGRESSION). To give a visual sense of the uncertainty, we can add error bars to our plot. Here we calculate the uncertainty for the aggregated data TO DO 14.2 Analyzing results when some trials have failed If your methods being evaluated sometimes fail, then when they tend to fail is something to investigate in its own right. Ideally, failure would not be too common, meaning we could drop those trials, or keep them, without really impacting our overall results. But one should at least know what one is ignoring. For example, in our cluster RCT running example, we know that our multilevel model, at least sometimes, has convergence issues. We also know that ICC is an important driver of when these convergence issues might occur, so we can explore how often we get a convergence message by ICC level: res %&gt;% group_by( method, ICC ) %&gt;% summarise( message = mean( message ) ) %&gt;% pivot_wider( names_from = &quot;method&quot;, values_from=&quot;message&quot; ) ## Warning: There were 15 warnings in `summarise()`. ## The first warning was: ## ℹ In argument: `message = mean(message)`. ## ℹ In group 1: `method = &quot;Agg&quot;` `ICC = 0`. ## Caused by warning in `mean.default()`: ## ! argument is not numeric or logical: returning NA ## ℹ Run `dplyr::last_dplyr_warnings()` to see the 14 remaining warnings. ## # A tibble: 5 × 4 ## ICC Agg LR MLM ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 NA NA NA ## 2 0.2 NA NA NA ## 3 0.4 NA NA NA ## 4 0.6 NA NA NA ## 5 0.8 NA NA NA We see that when the ICC is 0 we get a lot of convergence issues, but as soon as we pull away from 0 it drops off considerably. At this point we might decide to drop those runs with a message or keep them. In this case, we decide to keep. It should not matter much, except possibly when ICC = 0, and we know the convergence issues are driven by trying to estimate a 0 variance, and thus is in some sense expected. Furthermore, we know people using these methods would likely ignore these messages, and thus we are faithfully capturing how these methods would be used in practice. We might eventually, however, want to do a separate analysis of the ICC = 0 context to see if the MLM approach is actually falling apart, or if it is just throwing warnings. 14.3 More on modeling 14.3.1 Example 1: Biserial, revisited For example, consider the bias of the biserial correlation estimates from above. Visually, we see that several factors appear to impact bias, but we might want to get a sense of how much. In particular, does the population vs sample cutoff option matter, on average, for bias, across all the simulation factors considered? We can fit a regression model to see: options(scipen = 5) mod = lm( bias ~ fixed + rho + I(rho^2) + p1 + n, data = r_F) summary(mod, digits=2) ## ## Call: ## lm(formula = bias ~ fixed + rho + I(rho^2) + p1 + n, data = r_F) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.0215935 -0.0013608 0.0003823 0.0015677 0.0081802 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.00218473 0.00015107 14.462 &lt; 2e-16 *** ## fixedSample cutoff -0.00363520 0.00009733 -37.347 &lt; 2e-16 *** ## rho -0.00942338 0.00069578 -13.544 &lt; 2e-16 *** ## I(rho^2) 0.00720857 0.00070868 10.172 &lt; 2e-16 *** ## p1.L 0.00461700 0.00010882 42.426 &lt; 2e-16 *** ## p1.Q -0.00160546 0.00010882 -14.753 &lt; 2e-16 *** ## p1.C 0.00081464 0.00010882 7.486 8.41e-14 *** ## p1^4 -0.00011190 0.00010882 -1.028 0.3039 ## n.L 0.00362949 0.00010882 33.352 &lt; 2e-16 *** ## n.Q -0.00103981 0.00010882 -9.555 &lt; 2e-16 *** ## n.C 0.00027941 0.00010882 2.568 0.0103 * ## n^4 0.00001976 0.00010882 0.182 0.8559 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.003372 on 4788 degrees of freedom ## Multiple R-squared: 0.5107, Adjusted R-squared: 0.5096 ## F-statistic: 454.4 on 11 and 4788 DF, p-value: &lt; 2.2e-16 The above printout gives main effects for each factor, averaged across other factors. Because p1 and n are ordered factors, the lm() command automatically generates linear, quadradic, cubic and fourth order contrasts for them. We smooth our rho factor, which has many levels of a continuous measure, with a quadratic curve. We could instead use splines or some local linear regression if we were worried about model fit for a complex relationship. The main effects are summaries of trends across contexts. For example, averaged across the other contexts, the “sample cutoff” condition is around 0.004 lower than the population (the baseline condition). We can also use ANOVA to get a sense of the major sources of variation in the simulation results (e.g., identifying which factors have negligible/minor influence on the bias of an estimator). To do this, we use aov() to fit an analysis of variance model: anova_table &lt;- aov(bias ~ rho * p1 * fixed * n, data = r_F) summary(anova_table) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## rho 1 0.002444 0.002444 1673.25 &lt;2e-16 *** ## p1 4 0.023588 0.005897 4036.41 &lt;2e-16 *** ## fixed 1 0.015858 0.015858 10854.52 &lt;2e-16 *** ## n 4 0.013760 0.003440 2354.60 &lt;2e-16 *** ## rho:p1 4 0.001722 0.000431 294.71 &lt;2e-16 *** ## rho:fixed 1 0.003440 0.003440 2354.69 &lt;2e-16 *** ## p1:fixed 4 0.001683 0.000421 287.98 &lt;2e-16 *** ## rho:n 4 0.002000 0.000500 342.31 &lt;2e-16 *** ## p1:n 16 0.019810 0.001238 847.51 &lt;2e-16 *** ## fixed:n 4 0.013359 0.003340 2285.97 &lt;2e-16 *** ## rho:p1:fixed 4 0.000473 0.000118 80.87 &lt;2e-16 *** ## rho:p1:n 16 0.001470 0.000092 62.91 &lt;2e-16 *** ## rho:fixed:n 4 0.002929 0.000732 501.23 &lt;2e-16 *** ## p1:fixed:n 16 0.001429 0.000089 61.12 &lt;2e-16 *** ## rho:p1:fixed:n 16 0.000429 0.000027 18.36 &lt;2e-16 *** ## Residuals 4700 0.006866 0.000001 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The advantage here is the multiple levels of our categorical factors get bundled together in our table of results, making a tidier display. We can also summarise our anova table to see the contribution of the various factors and interactions to the total amount of variation in performance: library(lsr) etaSquared(anova_table) %&gt;% knitr::kable( digits = 2 ) eta.sq eta.sq.part rho 0.02 0.26 p1 0.21 0.77 fixed 0.14 0.70 n 0.12 0.67 rho:p1 0.02 0.20 rho:fixed 0.03 0.33 p1:fixed 0.02 0.20 rho:n 0.02 0.23 p1:n 0.18 0.74 fixed:n 0.12 0.66 rho:p1:fixed 0.00 0.06 rho:p1:n 0.01 0.18 rho:fixed:n 0.03 0.30 p1:fixed:n 0.01 0.17 rho:p1:fixed:n 0.00 0.06 Here we see which factors are explaining the most variation. E.g., p1 is explaining 21% of the variation in bias across simulations. The contribution of the three way interactions is fairly minimal, by comparison, and could be dropped to simplify our model. Modeling summarizes overall trends, and ANOVA allows us to identify what factors are relatively more important for explaining variation in our performance measure. We can fit our regression model for each performance measure in turn, to understand what drives our results. 14.3.2 Example 2: Estimators of treatment variation, revisited When we have many methods to compare, we can also use meta-regression to understand how these methods change as other simulation factors change TO DO: Add more on this, possibly cribbing from the commented out cluster RCT metaregression from the next chapter References Gilbert, Joshua, and Luke Miratrix. 2024. “Multilevel Metamodels: A Novel Approach to Enhance Efficiency and Generalizability in Monte Carlo Simulation Studies.” arXiv Preprint arXiv:2401.07294. "]]
