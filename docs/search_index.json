[["data-analysis-procedures.html", "Chapter 7 Data analysis procedures 7.1 Writing estimation functions 7.2 Including Multiple estimation procedures 7.3 Validating Estimation Procedures 7.4 Exercises", " Chapter 7 Data analysis procedures The overall aims of many simulation studies have to do with understand how a particular data-analysis procedure works or comparing the performance of multiple, competing procedures. Thus, the data-analysis procedure or procedures are the central object of study. Depending on the research question, the data-analysis procedure might be very simple—as simple as just computing a sample correlation–or it might involve a combination of several components. For example, the procedure might entail first computing a diagnostic test for heteroskedasticity and then, depending on the outcome of the test, applying either a conventional formula or a heteroskedasticity-robust formula for standard errors. As another example, a data-analysis procedure might involve using multiple imputation for missingness on key variables, then fitting a statistical model, and then generating predicted values based on the model. Also depending on the research question, we might need to create several functions that implement different estimation procedures to be compared. In this chapter, we demonstrate how to implement data-analysis procedures in the form of R functions, so that their performance can be evaluated by repeatedly applying them to artificial data. We start by discussing the high-level design of estimation functions and examining some simple but illustrative examples. We then discuss approaches for writing simulations that involve multiple estimation procedure. Next, we describe strategies for validating estimation functions. Finally, we examine methods for handling common computational problems with estimation procedures, such as non-convergence of maximum likelihood estimators. 7.1 Writing estimation functions In the abstract, a function that implements an estimation procedure should have the following form: estimate &lt;- function(data) { # calculations/model-fitting/estimation procedures return(estimates) } The function takes a dataset as input, fits a model or otherwise calculates an estimate, possibly with associated standard errors and so forth, and returns these quantities as output. The estimates could be point-estimates of parameters, standard errors, confidence intervals, p-values, predictions, or other quantities. The calculations in the body of the function should be set up to use datasets that have the same structure (i.e., same dimensions, same variable names) as the output of our function for generating simulated data. However, in principle, we should also be able to run the estimation function on real data as well. In Chapter 5 we wrote a function called ANOVA_Welch_F() for computing \\(p\\)-values from two different procedures for testing equality of means in a heteroskedastic ANOVA: ANOVA_Welch_F &lt;- function(data) { anova_F &lt;- oneway.test(x ~ group, data = data, var.equal = TRUE) Welch_F &lt;- oneway.test(x ~ group, data = data, var.equal = FALSE) result &lt;- tibble( ANOVA = anova_F$p.value, Welch = Welch_F$p.value ) return(result) } Apply this function to a simulated dataset returns two p-values, one for the usual ANOVA \\(F\\) test (which assumes homoskedasticity) and one for Welch’s heteroskedastic \\(F\\) test: sim_data &lt;- generate_ANOVA_data( mu = c(1, 2, 5, 6), sigma_sq = c(3, 2, 5, 1), sample_size = c(3, 6, 2, 4) ) ANOVA_Welch_F(sim_data) ## # A tibble: 1 × 2 ## ANOVA Welch ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.00285 0.0705 Our ANOVA_Welch_F() function is designed to work with the output of generate_ANOVA_data() in that it assumes that the grouping variable is called group and the outcome is called x. Relying on this assumption would be a poor choice if we were designing a function as part of an R package or for general-purpose use. However, because the primary use of the function is for simulation, it is reasonable to assume that the input data will always have appropriate variable names. In Chapter 6, we looked at a data-generating function for a bivariate Poisson distribution, an example of a non-normal bivariate distribution. We might use such a distribution to understand the behavior of Pearson’s sample correlation coefficient and its normalizing transformation, known as Fisher’s \\(z\\)-transformation, which is equivalent to the hyperbolic arc-tangent function (atanh() in R). When variables follow a bivariate normal distribution, Fisher’s \\(z\\)-transformed correlation is very close to normally distributed and its standard error is simply \\(1 / \\sqrt{N - 3}\\), and thus independent of the correlation. This makes \\(z\\)-transformation very useful for computing confidence intervals, which can then be back-transformed to the Pearson-\\(r\\) scale. In this problem, a simple estimation function would take a dataset with two variables as input and computes the sample correlation and its \\(z\\)-transformation, compute confidence intervals for \\(z\\), and then back-transform the confidence interval end-points: r_and_z &lt;- function(data) { r &lt;- cor(data$C1, data$C2) z &lt;- atanh(r) se_z &lt;- 1 / sqrt(nrow(data) - 3) ci_z &lt;- z + c(-1, 1) * qnorm(.975) * se_z ci_r &lt;- tanh(ci_z) tibble( r = r, z = z, CI_lo = ci_r[1], CI_hi = ci_r[2] ) } We can generate a small dataset using the r_bivariate_Poisson() function developed in the last chapter, then apply our estimation function to the result: Pois_dat &lt;- r_bivariate_Poisson(40, rho = 0.5, mu1 = 4, mu2 = 4) r_and_z(Pois_dat) ## # A tibble: 1 × 4 ## r z CI_lo CI_hi ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.141 0.142 -0.178 0.434 Although it is a bit cumbersome to do so, we could apply the estimation function to a real dataset by renaming variables as needed. Here is an example, which calculates the correlation between ratings of judicial integrity and familiarity with the law from the USJudgeRatings dataset (which is included in base R): data(USJudgeRatings) USJudgeRatings %&gt;% dplyr::select(C1 = INTG, C2 = FAMI) %&gt;% r_and_z() ## # A tibble: 1 × 4 ## r z CI_lo CI_hi ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.869 1.33 0.769 0.927 The resulting r is the same as what we get by computing the correlation directly from the original dataset: cor.test(~ INTG + FAMI, data = USJudgeRatings) ## ## Pearson&#39;s product-moment correlation ## ## data: INTG and FAMI ## t = 11.238, df = 41, p-value = 4.283e-14 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.7692563 0.9272343 ## sample estimates: ## cor ## 0.868858 It is a good practice to test out a newly-developed estimation function on real data as a check that it is working as intended. This type of test ensures that the estimation function is not using information outside of the dataset—not cheating, for example, by using known parameter values to construct an estimate—and so would be feasible to apply in real data analysis contexts. 7.2 Including Multiple estimation procedures Many simulations involve head-to-head comparisons between more than one data-analysis procedure. As a design principle, we generally recommend writing different functions for each estimation method one is planning on evaluating. Doing so makes it easier to add in additional methods as desired or to focus on just a subset of methods. Writing separate function also leads to a code base that is flexible and useful for other purposes (such as analyzing real data). Finally, repeating one of our primary mantras, separating functions makes debugging easier because it lets you focus attention on one thing at a time, without worrying about how errors in one area might propagate to others. To see how this works in practice, we will return to the case study from Section 6.6, in which we developed a data-generating function for simulating a cluster-randomized trial with student-level outcomes but school-level treatment assignment. Our data-generating process allowed for varying school sizes and heterogeneous treatment effects, which might be correlated with school size. Several different procedures could be used to estimate an overall average effect from such a clustered experiment, including: Estimating a multi-level regression model (also known as a hierarchical linear model), Estimating an ordinary least squares (OLS) regression model and applying cluster-robust standard errors, or Averaging the outcomes by school, then estimating a linear regression model on the mean outcomes. All three of these methods are are widely used and have some theoretical guarantees supporting their use. Education researchers tend to be more comfortable using multi-level regression models, whereas economists tend to use OLS with clustered standard errors. Let’s develop estimation functions for each of these procedures. For now we won’t worry about incorporating the school size covariate, and will focus only on getting a point estimate, standard error, and \\(p\\)-value for the average treatment effect. For the multi-level modeling strategy, there are several different existing packages that we could use. We will implement an estimator using the popular lme4 package, along with the lmerTest function for computing a \\(p\\)-value for the average effect. Here is a basic implementation: analysis_MLM &lt;- function( dat ) { require(lme4) require(lmerTest) M1 &lt;- lmer( Yobs ~ 1 + Z + (1 | sid), data = dat ) M1_summary &lt;- summary(M1)$coefficients tibble( ATE_hat = M1_summary[&quot;Z&quot;,&quot;Estimate&quot;], SE_hat = M1_summary[&quot;Z&quot;,&quot;Std. Error&quot;], p_value = M1_summary[&quot;Z&quot;, &quot;Pr(&gt;|t|)&quot;] ) } Linear Regression with Cluster-Robust Standard Errors (LM): analysis_OLS &lt;- function( dat ) { require(estimatr) M2 &lt;- lm_robust( Yobs ~ 1 + Z, data=dat, clusters=sid ) tibble( ATE_hat = M2$coefficients[[&quot;Z&quot;]], SE_hat = M2$std.error[[&quot;Z&quot;]], p_value = M2$p.value[[&quot;Z&quot;]] ) } and Aggregate data (Agg): analysis_agg &lt;- function( dat ) { require(dplyr) require(estimatr) datagg &lt;- dat %&gt;% group_by( sid, Z ) %&gt;% summarise( Ybar = mean( Yobs ), n = n(), .groups = &quot;drop&quot; ) stopifnot( nrow( datagg ) == length(unique(dat$sid) ) ) M3 &lt;- lm_robust( Ybar ~ 1 + Z, data=datagg, se_type = &quot;HC2&quot; ) tibble( ATE_hat = M3$coefficients[[&quot;Z&quot;]], SE_hat = M3$std.error[[&quot;Z&quot;]], p_value = M3$p.value[[&quot;Z&quot;]] ) } Note the stopifnot command: putting assert statements in your code like this is a good way to guarantee you are not introducing weird and hard-to-track errors in your code. For example, R likes to recycle vectors to make them the right length; if you gave it a wrong length in error, this can be a brutal error to discover. The stopifnot statements halt your code as soon as something goes wrong, rather than letting that initial wrongness flow on to further work, showing up in odd results that you don’t understand later on. See Section 17.2 for more. All of our methods give output in the similar format: dat &lt;- gen_cluster_RCT( J=16, n_bar = 30, alpha = 0.8, p = 0.5, gamma_0 = 0, gamma_1 = 0.2, gamma_2 = 0.2, sigma2_u = 0.4, sigma2_e = 0.6 ) analysis_MLM( dat ) ## Loading required package: lme4 ## Loading required package: Matrix ## ## Attaching package: &#39;Matrix&#39; ## The following objects are masked from &#39;package:tidyr&#39;: ## ## expand, pack, unpack ## Warning in check_dep_version(): ABI version mismatch: ## lme4 was built with Matrix ABI version 2 ## Current Matrix ABI version is 1 ## Please re-install lme4 from source or restore original &#39;Matrix&#39; package ## Loading required package: lmerTest ## ## Attaching package: &#39;lmerTest&#39; ## The following object is masked from &#39;package:lme4&#39;: ## ## lmer ## The following object is masked from &#39;package:stats&#39;: ## ## step ## # A tibble: 1 × 3 ## ATE_hat SE_hat p_value ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.586 0.313 0.0822 analysis_OLS( dat ) ## Loading required package: estimatr ## # A tibble: 1 × 3 ## ATE_hat SE_hat p_value ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.720 0.321 0.0452 analysis_agg( dat ) ## # A tibble: 1 × 3 ## ATE_hat SE_hat p_value ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.575 0.312 0.0863 This will allow us to make our simulation, for each iteration, call each method in turn on the same dataset, stack the results into a small table, and return that result. This will in turn get stacked to make one giant table of results, which makes evaluating performance quite easy. We will see this in the next chapter. 7.3 Validating Estimation Procedures Just as with data-generating functions, it is critical to verify the accuracy of the estimation functions. If an estimation function involves a known procedure that has been implemented in R or one of its contributed packages, then a straightforward verification is to compare your implementation to another existing implementation. For estimation functions that involve multi-step procedures or novel methods, then other approaches to verification will be needed. 7.3.1 Checking against existing implementations For our Welch test, we can check the output of ANOVA_Welch_F() against the built-in oneway.test function. Let’s do that with a fresh set of data: sim_data &lt;- generate_ANOVA_data( mu = c(1, 2, 5, 6), sigma_sq = c(3, 2, 5, 1), sample_size = c(3, 6, 2, 4) ) aov_results &lt;- oneway.test(x ~ factor(group), data = sim_data, var.equal = FALSE) aov_results ## ## One-way analysis of means (not assuming equal variances) ## ## data: x and factor(group) ## F = 6.9226, num df = 3.0000, denom df = 3.7485, p-value = 0.05145 Welch_results &lt;- ANOVA_Welch_F(sim_data) all.equal(aov_results$p.value, Welch_results$Welch) ## [1] TRUE We use all.equal() because it will check equality up to a tolerance in R, which can avoid some weird floating point errors due to rounding. For the bivariate correlation example, we can check the output of r_and_z() against R’s built-in cor.test() function, as we have already demonstrated in Section 7.1. Such a test is even more useful here because r_and_z() involves our own implementation of the confidence interval calculations, rather than relying on R’s built-in functions as we did with ANOVA_Welch_F(). 7.3.2 Checking novel procedures Simulations are usually an integral part of projects to develop novel statistical methods, but if an estimation procedure truly is new, how do you check that your code is correct? Effective methods for doing so will vary from problem to problem, but an over-arching strategy is to use theoretical results about the performance of an estimator to check that an estimator works as expected. For instance, we might work out the algebraic properties of an estimator for a special case and then check that the results of the estimation function agree with our algebra. In many problems, we might be able to find theoretical properties of an estimator when applied to a very large sample of data and when the model is correctly specified. such as its behavior in very large samples or under correct model specification. An estimation function implements a truly novel procedure. Well, one obvious answer is simulation! In principle, for large samples and data generated under the assumptions required by your new procedure, you should have a fairly good sense that your estimation procedures should work. It is often the case that as you design your simulation, and then start analyzing the results, you will find your estimators are really not working as planned. Such surprises will usually be due to (at least) three factors: you did not implement your method correctly, your method is not yet a good idea in the first place, or you do not yet understand something important about how your method works. When faced with poor performance you thus will debug your code, revise your method, and do some serious thinking. Ideally this will eventually lead you to a deeper understanding of a method that is a better idea in general, and correctly implemented in all likelihood. For example, in one research project Luke and other co-authors were working on a way to improve Instrumental Variable (IV) estimation using post-stratification. The idea is to group units based on a covariate that predicts compliance status, and then estimate within each group; hopefully this would improve overall estimation. In the first simulation, the estimates were full of NAs and odd results because we failed to properly account for what happens when the number of compliers was estimated to be zero. That was table stakes: after repairing that, we still found odd behavior and serious and unexpected bias, which turned out to be due to failing to implement the averaging of the groups step correctly. We fixed the estimator again and re-ran, and found that even when we had a variable that was almost perfectly predictive of compliance, gains were still surprisingly minimal. Eventually we understood that the groups with very few compliers were actually so unstable that they ruined the overall estimate. These results inspired us to introduce other estimators that dropped or down-weighted such strata, which gave our paper a deeper purpose and contribution. Simulation is an iterative process. It is to help you, the researcher, learn about your estimators so you can find a way forward with your work. What you learn then feeds back to the prior research, and you have a cycle that you eventually step off of, if you want to finish your paper. But do not expect it to be a single, well-defined, trajectory. 7.4 Exercises 7.4.1 More Welch and adding the BFF test Let’s continue to explore and tweak the simulation code we have developed to replicate the results of Brown and Forsythe (1974). Write a function that implements the Brown-Forsythe F*-test (the BFF* test!) as described on p. 130 of Brown and Forsythe (1974). Call it on a sample dataset to check it. BF_F &lt;- function(x_bar, s_sq, n, g) { # fill in the guts here return(pval) } Try calling your BF_F function on a variety of datasets of different sizes and shapes, to make sure it works. What kinds of datasets should you test out? 7.4.2 More estimators for Cluster Randomized Trials Sometimes you might want to consider two versions of an estimator. For example, in our cluster RCT code we used robust standard errors for the linear model estimator. Say we also want to include naive standard error estimates that we get out of the lm call. Extend the OLS call to be analysis_OLS &lt;- function( dat, robustSE = TRUE ) { and have the code inside calculate SEs based on the flag. Then modify the analyze_data() to include both approaches (you will have to call analysis_OLS twice). Efficiency-wise, estimating the OLS twice might not be ideal, but clarity-wise it might be considered helpful. Articulate two reasons for the design choice of implementing the two OLS calls separately, and articulate two reasons for instead having the analysis_OLS method generate both standard errors internally in a single call. "]]
