[["index.html", "Designing Monte Carlo Simulations in R Welcome License About the authors Acknowledgements", " Designing Monte Carlo Simulations in R Luke W. Miratrix and James E. Pustejovsky (Equal authors) 2025-06-19 Welcome Monte Carlo simulations are a computational technique for investigating how well something works, or for investigating what might happen in a given circumstance. When we write a simulation, we are able to control how data are generated, which means we can know what the “right answer” is. Then, by repeatedly generating data and then applying some statistical method that data, we can assess how well a statistical method works in practice. Monte Carlo simulations are an essential tool of inquiry for quantitative methodologists and students of statistics, useful both for small-scale or informal investigations and for formal methodological research. Despite the ubiquity of simulation work, most quantitative researchers get little formal training in the design and implementation of Monte Carlo simulations. As a result, the simulation studies presented in academic journal articles are highly variable in terms of their high-level logic, scope, programming, and presentation. Although there has long been discussion of simulation design and implementation among statisticians and methodologists, the available guidance is scattered across many different disciplines, and much of it is focused on mechanics and computing tools, rather than on principles. In this monograph, we aim to provide an introduction to the logic and mechanics of designing simulation studies, using the R programming language. Our focus is on simulation studies for formal research purposes (i.e., as might appear in a journal article or dissertation) and for informing the design of empirical studies (e.g., power analysis). That being said, the ideas of simulation are used in many different contexts and for many different problems, and we believe the overall concepts illustrated by these “conventional” simulations readily carry over into all sorts of other types of use, even statistical inference! Our focus is on the best practices of simulation design and how to use simulation to be a more informed and effective quantitative analyst. In particular, we try to provide a guide to designing simulation studies to answer questions about statistical methodology. Mainly, this book gives practical tools (i.e., lots of code to simply take and repurpose) along with some thoughts and guidance for writing simulations. We hope you find it to be a useful handbook to help you with your own projects, whatever they happen to be! License This book is licensed to you under Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License. The code samples in this book are licensed under Creative Commons CC0 1.0 Universal (CC0 1.0), i.e. public domain. About the authors We wrote this book in full collaboration, because we thought it would be fun to have some reason to talk about how to write simulations, and we wanted more people to be writing high-quality simulations. Our author order is alphabetical, but perhaps imagine it as a circle, or something with no start or end: But anymore, more about us. James E. Pustejovsky is an associate professor at the University of Wisconsin - Madison, where he teaches in the Quantitative Methods Program within the Department of Educational Psychology. He completed a Ph.D. in Statistics at Northwestern University. Luke Miratrix: I am currently an associate professor at Harvard University’s Graduate School of Education. I completed a Ph.D. in Statistics at UC Berkeley after having traveled through three different graduate programs (computer science at MIT, education at UC Berkeley, and then finally statistics at UC Berkeley). I then ended up as an assistant professor in Harvard’s statistics department, and moved (back) to Education a few years later. Over the years, simulation has become a way for me to think. This might be because I am fundamentally lazy, and the idea of sitting down and trying to do a bunch of math to figure something out seems less fun than writing up some code “real quick” so I can see how things operate. Of course, “real quick” rarely is that quick – and before I know it I got sucked into trying to learn some esoteric aspect of how to best program something, and then a few rabbit holes later I may have discovered something interesting! I find simulation quite absorbing, and I also find them reassuring (usually with regards to whether I have correctly implemented some statistical method). This book has been a real pleasure to write, because it’s given me actual license to sit down and think about why I do the various things I do, and also which way I actually prefer to approach a problem. And getting to write this book with my co-author has been a particular pleasure, for talking about the business of writing simulations is rarely done in practice. This has been a real gift, and I have learned so much. Acknowledgements The material in this book was initially developed through courses that we offered at the University of Texas at Austin (James) and Harvard University (Luke) as well as from a series of workshops that we offered through the Society for Research on Educational Effectiveness in June of 2021. We are grateful for feedback, questions, and corrections we have received from many students who participated in these courses. Some parts of this book are based on memos or other writings generated for various purposes, some of which were written by others. This is been attributed throughout. "],["introduction.html", "Chapter 1 Introduction 1.1 Some of simulation’s many uses 1.2 The perils of simulation as evidence 1.3 Simulating to learn 1.4 Why R? 1.5 Organization of the text", " Chapter 1 Introduction Monte Carlo simulations are a tool for studying the behavior of random processes, such as the behavior of a statistical estimation procedure when applied to a sample of data. Within quantitatively oriented fields, researchers developing new statistical methods or evaluating the use of existing methods nearly always use Monte Carlo simulations as part of their research process. In the context of methodological development, researchers use simulations in a way analogous to how a chef would use their test kitchen to develop new recipes before putting them on the menu, how a manufacturer would use a materials testing laboratory to evaluate the safety and durability of a new consumer good before bringing it to market, or how an astronaut would prepare for a spacewalk by practicing the process in an underwater mock-up. Simulation studies provide a clean and controlled environment for testing out data analysis approaches before putting them to use with real empirical data. More broadly, Monte Carlo studies are an essential tool in many different fields of science—climate science, engineering, and education research are three examples—and are used for a variety of different purposes. Simulations are used to model complex stochastic processes such as weather patterns (Jones, Maillardet, and Robinson 2012; Robert and Casella 2010); to generate parameter estimates from complex statistical models, as in Markov Chain Monte Carlo sampling (Gelman et al. 2013); and even to estimate uncertainty in statistical summaries, as in bootstrapping (Davison and Hinkley 1997). In this book, we shall focus on using simulation for the development and validation of methods for data analysis, which are everyday concerns within the fields of statistics and quantitative methodology. However, we also believe that many of the general principles of simulation design and execution that we will discuss are broadly applicable to these other purposes, and we note connections as they occur. At a very high level, Monte Carlo simulation provides a method for understanding the performance of a statistical model or data analysis method under conditions where the truth is known and can be controlled. The basic approach for doing so is as follows: Create artificial data using random number generators based on a specific statistical model, or more generally, a Data-Generating Process (DGP). Apply one or more data-analysis procedures to the artificial data. (These procedures might be something as simple as calculating a difference in sample means or fitting a regression model, or it might be an involved, multi-step procedure involving cleansing the data of apparent outliers, imputing missing values, applying a machine-learning algorithm, and carrying out further calculations on the predictions of the algorithm.) Repeat Steps 1 and 2 many times. Summarize the results across these repetitions in order to understand the general trends or patterns in how the method works. Simulation is useful because one can control the data-generating process and therefore fully know the truth—something that is almost always uncertain when analyzing real, empirical data. Having full control of the data-generating process makes it possible to assess how well a procedure works by comparing the estimates produced by the data analysis procedure against this known truth. For instance, we can see if estimates from a statistical procedure are consistently too high or too low (i.e., whether an estimator is systematically biased). We can also compare multiple data analysis procedures by assessing the degree of error in each set of results to determine which procedure is generally more accurate when applied to the same collection of artificial datasets. This basic process of simulation can be used to investigate an array of different questions that arise in statistics and quantitative methodology. To seed the field, we now give a high-level overview of some of the major use cases. We then discuss some of the major limitations and common pitfalls of simulation studies, which are important to keep in mind as we proceed. 1.1 Some of simulation’s many uses Monte Carlo simulations allow for rapid exploration of different data analysis procedures and, even more broadly, different approaches to designing studies and collecting measurements. Simulations are especially useful because they provide a means to answer questions that are difficult or impossible to answer by other means. Many statistical models and estimation methods can be analyzed mathematically, but only by using asymptotic approximations that describe how the methods work as sample size increases towards infinity. In contrast, simulation methods provide answers for specific, finite sample sizes. Thus, they allow researchers to study models and estimation methods where relevant mathematical formulas are not available, not easily applied, or not sufficiently accurate. Circumstances where simulations are helpful—or even essential—occur in a range of different situations within quantitative research. To set the stage for our subsequent presentation, consider the following areas where one might find need of simulation. 1.1.1 Comparing statistical approaches One of the more common uses of Monte Carlo simulation is to compare alternative statistical approaches to analyzing the same type of data. In the academic literature on statistical methodology, authors frequently report simulation studies comparing a newly proposed method against more traditional approaches, to make a case for the utility of their method. A classic example of such work is Brown and Forsythe (1974), who compared four different procedures for conducting a hypothesis test for equality of means in several populations (i.e., one-way ANOVA) when the population variances are not equal. A subsequent study by Mehrotra (1997) built on Brown and Forsythe’s work, proposing a more refined method and using simulations to demonstrate that it is superior to the existing methods. We explore the Brown and Forsythe (1974) study in the case study of Chapter 5. Comparative simulation can also have a practical application: In many situations, more than one data analysis approach is possible for addressing a given research question (or estimating a specified target parameter). Simulations comparing multiple approaches can be quite informative and can help to guide the design of an analytic plan (such as plans included in a pre-registered study protocol). For instance, researchers designing a multi-site randomized experiment might wonder whether they should use an analytic model that allows for variation in the site-specific impact estimates (Miratrix, Weiss, and Henderson 2021) or a simpler model that treats the impact as homogeneous across sites. What are the practical benefits and costs of using the more complex model? In the ideal case, simulations can identify best practices for how to approach analysis of a certain type of data and can surface trade-offs between competing approaches that occur in practice. 1.1.2 Assessing performance of complex pipelines In practice, statistical methods are often used as part of a multi-step workflow. For instance, in a regression model, one might first use a statistical test for heteroskedasticity (e.g., the White test or the Breusch-Pagan test) and then determine whether to use conventional or heteroskedasticity-robust standard errors depending on the result of the test. This combination of an initial diagnostic test followed by contingent use of different statistical procedures is quite difficult to analyze mathematically, but it is straight-forward to simulate (see, for example, Long and Ervin 2000). In particular, simulations are a straight-foward way to assess whether a proposed workflow is valid—that is, whether the conclusions from a pipeline are correct at a given level of certainty. Beyond just evaluating the performance characteristics of a workflow, simulating a multi-step workflow can actually be used as a technique for conducting statistical inference with real data. Data analysis approaches such as randomization inference and bootstrapping involve repeatedly simulating data and putting it through an analytic pipeline, in order to assess the uncertainty of the original estimate based on real data. In bootstrapping, the variation in a point estimate across replications of the simulation is used as the standard error for the context being simulated; an argument by analogy (the bootstrap analogy) is what connects this to inference on the original data and point estimate. See the first few chapters of Davison and Hinkley (1997) or Efron (2000) for further discussion of bootstrapping, and see Good (2013) or Lehmann et al. (1975) for more on permutation inference. 1.1.3 Assessing performance under misspecification Many statistical estimation procedures are known to perform well when the assumptions they entail are correct. However, data analysts must also be concerned with the robustness of estimation procedures—that is, their performance when one or more of the assumptions is violated to some degree. For example, in a multilevel model, how important is the assumption that the random effects are normally distributed? What about normality of the individual-level error terms? What about homoskedasticity of the individual-level error terms? Quantitative researchers routinely contend with such questions when analyzing empirical data, and simulation can provide some answers. Similar concerns arise for researchers considering the trade-offs between methods that make relatively stringent assumptions versus methods that are more flexible or adaptive. When the true data-generating process meets stringent assumptions (e.g., a treatment effect that is constant across the population of participants), what are the potential gain of exploiting such structure in the estimation process? Conversely, what are the costs (in terms of computation time or precision) of using more flexible methods that do not impose strong assumptions? A researcher designing an analytic plan would want to be well-informed of such trade-offs and, ideally, would want to situate their understanding in the context of the empirical phenomena that they study. Simulation allows for such investigation and comparison. 1.1.4 Assessing the finite-sample performance of a statistical approach Many statistical estimation procedures can be shown (through mathematical analysis) to work well asymptotically—that is, given an infinite amount of data—but their performance for data of a given, finite size is more difficult to quantify. Although mathematical theory can inform us about “asymptopia,” empirical researchers live in a world of finite sample sizes, where it can be difficult to gauge if one’s real data is large enough that the asymptotic approximations apply. For example, this is of particular concern with hierarchical data structures that include only 20 to 40 clusters—a common circumstance in many randomized field trials in education research. Simulation is a tractable approach for assessing the small-sample performance of such estimation methods or for determining minimum required sample sizes for adequate performance. One example of a simulation investigating questions of finite-sample behavior comes from Long and Ervin (2000), whose evaluated the performance of heteroskedasticity-robust standard errors (HRSE) in linear regression models. Asymptotic analysis indicates that HRSEs work well (in the sense of providing correct assessments of uncertainty) in sufficiently large samples (White (1980)), but what about in realistic contexts where small samples occur? Long and Ervin (2000) use extensive simulations to investigate the properties of different versions of HRSEs for linear regression across a range of sample sizes, demonstrating that the most commonly used form of these estimators often does not work well with sample sizes found in typical social science applications. Via simulation, they provided compelling evidence about a problem without having to wade into a technical (and potentially inaccessible) mathematical analysis of the problem. 1.1.5 Conducting Power Analyses During the process of proposing, seeking funding for, and planning an empirical research study, researchers need to justify the design of the study, including the size of the sample that they aim to collect. Part of such justifications may involve a power analysis, or an approximation of the probability that the study will show a statistically significant effect, given assumptions about the magnitude of true effects and other aspects of the data-generating process. Researchers may also wish to compare the power of different possible designs in order to inform decisions about how to carry out the proposed study given a set of monetary and temporal constraints. Many guidelines and tools are available for conducting power analysis for various research designs, including software such as PowerUp! (Dong and Maynard 2013), the Generalizer (Tipton 2013), G*Power (Faul et al. 2009), and PUMP (Hunter, Miratrix, and Porter 2024). These tools use analytic formulas for power, which are often derived using approximations and simplifying assumptions about a planned design. Simulation provides a very general-purpose alternative for power calculations, which can avoid such approximations and simplifications. By repeatedly simulating data based on a hypothetical process and then analyzing data following a specific protocol, one can computationally approximate the power to detect an effect of a specified size. Using simulation instead of analytic formulas allows for power analyses that are more nuanced and more tailored to the researcher’s circumstance than what can be obtained from available software. For example, simulation can be useful for the following: When estimating power in multi-site, block- or cluster-randomized trials, the formulas implemented in available software assume that sites are of equal size and that outcome distributions are unrelated to the size of each site. Small deviations from these assumptions are unlikely to change the results, but in practice, researchers may face situations where sites vary quite widely in size or where site-level outcomes are related to site size. Simulation can estimate power in this case. Available software such as PowerUp! allows investigators to build in assumptions about anticipated rates of attrition in cluster-randomized trials, under the assumption that attrition is completely at random and unrelated to anything. However, researchers might anticipate that, in practice, attrition will be related to baseline characteristics. Simulation can be used to assess how this might affect the power of a planned study. There are some closed-form expressions for power to test mediational relations (i.e., indirect and direct effects) in a variety of different experimental designs, and these formulas are now available in PowerUp!. However, the formulas involve a large number of parameters (including some where it may be difficult to develop credible assumptions) and they apply only to a specific analytic model for the mediating relationships. Researchers planning a study to investigate mediation might therefore find it useful to generate realistic data structures and conduct power analysis via simulation. 1.1.6 Simulating processess Yet another common use for Monte Carlo simulation is as a way to emulate a complex process as a means to better understand it or to evaluate the consequences of modifying it. A famous area of process simulation are climate models, where researchers simulate the process of climate change. These physical simulations mimic very complex systems to try and understand how perturbations (e.g., more carbon release) will impact downstream trends. Another example of process simulation arises in education research. Some large school districts such as New York City have centralized lotteries for school assignment, which entail having families rank schools by order of preference. The central office then assigns students to schools via a lottery procedure where each student gets a lottery number that breaks ties when there are too many students desiring to go to certain schools. Students’ school assignments are therefore based in part on random chance, but the the process is quite complex: each student has some probability of assignment to each school on their list, but the probabilities depend on their choices and the choices of other students. The school lottery process creates a natural experiment, based on which researchers can estimate the causal impact of being assigned to one school vs. another. A defensible analysis of the process requires knowing the probabilities of school assignment. Abdulkadiroğlu et al. (2017) conducted such an evaluation using the school lottery process in New York City. They calculated school assignment probabilities via simulation, by running the school lottery over and over, changing only students’ lottery numbers, and recording students’ school assignments in each repetition of the process. Simulating the lottery process a large number of times provided precise estimates of each students’ assignment probabilities, based on which Abdulkadiroğlu et al. (2017) were able to estimate causal impacts of school assignment. For another example, one that possibly illustrates the perils of simulation as taking us away from results that pass face validity, Staiger and Rockoff (2010) simulated the process of firing teachers depending on their estimated value-added scores. Based on their simulations, which model firing different proportions of teachers, they suggest that firing substantial portions of the teacher workforce annually would substantially improve student test scores. Their work offers a clear illustration of how simulations can be used to examine the potential consequences of various policy decisions, assuming the underlying assumptions hold true. This example also brings home a core concern of simulation: we only learn about the world we are simulating, and the relevance of simulation evidence to the real world is by no means guaranteed. 1.2 The perils of simulation as evidence Simulation has the potential to be a powerful tool for investigating quantitative methods. However, evidence from simulation studies is also fundamentally limited in certain ways, and thus very susceptible to critique. The core advantage of simulation studies is that they allow for evaluation of data analysis methods under specific and exact conditions, avoiding the need for approximation. The core limitation of simulations stems from this same property: they provide information about the performance of data analysis methods under specified conditions, but provide no guarantee that patterns of performance hold in general. One can partially address questions of generalization by examining a wide range of conditions, looking to see whether a pattern holds consistently or changes depending on features of the data-generating process. Even this strategy has limitations, though. Except for very simple processes, we can seldom consider every possible set of conditions. As we will see in later chapters, the design of a simulation study typically entails making choices over very large spaces of possibility. This flexibility leaves lots of room for discretion and judgement, and even for personal or professional biases (Boulesteix et al. 2020). Due to this flexibility, simulation findings are held in great skepticism by many. The following motto summarizes the skeptic’s concern: Simulations are doomed to succeed. As this motto captures, simulations are alluring: once a simulation framework is set up, it is easy to tweak and adjust. It is natural for us all to continue to do this until the simulation works “as it should.” If our goal is to show something that we already believe is correct (e.g., that our fancy new estimation procedure is better than existing methods), we could probably find a way to align our simulation with our intuition.1 Critiques of simulation studies often revolve around the realism, relevance, or generality of the data generating process. Are the simulated data realistic, in the sense that they follow similar patterns to what one would see in real empirical data? Are the explored aspects of the simulation relevant to what we would expect to find in practice? Was the simulation systematic in exploring a wide variety of scenarios, so that general conclusions are warranted? We see at least three principles for addressing such questions in one’s own work. Perhaps most fundamental is to be transparent in one’s methods and reasoning: explicitly state what was done, and provide code so that others can reproduce one’s results or tweak them to test variations of the data-generating process or alternative analysis strategies. Another important component of a robust argument is to systematically vary the conditions under examination. This is facilitated by writing code in a way to make it easy to simulate across a range of different data-generating scenarios. Once that is in place, one can systematically explore myriad scenarios and report all of the results. An aspiration of the simulation architect should be to explore the boundary conditions that separate where preferred methods work and where they break or fail. Finally, one can draw on relevant statistical theory to support the design of a simulation and interpretation of its results. Mathematical analysis might indicate that some features of a data-generating process will have a strong influence on the performance of a method, while other features will not matter at all when sample sizes are sufficiently large. Well designed simulations will examine conditions that are motivated by or complement what is known based on existing statistical theory. In addition to these principles, methodologists have proposed broader changes in practice to counter the potential for bias in methodological simulation studies. Morris, White, and Crowther (2019) introduced a formal framework, called ADEMP, to guide the reporting of methodological simulations. Boulesteix, Lauer, and Eugster (2013) argued for greater use of neutral comparison studies, in which the performance of alternative statistical methods are compared under a range of relevant conditions by methodological researchers who do not have vested interests in any specific method (see also Boulesteix, Wilson, and Hapfelmeier 2017). Further, Siepe et al. (2024) argue for more routine pre-registration of methodological simulations to bring greater transparency and reduce the possibility of bias arising from flexibility in their design. 1.3 Simulating to learn Most of the examples of Monte Carlo simulation that we have mentioned thus far are drawn from formal methodological research, published in methodologically focused research journals. If you do not identify as a methodologist, you may be wondering whether there is any benefit to learning how to do simulations—what’s the point, if you are never going to conduct methodological research studies or use simulation to aid in planning an empirical study? However, we believe that simulation is an incredibly useful tool—and well worth learning, even outside the context of formal methodological research—for at least two reasons. First, in order to do any sort of quantitative data analysis, you will need to make decisions about what methods to use. Across fields, existing guidance about data analysis practice is almost certainly informed by simulation research of some form, whether well-designed and thorough or haphazard and poorly reasoned. Consequently, having a high-level understanding of the logic and limitations of simulation will help you to be a critical consumer of methods research, even if you do not intend to conduct methods research of your own. Second, we believe conducting simulations deepens one’s understanding of the logic of statistical modeling and statistical inference. Learning a new statistical model (such as generalized linear mixed models) or analytic technique (such as multiple imputation by chained equations) requires taking in a lot of detailed information, from the assumptions of the model to the interpretation of parameter estimates to the best practices for estimation and what to do if some part of the process goes off. To thoroughly digest all these details, we have found it invaluable to simulate data based on the model under consideration. This usually requires translating mathematical notation into computer code, an exercise which makes the components of the model more tangible than just a jumble of Greek letters. The simulated data is then available for inspection, summarizing, graphing, and further calculation, all of which can aid comprehension and interpretation. Moreover, the process of simulating yields a dataset which can then be used to practice implementing the analysis procedure and interpreting the results. We have found that building a habit of simulating is a highly effective way to learn new models and methods, worthwhile even if one has no intention of carrying out methodological research. We might even go so far as to argue that whatever you might think, you don’t really understand a statistical model until you’ve done a simulation of it. 1.4 Why R? This book aims not only to introduce the conceptual principles of Monte Carlo simulation, but also to provide a practical guide to actually conducting simulation studies (whether for personal learning purposes or for formal methodological research). And conducting simulations requires writing computer code (sometimes, lots of code!). The computational principles and practices that we will describe are very general, not specific to any particular programming language, but for purposes of demonstrating, presenting examples, and practicing the process of developing a simulation, it helps to be specific. To that end, we will be using R, a popular programming language that is widely used among statisticians, quantitative methodologists, and data scientists. Our presentation will assume that readers are comfortable with writing R scripts to carry out tasks such as cleaning variables, summarizing data, creating data-based graphics, and running regression models (or more generally, estimating statistical models). We have chosen to focus on R (rather than some other programming language) because both of us are intimately familiar with R and use it extensively in our day-to-day work. Simply put, it is much easier to write in your native language than in one in which you are less fluent. But beyond our own habits and preferences, there are several more principled reasons for using R. R is free and open source software, which can be run under many different operating systems (Windows, Mac, Linux, etc.). This is advantageous not only because of the cost, but also because it means that anyone with a computer—anywhere in the world—can access the software and could, if they wanted, re-run our provided code for themselves. This makes R a good choice for practicing transparent and open science processes. There is a very large, active, and diverse community of people who use, teach, and develop R. It is used widely for applied data analysis and statistical work in such fields as education, psychology, economics, epidemiology, public health, and political science, and is widely taught in quantitative methods and applied statistics courses. Integral to the appeal of R is that it includes tens of thousands of contributed packages, which extend the core functionality of the language in myriad ways. New statistical techniques are often quickly available in R, or can be accessed through R interfaces. Increasingly, R can also be used to interface with other languages and platforms, such as running Python code via the reticulate package, running Stan programs for Bayesian modeling via RStan, or calling the h2o machine learning library using the h2o package (Fryda et al. 2014). The huge variety of statistical tools available in R makes it a fascinating place to learn and practice. R does have a persistent reputation as being a challenging and difficult language to use. This reputation might be partly attributable to its early roots, having been developed by highly technical statisticians who did not necessarily prioritize accessibility, legibility of code, or ease of use. However, as the R community has grown, the availability of introductory documentation and learning materials has improved drastically, so that it is now much easier to access pedagogical materials and find help. R’s reputation also probably partly stems from being a decentralized, open source project with many, many contributors. Contributed R packages vary hugely in quality and depth of development; there are some amazingly powerful tools available but also much that is half-baked, poorly executed, or flat out wrong. Because there is no central oversight or quality control, the onus is on the user to critically evaluate the packages that they use. For newer users especially, we recommend focusing on more established and widely used packages, seeking input and feedback from more knowledgeable users, and taking time to validate functionality against other packages or software when possible. A final contributor to R’s intimidating reputation might be its extreme flexibility. As both a statistical analysis package and a fully functional programming language, R can do many things that other software packages cannot, but this also means that there are often many different ways to accomplish the same task. In light of this situation, it is good to keep in mind that knowing a single way to do something is usually adequate—there is no need to learn six different words for hello, when one is enough to start a conversation. On balance, we think that the many strengths of R make it worthwhile to learn and continue exploring. For simulation, in particular, R’s facility to easily write functions (bundles of commands that you can easily call in different manners), to work with multiple datasets in play at the same time, and to leverage the vast array of other people’s work all make it a very attractive language. 1.5 Organization of the text We think of simulation studies as falling on a spectrum of formality. On the least formal end, we may use simulations to learn a statistical model, investigating questions purely to satisfy our own curiosity. On the most formal end, we may conduct carefully designed, pre-registered simulations that compare the performance of competing statistical methods across an array of data-generating conditions designed to inform practice in a particular research area. Our central goal is to help you learn to work anywhere along this spectrum, from the most casual to the most principled. To support that goal, the book is designed with a spiral structure, where we first present simpler and less formal examples that illustrate high-level principles, then revisit the component pieces, dissecting and exploring them in greater depth. We defer discussion of concepts that are relevant only to more formal use-cases (such as the ADEMP framework of Morris, White, and Crowther (2019)) until later chapters. We have also included many case studies throughout the book; these are designed to make the topics under discussion tangible, and are also designed to provide chunks of code that you emulate or even directly copy and use for your own purposes. We divided the book into several parts. In Part I (which you are reading now), we lay out our case for learning simulation, introduces some guidance on programming, and presents an initial, very simple simulation to set the stage for later discussion of design principles. Part II lays out the core components (generating artificial data, applying analytic procedures, executing the simulations, and analyzing the simulation results) for simulating a single scenario. It then presents some more involved case studies that illustrate the principles of modular, tidy simulation design. Part III moves to multifactor simulations, meaning simulations that look at more than one scenario or context. Multifactor simulation is central to the design of more formal simulation studies because it is only by evaluating or comparing estimation procedures across multiple scenarios that we can begin to understand their general properties. The book closes with two final parts. Part IV covers some technical challenges that are commonly encountered when programming simulations, including reproducibility, parallel computing, and error handling. Part V covers three extensions to specialized forms of simulation: simulating to calculate power, simulating within a potential outcomes framework for causal inference, and the parametric bootstrap. The specialized applications underscore how simulation can be used to answer a wide variety of questions across a range of contexts, thus connecting back to the broader purposes discussed above. The book also includes appendices with some further guidance on writing R code and pointers to further resources. References Abdulkadiroğlu, Atila, Joshua D Angrist, Yusuke Narita, and Parag A Pathak. 2017. “Research Design Meets Market Design: Using Centralized Assignment for Impact Evaluation.” Econometrica 85 (5): 1373–1432. Boulesteix, Anne-Laure, Sabine Hoffmann, Alethea Charlton, and Heidi Seibold. 2020. “A Replication Crisis in Methodological Research?” Significance 17 (5): 18–21. https://doi.org/10.1111/1740-9713.01444. Boulesteix, Anne-Laure, Sabine Lauer, and Manuel J. A. Eugster. 2013. “A Plea for Neutral Comparison Studies in Computational Sciences.” PLOS ONE 8 (4): e61562. https://doi.org/10.1371/journal.pone.0061562. Boulesteix, Anne-Laure, Rory Wilson, and Alexander Hapfelmeier. 2017. “Towards Evidence-Based Computational Statistics: Lessons from Clinical Research on the Role and Design of Real-Data Benchmark Studies.” BMC Medical Research Methodology 17 (1, 1): 1–12. https://doi.org/10.1186/s12874-017-0417-2. Brown, Morton B., and Alan B. Forsythe. 1974. “The Small Sample Behavior of Some Statistics Which Test the Equality of Several Means.” Technometrics 16 (1): 129–32. https://doi.org/10.1080/00401706.1974.10489158. Davison, A. C., and D. V. Hinkley. 1997. Bootstrap Methods and Their Applications. Cambridge: Cambridge University Press. Dong, Nianbo, and Rebecca Maynard. 2013. “PowerUp! : A Tool for Calculating Minimum Detectable Effect Sizes and Minimum Required Sample Sizes for Experimental and Quasi-Experimental Design Studies.” Journal of Research on Educational Effectiveness 6 (1): 24–67. https://doi.org/10.1080/19345747.2012.673143. Efron, Bradley. 2000. “The Bootstrap and Modern Statistics.” Journal of the American Statistical Association 95 (452): 1293–96. https://doi.org/10.2307/2669773. Faul, Franz, Edgar Erdfelder, Axel Buchner, and Albert-Georg Lang. 2009. “Statistical Power Analyses Using G*Power 3.1: Tests for Correlation and Regression Analyses.” Behavior Research Methods 41 (4): 1149–60. https://doi.org/10.3758/BRM.41.4.1149. Fryda, Tomas, Erin LeDell, Navdeep Gill, Spencer Aiello, Anqi Fu, Arno Candel, Cliff Click, et al. 2014. “H2o: R Interface for the ’H2O’ Scalable Machine Learning Platform.” Comprehensive R Archive Network. https://doi.org/10.32614/CRAN.package.h2o. Gelman, Andrew, John B. Carlin, Hal S. Stern, David B. Dunson, Aki Vehtari, and Donald B. Rubin. 2013. Bayesian Data Analysis. 0th ed. Chapman and Hall/CRC. https://doi.org/10.1201/b16018. Good, Phillip. 2013. Permutation Tests: A Practical Guide to Resampling Methods for Testing Hypotheses. Springer Science &amp; Business Media. Hunter, Kristen B., Luke Miratrix, and Kristin Porter. 2024. “PUMP: Estimating Power, Minimum Detectable Effect Size, and Sample Size When Adjusting for Multiple Outcomes in Multi-Level Experiments.” Journal of Statistical Software 108 (6): 1–43. https://doi.org/10.18637/jss.v108.i06. Jones, Owen, Robert Maillardet, and Andrew Robinson. 2012. Introduction to Scientific Programming and Simulation Using R. New York: Chapman and Hall/CRC. https://doi.org/10.1201/9781420068740. Lehmann, Erich Leo et al. 1975. “Statistical Methods Based on Ranks.” Nonparametrics. San Francisco, CA, Holden-Day 2. Long, J. Scott, and Laurie H. Ervin. 2000. “Using Heteroscedasticity Consistent Standard Errors in the Linear Regression Model.” The American Statistician 54 (3): 217–24. https://doi.org/10.1080/00031305.2000.10474549. Mehrotra, Devan V. 1997. “Improving the Brown-Forsythe Solution to the Generalized Behrens-Fisher Problem.” Communications in Statistics - Simulation and Computation 26 (3): 1139–45. https://doi.org/10.1080/03610919708813431. Miratrix, Luke W., Michael J. Weiss, and Brit Henderson. 2021. “An Applied Researcher’s Guide to Estimating Effects from Multisite Individually Randomized Trials: Estimands, Estimators, and Estimates.” Journal of Research on Educational Effectiveness 14 (1): 270–308. https://doi.org/10.1080/19345747.2020.1831115. Morris, Tim P., Ian R. White, and Michael J. Crowther. 2019. “Using Simulation Studies to Evaluate Statistical Methods.” Statistics in Medicine, January. https://doi.org/10.1002/sim.8086. Robert, Christian, and George Casella. 2010. Introducing Monte Carlo Methods with R. New York, NY: Springer. https://doi.org/10.1007/978-1-4419-1576-4. Siepe, Björn S., František Bartoš, Tim Morris, Anne-Laure Boulesteix, Daniel W. Heck, and Samuel Pawel. 2024. “Simulation Studies for Methodological Research in Psychology: A Standardized Template for Planning, Preregistration, and Reporting,” January. https://doi.org/10.31234/osf.io/ufgy6. Staiger, Douglas O, and Jonah E Rockoff. 2010. “Searching for Effective Teachers with Imperfect Information.” Journal of Economic Perspectives 24 (3): 97–118. Tipton, Elizabeth. 2013. “Stratified Sampling Using Cluster Analysis: A Sample Selection Strategy for Improved Generalizations from Experiments.” Evaluation Review 37 (2): 109–39. https://doi.org/10.1177/0193841X13516324. White, Halbert. 1980. “A Heteroskedasticity-Consistent Covariance Matrix Estimator and a Direct Test for Heteroskedasticity.” Econometrica 48 (4): 817–38. A comment from James: I recall attending seminars in the statistics department during graduate school, where guest speakers usually presented both some theory and some simulation results. A few years into my graduate studies, I realized that the simulation part of the presentation could nearly always be replaced with a single slide that said “we did some simulations and showed that our new method works better than old methods under conditions that we have cleverly selected to be favorable for our approach.” I hope that my own work is not as boring or predictable as my memory of these seminars.↩︎ "],["programming-preliminaries.html", "Chapter 2 Programming Preliminaries 2.1 Welcome to the tidyverse 2.2 Functions 2.3 \\&gt; (Pipe) dreams 2.4 Recipes versus Patterns 2.5 Exercises", " Chapter 2 Programming Preliminaries In this chapter, we introduce some essential programming concepts that may be less familiar to readers, but which are central to how we approach writing code for simulation studies. We also explain some of the rationale and reasoning behind how we present example code throughout the book. 2.1 Welcome to the tidyverse Layered on top of R are a collection of contributed packages that make data wrangling and management much, much easier. This collection is called tidyverse and it includes popular packages such as dplyr, tidyr, and ggplot2. We use methods from the “tidyverse” throughout the book because it facilitates writing clean, concise code. In particular, we make heavy use of the dplyr package for group-wise data manipulation, the purrr package for functional programming, and the ggplot2 package for statistical graphics. The 1st edition or 2nd edition of the free online textbook R for Data Science provide an excellent, thorough introduction to these packages, along with much more background on the tidyverse. We will cite portions of this text throughout the book. Loading the tidyverse packages is straightforward: library( tidyverse ) options(list(dplyr.summarise.inform = FALSE)) (The second line is to turn off some of the persistent warnings generated by the dplyr function summarize().) These lines of code appear in the header of nearly every script we use. 2.2 Functions If you are comfortable using R for data analysis tasks, you will be familiar with many of R’s functions. R has function to do things like calculate a summary statistic from a list of numbers (e.g., mean(), median(), or sd()), calculate linear regression coefficient estimates from a dataset (lm()), or count the number of rows in a dataset (nrow()). In the abstract, a function is a little machine for transforming ingredients into outputs, like a microwave (put a bag of kernels in and it will return hot, crunchy popcorn), a cheese shredder (put a block of mozzarella in and it transforms it into topping for your pizza), or a washing machine (put in dirty clothes and detergent and it will return clean but damp clothes). A function takes in pieces of information specified by the user (the inputs), follows a set of instructions for transforming or summarizing those inputs, and then returns the result of the calculations (the outputs). A function can do nearly anything as long as the calculation can be expressed in code—it can even produce output that is random. For example, the rnorm() function takes as input a number n and returns that many random numbers, drawn from a standard normal distribution: rnorm(3) ## [1] 0.7881078 -0.6695527 -0.3958219 Each time the function is called, it returns a different set of numbers: rnorm(3) ## [1] -0.06561908 -1.40027561 -0.78383165 The rnorm() function also has further input arguments that let the user specify the mean and standard deviation of the distribution from which numbers are drawn: rnorm(3, mean = 10, sd = 0.5) ## [1] 10.23538 10.24451 10.71261 In writing code for simulations, we will make extensive use of this particular function and other functions that produce sequences of random numbers. We will have more to say about random number generation later. 2.2.1 Rolling your own In R, you can create your own function by specifying the pieces of input information, the steps to follow in transforming the inputs, and the result to return as output. Learning to write your own functions to carry out calculations is an immensely useful skill that will greatly enhance your ability to accomplish a range of tasks. Writing custom functions is also central to our approach to coding Monte Carlo simulations, and so we highlight some of the key considerations here. Chapter 19 of R for Data Science (1st edition) provides an in-depth discussion of how to write your own functions. Here is an example of a custom function called one_run(): one_run &lt;- function( N, mn, sd ) { vals &lt;- rnorm( N, mean = mn, sd = sd ) tt &lt;- t.test( vals ) pvalue &lt;- tt$p.value return(pvalue) } The first line specifies that we are creating a function that takes inputs N, mn, and sd. These are called the parameters, inputs, or arguments of the function. The remaining lines inside the curly brackets are called the body of the function. These lines specify the instructions to follow in transforming the inputs into an output: Generate a random sample of N observations from a normal distribution with mean mn and store the result in vals. Use the built-in function t.test() to compute a one-sample t-test for the null hypothesis that the population mean is zero, then store the result in tt. Extract the p-value from the t-test store the result in pvalue. Return pvalue as output. Having created the function, we can then use it with any inputs that we like: one_run( 100, 5, 1 ) ## [1] 5.01524e-77 one_run( 10, 0.3, 1 ) ## [1] 0.5711102 one_run( 10, 0.3, 0.2 ) ## [1] 0.0002693702 In each case, the output of the function is a p-value from a simulated sample of data. The function produces a different answer each time because its instructions involve generating random numbers each time it is called. In essence, our custom function is just a short-cut for carrying out its instructions. Writing it saves us from having to repeatedly write or copy-paste the lines of code inside its body. 2.2.2 A dangerous function Writing custom functions will prove to be crucial for effectively implementing Monte Carlo simulations. However, designing custom functions does take practice to master. It also requires a degree of care above and beyond what is needed just to use R’s built-in functions. One of the common mistakes encountered in writing custom functions is to let the function depend on information that is not part of the input arguments. For example, consider the following script, which includes a nonsensical custom function called funky(): secret &lt;- 3 funky &lt;- function(input1, input2, input3) { # do funky stuff ratio &lt;- input1 / (input2 + 4) funky_output &lt;- input3 * ratio + secret return(funky_output) } funky(3, 2, 5) ## [1] 5.5 funky takes inputs input1, input2, and input3, but its instructions also depend on the quantity secret. What happens if we change the value of secret? secret &lt;- 100 funky(3, 2, 5) ## [1] 102.5 Even though we give it the same arguments as previously, the output of the function is different. This sort of behavior is confusing. Unless the function involves generating random numbers, we would generally expect it to return the exact same output if we give it the same inputs. Even worse, we get a rather cryptic error if the value of secret is not compatible with what the function expects: secret &lt;- &quot;A&quot; funky(3, 2, 5) ## Error in input3 * ratio + secret: non-numeric argument to binary operator If we are not careful, we will end up with very confusing code that can very easily lead to unintended results and errors. To avoid this issue, it is important for functions to only use information that is explicitly provided to it through its arguments. This is the principle of isolating the inputs. If the result of a function is supposed to depend on a quantity, then we should include that quantity among the input arguments. We can fix our example function by including secret as an argument: secret &lt;- 3 funkier &lt;- function(input1, input2, input3, secret) { # do funky stuff ratio &lt;- input1 / (input2 + 4) funky_output &lt;- input3 * ratio + secret return(funky_output) } funkier(3, 2, 5, 3) ## [1] 5.5 Now the output of the function is always the same, regardless of the value of other objects in R: secret &lt;- 100 funkier(3, 2, 5, 3) ## [1] 5.5 The input parameter secret holds sway here, even though there is also an object with the same name.2 If we want to try 100, we have to do so explicitly: funkier(3, 2, 5, 100) ## [1] 102.5 When writing your own functions, it may not be obvious that your function depends on external quantities and does not isolate the inputs. In our experience, one of the best ways to detect this issue is to clear the R environment and start from a fresh palette, run the code to create the function, and call the function (perhaps more than once) to ensure that it works as expected. Here is an illustration of what happens when we follow this process with our problematic custom function: # clear environment rm(list=ls()) # create function funky &lt;- function(input1, input2, input3) { # do funky stuff ratio &lt;- input1 / (input2 + 4) funky_output &lt;- input3 * ratio + secret return(funky_output) } # test the function funky(3, 2, 5) ## Error in funky(3, 2, 5): object &#39;secret&#39; not found We get an error because the external quantity is not available. Here is the same process using our corrected function: # clear environment rm(list=ls()) # create function funkier &lt;- function(input1, input2, input3, secret) { # do funky stuff ratio &lt;- input1 / (input2 + 4) funky_output &lt;- input3 * ratio + secret return(funky_output) } # test the function funkier(3, 2, 5, secret = 3) ## [1] 5.5 2.2.3 Using Named Arguments When calling a function (whether it is a built-in function or a custom function that you developed), you can specify which values correspond to which arguments using argument names. Using argument names greatly enhances the readability and flexibility of function calls. When you specify inputs by name, R matches the values to the arguments based on the names rather than the order in which they appear. This feature is particularly useful in complex functions with many optional arguments. For example, consider the function one_run() from 2.2.1: one_run &lt;- function( N, mn, sd ) { vals &lt;- rnorm( N, mean = mn, sd = sd ) tt &lt;- t.test( vals ) pvalue &lt;- tt$p.value return(pvalue) } You could call one_run() with named arguments in any order: result &lt;- one_run(sd = 0.5, mn = 2, N = 500) In this call, R knows which value to assign to each argument, so we could list the arguments however we like. Without naming, we would have to specify the arguments in the exact order that they appear in the function definition: result &lt;- one_run( 500, 2, 0.5 ) Without the argument names, this line of code is harder to follow—you have to know more about the design of the function to understand how it is being used in this instance. Getting in the habit of using named arguments will help you avoid errors. If you pass arguments without naming, and in the wrong order, you can end up with very strange results that are hard to diagnose. Even if you get it right, if someone later changes the function (say by adding a new argument in the middle of the list), your code will suddenly break with no explanation. 2.2.4 Argument Defaults Default arguments allow you to specify typical values for parameters that a user might not need to change every time they use the function. This can make the function easier to use and less error-prone because the defaults ensure that the function behaves sensibly even when some arguments are not explicitly provided. For example, let us revise the one_run() function from above to use default arguments: one_run &lt;- function( N = 10, mn = 0, sd = 1 ) { vals &lt;- rnorm( N, mean = mn, sd = sd ) tt &lt;- t.test( vals ) pvalue &lt;- tt$p.value return(pvalue) } Now our function has a default for N of 10, for mn of 0, and for s of 1. This means a user can run the function simply by calling one_run() without any inputs. Doing so will generate a p-value from a sample of 10 observations with a mean of zero and a standard deviation of 1. Once we have our function with defaults, we can call the function while specifying only the inputs that differ from the default values: bigger_sample &lt;- one_run( N = 50 ) The function will use its default values for mn and s. Using defaults lets the user call the function more succinctly. Later chapters will have much more to say about the process of writing custom functions, as well as many further illustrations and examples. 2.2.5 Function skeletons In discussing how to write functions for simulations, we will often present function skeletons. By a skeleton, we mean code that creates a function with a specific set of input arguments, but where the body is left partially or fully unspecified. Here is a cursory example of a function skeleton: run_simulation &lt;- function( N, J, mu, sigma, tau ) { # simulate data # apply estimation procedure # repeat # summarize results return(results) } In subsequent chapters, we will use function skeletons to outline the organization of code for simulation studies. The skeleton headers make clear what the inputs to the function need to be. Sometimes, we will leave comments in the body of the skeleton to sketch out the general flow of calculations that need to happen. Depending on the details of the simulation, the specifics of these steps might be quite different, but the general structure will often be quite consistent. Finally, the last line of the skeleton indicates the value that should be returned as output of the function. Thus, skeletons are kind of like Mad Libs, but with R code instead of parts of speech. 2.3 \\&gt; (Pipe) dreams Many of the functions from tidyverse packages are designed to make it easy to use them in sequence via the |&gt; symbol, or pipe.3 The pipe allows us to compose several functions, meaning to write a chain of several functions as a sequence, where the result of each function becomes the first input to the next function. In code written with the pipe, the order of function calls follows like a story book or cake recipe, making it easier to see what is happening at each step in the sequence. Consider the hypothetical functions f(), g(), and h(), and suppose we want to do a calculation that involves composing all three functions. One way to write this calculation is res1 &lt;- f(my_data, a = 4) res2 &lt;- g(res1, b = FALSE) result &lt;- h(res2, c = &quot;hot sauce&quot;) We have to store the result of each intermediate step in an object, and it takes a careful read of the code to see that we are using res1 as input to g() and res2 as input to h(). Alternately, we could try to write all the calculations as one line: result &lt;- h( g( f( my_data, a = 4 ), b = FALSE ), c = &quot;hot sauce&quot; ) This is a mess. It takes very careful parsing to see that the b argument is called as part of g() and the c argument is part of h()`, and the order in which the functions appear is not the same as the order in which they are calculated. With the pipe we can write the same calculation as result &lt;- my_data |&gt; # initial dataset f(a = 4) |&gt; # do f() to it g(b = FALSE) |&gt; # then do g() h(c = &quot;hot sauce&quot;) # then do h() This addresses the all the issues with our previous attempts: the order in which the functions appear is the same as the order in which they are executed; the additional arguments are clearly associated with the relevant functions; and there is only a single object holding the results of the calculations. Pipes are a very nice technique for writing clear code that is easy for others to follow.4 2.4 Recipes versus Patterns As we will elaborate in subsequent chapters, we follow a modular approach to writing simulations, in which each component of the simulation is represented by its own custom function or its own object in R. This modular approach leads to code that always has the same broad structure and where the process of implementing the simulation follows a set sequence of steps. We start by coding a data-generating process, then write one or more data-analysis methods, then determine how to evaluate the performance of the methods, and finally implement an experimental design to examine the performance of the methods across multiple scenarios. Over the next several chapters, we will walk through this process several times. Although we always follow the same broad process, the case studies that we will present are not intended as a cookbook that must be rigidly followed. In our experience, the specific features of a data-generating model, estimator, or research question sometimes require tweaking the template or switching up how we implement some aspect of the simulation. And sometimes, it might just be a question of style or preference. Because of this, we have purposely constructed the examples presented throughout the book to use different variations of our central theme rather than always following the exact same style and structure. We hope that presenting these variants and adaptations will both expand your sense of what is possible and also help you to recognize the core design principles—in other words, to distinguish the forest from the trees. Of course, we would welcome and encourage you to take any of the code verbatim, tweak and adapt it for your own purposes, and use it however you see fit. Adapting a good example is usually much easier than starting from a blank screen. 2.5 Exercises Revise the one_run() function to use |&gt; instead of storing the simulated sample in vals. Modify the one_run() function to return a tibble() that includes separate columns for the test statistic (called tt$statistic), the p-value, and the lower and upper end-points of the confidence interval (called tt$conf.int). Modify the one_run() function so that the sample of data is generated from a non-central t distribution by substituting R’s rt() function in place of rnorm(). Make sure to modify the arguments (and argument names) of one_run() to allow the user to specify the non-centrality and degrees of freedom parameters of the non-central t distribution. The non-central t distribution is usually parameterized in terms of non-centrality parameter \\(\\delta\\) and degrees of freedom \\(\\nu\\), and these parameters determine the mean and spread of the distribution. Specifically, the mean of the non-central t distribution is \\[\\text{E}(T) = \\delta \\times \\sqrt{\\frac{\\nu}{2}} \\times \\frac{\\Gamma((\\nu - 1) / 2)}{\\Gamma(\\nu / 2)},\\] where \\(\\Gamma()\\) is the gamma function (called gamma() in R). Create a version of the one_run() function that generates data based on a non-central t distribution, but where the input arguments are mn for the mean and df for the degrees of freedom. Here is a function skeleton to get started: one_run &lt;- function( N = 10, mn = 5, df = 4) { # generate data from non-central t distribution # vals &lt;- # calculate one-sample t-test tt &lt;- t.test( vals ) # compile results into a tibble and return # res &lt;- return(res) } Modify one_run() to allow the user to specify a hypothesized value for the population mean, to use when calculating the one-sample t-test results. To learn more about how R determines which values to use when executing a function, see Section 6.4 of Advance R.↩︎ The pipe is a relatively recent addition to R’s basic syntax. Prior to its inclusion in base R, the magrittr package provided—and still provides—a pipe symbol %&gt;% that works similarly but has some additional syntactic nuances. We use base R’s |&gt; because it is always available, even without loading any additional packages. To learn more about the nuanced %&gt;% pipe and similar operators, see the magrittr package.↩︎ Chapter 3.4 of R for Data Science (2nd edition) provides more discussion and examples of how to use |&gt;. Chapter 18 of R for Data Science (1st edition) provides more discussion and examples of how to use magrittr’s %&gt;%.↩︎ "],["t-test-simulation.html", "Chapter 3 An initial simulation 3.1 Simulating a single scenario 3.2 A non-normal population distribution 3.3 Simulating across different scenarios 3.4 Extending the simulation design 3.5 Exercises", " Chapter 3 An initial simulation To begin learning about simulation, a good starting place is to examine a small, concrete example. This example illustrates how simulation involves replicating the data-generating and data-analysis processes, followed by aggregating the results across replications. Our little example encapsulates the bulk of our approach to Monte Carlo simulation, touching on all the main components involved. In subsequent chapters we will look at each of these components in greater detail. But first, let us look at a simulation of a very simple statistical problem. The one-sample \\(t\\)-test is one of the most basic methods in the statistics literature. It tests a null hypothesis that a population mean of some variable is equal to a specific value by comparing the mean of a sample of data to the hypothesized value. If the sample average is discrepant (very different) from the null value, relative to how uncertain we are about our estimate, then the hypothesis is rejected. The test can also be used to generate a confidence interval for the population mean. If the sample consists of independent observations and the variable is normally distributed in the population, then the confidence interval will have exact coverage, in the sense that 95% intervals will include the population mean in 95 out of 100 tries. But what if the population variable is not normally distributed? To find out, let us look at the coverage of the \\(t\\)-test’s 95% confidence interval for the population mean when the method’s normality assumption is violated. Coverage is the chance of a confidence interval capturing the true parameter value. To examine coverage, we will simulate many samples from a non-normal population with a specified mean, calculate a confidence interval based on each sample, and see how many of the confidence intervals cover the known true population mean. Before getting to the simulation, let’s look at the data-analysis procedure we will be investigating. Here is the result of conducting a \\(t\\)-test on some fake data, generated from a normal distribution: # make fake data dat &lt;- rnorm( n = 10, mean = 4, sd = 2 ) # conduct the test tt &lt;- t.test( dat ) tt ## ## One Sample t-test ## ## data: dat ## t = 6.0878, df = 9, p-value = 0.0001819 ## alternative hypothesis: true mean is not equal to 0 ## 95 percent confidence interval: ## 2.164025 4.723248 ## sample estimates: ## mean of x ## 3.443636 # examine the confidence interval tt$conf.int ## [1] 2.164025 4.723248 ## attr(,&quot;conf.level&quot;) ## [1] 0.95 We generated data with a true (population) mean of 4. Did we capture it? To check, we can use the findInterval() function, which checks to see where the first number lies relative to the range given in the second argument. Here is an illustration of the syntax: findInterval( 1, c(20, 30) ) ## [1] 0 findInterval( 25, c(20, 30) ) ## [1] 1 findInterval( 40, c(20, 30) ) ## [1] 2 The findInterval() returns a 1 if the value of the first argument value is in the interval specified in the second argument. We can apply it to check whether our estimated CI covers the true population mean: findInterval( 4, tt$conf.int ) ## [1] 1 In this instance, findInterval() is equal to 1, which means our CI captured the true population mean of 4. Here is the full code for simulating data, computing the data-analysis procedure, and evaluating the result: # make fake data dat &lt;- rnorm( n = 10, mean = 4, sd = 2 ) # conduct the test tt &lt;- t.test( dat ) # evaluate the results findInterval( 4, tt$conf.int ) == 1 ## [1] TRUE The above code captures the basic form of a single simulation trial: make the data, analyze the data, decide how well we did. The code also illustrates a good way to figure out the details of a simulation: start by figuring out what a single iteration of the simulation might look like. Starting by mucking about this way also allows us to test and develop our code in an interactive, exploratory fashion. For instance, we can play with findInterval() to figure out how to use it to determine whether our confidence interval captured the truth. Once we have arrived at working code for a single iteration, we are in a good position to start writing functions to implement the actual simulation. For now, we have generated data from a normal distribution; we will later revise the code to generate data from a non-normal population distribution. 3.1 Simulating a single scenario We can estimate the coverage of the confidence interval by repeating the above data-generating and data-analysis processes many, many times. R’s replicate() function is a handy way to repeatedly call a line of code. Its first input argument is n, the number of times to repeat the calculation, followed by expr, which is one or more lines of code to be called. We can use replicate to repeat our simulation process 1000 times in a row, each time generating a new sample of 10 observations from a normal distribution with mean of 4 and a standard deviation of 2. For each replication, we store the result of using findInterval() to check whether the confidence interval includes the population mean of 4. rps &lt;- replicate( 1000, { dat &lt;- rnorm( n = 10, mean = 4, sd = 2 ) tt &lt;- t.test( dat ) findInterval( 4, tt$conf.int ) }) head(rps, 20) ## [1] 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 To see how well we did, we can look at a table of the results stored in rps and calculate the proportion of replications that the interval covered the population mean: table( rps ) ## rps ## 0 1 2 ## 27 957 16 mean( rps == 1 ) ## [1] 0.957 We got about 95% coverage, which is good news. In 27 out of the 1000 replications, the interval was too high (so the population mean was below the interval) and in 16 out of the 1000 replications, the interval was too low (so the population mean was above the interval). It is important to recognize that this set of simulations results, and our coverage rate of 95.7%, itself has some uncertainty in it. Because we only repeated the simulation 1000 times, what we really have is a sample of 1000 independent replications, out of an infinite number of possible simulation runs. Our coverage of 95.7% is an estimate of what the true coverage would be, if we ran more and more replications. The source of uncertainty of our estimate is called Monte Carlo simulation error (MCSE). We can actually assess the Monte Carlo simulation error in our simulation results using standard statistical procedures for independent and identically distributed data. Here we use a proportion test to check whether the estimated coverage rate is consistent with a true coverage rate of 95%: covered &lt;- as.numeric( rps == 1 ) prop.test( sum(covered), length(covered), p = 0.95 ) ## ## 1-sample proportions test with continuity ## correction ## ## data: sum(covered) out of length(covered), null probability 0.95 ## X-squared = 0.88947, df = 1, p-value = ## 0.3456 ## alternative hypothesis: true p is not equal to 0.95 ## 95 percent confidence interval: ## 0.9420144 0.9683505 ## sample estimates: ## p ## 0.957 The test indicates that our estimate is consistent with the possibility that the true coverage rate is 95%, just as it should be. Things working out should hardly be surprising. Mathematical theory tells us that the \\(t\\)-test is exact for normally distributed population variables, and we generated data from a normal distribution. In other words, all we have found so far is that the confidence intervals follow theory when the assumptions of the method are met. 3.2 A non-normal population distribution To see what happens when the normality assumption is violated, let us now look at a scenario where the population variable follows a geometric distribution. The geometric distribution is usually written in terms of a probability parameter \\(p\\), so that the distribution has a mean of \\((1 - p) / p\\). We will use a geometric distribution with a mean of 4 by setting \\(p = 1/5\\). Here is the population distribution of the variable: The distribution is highly right-skewed, which suggests that the normal confidence interval might not work very well. Now let’s revise our previous simulation code to use the geometric distribution: rps &lt;- replicate( 1000, { dat &lt;- rgeom( n = 10, prob = 1/5 ) tt &lt;- t.test( dat ) findInterval( 4, tt$conf.int ) }) table( rps ) ## rps ## 0 1 2 ## 8 892 100 Our confidence interval is often entirely too low (such that the population mean is above the interval) and very rarely does our interval fall fully above the population mean. Furthermore, our coverage rate is not the desired 95%: mean( rps == 1 ) ## [1] 0.892 To take account of Monte Carlo error, we will again do a proportion test. The following test result calculates a confidence interval for the true coverage rate under the scenario we are examining: covered &lt;- as.numeric( rps == 1 ) prop.test( sum(covered), length(covered), p = 0.95) ## ## 1-sample proportions test with continuity ## correction ## ## data: sum(covered) out of length(covered), null probability 0.95 ## X-squared = 69.605, df = 1, p-value &lt; ## 2.2e-16 ## alternative hypothesis: true p is not equal to 0.95 ## 95 percent confidence interval: ## 0.8707042 0.9102180 ## sample estimates: ## p ## 0.892 Our coverage is too low; the confidence interval based on the \\(t\\)-test misses the the true value more often than it should. We have learned that the \\(t\\)-test can fail when applied to non-normal (skewed) data. 3.3 Simulating across different scenarios So far, we have looked at coverage rates of the confidence interval under a single, specific scenario, with a sample size of 10, a population mean of 4, and a geometrically distributed variable. We know from statistical theory (specifically, the central limit theorem) that the confidence interval should work better if the sample size is big enough. But how big does it have to get? One way to examine this question is to expand the simulation to look at several different scenarios involving different sample sizes. We can think of this as a one-factor experiment, where we manipulate sample size and use simulation to estimate how confidence interval coverage rates change. To implement such an experiment, we first write our own function that executes the full simulation process for a given sample size: ttest_CI_experiment = function( n ) { rps &lt;- replicate( 1000, { dat &lt;- rgeom( n = n, prob = 1/5 ) # simulate data tt &lt;- t.test( dat ) # analyze data findInterval( 4, tt$conf.int ) # evaluate coverage }) coverage &lt;- mean( rps == 1 ) # summarize results return(coverage) } The code inside the body of the function is identical to what we have used above, with the sample size as a function argument, n, which enables us to easily run the code for different sample sizes. With our function in hand, we can now run the simulation for a single scenario just by calling it: ttest_CI_experiment(n = 10) ## [1] 0.885 Even though the sample size is still n = 10, the simulated coverage rate is a little bit different from what we found previously. That is because there is some Monte Carlo error in each simulated coverage rate. Our task is now to use this function for several different values of \\(n\\). We could just do this by copy-pasting and changing the value of n: ttest_CI_experiment(n = 10) ## [1] 0.922 ttest_CI_experiment(n = 20) ## [1] 0.91 ttest_CI_experiment(n = 30) ## [1] 0.914 However, this will quickly get cumbersome if we want to evaluate many different sample sizes. A better approach is to use a mapping function from the purrr package.5 The map_dbl() function takes a list of values and calls a function for each value in the list. This accomplishes the same thing as using a for loop to iterate through a list of items (if you happen to be familiar with these), but is more succinct. See Appendix chapter ?? for more on map().6 To proceed, we first create a list of sample sizes to test out: ns &lt;- c(10, 20, 30, 40, 60, 80, 100, 120, 160, 200, 300) Now we can use map_dbl() to evaluate the coverage rate for each sample size: coverage_est &lt;- map_dbl( ns, ttest_CI_experiment) This code will run our experiment for each value in ns, and then return a vector of the estimated coverage rates for each of the sample sizes. We advocate for depicting simulation results graphically. To do so, we store the simulation results in a dataset and then create a line plot using a log scale for the horizontal axis: res &lt;- tibble( n = ns, coverage = 100 * coverage_est ) ggplot( res, aes( x = n, y = coverage ) ) + geom_hline( yintercept=95, col=&quot;red&quot; ) + # A reference line for nominal coverage rate geom_line() + geom_point( size = 4 ) + scale_x_log10( breaks = ns, minor_breaks = NULL) + labs( title=&quot;Coverage rates for t-test on exponential data&quot;, x = &quot;n (sample size)&quot;, y = &quot;coverage (%)&quot; ) + coord_cartesian(xlim = c(9,320), ylim=c(85,100), expand = FALSE) + theme_minimal() We can see from the graph that the confidence interval’s coverage rate improves as sample size gets larger. For sample sizes over 100, the interval appears to have coverage quite close to the nominal 95% level. Although the general trend is pretty clear, the graph is still a bit messy because each point is an estimated coverage rate, with some Monte Carlo error baked in. 3.4 Extending the simulation design So far, we have executed a simple simulation to assess how well a statistical method works in a given circumstance, then expanded the simulation by running a single-factor experiment in which we varied the sample size to see how the method’s performance changes. In our example, we found that coverage is below what it should be for small sample sizes, but improves for sample sizes in the 100’s. This example captures all the major steps of a simulation study, which we outlined at the start of Chapter 1. We generated some hypothetical data according to a fully-specified data-generating process: we did both a normal distribution and a geometric distribution, each with a mean of 4. We applied a defined data-analysis procedure to the simulated data: we used a confidence interval based on the \\(t\\) distribution. We assessed how well the procedure worked across replications of the data-generating and data-analysis processes: in this case we focused on the coverage rate of the confidence interval. After creating a function to implement this whole process for a single scenario, we investigated how the performance of the confidence interval changed depending on sample size. In simulations of more complex models and data-analysis methods, some or all of the steps in the process might have more moving pieces or entail more complex calculations. For instance, we might want to compare the performance of different approaches to calculating a confidence interval. We might also want to examine how coverage rates are affected by other aspects of the data-generating process, such as looking at different population mean values for the geometric distribution—or even entirely different distributions. With such additional layers of complexity, we will need to think systematically about each of the component parts of the simulation. In the next chapter, we introduce an abstract, general framework for simulations that is helpful for guiding simulation design and managing all the considerations involved. 3.5 Exercises The simulation function we developed in this chapter uses 1000 replications of the data-generating and data-analysis process, which leads to some Monte Carlo error in the simulation results. Modify the ttest_CI_experiment() function to make the number of replications an input argument, then re-run the simulation and re-create the graph of the results with \\(R=10,000\\) or even \\(R=100,000\\). Is the graph more regular than the one in the text, above? Use your improved results to determine what sample size is large enough to give coverage of at least 94% (so only 1% off of desired). Modify the ttest_CI_experiment() function to make the \\(p\\) parameter an input argument. Repeat the one-factor simulation, but use \\(p = 1/10\\) so that the population mean is \\((1 - p) / p = 9\\). How do the coverage rates change? More challenging problems Below is a modified version of the ttest_CI_experiment() function that creates a tibble with lower and upper end-points of the simulated confidence intervals: lotsa_CIs_experiment = function( n ) { lotsa_CIs &lt;- replicate( 1000, { # simulate data dat &lt;- rgeom( n = n, prob = 1/5) # analyze data tt &lt;- t.test( dat ) # return CI tibble(lower = tt$conf.int[1], upper = tt.conf.int[2]) }) # summarize results # &lt;fill in the rest&gt; return(coverage) } Complete the function by writing code to compute the estimated coverage rate and average confidence interval length. Also calculate a 95% confidence interval for the true coverage rate (you can use prop.test() on your set of simulation coverage indicators to obtain this, treating your \\(R\\) simulation replicates as a random sample in its own right). Your modified function should return a one-row tibble with the coverage rate, average confidence interval length, and a CI for the true coverage rate. Modify the ttest_CI_experiment() function to include the extensions of the prior problem. Then re-run the simulations with your modified function, obtaining a data frame with each row being a simulation scenario and columns of sample size, estimated coverage, low end of the estimate’s confidence interval, and high end of the interval. You will likely want to use map() and then bind_rows() on your list of results; see Chapter ?? for more information about these techniques. Use these data to create a graph that depicts the estimated coverage rates. Try creating a graph that includes the 95% confidence intervals also, so that the Monte Carlo simulation error in the estimated coverage rates is represented in the graph. We recommend using the ggplot2 function geom_pointrange() to represent the confidence intervals. Modify ttest_CI_experiment() or lotsa_CIs_experiment() so that the user can specify the population mean of the data-generating process. Also let the user specify the number of replications to use. Here is a function skeleton to use as a starting point: ttest_CI_experiment &lt;- function( n, pop_mean, reps) { pop_prob &lt;- 1 / (pop_mean + 1) # &lt;fill in the rest&gt; return(coverage) } Using the modified function from the previous problem, implement a two-factor simulation study for several different values of n and several different population means. One way to do this is to run a few simulations with different population means, storing them in a series of datasets, res1, res2, res3, etc. Then use bind_rows( size1 = res1, ..., .id = \"mean\" ) to combine the datasets into a single dataset. Make a plot of your results, with n on the x-axis, coverage on the \\(y\\)-axis, and different lines for different population means. Alternately, readers familiar with the *apply() family of functions from Base R might prefer to use lapply() or sapply(), which do essentially the same thing as purrr::map_dbl().↩︎ You can also check out Section 21.5 of R for Data Science (1st edition), which provides an introduction to mapping.↩︎ "],["simulation-structure.html", "Chapter 4 Structure of a simulation study 4.1 General structure of a simulation 4.2 Tidy, modular simulations 4.3 Skeleton of a simulation study 4.4 Exercises", " Chapter 4 Structure of a simulation study Monte Carlo simulation is a very flexible tool that researchers use to study a vast array of different models and topics. Within the realm of methodological research, most simulations share a common structure, nearly always involving the same set of steps or component pieces. In learning to design your own simulations, it is very useful to recognize the core components that most simulation studies share. Identifying these components will help you to organize your work and structure the coding tasks involved in writing a simulation. In this chapter, we outline the component structure of a methodological simulation study, highlighting the four steps involved in a simulation of a single scenario and the three additional steps involved in multifactor simulations. We then describe a strategy for implementing simulations that mirrors the same component structure, where each step in the simulation is represented by a separate function or object. We call this strategy tidy, modular simulation. Finally, we show how the tidy, modular simulation strategy informs the structure and organization of code for a simulation study, walking through basic code skeletons (cf. 2.2.5) for each of the steps in a single-scenario simulation. 4.1 General structure of a simulation The four main steps involved in a simulation study, introduced in Chapter 1, are summarized in the top portion of Table 4.1. Table 4.1: Steps in the Simulation Process Step Description 1 Generate Generate a sample of artificial data based on a specific statistical model or data-generating process. 2 Analyze Apply one or more data-analysis procedures, estimators, or workflows to the artificial data. 3 Repeat Repeat steps (1) &amp; (2) \\(R\\) times, recording \\(R\\) sets of results. 4 Summarize Assess the performance of the procedure across the \\(R\\) replications. 5 Design Specify a set of conditions to examine 6 Execute Run the simulation for each condition in the design. 7 Synthesize Compare performance across conditions. In the simple \\(t\\)-test example presented in Chapter 3, we put each of these steps into action with R code: We used the geometric distribution as a data-generating process; We used the confidence interval from a one-sample \\(t\\)-test as the data-analysis procedure; We repeatedly simulated the confidence intervals with R’s replicate() function; and We summarized the results by estimating the fraction of the intervals that covered the population mean. We also saw that it was helpful to wrap all of these steps up into a single function, so that we could run the simulation across multiple sample sizes. These four initial steps are common and shared across nearly all simulations. In our first example, each of the steps was fairly simple, sometimes involving only a single line of code. More generally, each of the steps might be quite a bit more complex. The data-generating process might involve a more complex model with multiple variables or multilevel structure. The data analysis procedure might involve solving a multidimensional optimization problem to get parameter estimates, or might involve a data analysis workflow with multiple steps or contingencies. Further, we might use more than one metric for summarizing the results across replications and describing the performance of the data analysis procedure. Because each of the four steps involves its own set of choices, it will useful to recognize them as distinct from one another. In methodological research projects, we usually want to examine simulation results across an array of different conditions that differ not only in terms of sample size, but also in other parameters or features of the data-generating process. Running a simulation study across multiple conditions entails several further steps, which are summarized in the bottom portion of Table 4.1. We will first need to specify the factors and specific conditions to be examined in our experimental design. We will then need to execute the simulation for each of the conditions and store all the results for further analysis. Finally, we will need to find ways to synthesize or make sense of the main patterns in the results across all of the conditions in the design. Just as with the first four steps, it is useful to recognize these further steps as distinct from one another, each involving its own set of choices and techniques. The design step requires choosing which parameters and features of the data-generating process to vary, as well as which specific values to use for each factor that is varied. Executing a simulation might require a lot of computing power, especially if the simulation design has many conditions or the data analysis procedure takes a long time to compute. How to effectively implement the execution step will therefore depend on the computing requirements and available resources. Finally, many different techniques can be used to synthesizing findings from a multifactor simulation. Which ones are most useful will depend on your research questions and the choices you make in each of the preceeding steps. 4.2 Tidy, modular simulations It is apparent from Table 4.1 that writing a simulation in R involves a large space of possibilities and will requiring making a bunch of decisions. Considering the number of choices to be made, it is critical to stay organized and to approach the process systematically. Recognizing the components of a simulation is the starting point. Next is to see how to translate the components into R code. In our own methodological work, we have found it very useful to always follow the same approach to writing code for a simulation. We call this approach tidy, modular simulation. It involves two simple principles: Implement each component of a simulation as a distinct function or object. Store all results in rectangular data sets. Writing separate functions for each component step of a simulation has several benefits. The first is the practical benefit of turning the coding process from a monolithic (and potentially daunting) activity into a set of smaller, discrete tasks. This lets us focus on one task at a time and makes it easier to see progress. Second, following this principle makes for code that is easier to read, test, and debug. Rather than having to scan through an entire code file to understand how the data analysis procedure is implemented, we can quickly identify the function that implements it, then focus on understanding the workings of that function. Likewise, if another researcher wanted to test out the data analysis procedure on a dataset of their own, they could do so by running the corresponding function rather than having to dissect an entire script. Third, writing separate code for each component makes it possible to tweak the code or swap out pieces of the simulation, such as by adding additional estimation methods or trying out a data-generating process involving different distributional assumptions. We already saw this in Chapter 3, where we modified our initial data-generating process to use a geometric distribution rather than a normal distribution. In short, following the first principle makes for simpler, more robust code that is easier to navigate, easier to test, and easier to extend. The second principle of tidy, modular simulation is to store all results in rectangular datasets, such as the base R data.frame object or the tidyverse tibble object.7 This principle applies to any and all output, including the simulated data from Step 1, the results of data analysis procedures from Step 2, full sets of replicated simulation results from Step 3, and summarized results from Step 4. A primary benefit of following this principle is that it facilitates working with the output of each stage in the simulation process. If you are comfortable using R to analyze real data, you you can use the same skills and tools to examine simulation output as long as it is in tabular form. Many of the data processing and data analysis tools available in R work with—or even require—rectangular datasets. Thus, using rectangular datasets makes it easier to inspect, summarize, and visualize the output. 4.3 Skeleton of a simulation study The principles of tidy simulation imply that code for a simulation study should usually follow the same broad outline and organization of Table 4.1, with custom functions for each step in process. We will describe the outlines of simulation code using function skeletons to illustrate the inputs and outputs of each component. These skeletons skip over all the specific details, so that we can see the structure more clearly. We will first examine the structure of the code for simulating one specific scenario, then consider how to extend the code to systematically explore a variety of scenarios, as in a multifactor simulation. In code skeletons, the structure of the first four steps in a simulation looks like this: # Generate (data-generating process) generate_data &lt;- function( model_params ) { # stuff return(data) } # Analyze (data-analysis procedure) analyze &lt;- function( data ) { # stuff return(one_result) } # Repeat one_run &lt;- function( model_params ) { dat &lt;- generate_data( model_params ) one_result &lt;- analyze(dat) return(one_result) } results &lt;- replicate(R, one_run( params )) # Summarize (calculate performance measures) assess_performance &lt;- function( results, model_params ) { # stuff return(performance_measures) } assess_performance(results, model_params) The code above shows the full skeleton of a simulation. It involves four functions, where the outputs of one function get used as inputs in subsequent functions. We will now look at the inputs and outputs of each function to see how they align with the four steps in the simulation process. Subsequent chapters examine each piece in much greater detail—putting meat on the bones of each function skeleton, to torture our metaphor—and discuss specific statistical issues and programming techniques that are useful for designing each component. Besides illustrating the skeletal framework of a simulation, readers might find it useful to use it as a template from which to start writing their own code. The simhelpers package includes the function create_skeleton(), which will open a new R script that contains a template for a simulation study, with sections corresponding to each component: simhelpers::create_skeleton() The template that appears is a slightly more elaborate version of the code above, with the main difference being that it also includes some additional lines of code to wire the pieces together for a multifactor simulation. Starting from this template, you will already be on the road to writing a tidy, modular simulation. 4.3.1 Data-Generating Process The first step in a simulation is specifying a data-generating process. This is a hypothetical model for how data might arise, involving measurements or observations of one or more variables. The bare-bones skeleton of a data-generating function looks like the following: generate_data &lt;- function( model_params ) { # stuff return(data) } The function takes as input a set of model parameter values, denoted here as model_params. Based on those model parameters, the function generates a hypothetical dataset as output. Generating our own data based on a model allows us to know what the answer is (e.g., the true population mean or the true average effect of an intervention), so that we have benchmark against which to compare the results of a data analysis procedure that generates noisy estimates of the true value. In practice, model_params will usually not be just one input but rather multiple arguments. These arguments might include inputs such as the population mean for a variable, the standard deviation of a distribution of treatment effects, or a parameter controlling the degree of skewness in the population distribution. Many data-generating processes involving multiple variables, such as the response variable and predictor variables in a regression model. In such instances, the inputs of generate_data() might also include parameters that determine the degree of dependence or correlation between variables. Further, the generate_data() inputs will also usually include arguments relating to the sample size and structure of the hypothetical dataset. For instance, in a simulation of a multilevel dataset where individuals are nested within clusters, the inputs might include an arguments to specify the total number of clusters and the average number of individuals per cluster. We discuss the inputs and form of the data-generating function further in Chapter 6. 4.3.2 Data Analysis Procedure The second step in a simulation is specifying a data analysis procedure or set of procedures. The bare-bones skeleton of a data-generating function looks like the following: analyze &lt;- function( data ) { # stuff return(one_result) } The function should take a single dataset as input and produce a set of estimates or results (e.g., point estimates, standard errors, confidence intervals, p-values, predictions, etc.). Because we will be using the function to analyze hypothetical datasets simulated from the data-generating process, the analyze() function needs to work with data inputs that are produced by the generate_data() function. Thus, the code in the body of analyze() can assume that data will include relevant variables with specific names. Inside the body of the function, analyze() includes code to carry out a data analysis procedure. This might involve generating a confidence interval, as in the example from Chapter 3. In another context, it might involve estimating an average growth rate along with a standard error, given a dataset with longitudinal repeated measurements from multiple individuals. In still another context, it might involve generating individual-level predictions from a machine learning algorithm. In simulations that involve comparing multiple analysis methods, we might write an analyze() function for each of the methods of interest, or (generally less preferred) we might write one function that does the calculations for all of the methods together. A well-written estimation method should, in principle, work not only on a simulated, hypothetical dataset but also on a real empirical dataset that has the same format (i.e., appropriate variable names and structure). Because of this, the inputs of the analyze() function should not typically include any information about the parameters of the data-generating process. To be realistic, the code for our simulated data-analysis procedure should not make use of anything that the analyst could not know when analyzing a real dataset. Thus, analyze() has an argument for the sample dataset but not for model_params. We discuss the form and content of the data analysis function further in Chapter 7. 4.3.3 Repetition The third step in a simulation is to repeatedly evaluate the data-generating process and data analysis procedure. In practice, this amounts to repeatedly calling generate_data() and then calling analyze() on the result. Here is the skeleton from our simulation template: one_run &lt;- function( model_params ) { dat &lt;- generate_data( model_params ) one_result &lt;- analyze(dat) return(one_result) } results &lt;- simhelpers::repeat_and_stack(R, one_run( params )) We first create a helper function called one_run(), which takes model_params as input. Inside the body of the function, we call the generate_data() function to simulate a hypothetical dataset. We pass this dataset to analyze() and return a small dataset containing the results of the data-analysis procedure. The one_run() method is like a coordinator or dispatcher of the system: it generates the data, calls all the evaluation methods we want to call, combines all the results, and hands them back for recording. Making a helper method such as one_run() can be useful because it facilitates debugging. Once we have the one_run() helper function, we need a way to call it repeatedly. As with many things in R, there are a variety of different ways to do something over and over. In the above skeleton, we use the repeat_and_stack() function from simhelpers.8 In the first argument, we specify the number of times to repeat the process. In the second argument, we specify an expression that evaluates one_run() for specified values of the model parameters stored in params. The repeat_and_stack() function then evaluates the expression repeatedly, R times in all, and then stacks up all of the replications into a big dataset, with one or more rows per replication.9 We go into further detail about how to approach running the simulation process in Chapter 8. Among other things, we will illustrate how to use the bundle_sim() function from the simhelpers package to automate the process of coding this step, thereby avoiding the need to write a one_run() helper function. 4.3.4 Performance summaries The fourth step in a simulation is to summarize the distribution of simulation results across replications. Here is the skeleton from our simulation template: assess_performance &lt;- function( results, model_params ) { # stuff return(performance_measures) } assess_performance(results, params) The assess_performance() function takes results as input. results should be a dataset containing all of the replications of the data-generating and analysis process. In contrast to the analyze() function, assess_performance() also needs to know the true parameter values of the data-generating process, so it needs to have model_params as its other input. The function then uses both of these inputs to calculate performance measures and returns a summary of the performance measures in a dataset. Performance measures are the metrics or criteria used to assess the performance of a statistical method across repeated samples from the data-generating process. For example, we might want to know how close an estimator gets to the target parameter, on average. We might want to know if a confidence interval captures the true parameter the right proportion of the time, as in the simulation from Chapter 3. Performance is defined in terms of the sampling distribution of estimators or analysis results, across an infinite number of replications of the data-generating process. In practice, we use many replications of the process, but still only a finite number. Consequently, we actually estimate the performance measures and need to attend to the Monte Carlo error in the estimates. We discuss the specifics of different performance measures and assessment of Monte Carlo error in Chapter ??. 4.3.5 Multifactor simulations Thus far, we have sketched out the structure of a modular, tidy simulation for a single context. In our \\(t\\)-test case study, for example, we might ask how well the \\(t\\)-test works when we have \\(n=100\\) units and the observations follow geometric distribution. However, we rarely want to examine a method only in a single context. Typically, we want to explore how well a procedure works across a range of different contexts. If we choose conditions in a structured and thoughtful manner, we will be able to examine broad trends and potentially make more general claims about the behaviors of the data-analysis procedures under investigation. Thus, it is helpful to think of simulations as akin to a designed experiment: in seeking to understand the properties of one or more procedures, we test them under a variety of different scenarios to see how they perform, then seek to identify more general patterns that hold beyond the specific scenarios examined. This is the heart of simulation for methodological evaluation. To implement a multifactor simulation, we will follows the same principles of modular, tidy simulation. In particular, we will take the code developed for simulating a single context and bundle it into a function that can be evaluated for any and all scenarios of interest. Simulation studies often follow a full factorial design, in which each level of a factor (something we vary, such as sample size, true treatment effect, or residual variance) is crossed with every other level. The experimental design then consists of sets of parameter values (including design parameters, such as sample sizes), and these too can be represented in an object, distinct from the other components of the simulation. We will discuss multiple-scenario simulations in Part III (starting with Chapter ??), after we more fully develop the core concepts and techniques involved in simulating a single context. 4.4 Exercises Look back at the \\(t\\)-test simulation presented in Chapter 3. The code presented there did not entirely follow the formal structure outlined in this chapter. Revise the code by creating separate functions for each of four components in the simulation skeleton. Using the functions, re-run the simulation and recreate one or more graphs from the exercises in the previous chapter. References Wickham, Hadley. 2014. “Tidy Data.” Journal of Statistical Software 59 (10): 1–23. https://doi.org/10.18637/jss.v059.i10. Wickham (2014) provides a broader introduction to the concept of tidy data in the context of data-analysis tasks.↩︎ In the example from Chapter 3, we used the replicate() function from base R to repeat the process of generating and analyzing data. This function is a fine alternative to the repeat_and_stack() approach demonstrated in the skeleton. The only drawback is that it requires some further work to combine the results across replications. Here is a different version of the skeleton, which uses replicate() instead of repeat_and_stack(): results_list &lt;- replicate(n = R, expr = { dat &lt;- generate_data( params ) one_result &lt;- analyze(dat) return(one_result) }, simplify = FALSE) results &lt;- purrr::list_rbind(results_list) This version of the skeleton does not create a one_run() helper function, but instead puts the code from the body of one_run() directly into the expr argument of replicate(). To learn about other ways of repeatedly evaluating the simulation process, see Appendix ??.↩︎ If you would prefer the output as a list rather than a stacked dataset, set repeat_and_stack()’s optional argument stack = FALSE.↩︎ "],["case-ANOVA.html", "Chapter 5 Case Study: Heteroskedastic ANOVA 5.1 The data-generating model 5.2 The hypothesis testing procedures 5.3 Running the simulation 5.4 Summarizing test performance 5.5 Exercises", " Chapter 5 Case Study: Heteroskedastic ANOVA In this chapter, we present another detailed example of a simulation study to demonstrate how to put the principles of tidy, modular simulation into practice. To illustrate the process of programming a simulation, we reconstruct the simulations from Brown and Forsythe (1974). We will also use this case study as a recurring example in some of the following chapters. Brown and Forsythe (1974) studied methods for null hypothesis testing in studies that measure a characteristic \\(X\\) on samples from each of several groups. They consider a population consisting of \\(G\\) separate groups, with population means \\(\\mu_1,...,\\mu_G\\) and population variances \\(\\sigma_1^2,...,\\sigma_G^2\\) for the characteristic \\(X\\). A researcher obtains samples of size \\(n_1,...,n_G\\) from each of the groups and takes measurements of the characteristic for each sampled unit. Let \\(x_{ig}\\) denote the measurement from unit \\(i\\) in group \\(g\\), for \\(i = 1,...,n_g\\) for each \\(j = 1,..., G\\). The researcher’s goal is to use the sample data to test the hypothesis that the population means are all equal \\[ H_0: \\mu_1 = \\mu_2 = \\cdots = \\mu_G. \\] If the population variances were all equal (so \\(\\sigma_1^2 = \\sigma_2^2 = \\cdots = \\sigma_G^2\\)), we could use a conventional one-way analysis of variance (ANOVA) to conduct this test. However, one-way ANOVA might not work well if the variances are not equal. The question is then: what are best practices for testing the null of equal group means, allowing for the possibility that variances could differ across groups? To tackle this question, Brown and Forsythe evaluated two different hypothesis testing procedures, developed by James (1951) and Welch (1951), both of which avoid the assumption that all groups have equal equality of variances. Brown and Forsythe also evaluated the conventional one-way ANOVA F-test as a benchmark, even though this procedure maintains the assumption of equal variances. They also proposed and evaluated a new procedure of their own devising.10 Their simulation involved comparing the performance of these different hypothesis testing procedures (the methods) under a range of conditions (different data generating processes) with different sample sizes and different degrees of heteroskedasticity. They looked at the different scenarios shown as Table 5.1, varying number of groups, group size, and amount of variation within each group. In all, there are a total of 20 scenarios, covering conditions with between 10 and 6 groups. Table 5.1: Simulation scenarios explored by Brown and Forsythe (1974) Scenario Groups Sample Sizes Standard Deviations A 4 4,4,4,4 1,1,1,1 B 4 1,2,2,3 C 4 4,8,10,12 1,1,1,1 D 4 1,2,2,3 E 4 3,2,2,1 F 4 11,11,11,11 1,1,1,1 G 4 1,2,2,3 H 4 11,16,16,21 1,1,1,1 I 4 3,2,2,1 J 4 1,2,2,3 K 6 4,4,4,4,4,4 1,1,1,1,1,1 L 6 1,1,2,2,3,3 M 6 4,6,6,8,10,12 1,1,1,1,1,1 N 6 1,1,2,2,3,3 O 6 3,3,2,2,1,1 P 6 6,6,6,6,6,6 1,1,2,2,3,3 Q 6 11,11,11,11,11,11 1,1,2,2,3,3 R 6 16,16,16,16,16,16 1,1,2,2,3,3 S 6 21,21,21,21,21,21 1,1,2,2,3,3 T 10 20,20,20,20,20,20,20,20,20,20 1,1,1.5,1.5,2,2,2.5,2.5,3,3 When evaluating hypothesis testing procedures, there are two main performance metrics of interest: type-I error rate and power. The type-I error rate is the rate at which a test rejects the null hypothesis when the null hypothesis is actually true. To apply a hypothesis testing procedure, one has to specify a desired, or nominal, type-I error rate, often denoted as the \\(\\alpha\\)-level. For a specified \\(\\alpha\\), a valid or well-calibrated test should have an actual type-I error rate less than or equal to the nominal level, and ideally should be very close to nominal. Power is how often a test correctly rejects the null when it is indeed false. It is a measure of how sensitive a method is to violations of the null. Brown and Forsythe estimated error rates and power for nominal \\(\\alpha\\)-levels of 1%, 5%, and 10%. Table 1 of their paper reports the simulation results for type-I error (labeled as “size”). Our Table 5.2 reports some of their results with respect to Type I error. For a well-calibrated hypothesis testing method, the reported numbers should be very close to the desired alpha levels, as listed at the top of the table. We can compare the four tests to each other across each row, where each row is a specific scenario defined by a specific data generating process. Looking at ANOVA, for example, we see some scenarios with very elevated rates. For instance, in Scenario E, the ANOVA F-test has 21.9% rejection when it should only have 10%. In contrast, the ANOVA F works fine under scenario A, which is what one would expect because all the groups have the same variance. Brown and Forsythe’s choice of scenarios here illustrates a broader design principle: to provide a full picture of the performance of a method or set of methods, it is wise to always evaluate them under conditions where we expect things to work, as well as conditions where we expect them to not work well. Table 5.2: Portion of “Table 1” reproduced from Brown and Forsythe (1974) ANOVA F test B &amp; F’s F* test Welch’s test James’ test Scenario 10% 5% 1% 10% 5% 1% 10% 5% 1% 10% 5% 1% A 10.2 4.9 0.9 7.8 3.4 0.5 9.6 4.5 0.8 13.3 7.9 2.4 B 12.0 6.7 1.7 8.9 4.1 0.7 10.3 4.7 0.8 13.8 8.1 2.7 C 9.9 5.1 1.1 9.5 4.8 1.0 10.8 5.7 1.6 12.1 6.7 2.1 D 5.9 3.0 0.6 10.3 5.7 1.4 9.8 4.9 0.9 10.8 5.6 1.3 E 21.9 14.4 5.6 11.0 6.2 1.8 11.3 6.5 2.0 12.9 7.7 2.9 F 10.1 5.1 1.0 9.8 5.7 1.5 10.0 5.0 0.9 10.6 5.5 1.1 G 11.4 6.3 1.8 10.7 5.7 1.5 10.1 5.0 1.1 10.6 5.4 1.3 H 10.3 4.9 1.1 10.3 5.1 1.0 10.2 5.0 1.0 10.5 5.3 1.2 I 17.3 10.8 3.9 11.1 6.2 1.8 10.5 5.5 1.2 10.9 5.8 1.3 J 7.3 4.0 1.0 11.5 6.5 1.8 10.6 5.4 1.1 10.9 5.6 1.1 K 9.6 4.9 1.0 7.3 3.4 0.4 11.4 6.1 1.4 14.7 9.5 3.8 To replicate the Brown and Forsythe simulation, we will first write functions to generate data for a specified scenario and evaluate data of a given structure. We will then use these functions to evaluate the hypothesis testing procedures in a specific scenario with a particular set of parameters (e.g., sample sizes, number of groups, and so forth). We will then scale up to execute the simulations for a range of scenarios that vary the parameters of the data-generating model, just as reported in Brown and Forsythe’s paper. 5.1 The data-generating model In the heteroskedastic one-way ANOVA simulation, there are three sets of parameter values: population means, population variances, and sample sizes. Rather than attempting to write a general data-generating function immediately, it is often easier to write code for a specific case first and then use that code as a starting point for developing a function. For example, say that we have four groups with means of 1, 2, 5, 6; variances of 3, 2, 5, 1; and sample sizes of 3, 6, 2, 4: mu &lt;- c(1, 2, 5, 6) sigma_sq &lt;- c(3, 2, 5, 1) sample_size &lt;- c(3, 6, 2, 4) Following Brown and Forsythe (1974), we will assume that the measurements are normally distributed within each sub-group of the population. The following code generates a vector of group id’s and a vector of simulated measurements: N &lt;- sum(sample_size) # total sample size G &lt;- length(sample_size) # number of groups # group id factor group &lt;- factor(rep(1:G, times = sample_size)) # mean for each unit of the sample mu_long &lt;- rep(mu, times = sample_size) # sd for each unit of the sample sigma_long &lt;- rep(sqrt(sigma_sq), times = sample_size) # See what we have? tibble( group = group, mu = mu_long, sigma = sigma_long ) ## # A tibble: 15 × 3 ## group mu sigma ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 1.73 ## 2 1 1 1.73 ## 3 1 1 1.73 ## 4 2 2 1.41 ## 5 2 2 1.41 ## 6 2 2 1.41 ## 7 2 2 1.41 ## 8 2 2 1.41 ## 9 2 2 1.41 ## 10 3 5 2.24 ## 11 3 5 2.24 ## 12 4 6 1 ## 13 4 6 1 ## 14 4 6 1 ## 15 4 6 1 Now we have the pieces needed to generate a small dataset consisting of group memberships and the measured characteristic: # Now make our data x &lt;- rnorm(N, mean = mu_long, sd = sigma_long) dat &lt;- tibble(group = group, x = x) dat ## # A tibble: 15 × 2 ## group x ## &lt;fct&gt; &lt;dbl&gt; ## 1 1 1.24 ## 2 1 3.07 ## 3 1 -0.681 ## 4 2 2.43 ## 5 2 2.50 ## 6 2 2.15 ## 7 2 0.612 ## 8 2 0.860 ## 9 2 2.09 ## 10 3 1.56 ## 11 3 5.08 ## 12 4 5.68 ## 13 4 5.66 ## 14 4 5.92 ## 15 4 4.38 We have followed the strategy of first constructing a dataset with parameters for each observation in each group, making heavy use of base R’s rep() function to repeat values in a list. We then called rnorm() to generate N observations in all. This works because rnorm() is vectorized; if you give it a vector (or vectors) of parameter values, it will generate each subsequent observation according to the next entry in the vector. As a result, the first x value is simulated from a normal distribution with mean mu_long[1] and standard deviation sd_long[1], the second x is simulated using mu_long[2] and sd_long[2], and so on. As usual, there are many different and legitimate ways of doing this in R. For instance, instead of using rep() to do it all at once, we could generate observations for each group separately, then stack the observations into a dataset. Do not worry about trying to writing code the “best” way—especially when you are initially putting a simulation together. If you can find a way to accomplish your task at all, then that’s often enough (and you should feel good about it!). 5.1.1 Now make a function Because we will need to generate datasets over and over, we will wrap our code in a function. The inputs to the function will be the parameters of the model that we specified at the very beginning: the set of population means mu, the population variances sigma_sq, and sample sizes sample_size. We make these quantities arguments of the data-generating function so that we can make datasets of different sizes and shapes: generate_ANOVA_data &lt;- function(mu, sigma_sq, sample_size) { N &lt;- sum(sample_size) G &lt;- length(sample_size) group &lt;- factor(rep(1:G, times = sample_size)) mu_long &lt;- rep(mu, times = sample_size) sigma_long &lt;- rep(sqrt(sigma_sq), times = sample_size) x &lt;- rnorm(N, mean = mu_long, sd = sigma_long) sim_data &lt;- tibble(group = group, x = x) return(sim_data) } The function is simply the code we built previously, all bundled up. We developed the function by first writing code to make the data-generating process to work once, the way we want, and only then turning the final code into a function for later reuse. Once we have turned the code into a function, we can call it to get a new set of simulated data. For example, to generate a dataset with the same parameters as before, we can do: sim_data &lt;- generate_ANOVA_data( mu = mu, sigma_sq = sigma_sq, sample_size = sample_size ) str(sim_data) ## tibble [15 × 2] (S3: tbl_df/tbl/data.frame) ## $ group: Factor w/ 4 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;: 1 1 1 2 2 2 2 2 2 3 ... ## $ x : num [1:15] 0.777 2.115 1.31 1.848 3.041 ... To generate one with population means of zero in all the groups, but the same group variances and sample sizes as before, we can do: sim_data_null &lt;- generate_ANOVA_data( mu = c( 0, 0, 0, 0 ), sigma_sq = sigma_sq, sample_size = sample_size ) str(sim_data) ## tibble [15 × 2] (S3: tbl_df/tbl/data.frame) ## $ group: Factor w/ 4 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;: 1 1 1 2 2 2 2 2 2 3 ... ## $ x : num [1:15] 0.777 2.115 1.31 1.848 3.041 ... Following the principles of tidy, modular simulation, we have written a function that returns a rectangular dataset for further analysis. Also note that the dataset returned by generate_ANOVA_data() only includes the variables group and x, but not mu_long or sd_long. This is by design. Including mu_long or sd_long would amount to making the population parameters available for use in the data analysis procedures, which is not something that happens when analyzing real data. 5.1.2 Cautious coding In the above, we built some sample code, and then bundled it into a function by literally cutting and pasting the initial work we did into a function skeleton. In the process, we shifted from having variables in our workspace with different names to using those variable names as parameters in our function call. Developing code in this way is not without hazards. In particular, after we have created our function, our workspace is left with a variable mu in it and our function also has a parameter named mu. Inside the function, R will use the parameter mu first, but this is potentially confusing. Another potential source of confusion are lines such as mu = mu, which means “set the function’s parameter called mu to the variable called mu.” These are different things (with the same name). Once you have built a function, one way to check that it is working properly is to comment out the initial code (or delete it), clear out the workspace (or restart R), and then re-run the code that uses the function. If things still work, then you can be somewhat confident that you have successfully bundled your code into the function. Once you bundle your code, you can also do a search and replace to change the variable names inside your function to something more generic, to better clarify the distinction betwen object names and argument names. 5.2 The hypothesis testing procedures Brown and Forsythe considered four different hypothesis testing procedures for heteroskedastic ANOVA, but we will focus on just two of the tests for now. We start with the conventional one-way ANOVA that mistakenly assumes homoskedasticity. R’s oneway.test function will calculate this test automatically: sim_data &lt;- generate_ANOVA_data( mu = mu, sigma_sq = sigma_sq, sample_size = sample_size ) anova_F &lt;- oneway.test(x ~ group, data = sim_data, var.equal = TRUE) anova_F ## ## One-way analysis of means ## ## data: x and group ## F = 8.9503, num df = 3, denom df = 11, ## p-value = 0.002738 We can use the same function to calculate Welch’s test by setting var.equal = FALSE: Welch_F &lt;- oneway.test(x ~ group, data = sim_data, var.equal = FALSE) Welch_F ## ## One-way analysis of means (not assuming ## equal variances) ## ## data: x and group ## F = 22.321, num df = 3.0000, denom df = ## 3.0622, p-value = 0.01399 The main results we need here are the \\(p\\)-values of the tests, which will let us assess Type-I error and power for a given nominal \\(\\alpha\\)-level. The following function takes simulated data as input and returns as output the \\(p\\)-values from the one-way ANOVA test and Welch test: ANOVA_Welch_F &lt;- function(data) { anova_F &lt;- oneway.test(x ~ group, data = data, var.equal = TRUE) Welch_F &lt;- oneway.test(x ~ group, data = data, var.equal = FALSE) result &lt;- tibble( ANOVA = anova_F$p.value, Welch = Welch_F$p.value ) return(result) } ANOVA_Welch_F(sim_data) ## # A tibble: 1 × 2 ## ANOVA Welch ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.00274 0.0140 Following our tidy, modular simulation principles, this function returns a small dataset with the p-values from both tests. Eventually, we might want to use this function on some real data. Our estimation function does not care if the data are simulated or not; we call the input data rather than sim_data to reflect this. As an alternative to this function, we could instead write code to implement the ANOVA and Welch tests ourselves. This has some potential advantages, such as avoiding any extraneous calculations that oneway.test does, which take time and slow down our simulation. However, there are also drawbacks to doing so, including that writing our own code takes our time and opens up the possibility of errors in our code. For further discussion of the trade-offs, see Chapter ??, where we do implement these tests by hand and see what kind of speed-ups we can obtain. 5.3 Running the simulation We now have functions that implement steps 2 and 3 of the simulation. Given some parameters, generate_ANOVA_data produces a simulated dataset and, given some data, ANOVA_Welch_F calculates \\(p\\)-values two different ways. We now want to know which way is better, and by how much. To answer this question, we will need to repeat the chain of generate-and-analyze calculations a bunch of times. To facilitate repetition, we first put the components together into a single function: one_run = function( mu, sigma_sq, sample_size ) { sim_data &lt;- generate_ANOVA_data( mu = mu, sigma_sq = sigma_sq, sample_size = sample_size ) ANOVA_Welch_F(sim_data) } one_run( mu = mu, sigma_sq = sigma_sq, sample_size = sample_size ) ## # A tibble: 1 × 2 ## ANOVA Welch ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.0167 0.107 This function implements a single simulation trial by generating artificial data and then analyzing the data, ending with a tidy dataset that has results for the single run. We next call one_run() over and over; see Appendix ?? for some discussion of options. The following uses repeat_and_stack() from simhelpers to evaluate one_run() 4 times and then stack the results into a single dataset: library(simhelpers) sim_data &lt;- repeat_and_stack(4, one_run( mu = mu, sigma_sq = sigma_sq, sample_size = sample_size) ) sim_data ## # A tibble: 4 × 2 ## ANOVA Welch ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.0262 0.0125 ## 2 0.00451 0.0698 ## 3 0.00229 0.0380 ## 4 0.0108 0.0423 Voila! We have simulated \\(p\\)-values! 5.4 Summarizing test performance We now have all the pieces in place to reproduce the results from Brown and Forsythe (1974). We first focus on calculating the actual type-I error rate of these tests—that is, the proportion of the time that they reject the null hypothesis of equal means when that null is actually true—for an \\(\\alpha\\)-level of .05. To evaluate the type-I error rate, we need to simulate data from a process where the population means are indeed all equal. Arbitrarily, let’s start with \\(G = 4\\) groups and set all of the means equal to zero: mu &lt;- rep(0, 4) In the fifth row of Table 1 (Scenario E in our Table 5.1), Brown and Forsythe examine performance for the following parameter values for sample size and population variance: sample_size &lt;- c(4, 8, 10, 12) sigma_sq &lt;- c(3, 2, 2, 1)^2 With these parameter values, we can use map_dfr to simulate 10,000 \\(p\\)-values: p_vals &lt;- repeat_and_stack(10000, one_run( mu = mu, sigma_sq = sigma_sq, sample_size = sample_size ) ) We can estimate the rejection rates by summarizing across these replicated p-values. The rule is that the null is rejected if the \\(p\\)-value is less than \\(\\alpha\\). To get the rejection rate, we calculate the proportion of replications where the null is rejected: sum(p_vals$ANOVA &lt; 0.05) / 10000 ## [1] 0.1391 This is equivalent to taking the mean of the logical conditions: mean(p_vals$ANOVA &lt; 0.05) ## [1] 0.1391 We get a rejection rate that is much larger than \\(\\alpha = .05\\). We have learned that the ANOVA F-test does not adequately control Type-I error under this set of conditions. mean(p_vals$Welch &lt; 0.05) ## [1] 0.0697 The Welch test does much better, although it appears to be a little bit in excess of 0.05. Note that these two numbers are quite close (though not quite identical) to the corresponding entries in Table 1 of Brown and Forsythe (1974). The difference is due to the fact that both Table 1 and are results are actually estimated rejection rates, because we have not actually simulated an infinite number of replications. The estimation error arising from using a finite number of replications is called simulation error (or Monte Carlo error). In Chapter ??, we will look more at how to estimate and control the Monte Carlo simulation error in performance measures. So there you have it! Each part of the simulation is a distinct block of code, and together we have a modular simulation that can be easily extended to other scenarios or other tests. The exercises at the end of this chapter ask you to extend the framework further. In working through them, you will get to experience first-hand how the modular code that we have started to develop is easier to work with than a single, monolithic block of code. 5.5 Exercises The following exercises involve exploring and tweaking the above simulation code we have developed to replicate the results of Brown and Forsythe (1974). Table 1 from Brown and Forsythe reported rejection rates for \\(\\alpha = .01\\) and \\(\\alpha = .10\\) in addition to \\(\\alpha = .05\\). Calculate the rejection rates of the ANOVA F and Welch tests for all three \\(\\alpha\\)-levels and compare to the table. Try simulating the Type-I error rates for the parameter values in the first two rows of Table 1 of the original paper. Use 10,000 replications. How do your results compare to the report results? In the primary paper, Table 1 is about Type I error and Table 2 is about power. A portion of Table 2 follows: Table 5.3: Portion of “Table 2” reproduced from Brown and Forsythe (1974) Variances Means Brown’s F B &amp; F’s F* Welch’s W 1,1,1,1 0,0,0,0 4.9 5.1 5.0 1,0,0,0 68.6 67.6 65.0 3,2,2,1 0,0,0,0 NA 6.2 5.5 1.3,0,0,1.3 NA 42.4 68.2 In the table, the sizes of the four groups are 11, 16, 16, and 21, for all the scenarios. Try simulating the power levels for a couple of sets of parameter values from Table 5.3. Use 10,000 replications. How do your results compare to the results reported in the Table? Instead of making ANOVA_Welch_F return a single row with the columns for the \\(p\\)-values, one could instead return a dataset with one row for each test. The “long” approach is often nicer when evaluating more than two methods, or when each method returns not just a \\(p\\)-value but other quantities of interest. For our current simulation, we might also want to store the \\(F\\) statistic, for example. The resulting dataset would then look like the following: ANOVA_Welch_F_long(sim_data) ## # A tibble: 2 × 3 ## method Fstat pvalue ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 ANOVA 8.46 0.00338 ## 2 Welch 14.3 0.0241 Modify ANOVA_Welch_F() to return output in this format, update your simulation code, and then use group_by() plus summarise() to calculate rejection rates of both tests. group_by() is a method for dividing your data into distinct groups and conducting an operation on each. The classic form of this would be something like the following: sres &lt;- res %&gt;% group_by( method ) %&gt;% summarise( rejection_rate = mean( pvalue &lt; 0.05 ) ) The onewaytests package in R includes functions for calculating Brown and Forsythe’s \\(F^*\\) test and James’ test for differences in population means. Modify the data analysis function ANOVA_Welch_F (or, better yet, ANOVA_Welch_F_long from Exercise 4) to also include results from these hypothesis tests. Re-run the simulation to estimate the type-I error rate of all four tests under Scenarios A and B of Table 5.1. References Brown, Morton B., and Alan B. Forsythe. 1974. “The Small Sample Behavior of Some Statistics Which Test the Equality of Several Means.” Technometrics 16 (1): 129–32. https://doi.org/10.1080/00401706.1974.10489158. James, G. S. 1951. “The Comparison of Several Groups of Observations When the Ratios of the Population Variances Are Unknown.” Biometrika 38 (3/4): 324. https://doi.org/10.2307/2332578. Welch, B. L. 1951. “On the Comparison of Several Mean Values: An Alternative Approach.” Biometrika 38 (3/4): 330. https://doi.org/10.2307/2332579. This latter piece makes Brown and Forsythe’s study a prototypical example of a statistical methodology paper: find some problem that current procedures do not perfectly solve, invent something to do a better job, and then do simulations and/or math to build a case that the new procedure is better.↩︎ "],["data-generating-processes.html", "Chapter 6 Data-generating processes 6.1 Examples 6.2 Components of a DGP 6.3 A statistical model is a recipe for data generation 6.4 Plot the artificial data 6.5 Check the data-generating function 6.6 Example: Simulating clustered data 6.7 Sometimes a DGP is all you need 6.8 More to explore 6.9 Exercises", " Chapter 6 Data-generating processes As we saw in Chapter 4, the first step of a simulation is creating artificial data based on some process where we know (and can control) the truth. This step is what we call the data generating process (DGP). Think of it as a recipe for cooking up artificial data, which can be applied over and over, any time we’re hungry for a new dataset. Like a good recipe, a good DGP needs to be complete—it cannot be missing ingredients and it cannot omit any steps. Unlike cooking or baking, however, DGPs are usually specified in terms of a statistical model, or a set of equations involving constants, parameter values, and random variables, which we will instantiate as an R function (or perhaps a set of functions). More complex DGPs, such as those for hierarchical data or other latent variable models, will often involve a series of several equations that describe different dimensions or levels of the model, which need to be followed in sequence to produce an artificial dataset. Designing DGPs and implementing them in R code involves making choices about what aspects of the model we want to be able to control and how to set up the parameters of the model. In this chapter, we give a high-level overview of DGPs, discuss some of the choices and challenges involved in designing them, and demonstrate how to write R functions for the DGP. We then present a detailed example involving a hierarchical DGP for generating data on students nested within schools. 6.1 Examples Before diving in, it is helpful to consider a few examples that we will return to throughout this and subsequent chapters. 6.1.1 One-way analysis of variance We have already seen one example of a DGP in the ANOVA example from Chapter 5. Here, we consider observations on some variable \\(X\\) drawn from a population consisting of \\(G\\) groups, where group \\(g\\) has population mean \\(\\mu_g\\) and population variance \\(\\sigma_g^2\\) for \\(g = 1,...,G\\). A simulated dataset consists of \\(n_g\\) observations from each group \\(g = 1,...,G\\), where \\(X_{ig}\\) is the measurement for observation \\(i\\) in group \\(g\\). The statistical model for these data can be written as follows: \\[ X_{ig} = \\mu_g + \\epsilon_{ig}, \\quad \\mbox{with} \\quad \\epsilon_{ig} \\sim N( 0, \\sigma^2_g ) \\] Alternately, we could write the model as \\[ X_{ig} \\sim N( \\mu_g, \\sigma_g^2 ) \\] for \\(i = 1,...,n_g\\) and \\(g = 1,...,G\\). 6.1.2 Bivariate Poisson model As a second example, suppose that we want to understand how the usual Pearson sample correlation coefficient behaves with non-normal data or to investigate how the Pearson correlation relates to Spearman’s rank correlation coefficient. To look into such questions, one DGP we might entertain is a bivariate Poisson model, which is a distribution for a pair of counts, \\(C_1,C_2\\), where each count follows a Poisson distribution and where the pair of counts may be correlated. We will denote the expected values of the counts as \\(\\mu_1\\) and \\(\\mu_2\\) and the correlation between the counts as \\(\\rho\\). To simulate a dataset based on this model, we would first need to choose how many observations to generate. Call this sample size \\(N\\). One way to generate data following a bivariate Poisson model is to generate three independent Poisson random variables for each of the \\(N\\) observations: \\[ \\begin{aligned} Z_0 &amp;\\sim Pois\\left( \\rho \\sqrt{\\mu_1 \\mu_2}\\right) \\\\ Z_1 &amp;\\sim Pois\\left(\\mu_1 - \\rho \\sqrt{\\mu_1 \\mu_2}\\right) \\\\ Z_2 &amp;\\sim Pois\\left(\\mu_2 - \\rho \\sqrt{\\mu_1 \\mu_2}\\right) \\end{aligned} \\] and then combine the pieces to create two dependent observations: \\[ \\begin{aligned} C_1 &amp;= Z_0 + Z_1 \\\\ C_2 &amp;= Z_0 + Z_2. \\end{aligned} \\] An interesting feature of this model is that the range of possible correlations is constrained: only positive correlations are possible and, because each of the independent pieces must have a non-negative mean, the maximum possible correlation is \\(\\sqrt{\\frac{\\min\\{\\mu_1,\\mu_2\\}}{\\max\\{\\mu_1,\\mu_2\\}}}\\). 6.1.3 Hierarchical linear model for a cluster-randomized trial Cluster-randomized trials are randomized experiments where the unit of randomization is a group of individuals, rather than the individuals themselves. For example, suppose we have a collection of schools and the students within them. A cluster-randomized trial involves randomizing the schools into treatment or control conditions and then measuring an outcome such as academic performance on the multiple students within the schools. Typically, researchers will be interested in the extent to which average outcomes differ across schools assigned to different conditions, which captures the impact of the treatment relative to the control condition. We will index the schools using \\(j = 1,...,J\\) and let \\(n_j\\) denote the number of students observed in school \\(j\\). Say that \\(Y_{ij}\\) is the outcome measure for student \\(i\\) in school \\(j\\), for \\(1 = 1,...,n_j\\) and \\(j = 1,...,J\\), and let \\(Z_j\\) be an indicator equal to 1 if school \\(j\\) is assigned to the treatment condition and otherwise equal to 0. A widely used approach for estimating impacts from cluster-randomized trials is heirarchical linear modeling (HLM). One way to write an HLM is in two parts. First, we consider a regression model that describes the distribution of the outcomes across students within school \\(j\\): \\[ Y_{ij} = \\beta_{0j} + \\epsilon_{ij}, \\qquad \\epsilon_{ij} \\sim N(0, \\sigma_{\\epsilon}^2), \\] where \\(\\beta_{0j}\\) is the average outcome across students in school \\(j\\). Second, we allow that the school-level average outcomes differ by a treatment effect \\(\\gamma_{1}\\) and that, for schools within each condition, the average outcomes follow a normal distribution with variance \\(\\sigma_u^2\\). We can write these relationships as a regression equation for the school-specific average outcome: \\[ \\beta_{0j} = \\gamma_{0} + \\gamma_{10} Z_j + u_{0j}, \\quad u_{0j} \\sim N(0, \\tau^2), \\] where \\(\\gamma_{0}\\) is the average outcome among schools in the control condition. If we only consider the first stage of this model, it looks a bit like the one-way ANOVA model from the previous example: in both cases, we have multiple observations from each of several groups. The main distinction is that the ANOVA model treats the \\(G\\) groups as a fixed set, whereas the HLM treats the set of \\(J\\) schools as sampled from a larger population of schools and includes a regression model describing variation in the school-level average outcomes. 6.2 Components of a DGP A DGP involves a statistical model with parameters and random variables, but it also often includes further details as well, beyond those that we would consider to be part of the model as we would use it for analyzing real data. In statistical analysis of real data, we often use models that describe only part of the distribution of the data, rather than its full, multivariate distribution. For instance, when conducting a regression analysis, we are analyzing the distribution of an outcome or response variable, conditional on a set of predictor variables. When using an item response theory (IRT) model, we use responses to a set of items to estimate individual ability levels given the items on the test. In contrast, if we are going to generate data for simulating a regression model or IRT model, we need to specify distributions for these additional features (the predictors in a regression model, the items in an IRT model); we can no longer just take them as given. In designing and discussing DGPs, it is helpful to draw distinctions between the components of the focal statistical model and the remaining components of the DGP that are taken as given when analyzing real data. A first relevant distinction is between structural features, covariates, and outcomes (or more generally, endogenous quantities): Structural features are quantities that describe the structure of a dataset but do not enter directly into the focal statistical model, such as the per-group sample sizes in the one-way ANOVA example. When analyzing real data, we usually take the structural features as they come, but when simulating data, we will need to make choices about the structural features. For instance, in the HLM example involving students nested within schools, the number of students in each school is a structural feature. To simulate data based on HLM, we will need to make choices about the number of schools and the distribution of the number of students in each school (e.g., we might specify that school sizes are uniformly distributed between specified minimum and maximum sizes), even though we do not have to consider these quantities when estimating a hierarchical model on real data. Covariates are variables in a dataset that we typically take as given when analyzing real data. For instance, in the one-way ANOVA example, the group assignments of each observation is a covariate. In the HLM example, covariates would include the treatment indicators \\(Z_1,...,Z_J\\). In a more elaborate version of the HLM, they might also include variables such as student demographic information, measures of past academic performance, or school-level characteristics such as the school’s geographic region or treatment assignment. When analyzing real data, we condition on these quantities, but when specifying a DGP, we will need to make choices about how they are distributed (e.g., we might specify that students’ past academic performance is normally distributed). Outcomes and endogenous quantities are the variables whose distribution is described by the focal statistical model. In the one-way ANOVA example, the outcome variable consists of the measurements \\(X_{ig}\\) for \\(i = 1,...,n_g\\) and \\(g = 1,...,G\\). In the bivariate Poisson model, the outcomes consist of the component variables \\(Z_1,Z_2,Z_3\\) and the observed counts \\(C_1,C_2\\) because all of these quantities follow distributions that are specified as part of the focal model. The focal statistical model specifies the distribution of these variables, and we will be interested in estimating the parameters controlling their distribution. Note that the focal statistical model only determines this third component of the DGP. The focal model consists of the equations describing what we would aim to estimate when analyzing real data. In contrast, the full statistical model also includes additional elements specify how to generate the structural features and covariates—the pieces that are taken as given when analyzing real data. Table 6.1 contrasts the role of structural features, covariates, and outcomes in real data analysis versus in simulations. Table 6.1: Real Data Analysis versus Simulation Component Real world Simulation world Structural features We obtain data of a given sample size, sizes of clusters, etc. We specify sample sizes, we specify how to generate cluster sizes Covariates Data come with covariates We specify how to generate covariates Outcomes Data come with outcome variables We generate outcome data based on a focal model Parameter estimation We estimate a statistical model to learn about the unknown parameters We estimate a statistical model and compare the results to the true parameters For a given DGP, the full statistical model might involve distributions for structural features, distributions for covariates, and distributions for outcomes given the covariates. Each of these distributions will involve parameters that control the properties of the distribution (such as the average and degree of variation in a variable). We think of these parameters as falling into one of three categories: focal, auxiliary, or design. Focal parameters are the quantities that we care about and seek to estimate in real data analysis. These are typically parts of the focal statistical model, such as the population means \\(\\mu_1,...,\\mu_G\\) in the one-way ANOVA model, the correlation between counts \\(\\rho\\) in the bivariate Poisson model, or the treatment effect \\(\\gamma_{1}\\) in the HLM example. Auxiliary parameters are the other quantities that go into the focal statistical model or some other part of the DGP, which we might not be substantively interested in when analyzing real data but which nonetheless affect the analysis. For instance, in the one-way ANOVA model, we would consider the population variances \\(\\sigma_1^2,...,\\sigma_G^2\\) to be auxiliary if we are not interested in investigating how they vary from group to group. In the bivariate Poisson model we might consider the average counts \\(\\mu_1\\) and \\(\\mu_2\\) to be auxiliary parameters. Design parameters are the quantities that control how we generate structural features of the data. For instance, in a cluster-randomized trial, the fraction of schools assigned to treatment is a design parameter that can be directly controlled by the researchers. Additional design parameters might include the minimum and maximum number of students per school. Typically, we do not directly estimate such parameters because we take the distribution of structural features as given. It is evident from this discussion that DGPs can involve many moving parts. One of the central challenges in specifying DGPs is that the performance of estimation methods will generally be affected by the full statistical model—including the design parameters and distribution of structural features and covariates—even though they are not part of the focal model. 6.3 A statistical model is a recipe for data generation Once we have decided on a full statistical model and written it down in mathematical terms, we need to translate it into code. A function that implements a data-generating model should have the following form: generate_data &lt;- function( focal_parameters, auxiliary_parameters, design_parameters ) { # generate pseudo-random numbers and use those to make some data return(sim_data) } The function takes a set of parameter values as input, simulates random numbers and does calculations, and produces as output a set of simulated data. Typically, the inputs will consist of multiple parameters, and these will include not only the focal model parameters, but also the auxiliary parameters, sample sizes, and other design parameters. The output will typically be a dataset, mimicking what one would see in an analysis of real data. In some cases, the output data might be augmented with some other latent quantities (normally unobserved in the real world) that can be used later to assess whether an estimation procedure produces results that are close to the truth. We have already seen an example of a complete DGP function in the case study on one-way ANOVA (see Section 5.1). In this case study, we developed the following function to generate data for a single outcome from a set of \\(G\\) groups: generate_ANOVA_data &lt;- function(mu, sigma_sq, sample_size) { N &lt;- sum(sample_size) G &lt;- length(sample_size) group &lt;- factor(rep(1:G, times = sample_size)) mu_long &lt;- rep(mu, times = sample_size) sigma_long &lt;- rep(sqrt(sigma_sq), times = sample_size) x &lt;- rnorm(N, mean = mu_long, sd = sigma_long) sim_data &lt;- tibble(group = group, x = x) return(sim_data) } This function takes both the focal model parameters (mu, sigma_sq) and other design parameters that one might not think of as parameters per-se (sample_size). When simulating, we have to specify quantities that we take for granted when analyzing real data. How would we write a DGP function for the bivariate Poisson model? The equations in Section 6.1 give us the recipe, so it just a matter of re-expressing them in code. For this model, the only design parameter is the sample size, \\(N\\); the sole focal parameter is the correlation between the variates, \\(\\rho\\); and the auxiliary parameters are the expected counts \\(\\mu_1\\) and \\(\\mu_2\\). Our function should have all four of these quantities as inputs and should produce as output a dataset with two variables, \\(C_1\\) and \\(C_2\\). Here is one way to implement the model: r_bivariate_Poisson &lt;- function(N, rho, mu1, mu2) { # covariance term, equal to E(Z_3) EZ3 &lt;- rho * sqrt(mu1 * mu2) # Generate independent components Z1 &lt;- rpois(N, lambda = mu1 - EZ3) Z2 &lt;- rpois(N, lambda = mu2 - EZ3) Z3 &lt;- rpois(N, lambda = EZ3) # Assemble components dat &lt;- data.frame( C1 = Z1 + Z3, C2 = Z2 + Z3 ) return(dat) } Here we generate 5 observations from the bivariate Poisson with \\(\\rho = 0.5\\) and \\(\\mu_1 = \\mu_2 = 4\\): r_bivariate_Poisson(5, rho = 0.5, mu1 = 4, mu2 = 4) ## C1 C2 ## 1 4 4 ## 2 2 2 ## 3 2 1 ## 4 5 6 ## 5 2 3 6.4 Plot the artificial data The whole purpose of writing a DGP is to produce something that can be treated just as if it were real data. Considering that is our goal, we should act like it and engage in data analysis processes that we would apply whenever we analyze real data. In particular, we often find it worthwhile to create one or more plots of the data generated by a DGP, just as we would if we were exploring a new real dataset for the first time. This exercise can be very helpful for catching problems in the DGP function (about which more below). Beyond just debugging, constructing graphic visualizations can be a very effective way to study a model and strengthen your understanding of how to interpret its parameters. In the one-way ANOVA example, it would be conventional to visualize the data with box plots or some other summary statistics for the data from each group. For exploratory graphics, we prefer plots that include representations of the raw data points, not just summary statistics. The figure below uses a density ridge-plot, filled in with points for each observation in each group. The plot is based on a simulated dataset with 50 observations in each of five groups. Here is a plot of 30 observations from the bivariate Poisson distribution with means \\(\\mu_1 = 10, \\mu_2 = 7\\) and correlation \\(\\rho = .65\\) (points are jittered slightly to avoid over-plotting): Figure 6.1: \\(N = 30\\) observations from the bivariate Poisson distribution with \\(\\mu_1 = 10, \\mu_2 = 7, ho = .65\\). Plots like these are useful for building intuitions about a model. For instance, we can inspect 6.1 to get a sense of the order of magnitude and range of the observations, as well as the likelihood of obtaining multiple observations with identical counts. Depending on the analysis procedures we will apply to the dataset, we might even create plots of transformations of the dataset, such as a histogram of the differences \\(C_2 - C_1\\) or a scatterplot of the rank transformations of \\(C_1\\) and \\(C_2\\). 6.5 Check the data-generating function An important part of programming in R—and especially when writing custom functions—is finding ways to test and check the correctness of your code. Just writing a data-generating function is not enough. It is also critical to test whether the output it produces is correct. How best to do this will depend on the particulars of the DGP being implemented. For many DGPs, a broadly useful strategy is to generate a very large sample of data, so large that the sample distribution should very closely resemble the population distribution. One can then test whether features of the sample distribution closely align with corresponding parameters of the population model. For the heteroskedastic ANOVA problem, one basic thing we could do is check that the simulated data from each group follows a normal distribution. In the following code, we simulate very large samples from each of the four groups, and check that the means and variances agree with the input parameters: mu &lt;- c(1, 2, 5, 6) sigma_sq &lt;- c(3, 2, 5, 1) check_data &lt;- generate_ANOVA_data( mu = mu, sigma_sq = sigma_sq, sample_size = rep(10000, 4) ) chk &lt;- check_data %&gt;% group_by( group ) %&gt;% dplyr::summarise( n = n(), mean = mean(x), var = var(x) ) %&gt;% mutate(mu = mu, sigma2 = sigma_sq) %&gt;% relocate( group, n, mean, mu, var, sigma2 ) chk ## # A tibble: 4 × 6 ## group n mean mu var sigma2 ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 10000 0.988 1 2.97 3 ## 2 2 10000 1.99 2 1.99 2 ## 3 3 10000 5.02 5 5.00 5 ## 4 4 10000 5.98 6 0.993 1 It seems we are recovering our parameters. We can also make some diagnostic plots to assess whether we have normal data (using QQ plots, where we expect a straight line if the data are normal): ggplot( check_data ) + aes( sample = x, color = group ) + facet_wrap( ~ group ) + stat_qq() + stat_qq_line() This diagnostic looks good too. Here, these checks may seem a bit silly, but most bugs are silly—at least once you find them! In models that are even a little bit more complex, it is quite easy for small things such as a sign error to slip into your code. Even simple checks such as these can be quite helpful in catching such bugs. 6.6 Example: Simulating clustered data Writing code for a complicate DGP can feel like a daunting task, but if you first focus on a recipe for how the data is generated, it is often not too bad to then convert that recipe into code. We now illustrate this process with a detailed case study involving a more complex data-generating practice. Recent literature on multisite trials (where, for example, students are randomized to treatment or control within each of a series of sites) has explored how variation in the strength of effects can affect how different data-analysis procedures behave (e.g., Miratrix, Weiss, and Henderson 2021; Bloom et al. 2016). In this example, we are going to extend this work to explore best practices for estimating treatment effects in cluster randomized trials. In particular, we will investigate what happens when the treatment impact for each school is related to the size of the school. 6.6.1 A design decision: What do we want to manipulate? In designing a simulation study, we need to find a DGP that will allow us to address the specific questions we are interested in investigating. For instance, in the one-way ANOVA example, we wanted to see how different degrees of within-group variation impacted the performance of several hypothesis-testing procedures. We therefore needed a data generation process that allowed us to control the extent of within-group variation. To figure out what DGP to use for simulating data from a cluster-randomized trial, we need to consider how we are going to use those data in our simulation study. Because we are interested in understanding what happens when school-specific effects are related to school size, we will need data with the following features: observations for students in each of several schools; schools are different sizes and have different mean outcomes; school-specific treatment effects correlate with school size; and schools are assigned to different treatment conditions. A given dataset will consist of observations for individual students in schools, with each student having a school id, a treatment assignment (shared for all in the school), and an outcome. A good starting point for building a DGP is to first sketch out what a simulated dataset should look like. For this example, we need data like the following: schoolID Z size studentID Y 1 1 24 1 3.6 1 1 24 3 1.0 1 etc etc etc etc 1 1 24 24 2.0 2 0 32 1 0.5 2 0 32 2 1.5 2 0 32 3 1.2 etc etc etc etc etc When running simulations, it is good practice to look at simple scenarios along with complex ones. This lets us not only identify conditions where some aspect of the DGP is important, but also verify that the feature does not matter under scenarios where we know it should not. Given this principle, we land on the following points: We need a DGP that lets us generate schools that are all the same size or that are all different sizes. Our DGP should allow for variation in the school-specific treatment effects. We should have the option to generate school-specific effects that are related or unrelated to school size. 6.6.2 A model for a cluster RCT DGPs are expressed and communicated using mathematical models. In developing a DGP, we often start by considering the model for the outcomes (along with its focal parameters), which covers some but not all of the steps in the full recipe for generating data. It is helpful to write down the equations for the outcome model and then note what further quantities need to be generated (such as structural features and covariates). Then we can consider how to generate these quantities with auxiliary models. Section 6.1.3 introduced a basic HLM for a cluster-randomized trial. This model had two parts, starting with a model for our student outcome: \\[ Y_{ij} = \\beta_{0j} + \\epsilon_{ij} \\mbox{ with } \\epsilon_{ij} \\sim N( 0, \\sigma^2_\\epsilon ) \\] where \\(Y_{ij}\\) is the outcome for student \\(i\\) in site \\(j\\), \\(\\beta_{0j}\\) is the average outcome in site \\(j\\), and \\(\\epsilon_{ij}\\) is the residual error for student \\(i\\) in site \\(j\\). The model was completed by specifying how the site-specific outcomes vary as a function of treatment assignment: \\[ \\beta_{0j} = \\gamma_{0} + \\gamma_{1} Z_j + u_j, \\quad u_j \\sim N( 0, \\sigma^2_u ).\\] This model has a constant treatment effect: if a school is assigned to treatment, then all outcomes in the cluster are raised by the amount \\(\\gamma_{1}\\). But we also want to allow the size of impact to vary by school size. This suggests we will need to elaborate the model to include a treatment-by-size interaction term. One approach for allowing the school-specific impacts to depend on school size is to introduce school size as a predictor, as in \\[ \\beta_{0j} = \\gamma_{0} + \\gamma_{1} Z_j + \\gamma_{2} \\left(Z_j \\times n_j\\right) + u_j. \\] A drawback of this approach is that changing the average size of the schools will change the average treatment impact. A more interpretable approach is to allow treatment effects to depend on the relative school sizes. To do this, we can define a covariate that describes the deviation in the school size relative to the average size. Thus, let \\[ S_j = \\frac{n_j - \\bar{n}}{ \\bar{n} }, \\] where \\(\\bar{n}\\) is the overall average school size. Using this covariate, we then revise our equation for our site \\(j\\) to: \\[ \\beta_{0j} = \\gamma_{0} + \\gamma_{1} Z_j + \\gamma_{2} \\left( Z_j \\times S_j\\right) + u_j. \\] If \\(\\gamma_{2}\\) is positive, then bigger schools will have larger treatment impacts. Because \\(S_j\\) is centered at 0, the overall average impact across schools will be simply \\(\\gamma_{1}\\). (If \\(S_j\\) was not centered at zero, then the overall average impact would be some function of \\(\\gamma_{1}\\) and \\(\\gamma_{2}\\).) Putting all of the above together, we now have an HLM to describe the distribution of outcomes conditional on the covariates and structural features: \\[ \\begin{aligned} Y_{ij} &amp;= \\beta_{0j} + \\epsilon_{ij} \\quad &amp;\\epsilon_{ij} &amp;\\sim N( 0, \\sigma^2_\\epsilon ) \\\\ \\beta_{0j} &amp;= \\gamma_{0} + \\gamma_{1} Z_j + \\gamma_{2} Z_j S_j + u_j \\quad &amp; u_j &amp;\\sim N( 0, \\sigma^2_u ) \\end{aligned} \\] Substituting the second equation into the first leads to a single equation for generating the student-level outcomes (or what is called the reduced form of the HLM): \\[ Y_{ij} = \\gamma_{0} + \\gamma_{1} Z_j + \\gamma_{2} Z_j S_j + u_j + \\epsilon_{ij}\\] The parameters of this focal model are the mean outcome among control schools (\\(\\gamma_{0}\\)), the average treatment impact (\\(\\gamma_{1}\\)), the site-size by treatment interaction term (\\(\\gamma_{2}\\)), the amount of school-level variation (\\(\\sigma^2_u\\)), and the amount of within-school variation (\\(\\sigma^2_\\epsilon\\)). There are several ways that we could elaborate this model further. For one, we might want to include a main effect for \\(S_j\\), so that average outcomes in the absence of treatment are also dependent on school size. For another, we might revise the model to allow for school-to-school variation in treatment impacts that is not explained by school size. For simplicity, we do not build in these further features, but see the exercises at the end of the chapter. So far we have a mathematical model analogous to what we would write if we were analyzing the data. To generate data, we also need a way to generate the structural features and covariates involved in the model. First, we need to know the number of clusters (\\(J\\)) and the sizes of the clusters (\\(n_j\\), for \\(j = 1, ..., J\\)). For illustrative purposes, we will generate size sizes from a uniform distribution with average school size \\(\\bar{n}\\) and a fixed parameter \\(\\alpha\\) that controls the degree of variation in school size. Mathematically, \\[ n_j \\sim \\text{Unif}\\left[ (1-\\alpha)\\bar{n}, (1+\\alpha)\\bar{n} \\right].\\] Equivalently, we could generate site sizes by taking \\[n_j = \\bar{n}(1 + \\alpha U_j), \\quad U_j \\sim unif(-1, 1).\\] For instance, if \\(\\bar{n} = 100\\) and \\(\\alpha = 0.25\\) then schools would range in size from 75 to 125. This specification is nice because it is simple, with just two parameters,both of which are easy to interpret: \\(\\bar{n}\\) is the average school size and \\(\\alpha\\) is the degree of variation in school size, relative to the average. To round out the model, we also need to define how to generate the treatment indicator, \\(Z_j\\). To allow for different treatment allocations, we will specify a proportion \\(p\\) of clusters assigned to treatment. Because we are simulating a cluster-randomized trial, we do this by drawing a simple random sample (without replacement) of \\(p \\times J\\) schools out of the total sample of \\(J\\) schools, then setting \\(Z_j = 1\\) for these schools and \\(Z_j = 0\\) for the remaining schools. We will denote this process as \\(Z_1,...,Z_J \\sim SRS(p, J)\\), where SRS stands for simple random sample. Now that we have an auxiliary model for school sizes, let us look again at our treatment impact heterogeneity term: \\[ \\gamma_{2} Z_j S_j = \\gamma_{2} Z_j \\left(\\frac{n_j - \\bar{n}}{\\bar{n}}\\right) = \\gamma_{2} \\alpha Z_j U_j, \\] where \\(U_j \\sim \\text{Unif}(-1,1)\\) is the uniform variable used to generate \\(n_j\\). Because we have standardized by average school size, the importance of the covariate does not change as a function of ave rage school size, but rather as a function of the relative variation parameter \\(\\alpha\\). Setting up a DGP with standardized quantities will make it easier to interpret simulation results, especially if we are looking at results from multiple scenarios with different parameter values. To the extent feasible, we want the parameters of the DGP to change only one feature of the data, so that it is easier to isolate the influence of each parameter. 6.6.3 From equations to code When sketching out the equations for the DGP, we worked from the lowest level of the model (the students) to the higher level (schools) and then to the auxiliary models for covariates and structural features. For writing code to based on the DGP, we will proceed in the opposite direction, from auxiliary to focal and from the highest level to the lowest. First, we will generate the sites and their features: Generate school sizes Generate school-level covariates Generate school-level random effects Then we will generate the students inside the sites: Generate student residuals Add everything up to generate student outcomes The mathematical model gives us the details we need to execute with each of these steps. Here is the skeleton of a DGP function with arguments for each of the parameters we might want to control, including defaults for each (see ?? for more on function defaults): gen_cluster_RCT &lt;- function( J = 30, n_bar = 10, alpha = 0, p = 0.5, gamma_0 = 0, gamma_1 = 0, gamma_2 = 0, sigma2_u = 0, sigma2_e = 1 ) { # Code (see below) goes here } Note that the inputs to this function are a mix of model parameters (gamma_0, gamma_1, gamma_2, representing coefficients in regressions), auxilary parameters (sigma2_u, sigma2_e, alpha, n_bar), and design parameters (J, p) that directly inform data generation. We set default arguments (e.g., gamma_0=0) so that we can ignore aspects of the DGP that we do not care about for the moment. Inside the model, we will have a block of code to generate the variables pertaining to schools, and then another to generate the variables pertaining to students. We first make the schools: # generate schools sizes n_min &lt;- round( n_bar * (1 - alpha) ) n_max &lt;- round( n_bar * (1 + alpha) ) nj &lt;- sample( n_min:n_max, J, replace = TRUE ) # Generate average control outcome for all schools (the random effects) u0j &lt;- rnorm( J, mean = 0, sd = sqrt(sigma2_u) ) # randomize schools (proportion p to treatment) Zj &lt;- ifelse( sample( 1:J ) &lt;= J * p, 1, 0) # Calculate schools intercepts S_j &lt;- (nj - n_bar) / n_bar beta_0j &lt;- gamma_0 + gamma_1 * Zj + gamma_2 * Zj * S_j + u0j The code is a literal translation of the math we did before. Note the line with sample(1:J) &lt;= J*p; this is a simple trick to generate a 0/1 indicator for control and treatment conditions. There is also a serious error in the above code (serious in that the code will run and look fine in many cases, but not always do what we want); we leave it as an exercise (see below) to find and fix it. Next, we use the site characteristics to generate the individual-level variables: # Make individual site membership sid &lt;- as.factor( rep( 1:J, nj ) ) # Generate the individual-level errors and outcome N &lt;- sum( nj ) e &lt;- rnorm( N, mean = 0, sd = sqrt(sigma2_e) ) Y &lt;- beta_0j[sid] + e # Bundle into a dataset dd &lt;- data.frame( sid = sid, Z = Zj[ sid ], Yobs = Y ) A key piece here is the rep() function that takes a list and repeats each element of the list a specified number of times. In particular, rep() repeats each number (\\(1, 2, /ldots,J\\)), the corresponding number of times as listed in nj. Putting the code above into the function skeleton will produce complete DGP function (view the complete function here). We can then call the function as so: dat &lt;- gen_cluster_RCT( J=3, n_bar = 5, alpha = 0.5, p = 0.5, gamma_0 = 0, gamma_1 = 0.2, gamma_2 = 0.2, sigma2_u = 0.4, sigma2_e = 1 ) dat ## sid Z Yobs ## 1 1 1 2.3263686 ## 2 1 1 2.0202277 ## 3 1 1 3.5850632 ## 4 1 1 0.9581332 ## 5 1 1 2.6183761 ## 6 1 1 0.3198559 ## 7 2 0 -2.2305376 ## 8 2 0 1.0479261 ## 9 2 0 -0.6256389 ## 10 2 0 1.0891353 ## 11 2 0 -0.6051252 ## 12 2 0 -0.3099363 ## 13 2 0 -0.9828624 ## 14 2 0 -0.5571326 ## 15 3 0 0.1203995 ## 16 3 0 1.7086978 ## 17 3 0 0.1213685 With this function, we can control the average size of the clusters (n), the number of clusters (J), the proportion treated (p), the average outcome in the control group (gamma_0), the average treatment effect (gamma_1), the site size by treatment interaction (gamma_2), the amount of cross site variation (sigma2_u), the residual variation (sigma2_e), and the amount of site size variation (alpha). The next step is to test the code, making sure it is doing what we think it is. In fact, it is not–there is a subtle bug that only appears under some specifications of the parameters; see the exercises for more on diagnosing and repairing this error. 6.6.4 Standardization in the DGP One difficulty with the current implementation of the model is that the magnitude of the different parameters are inter-connected. For instance, raising or lowering the within-school variance (\\(\\sigma^2_u\\)) will increase the overall variation in \\(Y\\), and therefore affect our the interpretation of the treatment effect parameters, because a given value of \\(\\gamma_{1}\\) will be less consequential if there is more overall variation. We can fix this issue by standardizing the model parameters. Standardization will allow us to reduce the set of parameters we might want to manipulate and will ensure that varying the remaining parameters only affects one aspect of the DGP. For a continuous, normally distributed outcome variable, a common approach to scaling is to constrain the overall variance of the outcome to a fixed value, such as 1 or 100. The magnitude of the other parameters of the model can then be interpreted relative to this scale. Often, we can also constrain the mean of the outcome to a fixed value, such as setting \\(\\gamma_0 = 0\\) without affecting the interpretation of the other parameters. With the current model, the variance of the outcome across students in the control condition is \\[ \\begin{aligned} \\text{Var}( Y_{ij} | Z_j = 0) &amp;= \\text{Var}( \\beta_{0j} + \\epsilon_{ij} | Z_j = 0) \\\\ &amp;= \\text{Var}( \\gamma_{0} + \\gamma_{1} Z_j + \\gamma_{2} Z_j \\tilde{n}_j + u_j + \\epsilon_{ij} | Z_j = 0) \\\\ &amp;= \\text{Var}( \\gamma_{0} + u_j + \\epsilon_{ij} ) \\\\ &amp;= \\sigma^2_u + \\sigma^2_\\epsilon. \\end{aligned} \\] To ensure that the total variance is held constant, we can redefine the variance parameters in terms of the intra-class correlation (ICC). The ICC is defined as \\[ ICC = \\frac{ \\sigma^2_u }{ \\sigma^2_u + \\sigma^2_\\epsilon }.\\] The ICC measures the degree of between-group variation as a proportion of the total variation of the outcome. It plays an important role in power calculations for cluster-randomized trials. If we want the total variance of the outcome to be 1, we need to set \\(\\sigma^2_u + \\sigma^2_{\\epsilon} = 1\\), which the implies that \\(ICC = \\sigma^2_u\\), and \\(\\sigma^2_\\epsilon = 1 - ICC\\). Thus, we can call our DGP function as follows: ICC &lt;- 0.3 dat &lt;- gen_cluster_RCT( J = 30, n_bar = 20, alpha = 0.5, p = 0.5, gamma_0 = 0, gamma_1 = 0.3, gamma_2 = 0.2, sigma2_u = ICC, sigma2_e = 1 - ICC ) Manipulating the ICC rather than separately manipulating \\(\\sigma^2_u\\) and \\(\\sigma^2_\\epsilon\\) will let us change the degree of between-group variation without affecting the overall scale of the outcome. A further consequence of setting the overall scale of the outcome to 1 is that the parameters controlling the treatment impact can now be interpreted as standardized mean difference effect sizes. The standardized mean difference for a treatment impact is defined as the average impact over the standard deviation of the outcome among control observations.11 Letting \\(\\delta\\) denote the standardized mean difference parameter, \\[ \\delta = \\frac{E(Y | Z_j = 1) - E(Y | Z_j = 0)}{SD( Y | Z_j = 0 )} = \\frac{\\gamma_1}{\\sqrt{ \\sigma^2_u + \\sigma^2_\\epsilon } } \\] Because we have constrained the total variance, \\(\\gamma_1\\) is equivalent to \\(\\delta\\). This equivalence holds for any value of \\(\\gamma_0\\), so we do not have to worry about manipulating \\(\\gamma_0\\) in the simulations—we can simply leave it at its default value. 6.7 Sometimes a DGP is all you need We have introduced the data-generating process as only the first step in developing a simulation study. Indeed, there are many more considerations to come, which we will describe in subsequent chapters. However, this first step is still very useful in its own right, even apart from the other components of a simulation. Sometimes, writing a data-generating function is all you need to learn about a statistical model. Writing data-generating functions is a very effective way to study a statistical model, such as a model that you might be learning about in a course or through self-study. Writing code based on a model is a much more active process than listening to a lecture or reading a book or paper. Coding requires you to make the mathematical notation tangible and can help you to notice details of the model that might be easily missed through listening or reading alone. Suppose we are taking a first course psychometrics and have just been introduced to item response theory (IRT) models for binary response items. Our instructor has just laid out a bunch of notation: We have data from a sample of \\(N\\) individuals, each of whom responds to a set of \\(M\\) test items. We let \\(X_{im} = 1\\) if respondent \\(i\\) answers item \\(m\\) correctly, with \\(X_{im} = 0\\) otherwise, for \\(i = 1,...,N\\) and \\(m = 1,...,M\\). We imagine that each respondent has a latent ability \\(\\theta_i\\) on whatever domain the test measures. Now our instructor starts dropping models on us, putting up a slide showing the equation for a three-parameter IRT model: \\[ Pr(X_{im} = 1) = \\gamma_m + (1 - \\gamma_m) g\\left( \\alpha_m [\\theta_i - \\beta_m]\\right), \\] where \\(g(x)\\) is the cumulative logistic curve: \\(g(x) = e^x / (1 + e^x)\\). The instructor explains that \\(\\alpha_m\\) is discrimination parameter that can take any real value, \\(\\beta_m\\) is a difficulty parameter that has to be greater than zero, and \\(\\gamma_m\\) is a guessing parameter between 0 and 1. They also explain that the guessing parameter is often hard to estimate and so might get treated as fixed and known, based on the number of response options on the item. For instance, if all the items have four options, then we might take \\(\\gamma_m = \\frac{1}{4}\\) for \\(m = 1,...,M\\). Finally, they explain that the ability parameters are assumed to follow a standard normal distribution, so \\(\\theta_i \\sim N(0, 1)\\) in the population. What is this madnesses? It certainly is a lot of notation to try and follow. To make sense of all the moving pieces, let’s try simulating from the model using more-or-less arbitrary parameters. To begin, we will need to pick a test length \\(M\\) and a sample size \\(N\\). Let’s use \\(N = 7\\) participants and \\(M = 4\\) items for starters: N &lt;- 7 M &lt;- 4 The \\(\\theta_i\\) distribution seems like the next-simplest part of the model, so let’s generate some ability parameters: thetas &lt;- rnorm(N) Now we need sets of parameters \\(\\alpha_m, \\beta_m, \\gamma_m\\) for every item \\(m = 1,...,M\\). Where do we get these? For a particular fixed-length test, the set of item parameters would depend on the features of the actual test questions. But we are not (yet) dealing with actual testing data, so we will need to make something up. In other words, we need an auxiliary model for these parameters. Perhaps we could just simulate some values? Arbitrarily, let’s draw the difficulty parameters from a normal distribution with mean \\(\\mu_\\alpha = 0\\) and standard deviation \\(\\tau_\\beta = 1\\). The discrimination parameters have to be greater than zero, and values near \\(\\beta_m = 1\\) make the model simplify, so let’s draw them from a gamma distribution with mean \\(\\mu_\\beta = 1\\) and standard deviation \\(\\tau_\\beta = 0.2\\). For a gamma distribution parameterized in terms of shape and rate, shape is equal to \\(\\mu_\\beta^2 \\tau_\\beta^2 = 0.2^2\\) and rate is equal to \\(\\mu_\\beta \\tau_\\beta^2 = 0.2^2\\). We will let the difficulty and discrimination parameters be independent of each other. We will also imagine that all the test questions have four possible responses, and therefore set \\(\\gamma_m = \\frac{1}{4}\\) for all the items. With that, let’s make up some item parameters: alphas &lt;- rnorm(M, mean = 0, sd = 1.5) betas &lt;- rgamma(M, shape = 0.2^2, rate = 0.2^2) gammas &lt;- rep(1 / 4, M) The three-parameter IRT model describes the probability that a given respondent with ability \\(\\theta_i\\) answers each of the items on the test correctly. To generate data based on the model, we need to produce scores on every item for every respondent, leading to a matrix of \\(N\\) respondents by \\(M\\) items. To simplify this calculation, let’s write a function to compute the item probabilities for a single respondent: r_scores &lt;- function(theta_i, alphas, betas, gammas, M) { pi_i &lt;- gammas + (1 - gammas) * plogis(alphas * (theta_i - betas)) scores &lt;- rbinom(M, size = 1, prob = pi_i) names(scores) &lt;- paste0(&quot;q&quot;,1:M) return(scores) } r_scores(thetas[1], alphas = alphas, betas = betas, gammas = gammas, M = M) ## q1 q2 q3 q4 ## 0 1 0 1 In the first line of the function, we compute the probability of correctly answering all \\(M = 12\\) items, storing the result in a vector pi_i. In the next line, we simulate binary outcomes with the specified vector of probabilities. Finally, we assign names to each item so that we can keep track of which score is for which item. Now we can run the function for every one of our respondents: map_dfr( thetas, .f = r_scores, alphas = alphas, betas = betas, gammas = gammas, M = M ) ## # A tibble: 7 × 4 ## q1 q2 q3 q4 ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 0 0 1 0 ## 2 1 1 0 0 ## 3 1 0 0 0 ## 4 1 1 1 1 ## 5 0 0 0 1 ## 6 1 0 1 1 ## 7 0 1 0 0 To make it easier to generate datasets with different characteristics, let’s now combine the chunks of code we’ve written into a single data-generating function. To do so, we’ll need to consider what the input parameters should be. We’ve seen that we need to specify \\(N\\) and \\(M\\). We made assumptions about the item parameters, but we had to specify means and standard deviations for the difficulty and discrimination parameter distributions, so let’s allow those to be inputs also. Our data-generating function will then be r_3PL_IRT &lt;- function( N, M = 4, diff_M = 0, diff_SD = 1, disc_M = 1, disc_SD = 0.2, item_options = 4 ) { # generate ability parameters thetas &lt;- rnorm(N) # generate item parameters alphas &lt;- rnorm(M, mean = diff_M, sd = diff_SD) betas &lt;- rgamma(M, shape = disc_M^2 * disc_SD^2, rate = disc_M * disc_SD^2) gammas &lt;- rep(1 / item_options, M) # simulate item responses test_scores &lt;- map_dfr( thetas, .f = r_scores, alphas = alphas, betas = betas, gammas = gammas, M = M ) # calculate total score test_scores$total &lt;- rowSums(test_scores) return(test_scores) } r_3PL_IRT(N = 7, M = 4) ## # A tibble: 7 × 5 ## q1 q2 q3 q4 total ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 1 0 0 2 ## 2 1 1 1 1 4 ## 3 1 1 1 0 3 ## 4 1 1 1 1 4 ## 5 1 1 1 0 3 ## 6 1 1 1 1 4 ## 7 1 1 1 1 4 We will now look at a much larger sample of participants, with a longer test that includes \\(M = 12\\) items: test_scores &lt;- r_3PL_IRT(N = 10000, M = 12) Having written a function for the 3-parameter logistic IRT model makes it easy to explore properties of the model. For instance, we can easily visualize the distribution of the total scores on the test: ggplot(test_scores, aes(total)) + geom_bar() + scale_x_continuous(limits = c(0,12.5), breaks = seq(0,12,2)) ## Warning: Removed 1 row containing missing values or values ## outside the scale range (`geom_bar()`). Likewise, we can examine the probability of correctly responding to each of the items by computing sample means for each item: test_scores %&gt;% summarise(across(starts_with(&quot;q&quot;), mean)) ## # A tibble: 1 × 12 ## q1 q2 q3 q4 q5 q6 q7 q8 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.628 0.62 0.629 0.256 0.539 0.620 0.624 0.620 ## # ℹ 4 more variables: q9 &lt;dbl&gt;, q10 &lt;dbl&gt;, ## # q11 &lt;dbl&gt;, q12 &lt;dbl&gt; The percentage of correct responses varies from 25.6% to 63.3. What are the correlations between individual items and the total score? Let’s check: test_scores %&gt;% summarise(across(starts_with(&quot;q&quot;), ~ cor(.x, total))) ## # A tibble: 1 × 12 ## q1 q2 q3 q4 q5 q6 q7 q8 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.154 0.386 0.0195 0.227 0.445 0.411 0.386 0.421 ## # ℹ 4 more variables: q9 &lt;dbl&gt;, q10 &lt;dbl&gt;, ## # q11 &lt;dbl&gt;, q12 &lt;dbl&gt; Note that simulating a new dataset will produce different results for these summaries because generating each dataset entails sampling a new set of items (with different difficulty and discrimination). What if the items covered a much broader range of difficulties? Or what if the items had many more options so guessing was nearly impossible? We could investigate such questions by simulating new datasets with varying values of the model parameters. 6.8 More to explore This chapter has introduced the core components of a data-generating process and demonstrated how to move from formulating a full data-generating model to implementing the model in R code. Our main aim has been to provide enough scaffolding for you to get started with simulating and exploring a variety of models. The exercises below will give you further practice in developing, testing, and extending DGP functions. Of course, we have not covered every consideration and challenge that arises when writing DGPs for simulations—there is much more to explore! We will cover some further challenges in subsequent chapters. Yet more challenges will surely arise as you apply simulations in your own work. Even though such challenges which might not align with the examples or contexts that we have covered here, the concepts and principles that we have introduced should allow you to reason about potential solutions. In doing so, we encourage you adopt the attitude of a chef in a test kitchen by trying out your ideas with code. In the kitchen, there is often no way to know how a dish will turn out until you actually bake it and taste it. Likewise, in developing a DGP, the best way to understand and evaluate a DGP is to write it out in code and explore the datasets that you can produce with it. 6.9 Exercises 6.9.1 The Welch test on a shifted-and-scaled \\(t\\) distribution The shifted-and-scaled \\(t\\)-distribution has parameters \\(\\mu\\) (mean), \\(\\sigma\\) (scale), and \\(\\nu\\) (degrees of freedom). If \\(T\\) follows a student’s \\(t\\)-distribution with \\(\\nu\\) degrees of freedom, then \\(S = \\mu + \\sigma T\\) follows a shifted-and-scaled \\(t\\)-distribution. The following function will generate random draws from this distribution (the scaling of \\((\\nu-2)/\\nu\\) is to account for a non-scaled \\(t\\)-distribution having a variance of \\(\\nu/(\\nu-2)\\)). r_tss &lt;- function(n, mean, sd, df) { mean + sd * sqrt( (df-2)/df ) * rt(n = n, df = df) } r_tss(n = 8, mean = 3, sd = 2, df = 5) ## [1] 0.02822602 3.87035901 4.51376755 ## [4] 3.02447410 -0.26265918 2.29642811 ## [7] 1.16346358 7.59634125 Modify the Welch simulation’s simulate_data() function to generate data from shifted-and-scaled \\(t\\)-distributions rather than from normal distributions. Include the degrees of freedom as an input argument. Simulate a dataset with low degrees of freedom and plot it to see if you see a few outliers. Now generate more data and calculate the means and standard deviations to see if they are correctly calibrated (i.e., generate a big dataset to ensure you get reliable mean and standard deviation estimates). Check df equal to 500, 5, 3, and 2. Once you are satisfied you have a correct DGP function, re-run the Type-I error rate calculations from the prior exercises in Section 5.5 using a \\(t\\)-distribution with 5 degrees of freedom. Do the results change substantially? 6.9.2 Plot the bivariate Poisson In Section 6.3, we provided an example of a DGP function for the bivariate Poisson model. We demonstrated a plot of data simulated from this function in 6.4. Create a similar plot but for a much larger sample size of \\(N = 1000\\). With such a large dataset, it will likely be hard to distinguish individual observations because of over-plotting. Create a better visual representation of the same simulated dataset, such as a heatmap or a contour plot. 6.9.3 Check the bivariate Poisson function Although we presented a DGP function for the bivariate Poisson model, we have not demonstrated how to check that the function is correct—we’re leaving that to you! Write some code to verify that the function r_bivariate_Poisson() is working properly. Do this by generating a very large sample (say \\(N = 10^4\\) or \\(10^5\\)) and verifying the following: The sample means of \\(C_1\\) and \\(C_2\\) align with the specified population means. The sample variances of \\(C_1\\) and \\(C_2\\) are close to the specified population means (because for a Poisson distribution \\(\\mathbb{E}(C_p) = \\mathbb{V}(C_p)\\) for \\(p = 1,2\\)). The sample correlation aligns with the specified population correlation. The observed counts \\(C_1\\) and \\(C_2\\) follow Poisson distributions. 6.9.4 Add error-catching to the bivariate Poisson function In Section 6.1, we noted that the bivariate Poisson function as we described it can only produce a constrained range of correlations, which a maximum value that depends on the ratio of \\(\\mu_1\\) to \\(\\mu_2\\). Our current implementation of the model does not handle this aspect of the model very well: r_bivariate_Poisson(5, rho = 0.6, mu1 = 4, mu2 = 12) ## Warning in rpois(N, lambda = mu1 - EZ3): NAs ## produced ## C1 C2 ## 1 NA 11 ## 2 NA 11 ## 3 NA 13 ## 4 NA 15 ## 5 NA 9 For this combination of parameter values, \\(\\rho \\times \\sqrt{\\mu_1 \\mu_2}\\) is larger than \\(\\mu_1\\), which leads to simulated values for \\(C_1\\) that are all missing. That makes it pretty hard to compute the correlation between \\(C_1\\) and \\(C_2\\). Please help us fix this issue! Revise r_bivariate_Poisson() so that it checks for allowable values of \\(\\rho\\). If the user specifies a combination of parameters that does not make sense, make the function throw an error (using R’s stop() function). 6.9.5 A bivariate negative binomial distribution One potential limitation of the bivariate Poisson distribution described above is that the variances of the counts are necessarily equal to the means (i.e., unit dispersion). This limitation is inherited from the univariate Poisson distributions that each variate follows. Just as with the corresponding univariate distributions, one way to relax this limitation is to consider distributions with marginals that are negative binomial rather than Poisson, thereby allowing for overdispersion. Cho et al. (2023) describes one type of bivariate negative binomial distribution. They provide a method for constructing a bivariate negative binomial distribution by using latent, gamma-distributed components. Their algorithm involves first generating components from gamma distributions with specified shape and scale parameters: \\[ \\begin{aligned} Z_0 &amp;\\sim \\Gamma\\left( \\alpha_0, \\beta\\right) \\\\ Z_1 &amp;\\sim \\Gamma\\left( \\alpha_1, \\beta\\right) \\\\ Z_2 &amp;\\sim \\Gamma\\left( \\alpha_2, \\beta\\right) \\end{aligned} \\] for \\(\\alpha_0,\\alpha_1,\\alpha_2 &gt; 0\\) and \\(\\beta &gt; 0\\). Then simulate independent Poisson random variables as \\[ \\begin{aligned} C_1 &amp;\\sim Pois\\left( Z_0 + Z_1 \\right) \\\\ C_2 &amp;\\sim Pois\\left( \\delta(Z_0 + Z_2) \\right). \\end{aligned} \\] The resulting count variables follow marginal negative binomial distributions with moments \\[ \\begin{aligned} \\mathbb{E}(C_1) &amp;= (\\alpha_0 + \\alpha_1) \\beta &amp; \\mathbb{V}(C_1) &amp;= (\\alpha_0 + \\alpha_1) \\beta (\\beta + 1) \\\\ \\mathbb{E}(C_2) &amp;= (\\alpha_0 + \\alpha_2) \\beta \\delta &amp; \\mathbb{V}(C_2) &amp;= (\\alpha_0 + \\alpha_2) \\beta \\delta (\\beta \\delta + 1) \\\\ &amp; &amp; \\text{Cov}(C_1, C_2) &amp;= \\alpha_0 \\beta^2 \\delta. \\end{aligned} \\] The correlation between \\(C_1\\) and \\(C_2\\) is thus \\[ \\text{cor}(C_1, C_2) = \\frac{\\alpha_0}{\\sqrt{(\\alpha_0 + \\alpha_1)(\\alpha_0 + \\alpha_2)}} \\frac{\\beta \\sqrt{\\delta}}{\\sqrt{(\\beta + 1)(\\beta \\delta + 1)}}. \\] Write a DGP function that implements this distribution. Write some code to check that the function produces data where each variate follows a negative binomial distribution and where the correlation agrees with the formula given above. Consider parameter values that produce \\(\\mathbb{E}(C_1) = \\mathbb{E}(C_2) = 10\\) and \\(\\mathbb{V}(C_1) = \\mathbb{V}(C_2) = 15\\). What are the minimum and maximum possible correlations between \\(C_1\\) and \\(C_2\\)? 6.9.6 Another bivariate negative binomial distribution Another model for generating bivariate counts with negative binomial marginal distributions is by using Gaussian copulas. Here is a mathematical recipe for this distribution, which will produce counts with marginal means \\(\\mu_1\\) and \\(\\mu_2\\) and marginal variances \\(\\mu_1 + \\mu_1^2 / p_1\\) and \\(\\mu_2 + \\mu_2^2 / p_2\\). Start by generating variates from a bivariate standard normal distribution with correlation \\(\\rho\\): \\[ \\left(\\begin{array}{c}Z_1 \\\\ Z_2 \\end{array}\\right) \\sim N\\left(\\left[\\begin{array}{c}0 \\\\ 0\\end{array}\\right], \\ \\left[\\begin{array}{cc}1 &amp; \\rho \\\\ \\rho &amp; 1\\end{array}\\right]\\right) \\] Now find \\(U_1 = \\Phi(Z_1)\\) and \\(U_1 = \\Phi(Z_1)\\), where \\(\\Phi()\\) is the standard normal cumulative distribution function (called pnorm() in R). Then generate the counts by evaluating \\(U_1\\) and \\(U_2\\) with the negative binomial quantile function, \\(F_{NB}^{-1}(x | \\mu, p)\\) with mean parameters \\(\\mu\\) and size parameter \\(p\\) (this function is called qnbinom() in R): \\[ C_1 = F_{NB}^{-1}(U_1 | \\mu_1, p_1) \\qquad C_2 = F_{NB}^{-1}(U_2 | \\mu_2, p_2). \\] The resulting counts will be correlated, but the correlation will not be equal to \\(\\rho\\). Write a DGP function that implements this distribution. Write some code to check that the function produces data where each variate follows a negative binomial distribution Use the function to create a graph showing the population correlation between the observed counts as a function of \\(\\rho\\). Use \\(\\mu_1 = \\mu_2 = 10\\) and \\(p_1 = p_2 = 20\\). How does the range of correlations compare to the range from Exercise 6.9.5? 6.9.7 Plot the data from a cluster-randomized trial Run gen_cluster_RCT() with parameter values of your choice to produce data from a simulated cluster-randomized trial. Create a plot of the data that illustrates how student-level observations are nested within schools and how schools are assigned to different treatment conditions. 6.9.8 Checking the Cluster RCT DGP What is the variance of the outcomes generated by the model for the cluster-randomized trial if there are no treatment effects? (Try simulating data to check!) What other quick checks can you run on this DGP to make sure it is working correctly? In gen_cluster_RCT() we have the following line of code to generate the number of individuals per site. nj &lt;- sample( n_min:n_max, J, replace=TRUE ) This code has an error. Generate a variety of datasets where you vary n_min, n_max and J to discover the error. Then repair the code. Checking your data generating process across a range of scenarios is extremely important. 6.9.9 More school-level variation The DGP for the cluster-randomized trial allows for school-level treatment impact variation, but only to the extent that the variation is explained by school size. How could you modify your simulation to allow for school-level treatment impact variation that is not related to school size? Implement this change and generate some data to show how it works. 6.9.10 Cluster-randomized trial with baseline predictors Extend the DGP for the cluster-randomized trial to include an individual-level covariate \\(X\\) that is correlated with the outcome. Do this by modifying the model for student-level outcomes as \\[ Y_{ij} = \\beta_{0j} + \\beta_{1} X_{ij} + \\epsilon_{ij}. \\] Keep the same \\(\\beta_1\\) for all sites. To implement this model as a DGP, you will have to decide how to generate \\(X_{ij}\\). Use words and equations to explain your auxiliary model for \\(X_{ij}\\). Implement the model by modifying gen_cluster_RCT() accordingly. Use your implementation to find the unconditional variance of the outcome, \\(\\text{Var}(Y | Z_j = 0)\\), when \\(\\beta_1 = 0.6\\). 6.9.11 Random effects meta-regression Meta-analysis involves working with quantitative findings reported by previous studies on a particular topic, in the form of effect size estimates and their standard errors, to generate integrative summaries and identify patterns in the findings that might not be evident from any previous study considered in isolation. One model that is widely used in meta-analysis is the random effects meta-regression, which relates the effect sizes to known characteristics of the studies that are encoded in the form of predictors. Consider a collection of \\(K\\) studies. Let \\(T_i\\) denote an effect size estimate and \\(s_i\\) denote its standard error for study \\(i\\). Let \\(x_{i}\\) be a quantitative predictor variable that represents some characteristic of study \\(i\\) (such as its year of publication). The random effects meta-regression model assumes \\[ T_i = \\beta_0 + \\beta_1 x_i + u_i + e_i, \\] where \\(u_i \\sim N(0, \\tau^2)\\) and \\(e_i \\sim N(0, s_i^2)\\). In this model, \\(\\beta_0\\) corresponds to the expected effect size when \\(x_i = 0\\) and \\(\\beta_1\\) describes the expected difference in effect size per one unit difference in \\(x_i\\). The first error \\(u_i\\) corresponds to the heterogeneity in the effect sizes above and beyond the variation explained by \\(x_i\\), and the second error \\(e_i\\) corresponds to the sampling error, which we assume has known variance \\(s_i^2\\). Of the variables in the random effects meta-regression model, which are structural features, which are covariates, and which are outcomes? Of the parameters described above, which would you classify as focal, which as auxiliary, and which as design parameters? In addition to the focal model described above, what further assumptions or auxiliary models will be needed in order to simulate data for a random effects meta-regression? Propose an auxiliary model for any needed quantities. Using your proposed auxiliary model, write a function to generate data for a random effects meta-regression. Demonstrate your function by creating a plot based on a simulated dataset with \\(K = 30\\) effect sizes. Write code to check the properties of your data-generating function. 6.9.12 Meta-regression with selective reporting Vevea and Hedges (1995) proposed a meta-regression model that allows for the possibility that not all primary study results are published. They assume that primary study results are generated according to the random effects meta-regression model described in Exercise 6.9.11, but then only a subset of results are observed, where the probability of being included is a function of the result’s one-sided \\(p\\)-value for the null hypothesis \\(H_0: \\delta \\leq 0\\) against alternative \\(H_A: \\delta &gt; 0\\). Let \\(p_i = 1 - \\Phi(T_i / s_i)\\) be the one-sided \\(p\\)-value for study result \\(i\\), where \\(Phi()\\) is the standard normal cumulative distribution (pnorm() in R). In one version of the selection model, the selection probability follows a piece-wise constant distribution with \\[ \\text{Pr}(T_i \\text{ is observed}) = \\begin{cases} 1 &amp; \\text{if} \\quad 0 \\leq p_i &lt; .025 \\\\ \\lambda_1 &amp; \\text{if} \\quad .025 \\leq p_i &lt; .500 \\\\ \\lambda_2 &amp; \\text{if} \\quad .500 \\leq p_i &lt; 1 \\end{cases} \\] for selection probabilities \\(0 \\leq \\lambda_1 \\leq 1\\) and \\(0 \\leq \\lambda_2 \\leq 1\\). Modify your data-generating function from Exercise 6.9.11 to follow the Vevea-Hedges selection model. Ensure that the new data-generating function returns a total of \\(K\\) primary study results. Use the new function to generate a large number of effect size estimates, all with \\(s_i = 0.35\\), using parameters \\(\\beta_0 = 0.1\\), \\(\\beta_1 = 0.0\\), \\(\\tau = 0.1\\), \\(\\lambda_1 = 0.5\\), and \\(\\lambda_2 = 0.2\\). Plot the distribution of observed effect size estimates. Create several further plots using different values for \\(\\lambda_1\\) and \\(\\lambda_2\\). How do these parameters affect the shape of the distribution? Use the selmodel() function from the metafor package to estimate the Vevea-Hedges selection model based on one of your simulated datasets. (See Exercise 7.5.6 for example syntax.) How do the estimates compare to the model parameters you’ve specified? References Bloom, Howard S., Stephen W. Raudenbush, Michael J. Weiss, and Kristin Porter. 2016. “Using Multisite Experiments to Study Cross-Site Variation in Treatment Effects: A Hybrid Approach With Fixed Intercepts and a Random Treatment Coefficient.” Journal of Research on Educational Effectiveness 10 (4): 0–0. https://doi.org/10.1080/19345747.2016.1264518. Cho, Hunyong, Chuwen Liu, John S Preisser, and Di Wu. 2023. “A Bivariate Zero-Inflated Negative Binomial Model and Its Applications to Biomedical Settings.” Statistical Methods in Medical Research 32 (7): 1300–1317. https://doi.org/10.1177/09622802231172028. Miratrix, Luke W., Michael J. Weiss, and Brit Henderson. 2021. “An Applied Researcher’s Guide to Estimating Effects from Multisite Individually Randomized Trials: Estimands, Estimators, and Estimates.” Journal of Research on Educational Effectiveness 14 (1): 270–308. https://doi.org/10.1080/19345747.2020.1831115. Vevea, Jack L, and Larry V Hedges. 1995. “A General Linear Model for Estimating Effect Size in the Presence of Publication Bias.” Psychometrika 60 (3): 419–35. https://doi.org/10.1007/BF02294384. An alternative definition is based on the pooled standard deviation, but this is usually a bad choice if one suspects treatment variation. More treatment variation should not reduce the effect size for the same absolute average impact.↩︎ "],["data-analysis-procedures.html", "Chapter 7 Data analysis procedures 7.1 Writing estimation functions 7.2 Including Multiple estimation procedures 7.3 Validating Estimation Procedures 7.4 Handling errors, warnings, and other hiccups 7.5 Exercises", " Chapter 7 Data analysis procedures The overall aims of many simulation studies have to do with understand how a particular data-analysis procedure works or comparing the performance of multiple, competing procedures. Thus, the data-analysis procedure or procedures are the central object of study. Depending on the research question, the data-analysis procedure might be very simple—as simple as just computing a sample correlation–or it might involve a combination of several components. For example, the procedure might entail first computing a diagnostic test for heteroskedasticity and then, depending on the outcome of the test, applying either a conventional formula or a heteroskedasticity-robust formula for standard errors. As another example, a data-analysis procedure might involve using multiple imputation for missingness on key variables, then fitting a statistical model, and then generating predicted values based on the model. Also depending on the research question, we might need to create several functions that implement different estimation procedures to be compared. In this chapter, we demonstrate how to implement data-analysis procedures in the form of R functions, so that their performance can be evaluated by repeatedly applying them to artificial data. We start by describing the high-level design of estimation functions and examining some simple but illustrative examples. We then discuss approaches for writing simulations that involve multiple estimation procedure. Next, we describe strategies for validating estimation functions. Finally, we examine methods for handling common computational problems with estimation procedures, such as non-convergence of maximum likelihood estimators. 7.1 Writing estimation functions In the abstract, a function that implements an estimation procedure should have the following form: estimate &lt;- function(data) { # calculations/model-fitting/estimation procedures return(estimates) } The function takes a dataset as input, fits a model or otherwise calculates an estimate, possibly with associated standard errors and so forth, and returns these quantities as output. The estimates could be point estimates of parameters, standard errors, confidence intervals, p-values, predictions, or other quantities. The calculations in the body of the function should be set up to use datasets that have the same structure (i.e., same dimensions, same variable names) as the output of the corresponding function for generating simulated data. However, in principle, we should also be able to run the estimation function on real data as well. In Chapter 5 we wrote a function called ANOVA_Welch_F() for computing \\(p\\)-values from two different procedures for testing equality of means in a heteroskedastic ANOVA: ANOVA_Welch_F &lt;- function(data) { anova_F &lt;- oneway.test(x ~ group, data = data, var.equal = TRUE) Welch_F &lt;- oneway.test(x ~ group, data = data, var.equal = FALSE) result &lt;- tibble( ANOVA = anova_F$p.value, Welch = Welch_F$p.value ) return(result) } Apply this function to a simulated dataset returns two p-values, one for the usual ANOVA \\(F\\) test (which assumes homoskedasticity) and one for Welch’s heteroskedastic \\(F\\) test: sim_data &lt;- generate_ANOVA_data( mu = c(1, 2, 5, 6), sigma_sq = c(3, 2, 5, 1), sample_size = c(3, 6, 2, 4) ) ANOVA_Welch_F(sim_data) ## # A tibble: 1 × 2 ## ANOVA Welch ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.000293 0.0179 Our ANOVA_Welch_F() function is designed to work with the output of generate_ANOVA_data() in that it assumes that the grouping variable is called group and the outcome is called x. Relying on this assumption would be a poor choice if we were designing a function as part of an R package or for general-purpose use. However, because the primary use of the function is for simulation, it is reasonable to assume that the input data will always have appropriate variable names. In Chapter 6, we looked at a data-generating function for a bivariate Poisson distribution, an example of a non-normal bivariate distribution. We might use such a distribution to understand the behavior of Pearson’s sample correlation coefficient and its normalizing transformation, known as Fisher’s \\(z\\)-transformation, which is equivalent to the hyperbolic arc-tangent function (atanh() in R). When the sample measurements follow a bivariate normal distribution, Fisher’s \\(z\\)-transformed correlation is very close to normally distributed and its standard error is simply \\(1 / \\sqrt{N - 3}\\), and thus independent of the correlation. This makes \\(z\\)-transformation very useful for computing confidence intervals, which can then be back-transformed to the Pearson-\\(r\\) scale. In this problem, a simple estimation function would take a dataset with two variables as input and compute the sample correlation and its \\(z\\)-transformation, compute confidence intervals for \\(z\\), and then back-transform the confidence interval end-points. Here is an implementation of these calculations: r_and_z &lt;- function(data) { r &lt;- cor(data$C1, data$C2) z &lt;- atanh(r) se_z &lt;- 1 / sqrt(nrow(data) - 3) ci_z &lt;- z + c(-1, 1) * qnorm(.975) * se_z ci_r &lt;- tanh(ci_z) tibble( r = r, z = z, CI_lo = ci_r[1], CI_hi = ci_r[2] ) } To check that the function returns a result of the expected form, we generate a small dataset using the r_bivariate_Poisson() function developed in the last chapter, then apply our estimation function to the result: Pois_dat &lt;- r_bivariate_Poisson(40, rho = 0.5, mu1 = 4, mu2 = 4) r_and_z(Pois_dat) ## # A tibble: 1 × 4 ## r z CI_lo CI_hi ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.557 0.628 0.296 0.740 Although it is a little cumbersome to do so, we could also apply the estimation function to a real dataset. Here is an example, which calculates the correlation between ratings of judicial integrity and familiarity with the law from the USJudgeRatings dataset (which is included in base R). For the function to work on this dataset, we first need to rename the relevant variables. data(USJudgeRatings) USJudgeRatings %&gt;% dplyr::select(C1 = INTG, C2 = FAMI) %&gt;% r_and_z() ## # A tibble: 1 × 4 ## r z CI_lo CI_hi ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.869 1.33 0.769 0.927 The function returns a valid result—a quite strong correlation! It is a good practice to test out a newly-developed estimation function on real data as a check that it is working as intended. This type of test ensures that the estimation function is not using information outside of the dataset, such as by using known parameter values to construct an estimate. Applying the function to a real dataset demonstrates that the function implements a procedure that could actually be applied in real data analysis contexts. 7.2 Including Multiple estimation procedures Many simulations involve head-to-head comparisons between more than one data-analysis procedure. As a design principle, we generally recommend writing different functions for each estimation method one is planning on evaluating. Doing so makes it easier to add in additional methods as desired or to focus on just a subset of methods. Writing separate function also leads to a code base that is flexible and useful for other purposes (such as analyzing real data). Finally (repeating one of our favorite mantras), separating functions makes debugging easier because it lets you focus attention on one thing at a time, without worrying about how errors in one area might propagate to others. To see how this works in practice, we will return to the case study from Section 6.6, where we developed a data-generating function for simulating a cluster-randomized trial with student-level outcomes but school-level treatment assignment. Our data-generating process allowed for varying school sizes and heterogeneous treatment effects, which might be correlated with school size. Several different procedures might be used to estimate an overall average effect from a clustered experiment, including: Estimating a multi-level regression model (also known as a hierarchical linear model), Estimating an ordinary least squares (OLS) regression model and applying cluster-robust standard errors, or Averaging the outcomes by school, then estimating a linear regression model on the mean outcomes. All three of these methods are are widely used and have some theoretical guarantees supporting their use. Education researchers tend to be more comfortable using multi-level regression models, whereas economists tend to use OLS with clustered standard errors. Let’s develop estimation functions for each of these procedures. For now we won’t worry about incorporating the school size covariate, but will instead focus only on getting a point estimate, standard error, and \\(p\\)-value for the average treatment effect. For starters, we generate a sample dataset using a revised version of gen_cluster_RCT(), which corrects the bug discussed in Exercise 6.9.8: dat &lt;- gen_cluster_RCT( J=16, n_bar = 30, alpha = 0.8, p = 0.5, gamma_0 = 0, gamma_1 = 0.2, gamma_2 = 0.2, sigma2_u = 0.4, sigma2_e = 0.6 ) For the multi-level modeling strategy, there are several different existing packages that we could use. We will implement an estimator using the popular lme4 package, along with the lmerTest function for computing a \\(p\\)-value for the average effect. Here is a basic implementation: analysis_MLM &lt;- function( dat ) { M1 &lt;- lme4::lmer( Yobs ~ 1 + Z + (1 | sid), data = dat ) M1_test &lt;- lmerTest::as_lmerModLmerTest(M1) M1_summary &lt;- summary(M1_test)$coefficients tibble( ATE_hat = M1_summary[&quot;Z&quot;,&quot;Estimate&quot;], SE_hat = M1_summary[&quot;Z&quot;,&quot;Std. Error&quot;], p_value = M1_summary[&quot;Z&quot;, &quot;Pr(&gt;|t|)&quot;] ) } The function fits a multi-level model with a fixed coefficient for the treatment indicator and random intercepts for each school. To get a p-value for the treatment coefficient, we have to convert the model into an lmerModLmerTest object and then pass it through summary(). The function outputs only the statistics in which we are interested. Our function makes use of the lme4 and lmerTest packages. Rather than assuming that these packages will be loaded, we call relevant functions using the package name as a prefix, as in lme4::lmer(). This way, we can run the function even if we have not loaded the packages in the global environment. This approach is also preferable to loading packages inside the function itself (e.g., with require(lme4)) because calling the function does not change which packages are loaded in the global environment. Here is a function implementing OLS regression with cluster-robust standard errors: analysis_OLS &lt;- function( dat, se_type = &quot;CR2&quot; ) { M2 &lt;- estimatr::lm_robust( Yobs ~ 1 + Z, data = dat, clusters = sid, se_type = se_type ) tibble( ATE_hat = M2$coefficients[[&quot;Z&quot;]], SE_hat = M2$std.error[[&quot;Z&quot;]], p_value = M2$p.value[[&quot;Z&quot;]] ) } To get cluster-robust standard errors, we use the lm_robust() function from the estimatr() package, again calling only the relevant function using the package prefix rather than loading the whole package. A novel aspect of this estimation function is that it includes an additional intput argument, se_type, which allows us to control the type of standard error calculated by lm_robust(). Adding this option would let us use the same function to compute (and compare) different types of clustered standard errors for the average treatment effect estimate. We set a default option of \"CR2\", just like the default of lm_robust(). Finally, here is a function implementing the aggregate-then-analyze approach: analysis_agg &lt;- function( dat, se_type = &quot;HC2&quot; ) { datagg &lt;- dplyr::summarise( dat, Ybar = mean( Yobs ), n = n(), .by = c(sid, Z) ) stopifnot( nrow( datagg ) == length(unique(dat$sid) ) ) M3 &lt;- estimatr::lm_robust( Ybar ~ 1 + Z, data = datagg, se_type = se_type ) tibble( ATE_hat = M3$coefficients[[&quot;Z&quot;]], SE_hat = M3$std.error[[&quot;Z&quot;]], p_value = M3$p.value[[&quot;Z&quot;]] ) } Note the stopifnot command. Putting assert statements in your code like this is a good way to guarantee you are not introducing weird and hard-to-track errors in your code. For example, R likes to recycle vectors to make them the right length; if you gave it a wrong length in error, this can be a brutal error to discover. The stopifnot statements halt your code as soon as something goes wrong, rather than letting that initial wrongness flow on to further work, showing up in odd results that you don’t understand later on. See Section ?? for more. All of our functions produce output in the same format: analysis_MLM( dat ) ## # A tibble: 1 × 3 ## ATE_hat SE_hat p_value ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -0.111 0.323 0.737 analysis_OLS( dat ) ## # A tibble: 1 × 3 ## ATE_hat SE_hat p_value ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -0.177 0.307 0.576 analysis_agg( dat ) ## # A tibble: 1 × 3 ## ATE_hat SE_hat p_value ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -0.0818 0.339 0.813 Ensuring that the output of all the functions is structured in the same way will make it easy to keep the results organized once we start running multiple iterations of the simulation. If each estimation method returns a dataset with the same variables, we can simply stack the results on top of each other. Here is a function that bundles all the estimation procedures together: estimate_Tx_Fx &lt;- function( data, CR_se_type = &quot;CR2&quot;, agg_se_type = &quot;HC2&quot; ) { dplyr::bind_rows( MLM = analysis_MLM( dat ), OLS = analysis_OLS( dat, se_type = CR_se_type), agg = analysis_agg( dat, se_type = agg_se_type), .id = &quot;estimator&quot; ) } estimate_Tx_Fx(dat) ## # A tibble: 3 × 4 ## estimator ATE_hat SE_hat p_value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 MLM -0.111 0.323 0.737 ## 2 OLS -0.177 0.307 0.576 ## 3 agg -0.0818 0.339 0.813 This is a common coding pattern for in simulations that involve multiple estimation procedures. Each procedure is expressed in its own function, then these are assembled together in a single function so that they can all easily be applied to the same dataset. Stacking the results row-wise will make it easy to compute performance measures on the results. This will become more evident once we are working across multiple replications of the simulation process, as we will in Chapter 8. 7.3 Validating Estimation Procedures Just as with data-generating functions, it is critical to verify the accuracy of the estimation functions. If an estimation function involves a known procedure that has been implemented in R or one of its contributed packages, then a straightforward way to do this is to compare your implementation to another existing implementation. For estimation functions that involve multi-step procedures or novel methods, other approaches to verification may be needed, which rely more on statistical theory. 7.3.1 Checking against existing implementations For our Welch test function, we can check the output of ANOVA_Welch_F() against the built-in oneway.test function. Let’s do that with a fresh set of data: sim_data &lt;- generate_ANOVA_data( mu = c(1, 2, 5, 6), sigma_sq = c(3, 2, 5, 1), sample_size = c(3, 6, 2, 4) ) aov_results &lt;- oneway.test(x ~ factor(group), data = sim_data, var.equal = FALSE) aov_results ## ## One-way analysis of means (not assuming ## equal variances) ## ## data: x and factor(group) ## F = 28.367, num df = 3.0000, denom df = ## 3.3253, p-value = 0.007427 Welch_results &lt;- ANOVA_Welch_F(sim_data) all.equal(aov_results$p.value, Welch_results$Welch) ## [1] TRUE We use all.equal() because it will check equality up to a tolerance in R, which can avoid some perplexing errors due to rounding. For the bivariate correlation example, we can check the output of r_and_z() against R’s built-in cor.test() function: Pois_dat &lt;- r_bivariate_Poisson(15, rho = 0.6, mu1 = 14, mu2 = 8) my_result &lt;- r_and_z(Pois_dat) |&gt; subset(select = -z) my_result ## # A tibble: 1 × 3 ## r CI_lo CI_hi ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.503 -0.0125 0.807 R_result &lt;- cor.test(~ C1 + C2, data = Pois_dat) R_result &lt;- tibble(r = R_result$estimate[[&quot;cor&quot;]], CI_lo = R_result$conf.int[1], CI_hi = R_result$conf.int[2]) R_result ## # A tibble: 1 × 3 ## r CI_lo CI_hi ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.503 -0.0125 0.807 all.equal(R_result, my_result) ## [1] TRUE This type of test is even more useful here because r_and_z() uses our own implementation of the confidence interval calculations, rather than relying on R’s built-in functions as we did with ANOVA_Welch_F(). 7.3.2 Checking novel procedures Simulations are usually an integral part of projects to develop novel statistical methods. Checking estimation functions in such projects presents a challenge: if an estimation procedure truly is new, how do you check that your code is correct? Effective methods for doing so will vary from problem to problem, but an over-arching strategy is to use theoretical results about the performance of the estimator to check that your implementation works as expected. For instance, we might work out the algebraic properties of an estimator for a special case and then check that the result of the estimation function agrees with our algebra. For some estimation problems, we might be able to identify theoretical properties of an estimator when applied to a very large sample of data and when the model is correctly specified. If we can find results about large-sample behavior, then we can test an estimation function by applying it to a very large sample and checking whether the resulting estimates and very close to specified parameter values. We illustrate each of these approaches using our functions for estimating treatment effects from cluster-randomized trials. We start by testing an algebraic property. With each of the three methods we have implemented, the treatment effect estimator is a difference between the weighted average of the outcomes from students in each treatment condition; the only difference between the estimators is in what weights are used. In the special case where all schools have the same number of students, the weights used by all three methods end up being the same: all three methods allocate equal weight to each school. Therefore, we know that there should be no difference between the three point estimates. Furthermore, a bit of algebra will show that the cluster-robust standard error from the OLS approach will end up being identical to the robust standard error from the aggregation approach. If there are also equal numbers of schools assigned to both conditions, then the standard error from the multilevel model will also be identical to the other standard errors. Let’s verify that our estimation functions produce results that are consistent with these theoretical properties. To do so, we will need to generate a dataset with equal cluster sizes, setting \\(\\alpha = 0\\): dat &lt;- gen_cluster_RCT( J=12, n_bar = 30, alpha = 0, p = 0.5, gamma_0 = 0, gamma_1 = 0.2, gamma_2 = 0.2, sigma2_u = 0.4, sigma2_e = 0.6 ) table(dat$sid) # verify equal-sized clusters ## ## 1 2 3 4 5 6 7 8 9 10 11 12 ## 30 30 30 30 30 30 30 30 30 30 30 30 estimate_Tx_Fx(dat) ## # A tibble: 3 × 4 ## estimator ATE_hat SE_hat p_value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 MLM -0.525 0.366 0.183 ## 2 OLS -0.525 0.366 0.183 ## 3 agg -0.525 0.366 0.183 All three methods yield identical results. Now let’s try equal school sizes but unequal allocation to treatment: dat &lt;- gen_cluster_RCT( J=12, n_bar = 30, alpha = 0, p = 2 / 3, gamma_0 = 0, gamma_1 = 0.2, gamma_2 = 0.2, sigma2_u = 0.4, sigma2_e = 0.6 ) estimate_Tx_Fx(dat) ## # A tibble: 3 × 4 ## estimator ATE_hat SE_hat p_value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 MLM 0.189 0.547 0.737 ## 2 OLS 0.189 0.413 0.663 ## 3 agg 0.189 0.413 0.657 As expected, all three point estimators match, but the SE from the multilevel model is a little bit discrepant from the others. We can also use large-sample theory to check the multilevel modeling estimator. If the model is correctly specified, then all the parameters of the model should be accurately estimated if the model is fit to a very large sample of data. To check this property, we will need access to the full model output, not just the selected results returned by analysis_MLM(). One way to handle this is to make a small tweak to the estimation function, adding an option to control whether to return the entire model or just selected results. Here is the tweaked function: analysis_MLM &lt;- function( dat, all_results = FALSE) { M1 &lt;- lme4::lmer( Yobs ~ 1 + Z + (1 | sid), data = dat ) M1_test &lt;- lmerTest::as_lmerModLmerTest(M1) if (all_results) { return(summary(M1_test)) } M1_summary &lt;- summary(M1_test)$coefficients tibble( ATE_hat = M1_summary[&quot;Z&quot;,&quot;Estimate&quot;], SE_hat = M1_summary[&quot;Z&quot;,&quot;Std. Error&quot;], p_value = M1_summary[&quot;Z&quot;, &quot;Pr(&gt;|t|)&quot;] ) } Setting all_results to TRUE will return the entire function; keeping it at the default value of FALSE will return the same output as the other functions. Now let’s apply the estimation function to a very large dataset, with variation in cluster sizes. We set gamma_2 = 0 so that the estimation model is correctly specified: dat &lt;- gen_cluster_RCT( J=5000, n_bar = 20, alpha = 0.9, p = 2 / 3, gamma_0 = 2, gamma_1 = 0.30, gamma_2 = 0, sigma2_u = 0.4, sigma2_e = 0.6 ) analysis_MLM(dat, all_results = TRUE) ## Linear mixed model fit by REML. t-tests use ## Satterthwaite&#39;s method [lmerModLmerTest] ## Formula: Yobs ~ 1 + Z + (1 | sid) ## Data: dat ## ## REML criterion at convergence: 242678 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -4.3080 -0.6581 0.0007 0.6594 4.2134 ## ## Random effects: ## Groups Name Variance Std.Dev. ## sid (Intercept) 0.3972 0.6302 ## Residual 0.6030 0.7765 ## Number of obs: 98772, groups: sid, 5000 ## ## Fixed effects: ## Estimate Std. Error df ## (Intercept) 2.017e+00 1.636e-02 4.969e+03 ## Z 2.838e-01 2.003e-02 4.964e+03 ## t value Pr(&gt;|t|) ## (Intercept) 123.29 &lt;2e-16 *** ## Z 14.17 &lt;2e-16 *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) ## Z -0.817 The intercept and treatment effect coefficient estimates are very close to their true parameter values, as are the estimated school-level variance and student-level residual variance. This all gives some assurance that the analysis_MLM() function is working properly. Of course, it is important to bear in mind that these tests are only partial verifications. With the algebraic test, we’ve checked that the functions seem to be working properly for scenarios with equal school sizes, but they still might have errors that only appear when school sizes vary. Likewise, analysis_MLM() seems to be working properly for very large datasets, but our test does not rule out the possibility of bugs that only crawl out when \\(J\\) is small. Our large-sample test also relies on the correctness of the gen_cluster_RCT() function; if we had seen a discrepancy between parameters and estimates from the multilevel model, it could have been because of a problem with the data-generation function rather than with the estimation function. These limitations are typical of what can be accomplished through tests based on theoretical results, because theoretical results typically only hold under specific conditions. After all, if we had comprehensive theoretical results, we would not need to simulate anything in the first place! Nonetheless, it is good to work through such tests to the extent that relevant theory is available for the problem you are studying. 7.3.3 Checking with simulations Checking, debugging, and revising should not be limited to when you are initially developing estimation functions. It often happens that later steps in the process of conducting a simulation will reveal problems with the code for earlier steps. For instance, once you have run the data-generating and estimation steps repeatedly, calculated performance summaries, and created some graphs of the results, you might find an unusual or anomolous pattern in the performance of an estimator. This might be a legitimate result—it might be that the estimator really does behave weirdly or not work well—or it might be due to a problem in how you implemented the estimator or data-generating process. When faced with an unusual pattern, we recommend revisiting the estimation code to double check for bugs and also thinking further about what might lead to the anomoly. Further exploration might lead you to a deeper understanding of how a method works and perhaps even an idea for how to improve the estimator or refine the data-generating process. A good illustration of this process comes from one of Luke’s past research projects, in which he and other co-authors were working on a way to improve Instrumental Variable (IV) estimation using post-stratification. The method they studied involved grouping units based on a covariate that predicts compliance status, then calculating estimates within each group, then summarizing the estimates across groups. They used simulations to see whether this method would improve the accuracy of the overall summary effect estimate. In the first simulation, the estimates were full of NAs and odd results because the estimation function failed to account for what happens in groups of observations where the number of compliers was estimated to be zero. After repairing that problem and re-running everything, the simulation results still indicated serious and unexpected bias, which turned out to be due to an error in how the estimation function implemented the step of summarizing estimates across groups. After again correcting and re-running, the simulation results showed that the gains in accuracy from this new method were minimal, even when the groups were formed based on a variable that was almost perfectly predictive of compliance status. Eventually, we understood that the groups with very few compliers produced such unstable estimates that they spoiled the overall average estimate. This inspired us to revise our estimation strategy and introduce a method that dropped or down-weighted strata with few compliers, which ultimately helped us to strengthen the contribution of our work. As this experience highlights, simulations seldom follow a single, well-defined trajectory. The point of conducting simulations is to help us, as researchers, learn about estimation methods so that we can make progress with analysis of real data. What we learn from simulation gives us a better understanding of the methods (potentially including a better understanding of theoretical results), leading to ideas about better methods or new scenarios to explore in further simulations. Of course, at some point one needs to step off this merry-go-round, write up the findings, cook dinner, and clean the bathroom. But, just like many other research endeavors, simulations follow a highly iterative process. 7.4 Handling errors, warnings, and other hiccups Especially when working with more advanced estimation methods, it is possible that your estimation function will fail, throw an error, or return something uninterpretable for certain input datasets. For instance, maximum likelihood estimation often requires iterative, numerical optimization algorithms that sometimes fail to converge. This might happen rarely enough that it takes a while to even notice that it is a problem, but even quite rare things can occur when you run simulations with many thousands of repetitions. Less dire but still annoying, your estimation function might generate warnings, which can pile up if you are running many repetitions. In some cases, such warnings might also signal that the estimator produced a bad result, and it may not be clear whether we should retain this result (or include it in overall performance assessments). After all, the function tried to warn us that something is off! Errors and warnings in estimation functions pose two problems, one purely technical and one conceptual. On a technical level, R functions stop running if they hit errors (though not warnings), so we need ways to handle the errors in order to get our simulations up and running. On a conceptual level, we need to decide how to use the information contained in errors and warnings, whether that be by further elaborating the estimation procedures to address different contingencies or by evaluating the performance of the estimators in a way that appropriately accounts for errors. We consider each of the problems here, then revisit the conceptual considerations in Chapter ??. 7.4.1 Capturing errors and warnings Some estimation functions will require complicated or stochastic calculations that can sometimes produce errors. Intermittent errors can really be annoying and time-consuming if not addressed. To protect yourself, it is good practice to anticipate potential errors, preventing them from stopping code execution and allowing your simulations to keep running. We will demonstrate some techniques for error-handling using tools from the purrr package. For illustrative purposes, consider the following error-prone function that sometimes returns what we want, sometimes returns NaN due to taking the square root of a negative number, and sometimes crashes completely because broken_code() does not exist: my_complex_function = function( param ) { vals = rnorm( param, mean = 0.5 ) if ( sum( vals ) &gt; 5 ) { broken_code( 4 ) } else { sqrt( sum( vals ) * sign( vals )[[1]] ) } } Running it produces some results and an occasional warning, and some errors: set.seed(3) my_complex_function( 1 ) ## [1] 0.6796568 my_complex_function( 10 ) ## [1] 2.132087 my_complex_function( 5 ) ## Warning in sqrt(sum(vals) * sign(vals)[[1]]): ## NaNs produced ## [1] NaN Running it many times produces warnings, then an error: resu &lt;- replicate(20, my_complex_function( 7 )) ## Warning in sqrt(sum(vals) * sign(vals)[[1]]): ## NaNs produced ## Warning in sqrt(sum(vals) * sign(vals)[[1]]): ## NaNs produced ## Error in broken_code(4): could not find function &quot;broken_code&quot; The purrr package includes a function called safely that makes it easy to trap errors. To use it, we feed the estimation function into safely() to create a new version: my_safe_function &lt;- safely( my_complex_function, otherwise = NA ) my_safe_function( 7 ) ## $result ## [1] 2.175561 ## ## $error ## NULL The safe version of the function returns a list with two entries: the result (or NULL if there was an error), and the error message (or NULL if there was no error). safely() is an example of a functional (or an abverb), which takes a function and returns a new function that does something slightly different. We include otherwise = NA so we always get a result, rather than a NULL when there is an error. We can use the safe function repeatedly and it will always return a result: resu &lt;- replicate(20, my_safe_function( 7 ), simplify = FALSE) ## Warning in sqrt(sum(vals) * sign(vals)[[1]]): ## NaNs produced ## Warning in sqrt(sum(vals) * sign(vals)[[1]]): ## NaNs produced ## Warning in sqrt(sum(vals) * sign(vals)[[1]]): ## NaNs produced ## Warning in sqrt(sum(vals) * sign(vals)[[1]]): ## NaNs produced ## Warning in sqrt(sum(vals) * sign(vals)[[1]]): ## NaNs produced resu &lt;- transpose( resu ) unlist(resu$result) ## [1] 1.3870195 0.5638654 NA 1.6995292 ## [5] NA NA NaN 2.2121710 ## [9] NA NaN NaN 1.8801925 ## [13] NaN 1.9154618 NaN NA ## [17] 2.2245636 NA NA 0.8854747 The transpose() function takes a list of lists, and reorganizes them to give you a list of all the first elements, a list of all the second elements, etc. This is very powerful for wrangling data, because then we can make a tibble with list columns as so: tb &lt;- tibble( result = unlist( resu$result ), error = resu$error ) head( tb, n = 4 ) ## # A tibble: 4 × 2 ## result error ## &lt;dbl&gt; &lt;list&gt; ## 1 1.39 &lt;NULL&gt; ## 2 0.564 &lt;NULL&gt; ## 3 NA &lt;smplErrr&gt; ## 4 1.70 &lt;NULL&gt; The purrr package includes several other functionals that are useful for handling errors and warnings. The possibly() wrapper will try to run a function and will return a specified value in the event of an error: my_possible_function &lt;- possibly( my_complex_function, otherwise = NA ) my_possible_function( 7 ) ## [1] 1.506734 rs &lt;- replicate(20, my_possible_function( 7 )) ## Warning in sqrt(sum(vals) * sign(vals)[[1]]): ## NaNs produced ## Warning in sqrt(sum(vals) * sign(vals)[[1]]): ## NaNs produced ## Warning in sqrt(sum(vals) * sign(vals)[[1]]): ## NaNs produced rs ## [1] NA 0.7023915 0.9495728 NA ## [5] NA 1.8784947 0.9838187 NA ## [9] NA 1.7676572 1.4809897 NA ## [13] 2.1019082 NA NaN NaN ## [17] 1.8629289 1.3467997 NaN 1.3017348 It works as a simpler version of safely(), which does not record error messages. The quietly functional leads to results that are bundled together with any console output, warnings, and messages, rather than printing anything to the console: my_quiet_function &lt;- quietly( my_complex_function ) my_quiet_function( 1 ) ## $result ## [1] 0.1724504 ## ## $output ## [1] &quot;&quot; ## ## $warnings ## character(0) ## ## $messages ## character(0) This can be especially useful to reduce extraneous printing in a simulation, which can slow down code execution more than you might expect. However, quietly() does not trap errors: rs &lt;- replicate(20, my_quiet_function( 7 )) ## Error in broken_code(4): could not find function &quot;broken_code&quot; Double-wrapping your function will handle both errors and warnings, but the structure it produces gets a bit complicated: my_safe_quiet_function &lt;- quietly( safely( my_complex_function, otherwise = NA ) ) my_safe_quiet_function(7) ## $result ## $result$result ## [1] NA ## ## $result$error ## &lt;simpleError in broken_code(4): could not find function &quot;broken_code&quot;&gt; ## ## ## $output ## [1] &quot;&quot; ## ## $warnings ## character(0) ## ## $messages ## character(0) Even though the result is a bit of a mess, this structure provides all the pieces that we need to do further calculations on the result (when available), along with errors, warnings, and other output. To see how this works in practice, we will adapt our analysis_MLM() function, which makes use of lmer() for fitting a multilevel model. Currently, the estimation function sometimes prints messages to the console: set.seed(101012) # (I picked this to show a warning.) dat &lt;- gen_cluster_RCT( J = 50, n_bar = 100, sigma2_u = 0 ) mod &lt;- analysis_MLM(dat) ## boundary (singular) fit: see help(&#39;isSingular&#39;) Wrapping lmer() with quiet() makes it possible to catch such output and store it along with other results, as in the following: quiet_lmer &lt;- quietly(lme4::lmer) qmod &lt;- quiet_lmer( Yobs ~ 1 + Z + (1|sid), data=dat ) qmod ## $result ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: Yobs ~ 1 + Z + (1 | sid) ## Data: ..2 ## REML criterion at convergence: 14026.44 ## Random effects: ## Groups Name Std.Dev. ## sid (Intercept) 0.0000 ## Residual 0.9828 ## Number of obs: 5000, groups: sid, 50 ## Fixed Effects: ## (Intercept) Z ## -0.013930 -0.008804 ## optimizer (nloptwrap) convergence code: 0 (OK) ; 0 optimizer warnings; 1 lme4 warnings ## ## $output ## [1] &quot;&quot; ## ## $warnings ## character(0) ## ## $messages ## [1] &quot;boundary (singular) fit: see help(&#39;isSingular&#39;)\\n&quot; However, the lmerTest package does not like the structure of the results, and produces an error: lmerTest::as_lmerModLmerTest(qmod$result) ## Error in lmerTest::as_lmerModLmerTest(qmod$result): Unable to extract deviance function from model fit We can side-step this by combining as_lmerModLmerTest() and lmer() into a single function. While we are at it, we also layer on summary(). To do so, we use the compose() functional from purrr, which takes a list of functions and wraps them into one: lmer_with_test &lt;- purrr::compose( summary, lmerTest::as_lmerModLmerTest, lme4::lmer ) The resulting lmer_with_test() function acts as if we were calling lmer(), then feeding the result into as_lmerModLmerTest(), then feeding the result into summary(). We then wrap the combination function with safely() and quietly(): quiet_safe_lmer &lt;- purrr::quietly(purrr::safely(lmer_with_test)) Now we can use our suitably quieted and safe function in a new version of the estimation function: analysis_MLM_safe &lt;- function( dat, all_results = FALSE ) { M1 &lt;- quiet_safe_lmer( Yobs ~ 1 + Z + (1 | sid), data=dat ) if (all_results) { return(M1) } message &lt;- ifelse( length( M1$message ) &gt; 0, M1$message, NA_character_ ) warning &lt;- ifelse( length( M1$warning ) &gt; 0, M1$warning, NA_character_ ) error &lt;- ifelse( length( M1$result$error) &gt; 0, M1$result$error$message, NA_character_ ) tibble( ATE_hat = M1$result$result$coefficients[&quot;Z&quot;,&quot;Estimate&quot;], SE_hat = M1$result$result$coefficients[&quot;Z&quot;,&quot;Std. Error&quot;], p_value = M1$result$result$coefficients[&quot;Z&quot;, &quot;Pr(&gt;|t|)&quot;], message = message, warning = warning, error = error ) } This quiet version runs without extraneous messages: mod &lt;- analysis_MLM_safe(dat) mod ## # A tibble: 1 × 6 ## ATE_hat SE_hat p_value message warning error ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 -0.00880 0.0278 0.751 &quot;boundary… &lt;NA&gt; &lt;NA&gt; Now we have the estimation results along with any diagnostic information from messages or warnings. Storing this information will let us evaluate what proportion of the time there was a warning or message, run additional analyses on the subset of replications where there was no such warning, or even modify the estimation procedure to take the diagnostics into account. 7.4.2 Adapting estimation procedures for errors and warnings So far, we have seen techniques for handling technical hiccups that occur when data analysis procedures do not always produce results. But how do we account for the absence of results in a simulation? In Chapter ??, we will delve into the conceptual issues with summarizing the performance of methods that do not always provide an answer. One of the best solutions to such problems still concerns the formulation of estimation functions, and so we introduce it here. That solution is to re-define the estimator to include contingencies for handling lack of results. Consider a data analyst who was planning to apply a fancy statistical model to their data, but then finds that the model does not converge. What would that analyst do in practice (besides cussing and taking a snack break)? Rather than giving up entirely, they would probably think of an alternative analysis and attempt to apply it, perhaps by simplifying the model in some way. To the extent that we can anticipate such possibilities, we can build these error-contingent alternative analyses into our estimation function. To illustrate, let’s look at an error (a not-particularly-subtle one) that can crop up in the cluster-randomized trial example when clusters are very small: set.seed(65842) tiny_dat &lt;- gen_cluster_RCT( J = 10, n_bar = 2, alpha = 0.5) analysis_MLM_safe(tiny_dat) ## # A tibble: 1 × 3 ## message warning error ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 &lt;NA&gt; &lt;NA&gt; number of levels of each groupi… table(tiny_dat$sid) ## ## 1 2 3 4 5 6 7 8 9 10 ## 1 1 1 1 1 1 1 1 1 1 The error occurs because all 10 simulated schools include a single student, making it impossible to estimate a random-intercepts multilevel model. A natural fall-back analysis here would be to estimate an ordinary least squares regression analysis. Suppose that our imaginary analyst is not especially into nuance, and so will fall back onto ordinary least squares whenever the multilevel model produces an error. We can express this logic in our estimation function by first catching the error thrown by lmer() and then running an OLS regression in the event an error is thrown: analysis_MLM_contingent &lt;- function( dat, all_results = FALSE ) { M1 &lt;- quiet_safe_lmer( Yobs ~ 1 + Z + (1 | sid), data=dat ) if (all_results) { return(M1) } if (!is.null(M1$result$result)) { # If lmer() returns a result res &lt;- tibble( ATE_hat = M1$result$result$coefficients[&quot;Z&quot;,&quot;Estimate&quot;], SE_hat = M1$result$result$coefficients[&quot;Z&quot;,&quot;Std. Error&quot;], p_value = M1$result$result$coefficients[&quot;Z&quot;, &quot;Pr(&gt;|t|)&quot;], ) } else { # If lmer() errors, fall back on OLS M_ols &lt;- summary(lm(Yobs ~ Z, data = dat)) res &lt;- tibble( ATE_hat = M_ols$coefficients[&quot;Z&quot;,&quot;Estimate&quot;], SE_hat = M_ols$coefficients[&quot;Z&quot;, &quot;Std. Error&quot;], p_value = M_ols$coefficients[&quot;Z&quot;,&quot;Pr(&gt;|t|)&quot;] ) } # Store original messages, warnings, errors res$message &lt;- ifelse( length( M1$message ) &gt; 0, M1$message, NA_character_ ) res$warning &lt;- ifelse( length( M1$warning ) &gt; 0, M1$warning, NA_character_ ) res$error &lt;- ifelse( length( M1$result$error) &gt; 0, M1$result$error$message, NA_character_ ) return(res) } We still store the messages, warnings, and errors from the initial lmer() fit so that we can keep track of how often errors occur. The function now returns an treatment effect estimate even if lmer() errors: analysis_MLM_contingent(tiny_dat) ## # A tibble: 1 × 6 ## ATE_hat SE_hat p_value message warning error ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 0.400 0.603 0.525 &lt;NA&gt; &lt;NA&gt; number o… Of course, we can easily anticipate the conditions under which this particular error will occur: all we need to do is check whether all the clusters are single observations. Because it is easily anticipated, a better strategy for handling this error is to check before fitting the multilevel model and proceeding accordingly in the event that the clusters are all singletons. Exercise 7.5.5 asks you to implement this approach and further refine this contingent analysis strategy. Adapting estimation functions to address errors can be an effective—and often very interesting—strategy for studying the performance of estimation methods. Rather than studying the performance of a data-analysis method that is only sometimes well-defined, we shift to studying a stylized cognitive model for the analyst’s decision-making process, which handles contingencies that might crop up whether analyzing simulated data or real empirical data. Of course, studying such a model is only interesting to the extent that the decision-making process it implements is a plausible representation of what an analyst might actually do in practice. This approach to handling errors also leads to more complex estimation functions, which entail implementing multiple estimation methods and a set of decision rules for applying them. Often, the set of contingencies that need to be handled will not be immediately obvious, so you may find that you need to build and refine the decision rules as you learn more about how they work. Running an estimation procedure over multiple, simulated datasets is an excellent (if aggravating!) way to identify errors and edge cases. We turn to procedures for doing so in the next chapter. 7.5 Exercises 7.5.1 More Heteroskedastic ANOVA In the classic simulation by Brown and Forsythe (1974), they not only looked at the performance of the homoskedastic ANOVA-F test and Welch’s heteroskedastic-F test, they also proposed their own new hypothesis testing procedure. Write a function that implements the Brown-Forsythe F* test (the BFF* test!) as described on p. 130 of Brown and Forsythe (1974), using the following code skeleton: BF_F &lt;- function( data ) { # fill in the guts here return(pval) } Run the following code to check that your function produces a result: sim_data &lt;- generate_ANOVA_data( mu = c(1, 2, 5, 6), sigma_sq = c(3, 2, 5, 1), sample_size = c(3, 6, 2, 4) ) BF_F( sim_data ) Try calling your BF_F function on a variety of datasets of different sizes and shapes, to make sure it works. What kinds of datasets should you test out? Add the BFF* test into the output of Welch_ANOVA_F() by calling your BF_F() function inside the body of Welch_ANOVA_F(). The onewaytests package implements a variety of different hypothesis testing procedures for one-way ANOVA. Validate your Welch_ANOVA_F() function by comparing the results to the output of the relevant functions from onewaytests. 7.5.2 Contingent testing In the one-way ANOVA problem, one approach that an analyst might think to take is to conduct a preliminary significance test for heterogeneity of variances (such as Levene’s test or Bartlett’s test), and then report the \\(p\\)-value from the homoskedastic ANOVA F test if variance heterogeneity is not detected but the \\(p\\)-value from the BFF* test if variance heteogeneity is detected. Modify the Welch_ANOVA_F() function to return the \\(p\\)-value from this contingent BFF* test in addition to the \\(p\\)-values from the (non-contingent) ANOVA-F, Welch, and BFF* tests. Include an input option that allows the user to control the \\(\\alpha\\) level of the preliminary test for heterogeneity of variances, as in the following skeleton. Welch_ANOVA_F &lt;- function( data , pre_alpha = .05) { # preliminary test for variance heterogeneity # compute non-contingent F tests for group differences # compute contingent test # compile results return(result) } 7.5.3 Check the cluster-RCT functions Section 7.2 presented functions implementing several different strategies for estimating an average treatment effect from a cluster-randomized trial. Write some code to validate these functions by comparing their output to the results of other tools for doing the same calculation. Use one or more datasets simulated with gen_cluster_RCT(). For each of these tests, you will need to figure out the appropriate syntax by reading the package documentation. For analysis_MLM(), check the output by fitting the same model using lme() from the nlme() package or glmmTMB() from the package of the same name. For analysis_OLS(), check the output by fitting the linear model using the base R function lm(), then computing standard errors using vcovCL() from the sandwich package. Also compare the output to the results of feeding the fitted model through coef_test() from the clubSandwich package. For analysis_agg(), check the output by aggregating the data to the school-level, fitting the linear model using lm(), and computing standard errors using vcovHC() from the sandwich package. 7.5.4 Extending the cluster-RCT functions Exercise 6.9.10 from Chapter 6 asked you to extend the data-generating function for the cluster-randomized trial to include generating a student-level covariate, \\(X\\), that is predictive of the outcome. Use your modified function to generate a dataset. Modify the estimation functions from Section 7.2 to use models that include the covariate as a predictor. Further extend the functions to include an input argument for the set of predictors to be included in the model, as in the following skeleton for the multi-level model estimator: analysis_MLM &lt;- function(dat, predictors = &quot;Z&quot;) { } analysis_MLM( dat ) analysis_MLM( dat, predictors = c(&quot;Z&quot;,&quot;X&quot;)) analysis_MLM( dat, predictors = c(&quot;Z&quot;,&quot;X&quot;, &quot;X:Z&quot;)) Hint: Check out the reformulate() function, which makes it easy to build formulas for different sets of predictors. 7.5.5 Contingent estimator processing In Section 7.4.2 we developed a version of analysis_MLM() that fell back on OLS regression in the event that lmer() produced any error. The implementation that we demonstrated is not especially smart. For one, it does not anticipate that the error will occur. For another, if we are using this function as one of several estimation strategies, it will require fitting the OLS regression multiple times. Can you fix these problems? Revise analysis_MLM_contingent() so that it checks whether all clusters are single observations before fitting the multilevel model. Handle event the contingency where all clusters are singletons by skipping the model-fitting step, returning NA values for the point estimator, standard error, and p-value, and returning an informative message in the error variable. Test that the function is working as expected. Revise estimate_Tx_Fx() to use your new version of analysis_MLM_contingent(). The revised function will sometimes return NA values for the MLM results. To implement the strategy of falling-back on OLS regression, add some code that replaces any NA values with corresponding results of analysis_OLS(). Test that the function is working as expected. 7.5.6 Meta-regression with selective reporting Exercise 6.9.12 asked you to write a data-generating function for the Vevea and Hedges (1995) selection model. The metafor package includes a function for fitting this model (as well as a variety of other selection models). Here is an example of the syntax for estimating this model, using a dataset from the metadat package: library(metafor, quietly = TRUE) ## ## Loading the &#39;metafor&#39; package (version 4.8-0). For an ## introduction to the package please type: help(metafor) data(&quot;dat.assink2016&quot;, package = &quot;metadat&quot;) # rename variables and tidy up dat &lt;- dat.assink2016 %&gt;% mutate( si = sqrt(vi) ) %&gt;% rename( Ti = yi, s_sq = vi, Xi = year ) # fit a random effects meta-regression model rma_fit &lt;- rma.uni(yi = Ti, sei = si, mods = ~ Xi, data = dat) # fit two-step selection model selmodel(rma_fit, type = &quot;step&quot;, steps = c(.025, .500)) ## ## Mixed-Effects Model (k = 100; tau^2 estimator: ML) ## ## tau^2 (estimated amount of residual heterogeneity): 0.2314 (SE = 0.0500) ## tau (square root of estimated tau^2 value): 0.4810 ## ## Test for Residual Heterogeneity: ## LRT(df = 1) = 349.3718, p-val &lt; .0001 ## ## Test of Moderators (coefficient 2): ## QM(df = 1) = 42.1433, p-val &lt; .0001 ## ## Model Results: ## ## estimate se zval pval ## intrcpt 0.4149 0.1013 4.0942 &lt;.0001 ## Xi -0.0782 0.0120 -6.4918 &lt;.0001 ## ci.lb ci.ub ## intrcpt 0.2163 0.6135 *** ## Xi -0.1018 -0.0546 *** ## ## Test for Selection Model Parameters: ## LRT(df = 2) = 1.7814, p-val = 0.4104 ## ## Selection Model Results: ## ## k estimate se zval ## 0 &lt; p &lt;= 0.025 59 1.0000 --- --- ## 0.025 &lt; p &lt;= 0.5 23 0.6990 0.2307 -1.3049 ## 0.5 &lt; p &lt;= 1 18 0.5027 0.2743 -1.8132 ## pval ci.lb ci.ub ## 0 &lt; p &lt;= 0.025 --- --- --- ## 0.025 &lt; p &lt;= 0.5 0.1919 0.2470 1.1511 ## 0.5 &lt; p &lt;= 1 0.0698 0.0000 1.0403 . ## ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The selmodel() function can also be used to fit selection models in which one or both of the selection parameters are fixed to user-specified values. For example: # Fixing lambda_1 = 0.5, lambda_2 = 0.2 fix_both &lt;- selmodel(rma_fit, type = &quot;step&quot;, steps = c(.025, .500), delta = c(1, 0.5, 0.2)) # Fixing lambda_1 = 0.5, estimating lambda_2 fix_one &lt;- selmodel(rma_fit, type = &quot;step&quot;, steps = c(.025, .500), delta = c(1, 0.5, NA)) Write an estimation function that fits the selection model and returns estimates, standard errors, and confidence intervals for each of the model parameters \\((\\beta_0,\\beta_1,\\tau,\\lambda_1,\\lambda_2)\\). The selmodel() fitting function sometimes returns errors, as in the following example: dat_sig &lt;- dat %&gt;% filter(Ti / si &gt; 0) rma_pos &lt;- rma.uni(yi = Ti, sei = si, mods = ~ Xi, data = dat_sig) # fit two-step selection model sel_fit &lt;- selmodel(rma_pos, type = &quot;step&quot;, steps = c(.025, .500)) ## Warning: One or more intervals do not contain any ## observed p-values. ## Warning: One or more &#39;delta&#39; estimates are (almost) equal to their lower or upper bound. ## Treat results with caution (or consider adjusting &#39;delta.min&#39; and/or &#39;delta.max&#39;). Modify your estimation function to catch and return warnings such as these. Write code demonstrating that the function works as expected. The selmodel() throws warnings when the dataset contains no observations with \\(p_i\\) in one of the specified intervals. Modify your estimation function to set the selection probability for an interval to \\(\\lambda_1 = 0.1\\) if there are no \\(p_i\\) values between .025 and .500 and to set \\(\\lambda_2 = 0.1\\) if there are no \\(p_i\\) values larger than .500. Write code demonstrating that the function works as expected. References Vevea, Jack L, and Larry V Hedges. 1995. “A General Linear Model for Estimating Effect Size in the Presence of Publication Bias.” Psychometrika 60 (3): 419–35. https://doi.org/10.1007/BF02294384. "],["running-the-simulation-process.html", "Chapter 8 Running the Simulation Process 8.1 Writing simulations quick with the simhelpers package 8.2 Adding Checks and Balances 8.3 Exercises", " Chapter 8 Running the Simulation Process In the prior two chapters we saw how to write functions that generate data according to a specified model (and parameters) and functions that implement estimation procedures on simulated data. We next put those two together and repeat a bunch of times to obtain a lot of results such as point estimates, estimated standard errors and/or confidence intervals. We use two primary ways of doing this in this textbook. The first is to write a function that does a single step of a simulation, and then use the map() function to run that single step multiple times. For our Cluster RCT case study, for example, we would write the following that takes our simulation parameters and runs a single trial of our simulation: one_run &lt;- function( n_bar = 30, J=20, gamma_1 = 0.3, gamma_2 = 0.5, sigma2_u = 0.20, sigma2_e = 0.80, alpha = 0.75 ) { dat &lt;- gen_cluster_RCT( n_bar = n_bar, J=J, gamma_1 = gamma_1, gamma_2 = gamma_2, sigma2_u = sigma2_u, sigma2_e = sigma2_e, alpha = alpha ) MLM = analysis_MLM( dat ) LR = analysis_OLS( dat ) Agg = analysis_agg( dat ) bind_rows( MLM = MLM, LR = LR, Agg = Agg, .id = &quot;method&quot; ) } We have added a bunch of defaults to our function, so we can easily run it without remembering all the things we can change. When we call it, we get a nice table of results that we can evaluate: one_run( n_bar = 30, J = 20, alpha=0.5 ) ## # A tibble: 3 × 4 ## method ATE_hat SE_hat p_value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 MLM 0.334 0.206 0.123 ## 2 LR 0.382 0.230 0.116 ## 3 Agg 0.329 0.203 0.122 The results for each method is a single line. We record estimated impact, estimated standard error, and a nominal \\(p\\)-value. Note how the bind_rows() method can take naming on the fly, and give us a column of method, which will be very useful for keeping track of what estimated what. We intentionally wrap up our results with a data frame to make later processing of data with the tidyverse package much easier. We then use the map() function to run this function multiple times: set.seed( 40404 ) R = 1000 ATE = 0.30 runs &lt;- map_df( 1:R, ~one_run( n_bar = 30, J=20, gamma_1 = ATE ), .id=&quot;runID&quot; ) saveRDS( runs, file = &quot;results/cluster_RCT_simulation.rds&quot; ) What the map() function is doing is first making a list from 1 to R, and then for each element in that list, it is calling one_run() with the parameters n_bar = 30, J=20. The ~ is a shorthand way of writing a function that takes one argument, and then calls one_run() with that argument; the argument is the iteration number (1, 2, 3, …, R), but we are ignoring it. The .id = \"runID\" argument is a way of keeping track of which iteration number produced which result. The _df at the end of map_df() is a way of telling map() to take the results of each iteration and bind them together into a single data frame. Once our simulation is complete, we save our results to a file for future use; this speeds up our lives since we will not have to constantly re-run our simulation each time we want to explore the results. We have arrived! We now have the individual results of all our methods applied to each of 1000 generated datasets. The next step is to evaluate how well the estimators did. Regarding our point estimate, for example, we have these primary questions: Is it biased? (bias) Is it precise? (standard error) Does it predict well? (RMSE) In the next chapter, we systematically go through answering these questions for our initial scenario. 8.1 Writing simulations quick with the simhelpers package The map approach is a bit strange, with building a secret function on the fly with ~, and also having the copy over all the parameters we pass from one_run() to gen_cluster_RCT(). The simhelpers package provides a shortcut that makes this step easier. To do it, we first need to write a single estimation procedure function that puts all of our estimators together: analyze_data = function( dat ) { MLM = analysis_MLM( dat ) LR = analysis_OLS( dat ) Agg = analysis_agg( dat ) bind_rows( MLM = MLM, LR = LR, Agg = Agg, .id = &quot;method&quot; ) } This is simply the one_run() method from above, but without the data generating part. When we pass a dataset to it, we get a nice table of results that we can evaluate, as we did before. dat = gen_cluster_RCT( n=30, J = 20, gamma_1 = 0.30 ) analyze_data( dat ) ## boundary (singular) fit: see help(&#39;isSingular&#39;) ## # A tibble: 3 × 4 ## method ATE_hat SE_hat p_value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 MLM 0.382 0.0813 0.00000325 ## 2 LR 0.382 0.0672 0.0000213 ## 3 Agg 0.382 0.0672 0.0000213 We can now use simhelpers to write us a new function for the entire simulation: library(simhelpers) sim_function &lt;- bundle_sim( gen_cluster_RCT, analyze_data ) We can then use it as so: sim_function( 2, n_bar = 30, J = 20, gamma_1 = ATE ) ## boundary (singular) fit: see help(&#39;isSingular&#39;) ## # A tibble: 6 × 4 ## method ATE_hat SE_hat p_value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 MLM 0.294 0.0854 0.00291 ## 2 LR 0.294 0.0854 0.00291 ## 3 Agg 0.294 0.0854 0.00291 ## 4 MLM 0.375 0.0781 0.00000202 ## 5 LR 0.375 0.0666 0.0000244 ## 6 Agg 0.375 0.0666 0.0000244 The bundle_sim() command takes our DGP function and our estimation procedures function and gives us back a function, which we have called sim_function, that will run a simulation using whatever parameters we give it. The bundle_sim() command examines gen_cluster_RCT function, figures out what parameters it needs, and makes sure that the newly created function is able to take those parameters from the user. To use it for our simulation, we would then write rns &lt;- sim_function( R, n_bar = 30, J = 20, gamma_1 = ATE ) saveRDS( runs, file = &quot;results/cluster_RCT_simulation.rds&quot; ) This is a bit more elegant than the map() approach, and is especially useful when we have a lot of parameters to pass around. 8.2 Adding Checks and Balances In the extensions of the prior DGP chapter, we discussed indexing our DGP by the ICC instead of the two variance components. We can do this, and also translate some of the more obscure model parameters to easier to interpret parameters from within our simulation driver as follows: one_run &lt;- function( n_bar = 30, J=20, ATE = 0.3, size_coef = 0.5, ICC = 0.4, alpha = 0.75 ) { stopifnot( ICC &gt;= 0 &amp;&amp; ICC &lt; 1 ) dat &lt;- gen_cluster_RCT( n_bar = n_bar, J=J, gamma_1 = ATE, gamma_2 = size_coef, sigma2_u = ICC, sigma2_e = 1-ICC, alpha = alpha ) MLM = analysis_MLM( dat ) LR = analysis_OLS( dat ) Agg = analysis_agg( dat ) bind_rows( MLM = MLM, LR = LR, Agg = Agg, .id = &quot;method&quot; ) } Note the stopifnot: it is wise to ensure our parameter transforms are all reasonable, so we do not get unexplained errors or strange results later on. It is best if your code fails as soon as possible! Otherwise debugging can be quite hard. In our modified one_run() we are transforming our ICC parameter into specific other parameters that are used in our actual model to maintain our effect size interpretation of our simulation. We have not even modified our gen_cluster_RCT() DGP method: we are just specifying the constellation of parameters as a function of the parameters we want to directly control in the simulation. Controlling how we use the foundational elements such as our data generating code is a key tool for making the higher level simulations sensible and more easily interpretable. Here we have put our entire simulation into effect size units, and are now providing “knobs” to the simulation that are directly interpretable. 8.3 Exercises In the prior chapter’s exercises, you made a new BF_F function for the Welch simulation. Now incorporate the BF_F function into the one_run() function, and use your revised function to generate simulation results for this additional estimator. Abdulkadiroğlu, Atila, Joshua D Angrist, Yusuke Narita, and Parag A Pathak. 2017. “Research Design Meets Market Design: Using Centralized Assignment for Impact Evaluation.” Econometrica 85 (5): 1373–1432. Bloom, Howard S., Stephen W. Raudenbush, Michael J. Weiss, and Kristin Porter. 2016. “Using Multisite Experiments to Study Cross-Site Variation in Treatment Effects: A Hybrid Approach With Fixed Intercepts and a Random Treatment Coefficient.” Journal of Research on Educational Effectiveness 10 (4): 0–0. https://doi.org/10.1080/19345747.2016.1264518. Boulesteix, Anne-Laure, Sabine Hoffmann, Alethea Charlton, and Heidi Seibold. 2020. “A Replication Crisis in Methodological Research?” Significance 17 (5): 18–21. https://doi.org/10.1111/1740-9713.01444. Boulesteix, Anne-Laure, Sabine Lauer, and Manuel J. A. Eugster. 2013. “A Plea for Neutral Comparison Studies in Computational Sciences.” PLOS ONE 8 (4): e61562. https://doi.org/10.1371/journal.pone.0061562. Boulesteix, Anne-Laure, Rory Wilson, and Alexander Hapfelmeier. 2017. “Towards Evidence-Based Computational Statistics: Lessons from Clinical Research on the Role and Design of Real-Data Benchmark Studies.” BMC Medical Research Methodology 17 (1, 1): 1–12. https://doi.org/10.1186/s12874-017-0417-2. Brown, Morton B., and Alan B. Forsythe. 1974. “The Small Sample Behavior of Some Statistics Which Test the Equality of Several Means.” Technometrics 16 (1): 129–32. https://doi.org/10.1080/00401706.1974.10489158. Cho, Hunyong, Chuwen Liu, John S Preisser, and Di Wu. 2023. “A Bivariate Zero-Inflated Negative Binomial Model and Its Applications to Biomedical Settings.” Statistical Methods in Medical Research 32 (7): 1300–1317. https://doi.org/10.1177/09622802231172028. Davison, A. C., and D. V. Hinkley. 1997. Bootstrap Methods and Their Applications. Cambridge: Cambridge University Press. Dong, Nianbo, and Rebecca Maynard. 2013. “PowerUp! : A Tool for Calculating Minimum Detectable Effect Sizes and Minimum Required Sample Sizes for Experimental and Quasi-Experimental Design Studies.” Journal of Research on Educational Effectiveness 6 (1): 24–67. https://doi.org/10.1080/19345747.2012.673143. Efron, Bradley. 2000. “The Bootstrap and Modern Statistics.” Journal of the American Statistical Association 95 (452): 1293–96. https://doi.org/10.2307/2669773. Faul, Franz, Edgar Erdfelder, Axel Buchner, and Albert-Georg Lang. 2009. “Statistical Power Analyses Using G*Power 3.1: Tests for Correlation and Regression Analyses.” Behavior Research Methods 41 (4): 1149–60. https://doi.org/10.3758/BRM.41.4.1149. Fryda, Tomas, Erin LeDell, Navdeep Gill, Spencer Aiello, Anqi Fu, Arno Candel, Cliff Click, et al. 2014. “H2o: R Interface for the ’H2O’ Scalable Machine Learning Platform.” Comprehensive R Archive Network. https://doi.org/10.32614/CRAN.package.h2o. Gelman, Andrew, John B. Carlin, Hal S. Stern, David B. Dunson, Aki Vehtari, and Donald B. Rubin. 2013. Bayesian Data Analysis. 0th ed. Chapman and Hall/CRC. https://doi.org/10.1201/b16018. Good, Phillip. 2013. Permutation Tests: A Practical Guide to Resampling Methods for Testing Hypotheses. Springer Science &amp; Business Media. Hunter, Kristen B., Luke Miratrix, and Kristin Porter. 2024. “PUMP: Estimating Power, Minimum Detectable Effect Size, and Sample Size When Adjusting for Multiple Outcomes in Multi-Level Experiments.” Journal of Statistical Software 108 (6): 1–43. https://doi.org/10.18637/jss.v108.i06. James, G. S. 1951. “The Comparison of Several Groups of Observations When the Ratios of the Population Variances Are Unknown.” Biometrika 38 (3/4): 324. https://doi.org/10.2307/2332578. Jones, Owen, Robert Maillardet, and Andrew Robinson. 2012. Introduction to Scientific Programming and Simulation Using R. New York: Chapman and Hall/CRC. https://doi.org/10.1201/9781420068740. Lehmann, Erich Leo et al. 1975. “Statistical Methods Based on Ranks.” Nonparametrics. San Francisco, CA, Holden-Day 2. Long, J. Scott, and Laurie H. Ervin. 2000. “Using Heteroscedasticity Consistent Standard Errors in the Linear Regression Model.” The American Statistician 54 (3): 217–24. https://doi.org/10.1080/00031305.2000.10474549. Mehrotra, Devan V. 1997. “Improving the Brown-Forsythe Solution to the Generalized Behrens-Fisher Problem.” Communications in Statistics - Simulation and Computation 26 (3): 1139–45. https://doi.org/10.1080/03610919708813431. Miratrix, Luke W., Michael J. Weiss, and Brit Henderson. 2021. “An Applied Researcher’s Guide to Estimating Effects from Multisite Individually Randomized Trials: Estimands, Estimators, and Estimates.” Journal of Research on Educational Effectiveness 14 (1): 270–308. https://doi.org/10.1080/19345747.2020.1831115. Morris, Tim P., Ian R. White, and Michael J. Crowther. 2019. “Using Simulation Studies to Evaluate Statistical Methods.” Statistics in Medicine, January. https://doi.org/10.1002/sim.8086. Robert, Christian, and George Casella. 2010. Introducing Monte Carlo Methods with R. New York, NY: Springer. https://doi.org/10.1007/978-1-4419-1576-4. Siepe, Björn S., František Bartoš, Tim Morris, Anne-Laure Boulesteix, Daniel W. Heck, and Samuel Pawel. 2024. “Simulation Studies for Methodological Research in Psychology: A Standardized Template for Planning, Preregistration, and Reporting,” January. https://doi.org/10.31234/osf.io/ufgy6. Staiger, Douglas O, and Jonah E Rockoff. 2010. “Searching for Effective Teachers with Imperfect Information.” Journal of Economic Perspectives 24 (3): 97–118. Tipton, Elizabeth. 2013. “Stratified Sampling Using Cluster Analysis: A Sample Selection Strategy for Improved Generalizations from Experiments.” Evaluation Review 37 (2): 109–39. https://doi.org/10.1177/0193841X13516324. Vevea, Jack L, and Larry V Hedges. 1995. “A General Linear Model for Estimating Effect Size in the Presence of Publication Bias.” Psychometrika 60 (3): 419–35. https://doi.org/10.1007/BF02294384. Welch, B. L. 1951. “On the Comparison of Several Mean Values: An Alternative Approach.” Biometrika 38 (3/4): 330. https://doi.org/10.2307/2332579. White, Halbert. 1980. “A Heteroskedasticity-Consistent Covariance Matrix Estimator and a Direct Test for Heteroskedasticity.” Econometrica 48 (4): 817–38. Wickham, Hadley. 2014. “Tidy Data.” Journal of Statistical Software 59 (10): 1–23. https://doi.org/10.18637/jss.v059.i10. "]]
