% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{book}
\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{booktabs}
\usepackage{amsthm}
\makeatletter
\def\thm@space@setup{%
  \thm@preskip=8pt plus 2pt minus 4pt
  \thm@postskip=\thm@preskip
}
\makeatother

% Added by Miratrix
\usepackage{color}
\newcommand\cmnt[2]{\qquad{{\color{red} \em #1---#2} \qquad}}
\newcommand\cmntM[1]{\cmnt{#1}{Miratrix}}
\newcommand\awk{{{\color{red} {$\leftarrow$ Awkward phrasing}}\qquad}}
\newcommand\cmntMp[1]{{\color{red} $\leftarrow$ {\em #1 -Miratrix} \qquad}}

\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[]{natbib}
\bibliographystyle{apalike}
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Designing Monte Carlo Simulations in R},
  pdfauthor={Luke W. Miratrix and James E. Pustejovsky (Equal authors)},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{Designing Monte Carlo Simulations in R}
\author{Luke W. Miratrix and James E. Pustejovsky
(Equal authors)}
\date{2024-06-12}

\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\chapter*{Welcome}\label{welcome}
\addcontentsline{toc}{chapter}{Welcome}

Monte Carlo simulations are a computational technique for investigating how well something works, or for investigating what might happen in a given circumstance.
When we write a simulation, we are able to control how data are generated, which means we can know what the ``right answer'' is.
Then, by repeatedly generating data and then applying some statistical method that data, we can assess how well a statistical method works in practice.

Monte Carlo simulations are an essential tool of inquiry for quantitative methodologists and students of statistics, useful both for small-scale or informal investigations and for formal methodological research.
In this monograph, we aim to provide an introduction to the logic and mechanics of designing simulation studies, using the R programming language.
Our focus is on simulation studies for formal research purposes (i.e., as might appear in a journal article or dissertation) and for informing the design of empirical studies (e.g., power analysis).
That being said, the ideas of simulation are used in many different contexts and for many different problems, and we believe the overall concepts illustrated by these ``conventional'' simulations readily carry over into all sorts of other types of use, even statistical inference!

Mainly, this book gives practical tools (i.e., lots of code to simply take and repurpose) along with some thoughts and guidance for writing simulations.
We hope you find it to be a useful handbook to help you with your own projects, whatever they happen to be!

\section*{License}\label{license}
\addcontentsline{toc}{section}{License}

This book is licensed to you under \href{http://creativecommons.org/licenses/by-nc-nd/4.0/}{Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License}.

The code samples in this book are licensed under \href{https://creativecommons.org/publicdomain/zero/1.0/}{Creative Commons CC0 1.0 Universal (CC0 1.0)}, i.e.~public domain.

\section*{About the authors}\label{about-the-authors}
\addcontentsline{toc}{section}{About the authors}

We wrote this book in full collaboration, because we thought it would be fun to have some reason to talk about how to write simulations, and we wanted more people to be writing high-quality simulations.
Our author order is alphabetical, but perhaps imagine it as a circle, or something with no start or end:

\begin{center}\includegraphics[width=0.75\linewidth]{Designing-Simulations-in-R_files/figure-latex/unnamed-chunk-2-1} \end{center}

But anymore, more about us.

\textbf{James E. Pustejovsky} is an associate professor at the University of Wisconsin - Madison, where he teaches in the Quantitative Methods Program within the Department of Educational Psychology. He completed a Ph.D.~in Statistics at Northwestern University.

\textbf{Luke Miratrix}: I am currently an associate professor at Harvard University's Graduate School of Education. I completed a Ph.D.~in Statistics at UC Berkeley after having traveled through three different graduate programs (computer science at MIT, education at UC Berkeley, and then finally statistics at UC Berkeley).
I then ended up as an assistant professor in Harvard's statistics department, and moved (back) to Education a few years later.

Over the years, simulation has become a way for me to think.
This might be because I am fundamentally lazy, and the idea of sitting down and trying to do a bunch of math to figure something out seems less fun than writing up some code ``real quick'' so I can see how things operate. Of course, ``real quick'' rarely is that quick -- and before I know it I got sucked into trying to learn some esoteric aspect of how to best program something, and then a few rabbit holes later I may have discovered something interesting! I find simulation quite absorbing, and I also find them reassuring (usually with regards to whether I have correctly implemented some statistical method). This book has been a real pleasure to write, because it's given me actual license to sit down and think about why I do the various things I do, and also which way I actually prefer to approach a problem. And getting to write this book with my co-author has been a particular pleasure, for talking about the business of writing simulations is rarely done in practice. This has been a real gift, and I have learned so much.

\section*{Acknowledgements}\label{acknowledgements}
\addcontentsline{toc}{section}{Acknowledgements}

The material in this book was initially developed through courses that we offered at the University of Texas at Austin (James) and Harvard University (Luke) as well as from a series of workshops that we offered through the Society for Research on Educational Effectiveness in June of 2021. We are grateful for feedback, questions, and corrections we have received from many students who participated in these courses. Some parts of this book are based on memos or other writings generated for various purposes, some of which were written by others. This is been attributed throughout.

\chapter{Introduction}\label{introduction}

In this text we present an approach for writing Monte Carlo simulations in R.
Our focus in this text is on the best practices of simulation design and how to use simulation to be a more informed and effective quantitative analyst.
In particular, we try to provide a guide to designing simulation studies to answer questions about statistical methodology.

In general, simulation studies allow for investigating the performance of a statistical model or method under known data-generating processes.
By controlling the data generating process (e.g., by specifying true values for the parameters of a statistical model) and then repeatedly applying a statistical method to data generated by that process, it becomes possible to assess how well a statistical method works.

Overall, we will show how simulation frameworks allow for rapid exploration of the impact of different design choices and data concerns, and how simulation can answer questions that are hard to answer using direct computation (e.g., with power calculators or mathematical formula).
Simulations are particularly useful for studying models and estimation methods where relevant algebraic formulas are not available, not easily applied, or not sufficiently accurate.
For example, available algebraic formulas are often based on asymptotic approximations, which might not ``kick in'' if sample sizes are moderate.
This is, for example, a particular concern with hierarchical data structures that include only 20 to 40 clusters, which is the range of common sample sizes in many large-scale randomized trials in education research.

\section{Some of simulation's many uses}\label{some-of-simulations-many-uses}

Simulation can be useful across a wide range of areas.
To wet the appetite, consider the following areas where one might find need of simulation.

\subsection{Comparing statistical approaches}\label{comparing-statistical-approaches}

Comparing statistical approaches is perhaps the most common use of Monte Carlo simulation.
In the statistical methodology literature, for example, authors will frequently use simulation to compare their newly proposed method to more traditional approaches to make a case for their method being of real value.
Other simulation-based research will often work to align a literature by systematically comparing a suite of methods all designed to achieve a given task to one another.
In the best case, simulation can show how trade-offs between methods can occur in practice.

For a classic example, Brown and Forsythe (1974) compared four different procedures for conducting a hypothesis test for equality of means in several populations (i.e., one-way ANOVA) when the population variances are not equal.
We revisit this example later.
Overall, simulation can be critical for understanding the benefits and drawbacks of analytic methods in practice.

Comparitive simulation can also have a practical application: In many situations, more than one modeling approach is possible for addressing the same research question (or estimating the same target parameter).
Comparing the costs of one vs.~another using simulation is informative for guiding the design of analytic plans (such as plans included in pre-registered study protocols).
As an example of the type of questions that researchers might encounter in designing analytic plans: what are the practical benefits and costs of using a model that allows for cross-site impact variation for a multi-site trial \citep{miratrix2021applied}?

\subsection{Assessing performance of complex pipelines}\label{assessing-performance-of-complex-pipelines}

In practice, statistical methods are often used in combination.
For instance, in a regression model, one could first use a statistical test for heteroskedasticity (e.g., the White test or the Breusch-Pagan test) and then determine whether to use conventional or robust standard errors depending on the result of the test. This combination of an initial diagnostic test followed by contingent use of different statistical procedures is all but impossible to analyze mathematically, but it is straight-forward to simulate (see, for example, Long \& Ervin, 2000).
In particular, with simulation, we can verify a proposed pipeline is \emph{valid}, meaning that the conclusions it draws are correct at a given level of certainty.

Simulating an analytic pipeline can be used for statistical inference as well.
With bootstrap or parametric bootstrap approaches, for example, one is, in essence, repeatedly simulating data and putting it through an entire analytic pipeline to assess how stable estimation is.
How much a final point estimate varies across the simulation trials is the standard error for the context being simulated; an argument by analogy (the bootstrap analogy) is what connects this to inference on the original data and point estimate.

\subsection{Assessing performance under misspecification}\label{assessing-performance-under-misspecification}

Many statistical estimation procedures can be shown (through mathematical analysis) to perform well when the assumptions they entail are correct.
However, in practice it is often of interest to also understand their robustness---that is, their performance when one or more of the assumptions is incorrect.
For example, how important is the normality assumption underlying multilevel modeling?
What about homoskedasticity?

In a similar vein, when the true data-generating process meets stringent assumptions (e.g., constant treatment effect), what are the potential gain of exploiting such structure in the estimation process?
Conversely, what are the costs of using flexible methods that do not impose the stringent assumption?
A researcher designing an analytic plan would want to be well informed of such tradeoffs in the context they are working in.
Simulation allows for such investigation and comparison.

\subsection{Assessing the finite sample performance of a statistical approach}\label{assessing-the-finite-sample-performance-of-a-statistical-approach}

Many statistical estimation procedures can be shown (through mathematical analysis) to work well \emph{asymptotically}---that is, given an infinite amount of data---but their performance in small samples is more difficult to quantify.
Simulation is a tractable approach for assessing the small-sample performance of such methods, or for determining minimum required sample sizes for adequate performance.
This is perhaps one of the most important uses for simulation: mathematical theory generally is asymptotic in nature, but we are living in the finite world and practice.
In order to know whether the asymptotics ``kick in'' we must rely on simulation.

For example, heteroskedasticity-robust standard errors (HRSE) are known to work asymptotically, but can be misleading in small samples.
Long and Ervin (2000) use extensive simulations to investigate the properties of different heteroscedasticity robust standard error estimators for linear regression across a range of sample sizes, demonstrating that the most commonly used form of these estimators often does \emph{not} work well with sample sizes typical in the social sciences.
Simulation could answer what asymptotics could not: how these estimators work in typical practice.

For another example, recent work has developed the Fixed-Intercept, Random Coefficient method for estimating and accounting for cross site treatment variation in multisite trials. When there are a moderate number of clusters it appears that the numerical (asymptotic based) estimates of performance are not very accurate. Simulation can unpack these trends and give a more accurate picture of effectiveness in these real contexts.

\subsection{Conducting Power Analyses}\label{conducting-power-analyses}

By repeatedly simulating and then analyzing data from a guessed-at world, a researcher can easily calculate the power to detect the effects so modeled, if that world were true.
This can allow for power analyses far more nuanced and tailored to a given circumstance than typical power calculators. In particular, simulation can be useful for the following:

\begin{itemize}
\item
  Available formulas for power analysis in multi-site block- or cluster-randomized trials (such those implemented in the Optimal Design and PowerUp! Software) assume that sites are of equal size and that outcome distributions are unrelated to site size. Small deviations from these assumptions are unlikely to change the results, but in practice, researchers may face situations where sites vary quite widely in size or where site-level outcomes are related to site size. Simulation can estimate power in this case.
\item
  Available software (such as PowerUp!) allows investigators to make assumptions about anticipated rates of attrition in cluster-randomized trials, under the assumption that attrition is completely at random. However, researchers might anticipate that attrition will be related to baseline characteristics, leading to data that is missing at random but not completely at random. How will this affect the power of a planned study?
\item
  There are some closed-form expressions for power to test mediational relations (i.e., indirect and direct effects) in a variety of different experimental designs, and these formulas are now available in PowerUp!. However, the formulas involve a large number of parameters (including some where it may be difficult in practice to develop credible assumptions) and they apply only to a specific analytic model for the mediating relationships. Researchers planning a study to investigate mediation might therefore find it easier to generate realistic data structures and conduct power analysis via simulation.
\end{itemize}

\subsection{Simulating processess}\label{simulating-processess}

Less central to this book, but a very common use for simulation, is to simulation some sort of complex process to better understand it or the consequences of it.
For example, some larger school districts (e.g., New York City) have centralized lotteries for school assignment where families rank some number of schools by order of preference.
The central office then assigns students to schools via a lottery procedure where each student gets a lottery number that breaks ties when there are too many students desiring to go to a specific school.
As a consequence, students have a random probability of assignment to the schools on their list, depending on their choices, the choices of other students, and their lottery numbers.

We can exploit this process to estimate the causal impact of being assigned to one school vs.~another, treating the lottery as a natural experiment, but only if we have those probabilities of school assignment.
We can obtain them via simulation: we repeatedly run the school lottery over and over, and record where everyone gets assigned.
Using these final propensity scores we can move forward with our analysis \citep{abdulkadirouglu2017research}.

For another example, one that possibly illustrates the perils of simulation as taking us away from results that pass face validity, \citet{staiger2010searching} simulate the process of firing teachers depending on their estimated value added scores.
Using simulation of different levels of draconian policy, they argue that substantial portions of teachers should be fired each year.
Here we see a clean example of how the assumptions driving a simulation can be explored, so we can see what the consequences of a system would be\ldots{} if the assumptions behind the simulation were true.

A famous area of process simulation are climate models, where researchers simulate the process of climate change.
These physical simulations mimic very complex systems to try and understand how perturbations (e.g., more carbon release) will impact downstream trends.

\section{The perils of simulation as evidence}\label{the-perils-of-simulation-as-evidence}

Simulation has the potential to be a powerful tool for investigating quantitative methods.
Unfortunately, simulation-based argument also opens up a large can of worms, and is very susceptible to critique.
These critiques usually revolve around what the data generating process of the simulations is.
Are the simulated data realistic?
Was the simulation systematic in exploring a wide variety of scenarios, allowing for truly general conclusions?

The best way to answer these arguments is through transparency: explicitly state what was done, and provide code so people can tweak it to run their own simulations.
Another important component of a robust argument is systematic variation: design simulations so that one can easily simulate across a range of scenarios.
Once that is in place, systematically explore myriad scenarios and report all of the results.

Due to the flexibility in the design of simulations, they are held in great skepticism by many.
A summary of this is the motto

\begin{quote}
Simulations are doomed to succeed.
\end{quote}

Simulations are alluring: once a simulation framework is set up, it is easy to tweak and adjust.
It is natural for us all to continue to do this until the simulation works ``as it should.''
This means, if our goal was to show something we ``know'' is right (e.g., that our new estimation procedure is better than another), we will eventually find a way to align our simulation with our intuition.
This is, simply put, a version of fishing.

To counteract that, challenge yourself to design scenarios where things do not work as you expect.
Try to learn the edges that separate where things work, and where things do not.

\section{Why R and RStudio?}\label{why-r-and-rstudio}

The statistical software package R runs on both PCs and Macs. The software is free and available online.
R is straightforward to learn, but is sufficiently powerful and versatile to be useful for real projects that you might carry out.
It is used widely for statistical work in such fields as education, psychology, economics, medical research, epidemiology, public health, and political science.

We highly recommend using RStudio, which makes using R easier. RStudio is an Integrated Development Environment (IDE) that structures your experience, helps keep things organized, and offers multiple time-saving features to make your programming experience better. You might also consider using R Markdown. R Markdown allows for generating documents with embedded R code and R output in a clean format, which can greatly help report generation.
(In fact, this book is in a variant of R Markdown.)

Many people seem to believe that R is particularly technically challenging and difficult to master. This probably stems from its extreme flexibility; it is a fully functional programming language as well as a statistical analysis package. R can do things that many other software packages (we're looking at you, Stata) essentially cannot.
But these more involved things are frequently hard to do because they require you to think like a statistical programmer rather than a data analyst.
As a result, R is perceived as a ``hard'' language to use.
For simulation, in particular, the ability to easily write functions (bundles of commands that you can easily call in different manners), to have multiple tables of data in play at the same time, and to leverage the vast array of other people's work all make R an attractive option.

\subsection{Templates vs.~Patterns}\label{templates-vs.-patterns}

We generally adhere to a simple, modular approach to building simulations.
We (repeatedly) demonstrate a set sequence of steps, going from coding a data generating process, to the estimation methods, to the code for evaluating a simulation result, to the final multifactor experiment.
But our many case studies are not all precisely the same; the idea of this sequence of steps is paramount, but coding is not a rigid process.
Different aspects of a particular problem may call for doing things in a slightly different manner.
And often, it is merely an issue of style or choice.

We believe that the variants of the coding pattens we showcase are just as important as the patterns themselves, as they take us out of a rigid model of thinking about the creation of simulations.
The different takes on the same idea will, we hope, expand the sense of what is possible and also triangulate the core coding principles we are attempting to espouse.

That being said, much of this code can be taken verbatim, tweaked for your own ends, and used.
We hope you end up doing just that!

\subsection{The tidyverse and a recommended text}\label{the-tidyverse-and-a-recommended-text}

Layered on top of R are a collection of packages that make data wrangling and management much, much easier.
This collection is called the ``tidyverse,'' and much of this book heavily relies on it.
Loading the tidyverse packages is straightforward

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{( tidyverse )}
\FunctionTok{options}\NormalTok{(}\FunctionTok{list}\NormalTok{(}\AttributeTok{dplyr.summarise.inform =} \ConstantTok{FALSE}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

(The second line is to shut up some of tidyverse's weird summarize warnings.)
These lines of code are pretty much the header of any script we use.

We use methods from the ``tidyverse'' for cleaner code and some nice shortcuts.
See the online, free and excellent (\url{https://r4ds.had.co.nz/}){[}R for Data Science textbook{]} for more on the tidyverse.
We will cite portions of this text throughout this manuscript.

\subsection{Functions}\label{functions}

\begin{figure}
\centering
\includegraphics[width=0.5\textwidth,height=\textheight]{image/function-machine.png}
\caption{A function as a machine}
\end{figure}

A critical component of simulation design is the use of functions.
A function is a bundle of commands that you can name, so you can use those commands over and over.
You can think of it as a machine, with a hopper that takes some inputs, and a chute that spits out an output based on those inputs (see figure above).
A function can do anything, and it can even be random in its behavior.
For example, the \texttt{rnorm()} function in R takes a number, and gives you that many random, normally distributed, numbers in response.
See \href{https://r4ds.had.co.nz/functions.html}{Chapter 19 of R for Data Science} for an extended discussion, but here is an example function to get you started:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{one\_run }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{( N, mn ) \{}
\NormalTok{  vals }\OtherTok{=} \FunctionTok{rnorm}\NormalTok{( N, }\AttributeTok{mean =}\NormalTok{ mn )}
\NormalTok{  tt }\OtherTok{=} \FunctionTok{t.test}\NormalTok{( vals )}
\NormalTok{  tt}\SpecialCharTok{$}\NormalTok{p.value}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

The above makes a new command, \texttt{one\_run()} that takes a desired sample size \(N\) and mean \texttt{mn} and generates \(N\) normally distributed points centered on \texttt{mn}, conducts a \(t\)-test on the results, and returns the \(p\)-value for testing whether the mean is zero or not.
The things we pass to the function, \texttt{N} and \texttt{mn}, are called \emph{parameters}, or \emph{inputs}.
Inside the function, we can use these to make calculations and so forth.

We call our new method as so:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{one\_run}\NormalTok{( }\DecValTok{100}\NormalTok{, }\DecValTok{5}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1.390038e-75
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{one\_run}\NormalTok{( }\DecValTok{10}\NormalTok{, }\FloatTok{0.3}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.2827601
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{one\_run}\NormalTok{( }\DecValTok{10}\NormalTok{, }\FloatTok{0.3}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.2427308
\end{verbatim}

In this case, each time we run our code, we get a different answer since we are generating random numbers with each call.

We can call it a lot, like so:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{( }\DecValTok{30303}\NormalTok{ )}
\NormalTok{pvs }\OtherTok{=} \FunctionTok{replicate}\NormalTok{( }\DecValTok{1000}\NormalTok{, }\FunctionTok{one\_run}\NormalTok{( }\DecValTok{100}\NormalTok{, }\FloatTok{0.2}\NormalTok{ ) )}
\FunctionTok{qplot}\NormalTok{( pvs, }\AttributeTok{binwidth=}\FloatTok{0.02}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{Designing-Simulations-in-R_files/figure-latex/pvalue_plot-1} \end{center}

We see that if our sample size is 100, and the true mean is 0.2, we often get low \(p\)-values, but not always. We can calculate the power of our test as so:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sum}\NormalTok{( pvs }\SpecialCharTok{\textless{}=} \FloatTok{0.05}\NormalTok{ ) }\SpecialCharTok{/} \DecValTok{1000}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.492
\end{verbatim}

Via simulation, we have discovered we have about a 49\% chance of rejecting the null, if the alternative is 0.2 and our sample size is 100.

Basically the rest of the book is an elaboration of the ideas above.

\subsection{A dangerous function}\label{a-dangerous-function}

Functions are awesome, but if you violate their intention, you can get into trouble.
For example, consider the following script:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{secret\_ingredient }\OtherTok{\textless{}{-}} \DecValTok{3}

\CommentTok{\# blah blah blah}

\NormalTok{funky }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(input1, input2, input3) \{}
  
  \CommentTok{\# do funky stuff}
\NormalTok{  ratio }\OtherTok{\textless{}{-}}\NormalTok{ input1 }\SpecialCharTok{/}\NormalTok{ (input2 }\SpecialCharTok{+} \DecValTok{4}\NormalTok{)}
\NormalTok{  funky\_output }\OtherTok{\textless{}{-}}\NormalTok{ input3 }\SpecialCharTok{*}\NormalTok{ ratio }\SpecialCharTok{+}\NormalTok{ secret\_ingredient}
  
  \FunctionTok{return}\NormalTok{(funky\_output)  }
\NormalTok{\}}

\FunctionTok{funky}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 5.5
\end{verbatim}

You then call it like so:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{secret\_ingredient }\OtherTok{\textless{}{-}} \DecValTok{100}
\FunctionTok{funky}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 102.5
\end{verbatim}

This is bad: our function acts differently even when we give it the same arguments.
Such behavior can be quite confusing, as we generally expect the function to work a certain way, given the inputs we provided it.

Even worse, we can get errors depending on this extra feature:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{secret\_ingredient }\OtherTok{\textless{}{-}} \StringTok{"A"}
\FunctionTok{funky}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Error in input3 * ratio + secret_ingredient: non-numeric argument to binary operator
\end{verbatim}

This is the \#1 gotcha with function writing.
Be careful to, in a function, only use what you are \emph{passed}, as in only use those parameters that are specified at the head of the function.
It is easy to write terrible, confusing code in R.

You can fix it by \emph{isolating the inputs}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{secret\_ingredient }\OtherTok{\textless{}{-}} \DecValTok{3}

\CommentTok{\# blah blah blah}

\NormalTok{funky }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(input1, input2, input3, secret\_ingredient) \{}
  
  \CommentTok{\# do funky stuff}
\NormalTok{  ratio }\OtherTok{\textless{}{-}}\NormalTok{ input1 }\SpecialCharTok{/}\NormalTok{ (input2 }\SpecialCharTok{+} \DecValTok{4}\NormalTok{)}
\NormalTok{  funky\_output }\OtherTok{\textless{}{-}}\NormalTok{ input3 }\SpecialCharTok{*}\NormalTok{ ratio }\SpecialCharTok{+}\NormalTok{ secret\_ingredient}
  
  \FunctionTok{return}\NormalTok{(funky\_output)  }
\NormalTok{\}}

\FunctionTok{funky}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 5.5
\end{verbatim}

Now things are nice:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{secret\_ingredient }\OtherTok{\textless{}{-}} \DecValTok{100}
\FunctionTok{funky}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 5.5
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{funky}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{100}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 102.5
\end{verbatim}

\subsection{Function skeletons}\label{function-skeletons}

When we say ``skeleton'' we simply mean the header of a function, without the middle stuff. E.g.,

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{run\_simulation }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{( N, J, mu, sigma, tau ) \{}
  
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

These are useful as documentation for sketching out a general plan of how to organize code.

\subsection{\texorpdfstring{\texttt{\%\textgreater{}\%} (Pipe) dreams}{\%\textgreater\% (Pipe) dreams}}\label{pipe-dreams}

We extensively use the ``pipe'' in our code.
For those unfamiliar, we here spend a moment discussing it, but see R for Data Science, Chapter 18, for more.
The \texttt{\%\textgreater{}\%} command allows you to apply a \textbf{sequence of functions} to a data frame; this makes your code read like a story book.

With conventional code we have

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{res1 }\OtherTok{\textless{}{-}} \FunctionTok{f}\NormalTok{(my\_data, }\AttributeTok{a =} \DecValTok{4}\NormalTok{)}
\NormalTok{res2 }\OtherTok{\textless{}{-}} \FunctionTok{g}\NormalTok{(res1, }\AttributeTok{b =} \ConstantTok{FALSE}\NormalTok{)}
\NormalTok{result }\OtherTok{\textless{}{-}} \FunctionTok{h}\NormalTok{(res2, }\AttributeTok{c =} \StringTok{"hot sauce"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Or

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{result }\OtherTok{\textless{}{-}} \FunctionTok{h}\NormalTok{(}\FunctionTok{g}\NormalTok{(}\FunctionTok{f}\NormalTok{(my\_data, }\AttributeTok{a =} \DecValTok{4}\NormalTok{), }
              \AttributeTok{b =} \ConstantTok{FALSE}\NormalTok{), }
            \AttributeTok{c =} \StringTok{"hot sauce"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Ouch.

With the pipe we have

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{result }\OtherTok{\textless{}{-}} 
\NormalTok{  my\_data }\SpecialCharTok{\%\textgreater{}\%}        \CommentTok{\# initial dataset}
  \FunctionTok{f}\NormalTok{(}\AttributeTok{a =} \DecValTok{4}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}       \CommentTok{\# do f() to it}
  \FunctionTok{g}\NormalTok{(}\AttributeTok{b =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}   \CommentTok{\# then do g()}
  \FunctionTok{h}\NormalTok{(}\AttributeTok{c =} \StringTok{"hot sauce"}\NormalTok{) }\CommentTok{\# then do h()}
\end{Highlighting}
\end{Shaded}

Nice!

\chapter{An initial simulation}\label{an-initial-simulation}

We begin with the concrete before we get abstract, with an initial simulation study that looks at the lowly
one-sample \(t\)-test under violations of the normality assumption.
In particular, we will examine the coverage of our \(t\)-test.
\emph{Coverage} is the chance of a confidence interval capturing the true parameter value.

The goal of this chapter is to make the idea of Monte Carlo simulation concrete, and to illustrate the idea of replication
and aggregation of results.
In a sense, this chapter is the entire book.
That being said, we hope to provide some deeper thinking on all the component parts in everything that follows.

Before simulation, we want to understand what we are investigating.
Let's first look at the \(t\)-test on some fake data:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# make fake data}
\NormalTok{dat }\OtherTok{=} \FunctionTok{rnorm}\NormalTok{( }\DecValTok{10}\NormalTok{, }\AttributeTok{mean=}\DecValTok{3}\NormalTok{, }\AttributeTok{sd=}\DecValTok{1}\NormalTok{ )}

\CommentTok{\# conduct the test}
\NormalTok{tt }\OtherTok{=} \FunctionTok{t.test}\NormalTok{( dat )}
\NormalTok{tt}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  One Sample t-test
## 
## data:  dat
## t = 9.6235, df = 9, p-value = 4.92e-06
## alternative hypothesis: true mean is not equal to 0
## 95 percent confidence interval:
##  2.082012 3.361624
## sample estimates:
## mean of x 
##  2.721818
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# examine the results}
\NormalTok{tt}\SpecialCharTok{$}\NormalTok{conf.int}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 2.082012 3.361624
## attr(,"conf.level")
## [1] 0.95
\end{verbatim}

For us, we have a true mean of 3. Did we capture it? To find out, we use \texttt{findInterval()}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{findInterval}\NormalTok{( }\DecValTok{3}\NormalTok{, tt}\SpecialCharTok{$}\NormalTok{conf.int )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1
\end{verbatim}

\texttt{findInterval()} checks to see where the first number lies relative to the
range given in the second argument. E.g.,

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{findInterval}\NormalTok{( }\DecValTok{1}\NormalTok{, }\FunctionTok{c}\NormalTok{(}\DecValTok{20}\NormalTok{, }\DecValTok{30}\NormalTok{) )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{findInterval}\NormalTok{( }\DecValTok{25}\NormalTok{, }\FunctionTok{c}\NormalTok{(}\DecValTok{20}\NormalTok{, }\DecValTok{30}\NormalTok{) )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{findInterval}\NormalTok{( }\DecValTok{40}\NormalTok{, }\FunctionTok{c}\NormalTok{(}\DecValTok{20}\NormalTok{, }\DecValTok{30}\NormalTok{) )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 2
\end{verbatim}

So, for us, \texttt{findInterval\ ==\ 1} means we got it! Packaging the above gives
us the following code:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# make fake data}
\NormalTok{dat }\OtherTok{=} \FunctionTok{rnorm}\NormalTok{( }\DecValTok{10}\NormalTok{, }\AttributeTok{mean=}\DecValTok{3}\NormalTok{, }\AttributeTok{sd=}\DecValTok{1}\NormalTok{ )}

\CommentTok{\# conduct the test}
\NormalTok{tt }\OtherTok{=} \FunctionTok{t.test}\NormalTok{( dat )}

\CommentTok{\# evaluate the results}
\FunctionTok{findInterval}\NormalTok{( }\DecValTok{3}\NormalTok{, tt}\SpecialCharTok{$}\NormalTok{conf.int ) }\SpecialCharTok{==} \DecValTok{1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] TRUE
\end{verbatim}

\section{Simulation for a single scenario}\label{simulation-for-a-single-scenario}

The above shows the canonical form of a single simulation trial: make the
data, analyze the data, decide how well we did.
Before writing a simulation, it is wise to understand the thing we plan on simulating.
Mucking around with code like this gets us ready.

Now let's look at coverage by doing the above many, many times and seeing how
often we capture the true parameter:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rps }\OtherTok{=} \FunctionTok{replicate}\NormalTok{( }\DecValTok{1000}\NormalTok{, \{}
\NormalTok{  dat }\OtherTok{=} \FunctionTok{rnorm}\NormalTok{( }\DecValTok{10}\NormalTok{ )}
\NormalTok{  tt }\OtherTok{=} \FunctionTok{t.test}\NormalTok{( dat )}
  \FunctionTok{findInterval}\NormalTok{( }\DecValTok{0}\NormalTok{, tt}\SpecialCharTok{$}\NormalTok{conf.int )}
\NormalTok{\})}
\FunctionTok{table}\NormalTok{( rps )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## rps
##   0   1   2 
##  27 957  16
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mean}\NormalTok{( rps }\SpecialCharTok{==} \DecValTok{1}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.957
\end{verbatim}

The \texttt{replicate()} function is one of many ways we can do things over and over in R.
We got about 95\% coverage, which is good news. We can also assess
\emph{simulation uncertainty} by recognizing that our simulation results are an
i.i.d. sample of the infinite possible simulation runs. We analyze this
sample to see a range for our true coverage.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{hits }\OtherTok{=} \FunctionTok{as.numeric}\NormalTok{( rps }\SpecialCharTok{==} \DecValTok{1}\NormalTok{ )}
\FunctionTok{prop.test}\NormalTok{( }\FunctionTok{sum}\NormalTok{(hits), }\FunctionTok{length}\NormalTok{(hits), }\AttributeTok{p =} \FloatTok{0.95}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  1-sample proportions test with continuity
##  correction
## 
## data:  sum(hits) out of length(hits), null probability 0.95
## X-squared = 0.88947, df = 1, p-value =
## 0.3456
## alternative hypothesis: true p is not equal to 0.95
## 95 percent confidence interval:
##  0.9420144 0.9683505
## sample estimates:
##     p 
## 0.957
\end{verbatim}

We have no evidence that our coverage is not what it should be: 95\%.

Things working out should hardly be surprising. The \(t\)-test is designed for
normal data and we generated normal data. In other words, our test is
following theory when we meet our assumptions. Now let's look at an
exponential distribution to see what happens when we don't have normally
distributed data. We are simulating to see what happens when we violate
our assumptions behind the \(t\)-test. Here, the true mean is 1 (the mean of a
standard exponential is 1).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rps }\OtherTok{=} \FunctionTok{replicate}\NormalTok{( }\DecValTok{1000}\NormalTok{, \{}
\NormalTok{  dat }\OtherTok{=} \FunctionTok{rexp}\NormalTok{( }\DecValTok{10}\NormalTok{ )}
\NormalTok{  tt }\OtherTok{=} \FunctionTok{t.test}\NormalTok{( dat )}
  \FunctionTok{findInterval}\NormalTok{( }\DecValTok{1}\NormalTok{, tt}\SpecialCharTok{$}\NormalTok{conf.int )}
\NormalTok{\})}
\FunctionTok{table}\NormalTok{( rps )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## rps
##   0   1   2 
##   3 905  92
\end{verbatim}

Our interval is often entirely too high and very rarely does our interval miss
because it is entirely too low.
Furthermore, our average coverage is not 95\% as it should be:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mean}\NormalTok{( rps }\SpecialCharTok{==} \DecValTok{1}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.905
\end{verbatim}

Again, to take simulation uncertainty into account we do a proportion test.
Here we have a confidence interval of our true coverage rate under our model
misspecification:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{hits }\OtherTok{=} \FunctionTok{as.numeric}\NormalTok{( rps }\SpecialCharTok{==} \DecValTok{1}\NormalTok{ )}
\FunctionTok{prop.test}\NormalTok{( }\FunctionTok{sum}\NormalTok{(hits), }\FunctionTok{length}\NormalTok{(hits) )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  1-sample proportions test with continuity
##  correction
## 
## data:  sum(hits) out of length(hits), null probability 0.5
## X-squared = 654.48, df = 1, p-value <
## 2.2e-16
## alternative hypothesis: true p is not equal to 0.5
## 95 percent confidence interval:
##  0.8847051 0.9221104
## sample estimates:
##     p 
## 0.905
\end{verbatim}

Our coverage is \emph{too low}. Our \(t\)-test based confidence interval is missing
the true value (1) more than it should.

\section{Simulating across different scenarios}\label{simulating-across-different-scenarios}

The above gives us an answer for a single, specific circumstance. We next
want to examine how the coverage changes as the sample size varies. So let's
do a one-factor experiment, with the factor being sample size, i.e., we will
conduct the above simulation for a variety of sample sizes and see how
coverage changes.

We first make a function, wrapping up our \emph{specific, single-scenario}
simulation into a bundle so we can call it under a variety of different
scenarios.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{run.experiment }\OtherTok{=} \ControlFlowTok{function}\NormalTok{( n ) \{}
\NormalTok{  rps }\OtherTok{=} \FunctionTok{replicate}\NormalTok{( }\DecValTok{10000}\NormalTok{, \{}
\NormalTok{    dat }\OtherTok{=} \FunctionTok{rexp}\NormalTok{( n )}
\NormalTok{    tt }\OtherTok{=} \FunctionTok{t.test}\NormalTok{( dat )}
    \FunctionTok{findInterval}\NormalTok{( }\DecValTok{1}\NormalTok{, tt}\SpecialCharTok{$}\NormalTok{conf.int )}
\NormalTok{  \})}

  \FunctionTok{mean}\NormalTok{( rps }\SpecialCharTok{==} \DecValTok{1}\NormalTok{ )}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Now we run \texttt{run.experiment} for different \(n\). We do this with \texttt{map\_dbl()},
which takes a list and calls a function for each value in the list (See R for
DS, Chapter 21.5).
This is kind of like a for loop, and is the tidyverse form of the \texttt{sapply()} method.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ns }\OtherTok{=} \FunctionTok{c}\NormalTok{( }\DecValTok{5}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{20}\NormalTok{, }\DecValTok{40}\NormalTok{, }\DecValTok{80}\NormalTok{, }\DecValTok{160}\NormalTok{, }\DecValTok{320}\NormalTok{, }\DecValTok{740}\NormalTok{ )}
\NormalTok{cover }\OtherTok{=} \FunctionTok{map\_dbl}\NormalTok{( ns, run.experiment )}
\end{Highlighting}
\end{Shaded}

We next take our results, make a data.frame out of them, and plot with a log scale for our \(x\)-axis:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{res }\OtherTok{=} \FunctionTok{data.frame}\NormalTok{( }\AttributeTok{n =}\NormalTok{ ns, }\AttributeTok{coverage=}\NormalTok{cover )}
\FunctionTok{ggplot}\NormalTok{( res, }\FunctionTok{aes}\NormalTok{( }\AttributeTok{x=}\NormalTok{n, }\AttributeTok{y=}\DecValTok{100}\SpecialCharTok{*}\NormalTok{coverage ) ) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{geom\_point}\NormalTok{( }\AttributeTok{size=}\DecValTok{4}\NormalTok{ ) }\SpecialCharTok{+}
  \FunctionTok{geom\_hline}\NormalTok{( }\AttributeTok{yintercept=}\DecValTok{95}\NormalTok{, }\AttributeTok{col=}\StringTok{"red"}\NormalTok{ ) }\SpecialCharTok{+}
  \FunctionTok{scale\_x\_log10}\NormalTok{( }\AttributeTok{breaks=}\NormalTok{ns ) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{( }\AttributeTok{title=}\StringTok{"Coverage rates for t{-}test on exponential data"}\NormalTok{,}
        \AttributeTok{x =} \StringTok{"n (sample size)"}\NormalTok{, }\AttributeTok{y =} \StringTok{"coverage (\%)"}\NormalTok{ ) }\SpecialCharTok{+}
  \FunctionTok{coord\_cartesian}\NormalTok{( }\AttributeTok{ylim=}\FunctionTok{c}\NormalTok{(}\DecValTok{80}\NormalTok{,}\DecValTok{100}\NormalTok{) )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{Designing-Simulations-in-R_files/figure-latex/ttest_result_figure-1} \end{center}

So far we have done a very simple simulation to assess how well a statistical method works in a given circumstance.
We have run a single factor experiment, systematically varying the sample size to examine how the behavior of our estimator changes.
In this case, we find that coverage is poor for small sample sizes, and still a bit low for higher sample sizes is well.

More broadly, the overall simulation framework, for a given scenario, is to repeatedly do the following:

\begin{itemize}
\tightlist
\item
  Generate data according to some decided upon data generation process (DGP).
  This is our model.
\item
  Analyze data according to some other process (and possibly some other assumed model).
\item
  Assess whether the analysis ``worked'' by some measure of working (such as coverage).
\end{itemize}

Frequently we would analyze our data with different methods, and compare performances across the methods.
We might do this, for example, if we were trying to see how our new, nifty method we just invented compares to business as usual.
We would also want to vary multiple aspects of our simulation (which we call factors), such as exploring coverage across a range of sample sizes (as we did) and also different disributions for the data (e.g., normal, exponential, t, and so forth).
In the next chapter we provide a framework for simulation studies, building on the core arc of this example.

\chapter{Structure of a simulation study}\label{structure-of-a-simulation-study}

In the prior chapter we saw a simple simulation evaluation of a \(t\)-test.
We next break that simulation down into components, and then in the subsequent chapters we dig into how to think about each component.
When writing simulations in our methodological work, we try to always follow the same workflow that we outline below.
We make no claim that this is the only or best way to do things, but it does work for us, and we, at least, like it!

\section{General structure of a simulation}\label{general-structure-of-a-simulation}

The easiest way to compare two estimation procedures is to generate some data from scratch, generating it in such a way that we know what the ``right answer'' is, and then to analyze our data using the procedures we wish to study.
For each dataset we generate, we can write down how close the each procedure got to the right answer, whether they ended up with an answer that was too high or too low, and so forth.
If our procedures include a hypothesis test against some hypothetical null, we can write down whether we rejected or did not reject this null.

If we do this once, we have some idea of whether the estimators worked in that specific example, but we do not know if this is due to random chance.
To assess general trends, therefore, we repeat this process over and over, keeping track of how well our estimators did each time.
We finally aggregate our results, and see if one estimator systematically outperformed the other.
But that is not quite enough: we might learn, by doing this, that estimator \(A\) is better than estimator \(B\) in the one context we have just simulated, but we will not know if it is \emph{generally} superior.

To assess superiority more broadly, we could generate data from a variety of different scenarios, seeing if our estimator is systematically winning across simulation contexts. If we do this in a a structured and thoughtful manner, we can eventually make claims as to the behaviors of our estimators that we are investigating.
This is the heart of simulation for methodological evaluation.

A simulation study can be thought of as something like a controlled scientific experiment: we want to understand the properties of our estimators, so we test them in a variety of different scenarios to see how they perform.
Found general trends across these scenarios help us understand general patterns of behavior.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.0769}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4231}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.5000}}@{}}
\caption{Steps in the Simulation Process \{\#tbl:simulation\}}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Step
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Step
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & \textbf{Generate} & Generate a sample of data based on a specified statistical model/process. \\
2 & \textbf{Analyze} & Analyze data using one or more procedures/workflows. \\
3 & \textbf{Repeat} & Repeat steps (1) \& (2) \(R\) times, recording \(R\) sets of results. \\
4 & \textbf{Summarize} & Assess the performance of the procedure across our \(R\) repetitions. \\
\end{longtable}

As we saw in our initial example, we can break the the logic of simulation for a specific and specified scenario down into the four steps outlined on Table \citet{tbl:simulation}.
We will then repeat this evaluation across different scenarios, so we can see how performance changes as we chance circumstance.
But first, we just focus on a single scenario.

\section{Tidy simulations}\label{tidy-simulations}

We advocate for writing \emph{tidy simulations}, meaning we keep all the components of a simulation separate, and tend to store all results and intermediate results as data frames (rectangular data sets).
The main way to keep things tidy is to follow a \textbf{modular approach}, in which each component of the simulation is implemented \emph{as a separate function} (or potentially a set of several functions).

Writing separate functions for the different components of the simulation makes the code easier to read, test, and debug.
Furthermore, it makes it possible to swap components of the simulation in or out, such as by adding additional estimation methods or trying out a data-generating model that involves different distributional assumptions.
In particular, we write separate functions for each of the steps given in Table \citet{tbl:simulation}.

We first write code to run a specific simulation for a specific scenario.
Once that is working, we re-use the code to systematically explore a variety of scenarios so we can
see how things change as scenario changes.

In code, we can start with the skeletons of:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Generate}
\NormalTok{generate\_data }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{( params ) \{}
  \CommentTok{\# stuff}
\NormalTok{\}}

\CommentTok{\# Analyze}
\NormalTok{analyze }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{( data ) \{}
  \CommentTok{\# stuff}
\NormalTok{\}}

\CommentTok{\# Repeat}
\NormalTok{one\_run }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{( params ) \{}
\NormalTok{  dat }\OtherTok{\textless{}{-}} \FunctionTok{generate\_data}\NormalTok{( params )}
  \FunctionTok{analyze}\NormalTok{(dat)}
\NormalTok{\}}
\NormalTok{results }\OtherTok{\textless{}{-}} \FunctionTok{map\_df}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{R, }\SpecialCharTok{\textasciitilde{}} \FunctionTok{one\_run}\NormalTok{( params ))}

\CommentTok{\# Summarize}
\NormalTok{assess\_performance }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{( results ) \{}
  \CommentTok{\# stuff}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

We next briefly describe each piece of the above.
The following four chapters then dig into each piece in more detail.

\subsection{The Data Generating Process (DGP)}\label{the-data-generating-process-dgp}

The data-generating process takes a set of parameter values as input and generates a set of simulated data as output.
When we generate data, we control the ground truth!
Generating our own data allows us to know what the answer is (e.g., what the real treatment effect is), so we can later know if our methods are doing the right thing.

\subsection{Estimation procedures}\label{estimation-procedures}

The estimation procedures are the statistical procedures under examination.
For example, an estimation procedure might be to estimate the average growth rate along with a standard error.
Or it might be to conduct a hypothesis test, like we saw earlier.

Each estimation procedure we examine should take a dataset and produce a set of estimates or results (e.g., point estimates, standard errors, confidence intervals, p-values, etc.).
You often will have different functions for each estimation procedure you are investigating.

A well written estimation methods should, in principle, work on real data as well as simulated data; the more we can ``black box'' our methods into a single function call, the easier it will be to separate out the structure of the simulation from the complexity of the methods being evaluated.

\subsection{Repetition}\label{repetition}

There are a variety of ways in R to do something over and over.
Using \texttt{map()}, as above, might look a bit strange with the \texttt{\textasciitilde{}}, but it is currently the most straightforward within the tidyverse.
Later on, we will illustrate how to use the \texttt{simhelpers} package to do this step automatically.

Making a helper method such as \texttt{one\_run()} makes debugging our simulations a lot, lot easier.
The \texttt{one\_run()} method is like a coordinator or dispatcher of our system: it generates the data, calls all the evaluation methods we want to call, combines all the results, and hands them back for recording.

Each iteration of \texttt{one\_run()} returns a small data frame of results.
We then stack all these returned results into one large dataframe of simulation results to ready it for the next step, which is assessing performance.

\subsection{Performance summaries}\label{performance-summaries}

Performance summaries are the metrics used to assess the performance of a statistical method.
Interest usually centers on understanding the performance of a method over repeated samples from a data-generating process.
For example, we might want to know how close our estimator gets to the target parameter, on average.
Or we might want to know if a confidence interval captures the truth the right proportion of the time.
To estimate these quantities we repeat steps 2 and 3 many times to get a large number of simulated estimates.
We then \emph{summarize the distribution} of the estimates to characterize performance of a method.

We generally want our analysis method to give a dataframe back because we are going to eventually stack the
results up across different scenarios to make one long dataframe of results. Happily the \texttt{dplyr}
package generally gives us dataframes so this will not usually be a problem.
But each step of the way, we will be generating data frames to keep things tidy.

\subsection{Why the tidy approach?}\label{why-the-tidy-approach}

To repeat, the tidy approach we propose has several advantages:

\begin{itemize}
\item
  Easier to check \& debug.
\item
  Easier to modify your code.
\item
  Easier to make everything run fast.
\item
  Facilitates creative re-use.
\end{itemize}

To make things even easier, the \texttt{simhelpers} package will build skeletons for each component (along with some other useful code to wire the pieces together) via the \texttt{create\_skeleton()} method.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{simhelpers}\SpecialCharTok{::}\FunctionTok{create\_skeleton}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

The \texttt{create\_skeleton()} function will open up a new R script for you that contains a template for a simulation study, with sections corresponding to each component.
Starting with this, you will already be well on the road to writing a tidy simulation.

\section{Multiple Scenarios}\label{multiple-scenarios}

We have walked through, at a high level, a modular approach to run a simulation for a single context.
In our \(t\)-test case study, for example, we might ask how well the \(t\)-test works when we have \(n=100\) units and an exponential distribution to our data.
But we rarely want to examine a single context, but instead want to explore how well a procedure works across a range of contexts.

To do this, we again use the principles of modular coding: we take all our above code for a single scenario and wrap that in a function.
Once we are sure that function works correctly, we can then call it for all the scenarios we wish.

A multiple-scenario simulation is a type of \emph{designed experiment}, in which factors such as sample size and true parameter values are systematically varied.
In fact, simulation studies typically follow a \textbf{full factorial design}, in which each level of a factor (something we vary, such as sample size, true treatment effect, or residual variance) is crossed with every other level.
The experimental design then consists of sets of parameter values (including design parameters, such as sample sizes) that will be considered as possible elements that can impact our estimation procedures performances.
We will discuss multiple-scenario simulations in Part III (starting with Chapter \citet{exp_design}), after we more fully develop the core concepts listed above.

\chapter{Case Study: Heteroskedastic ANOVA}\label{case_ANOVA}

To illustrate the process of programming a simulation, we reconstruct the simulations from Brown and Forsythe (1974).
We also use this case study as a reoccuring example in some of the following chapters.

Brown and Forsythe wanted to study methods for null hypothesis testing in the following model: Consider a population consisting of \(g\) separate groups, with population means \(\mu_1,...,\mu_g\) and population variances \(\sigma_1^2,...,\sigma_g^2\) for some characteristic \(X\).
We obtain samples of size \(n_1,...,n_g\) from each of the groups, and take measurements of the characteristic for each sampled unit.
Let \(x_{ij}\) denote the measurement from unit \(i\) in group \(j\), for \(i = 1,...,n_j\) for each \(j = 1,..., g\).
Our goal is to use the sample data to test the hypothesis that the population means are all equal, i.e.,
\[
H_0: \mu_1 = \mu_2 = \cdots = \mu_g.
\]
Now, if the population \emph{variances} were all equal (i.e., \(\sigma_1^2 = \sigma_2^2 = \cdots = \sigma_g^2\)), we could use a conventional one-way analysis of variance (ANOVA) to test.
However, one-way ANOVA might not work well if the variances are not equal.
The question is then what are best practices for testing, when in this heteroskedastic case.

To tackle this question, Brown and Forsythe evaluated two different hypothesis testing procedures, developed by James (1951) and Welch (1951), that had been proposed for testing this hypothesis without assuming equality of variances, along with the conventional one-way ANOVA F-test as a benchmark.
They also proposed and evaluated a new procedure of their own devising.
(This latter pieces makes this paper one of a canonical format for statistical methodology papers: find some problem that current procedures do not perfectly solve, invent something to do a better job, and then do simulation and/or math to build a case that the new procedure is better.)
Overall, the simulation involves comparing the performance of these different hypothesis testing procedures (the methods) under a range of conditions (different data generating processes).

For hypothesis testing, there are two main performance metrics of interest: type-I error rate and power.
The type-I error rate is, when the null hypothesis is true, how often a test falsely rejects the null.
It is a measure of how \emph{valid} a method is.
Power is how often a test correctly rejects the null when it is indeed false.
It is a measure of how \emph{powerful} (or sensitive) a method is.
The authors explored error rates and power for nominal \(\alpha\)-levels of 1\%, 5\%, and 10\%.
Table 1 of their paper reports the simulation results for type-I error (labeled as ``size''); ideally, a test should have true type-I error very close to the nominal \(\alpha\).

They looked at ten different scenarios:

\begin{table}

\caption{\label{tab:BF-scenarios}Scenarios explored by Brown and Forsythe from their Table 1}
\centering
\begin{tabular}[t]{l|l|l}
\hline
Groups & Sample Sizes & Standard Deviations\\
\hline
4 & 4,4,4,4 & 1,1,1,1\\
\hline
4 & 4,8,10,12 & 1,2,2,3\\
\hline
4 & 11,11,11,11 & 1,1,1,1\\
\hline
4 & 11,16,16,21 & 1,2,2,3\\
\hline
6 & 4,4,4,4,4,4 & 1,1,1,1,1,1\\
\hline
6 & 4,6,6,8,10,12 & 1,2,2,3\\
\hline
6 & 6,6,6,6,6,6 & 1,1,1,1,1,1\\
\hline
6 & 11,11,11,11,11,11 & 1,1,1,1,1,1\\
\hline
6 & 16,16,16,16,16,16 & 1,2,2,3\\
\hline
6 & 21,21,21,21,21,21 & 1,2,2,3\\
\hline
10 & 20,20,20,20,20,20,20,20,20,20 & 1,1,1.5,1.5,2,2,2.5,2.5,3,3\\
\hline
\end{tabular}
\end{table}

We also provide some of the numbers they reported in their Table 1 for these scenarios on Table \citet{tab:BF-table1}.

\begin{table}

\caption{\label{tab:BF-table1}Portion of "Table 1," reproduced from Brown and Forsythe}
\centering
\begin{tabular}[t]{r|r|r|r|r|r|r|r|r|r}
\hline
group & F\_10\% & F\_5\% & F\_1\% & F*\_10\% & F*\_5\% & F*\_1\% & W\_10\% & W\_5\% & W\_1\%\\
\hline
1 & 10.2 & 4.9 & 0.9 & 7.8 & 3.4 & 0.5 & 9.6 & 4.5 & 0.8\\
\hline
2 & 12.0 & 6.7 & 1.7 & 8.9 & 4.1 & 0.7 & 10.3 & 4.7 & 0.8\\
\hline
3 & 9.9 & 5.1 & 1.1 & 9.5 & 4.8 & 1.0 & 10.8 & 5.7 & 1.6\\
\hline
4 & 5.9 & 3.0 & 0.6 & 10.3 & 5.7 & 1.4 & 9.8 & 4.9 & 0.9\\
\hline
5 & 21.9 & 14.4 & 5.6 & 11.0 & 6.2 & 1.8 & 11.3 & 6.5 & 2.0\\
\hline
6 & 10.1 & 5.1 & 1.0 & 9.8 & 5.7 & 1.5 & 10.0 & 5.0 & 0.9\\
\hline
7 & 11.4 & 6.3 & 1.8 & 10.7 & 5.7 & 1.5 & 10.1 & 5.0 & 1.1\\
\hline
8 & 10.3 & 4.9 & 1.1 & 10.3 & 5.1 & 1.0 & 10.2 & 5.0 & 1.0\\
\hline
9 & 17.3 & 10.8 & 3.9 & 11.1 & 6.2 & 1.8 & 10.5 & 5.5 & 1.2\\
\hline
10 & 7.3 & 4.0 & 1.0 & 11.5 & 6.5 & 1.8 & 10.6 & 5.4 & 1.1\\
\hline
11 & 9.6 & 4.9 & 1.0 & 7.3 & 3.4 & 0.4 & 11.4 & 6.1 & 1.4\\
\hline
\end{tabular}
\end{table}

Table 2 reports results on power; it is desirable to have higher power to reject null hypotheses that are false, so higher rates are better here.

To replicate this simulation we are going to first write code to evaluate our three procedures in a specific scenario with a specific set of core parameters (e.g., sample sizes, number of groups, and so forth), and then scale up to do a range of scenarios where we vary these parameters.

\section{The data-generating model}\label{the-data-generating-model}

In the heteroskedastic one-way ANOVA simulation, there are three sets of parameter values: population means, population variances, and sample sizes.
Rather than attempting to write a general data-generating function immediately, it is often easier to write code for a specific case first and then use that code as a launch point for the rest.
For example, say that we have four groups with means of 1, 2, 5, 6; variances of 3, 2, 5, 1; and sample sizes of 3, 6, 2, 4:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mu }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{6}\NormalTok{)}
\NormalTok{sigma\_sq }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{sample\_size }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{4}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Following Brown and Forsythe, we'll assume that the measurements are normally distributed within each sub-group of the population. The following code generates a vector of group id's and a vector of simulated measurements:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{N }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(sample\_size) }\CommentTok{\# total sample size}
\NormalTok{g }\OtherTok{\textless{}{-}} \FunctionTok{length}\NormalTok{(sample\_size) }\CommentTok{\# number of groups}

\CommentTok{\# group id}
\NormalTok{group }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{g, }\AttributeTok{times =}\NormalTok{ sample\_size)}

\CommentTok{\# mean for each unit of the sample}
\NormalTok{mu\_long }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(mu, }\AttributeTok{times =}\NormalTok{ sample\_size) }

\CommentTok{\# sd for each unit of the sample}
\NormalTok{sigma\_long }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(}\FunctionTok{sqrt}\NormalTok{(sigma\_sq), }\AttributeTok{times =}\NormalTok{ sample\_size) }

\CommentTok{\# See what we have?}
\FunctionTok{tibble}\NormalTok{( }\AttributeTok{group=}\NormalTok{group, }\AttributeTok{mu=}\NormalTok{mu\_long, }\AttributeTok{sigma=}\NormalTok{sigma\_long)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 15 x 3
##    group    mu sigma
##    <int> <dbl> <dbl>
##  1     1     1  1.73
##  2     1     1  1.73
##  3     1     1  1.73
##  4     2     2  1.41
##  5     2     2  1.41
##  6     2     2  1.41
##  7     2     2  1.41
##  8     2     2  1.41
##  9     2     2  1.41
## 10     3     5  2.24
## 11     3     5  2.24
## 12     4     6  1   
## 13     4     6  1   
## 14     4     6  1   
## 15     4     6  1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Now make our data}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(N, }\AttributeTok{mean =}\NormalTok{ mu\_long, }\AttributeTok{sd =}\NormalTok{ sigma\_long)}
\FunctionTok{tibble}\NormalTok{(}\AttributeTok{group =}\NormalTok{ group, }\AttributeTok{x =}\NormalTok{ x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 15 x 2
##    group      x
##    <int>  <dbl>
##  1     1 -0.842
##  2     1  1.14 
##  3     1  2.31 
##  4     2  1.40 
##  5     2  1.92 
##  6     2  2.54 
##  7     2 -1.09 
##  8     2  4.47 
##  9     2 -0.658
## 10     3  6.39 
## 11     3  5.72 
## 12     4  5.25 
## 13     4  5.02 
## 14     4  4.66 
## 15     4  6.83
\end{verbatim}

We have made a small dataset of group membership and outcome.
We note that there are many different and legitimate ways of doing this in R.
E.g., we could generate each group separately, and then stack our groups, instead of using \texttt{rep} to do it all at once.
In general, we advocate the adage that if you can do it at all, then you should feel good about yourself.
Do not worry about writing code the ``best'' way when you are initially putting a simulation together.

To continue, as we are going to generate data over and over, we wrap this code in a function.
We also make our means, variances and sample sizes be parameters of our function so we can make datasets of different sizes and shapes, like so:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{generate\_data }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(mu, sigma\_sq, sample\_size) \{}

\NormalTok{  N }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(sample\_size)}
\NormalTok{  g }\OtherTok{\textless{}{-}} \FunctionTok{length}\NormalTok{(sample\_size)}

\NormalTok{  group }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{g, }\AttributeTok{times =}\NormalTok{ sample\_size)}
\NormalTok{  mu\_long }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(mu, }\AttributeTok{times =}\NormalTok{ sample\_size)}
\NormalTok{  sigma\_long }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(}\FunctionTok{sqrt}\NormalTok{(sigma\_sq), }\AttributeTok{times =}\NormalTok{ sample\_size)}

\NormalTok{  x }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(N, }\AttributeTok{mean =}\NormalTok{ mu\_long, }\AttributeTok{sd =}\NormalTok{ sigma\_long)}
\NormalTok{  sim\_data }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}\AttributeTok{group =}\NormalTok{ group, }\AttributeTok{x =}\NormalTok{ x)}

  \FunctionTok{return}\NormalTok{(sim\_data)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

The above code is simply the code we built previously, all bundled up.
Our workflow is to scrabble around to get it to work once, the way we want, and then bundle up our final work into a function for later reuse.

Given our method, we can easily call our function to get a new set of simulated data.
For example, to generate a dataset with the same parameters as before, we can do:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sim\_data }\OtherTok{\textless{}{-}} \FunctionTok{generate\_data}\NormalTok{(}\AttributeTok{mu =}\NormalTok{ mu, }\AttributeTok{sigma\_sq =}\NormalTok{ sigma\_sq, }
                          \AttributeTok{sample\_size =}\NormalTok{ sample\_size)}
\end{Highlighting}
\end{Shaded}

To generate one with 0s for the averages of each group, but the same group variances and sample sizes as before, we can do:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sim\_data\_null }\OtherTok{\textless{}{-}} \FunctionTok{generate\_data}\NormalTok{( }\AttributeTok{mu =} \FunctionTok{c}\NormalTok{( }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{ ),}
                                \AttributeTok{sigma\_sq =}\NormalTok{ sigma\_sq, }
                                \AttributeTok{sample\_size =}\NormalTok{ sample\_size)}
\end{Highlighting}
\end{Shaded}

\subsection{Coding remark}\label{coding-remark}

In the above, we built some sample code, and then bundled it into a function by literally cutting and pasting the initial work we did into a function skeleton.
In the process, we shifted from having variables in our workspace with different names to using those variable names as parameters in our function call.

Developing code in this way is not without hazards.
In particular, after finishing making our function, our workspace has a variable \texttt{mu} in it and our function also has a parameter named \texttt{mu}.
Inside the function, R will use the parameter \texttt{mu} first, but this is potentially confusing.
As are, potentially, lines such as \texttt{mu\ =\ mu}, which means ``set the function's parameter called \texttt{mu} to the variable called \texttt{mu}.''
These are different things (with the same name).

One way to check your code, once a function is built, is to comment out the initial code (or delete it), restart R or at least clear out the workspace, and then re-run the code that uses the function.
If things still work, then you should be somewhat confident you successfully bundled your code into the function.

You can also, once you bundle your code, do a search and replace to change variable names in your function to something more generic, to make the separation more clear.

\section{The estimation procedures}\label{the-estimation-procedures}

Brown and Forsythe considered four different hypothesis testing procedures for heteroskedastic ANOVA.
We start with the simplest one, which is just to use a conventional one-way ANOVA (while mistakenly assuming homoskedasticity). R's \texttt{oneway.test} function will actually calculate this test automatically:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sim\_data }\OtherTok{\textless{}{-}} \FunctionTok{generate\_data}\NormalTok{(}\AttributeTok{mu =}\NormalTok{ mu, }\AttributeTok{sigma\_sq =}\NormalTok{ sigma\_sq,}
                          \AttributeTok{sample\_size =}\NormalTok{ sample\_size)}
\FunctionTok{oneway.test}\NormalTok{(x }\SpecialCharTok{\textasciitilde{}} \FunctionTok{factor}\NormalTok{(group), }\AttributeTok{data =}\NormalTok{ sim\_data, }\AttributeTok{var.equal =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  One-way analysis of means
## 
## data:  x and factor(group)
## F = 8.0596, num df = 3, denom df = 11,
## p-value = 0.004044
\end{verbatim}

The main result we need here is the \(p\)-value, which will let us assess the test's Type-I error and power for a given nominal \(\alpha\)-level. The following function takes simulated data as input and returns as output the \(p\)-value from a one-way ANOVA:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ANOVA\_F\_aov }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(sim\_data) \{}
\NormalTok{  oneway\_anova }\OtherTok{\textless{}{-}} \FunctionTok{oneway.test}\NormalTok{(x }\SpecialCharTok{\textasciitilde{}} \FunctionTok{factor}\NormalTok{(group), }\AttributeTok{data =}\NormalTok{ sim\_data,}
                              \AttributeTok{var.equal =} \ConstantTok{TRUE}\NormalTok{)}
  \FunctionTok{return}\NormalTok{(oneway\_anova}\SpecialCharTok{$}\NormalTok{p.value)}
\NormalTok{\}}

\FunctionTok{ANOVA\_F\_aov}\NormalTok{(sim\_data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.004043699
\end{verbatim}

We might instead write the code to implement the ANOVA test ourselves.
This has some plusses (e.g., \texttt{oneway.test} maybe is doing a lot of other stuff which could take time and slow down our simulation) and minuses (e..g, writing our own code takes \emph{our} time, and it gives us lots of room to make mistakes and thus make life hard on ourself).
For further discussion of the trade-offs, see @ref(optimize\_code), where we do implement it by hand and see what kind of speed-ups we can obtain by doing that.

We next implement the Welch test, another one of the tests considered by Brown and Forsythe.
Here is a function that calculates the Welch test by hand, again following the notation and formulas from the paper:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Welch\_F }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(sim\_data) \{}

\NormalTok{  stats }\OtherTok{\textless{}{-}}\NormalTok{ sim\_data }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{group\_by}\NormalTok{( group ) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{summarise}\NormalTok{( }\AttributeTok{x\_bar =} \FunctionTok{mean}\NormalTok{( x ),}
               \AttributeTok{s\_sq =} \FunctionTok{var}\NormalTok{( x ),}
               \AttributeTok{n =} \FunctionTok{n}\NormalTok{() )}
  
\NormalTok{  g }\OtherTok{=} \FunctionTok{nrow}\NormalTok{( stats )}

\NormalTok{  stats }\OtherTok{\textless{}{-}} \FunctionTok{mutate}\NormalTok{( stats, }
                   \AttributeTok{w =}\NormalTok{ n }\SpecialCharTok{/}\NormalTok{ s\_sq )}
  
\NormalTok{  res }\OtherTok{\textless{}{-}}\NormalTok{ stats }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{summarise}\NormalTok{( }\AttributeTok{u =} \FunctionTok{sum}\NormalTok{(w),}
               \AttributeTok{x\_tilde =} \FunctionTok{sum}\NormalTok{( w }\SpecialCharTok{*}\NormalTok{ x\_bar ) }\SpecialCharTok{/}\NormalTok{ u,}
               \AttributeTok{msbtw =} \FunctionTok{sum}\NormalTok{( w }\SpecialCharTok{*}\NormalTok{ (x\_bar }\SpecialCharTok{{-}}\NormalTok{ x\_tilde)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{ ) }\SpecialCharTok{/}\NormalTok{ (g }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{),}
               \AttributeTok{G =} \FunctionTok{sum}\NormalTok{( (}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ w }\SpecialCharTok{/}\NormalTok{ u)}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{/}\NormalTok{ (n }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{) ),}
               \AttributeTok{denom =} \DecValTok{1} \SpecialCharTok{+}\NormalTok{  G }\SpecialCharTok{*} \DecValTok{2} \SpecialCharTok{*}\NormalTok{ (g }\SpecialCharTok{{-}} \DecValTok{2}\NormalTok{) }\SpecialCharTok{/}\NormalTok{ (g}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{{-}} \DecValTok{1}\NormalTok{),}
               \AttributeTok{W =}\NormalTok{ msbtw }\SpecialCharTok{/}\NormalTok{ denom,}
               \AttributeTok{f =}\NormalTok{ (g}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{{-}} \DecValTok{1}\NormalTok{) }\SpecialCharTok{/}\NormalTok{ (}\DecValTok{3} \SpecialCharTok{*}\NormalTok{ G) )}

\NormalTok{  pval }\OtherTok{\textless{}{-}} \FunctionTok{pf}\NormalTok{(res}\SpecialCharTok{$}\NormalTok{W, }\AttributeTok{df1 =}\NormalTok{ g }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{, }\AttributeTok{df2 =}\NormalTok{ res}\SpecialCharTok{$}\NormalTok{f, }\AttributeTok{lower.tail =} \ConstantTok{FALSE}\NormalTok{)}

  \FunctionTok{return}\NormalTok{(pval)}
\NormalTok{\}}

\FunctionTok{Welch\_F}\NormalTok{(sim\_data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.04428594
\end{verbatim}

Our estimation function does not care if the data are simulated or not--we call the parameter \texttt{data} not \texttt{sim\_data} to reflect this.
Eventually, we might imagine using this function on our real data!
Also, we can put a lot of complex stuff in a function--and then forget all about it.
In the above, for example, you can understand everything we say about writing simulations even if you do not understand anything at all about what this function is doing: it is just a black box that takes data and returns a \(p\)-value.

\section{Running the simulation}\label{running-the-simulation}

We now have functions that implement steps 2 and 3 of the simulation.
Given some parameters, \texttt{generate\_data} produces a simulated dataset and \texttt{ANOVA\_F\_aov} and \texttt{Welch\_F} use the simulated data to calculate \(p\)-values two different ways.
We now want to know which way is better, and how.
To answer this question, we next need to repeat this chain of calculations a bunch of times.

We first make a function that puts our chain together in a single method.
This method is also responsible for putting the results together in a tidy structure that is easy to aggregate and analyze.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{one\_run }\OtherTok{=} \ControlFlowTok{function}\NormalTok{( mu, sigma\_sq, sample\_size ) \{}
\NormalTok{  sim\_data }\OtherTok{\textless{}{-}} \FunctionTok{generate\_data}\NormalTok{(}\AttributeTok{mu =}\NormalTok{ mu, }\AttributeTok{sigma\_sq =}\NormalTok{ sigma\_sq,}
                            \AttributeTok{sample\_size =}\NormalTok{ sample\_size)}
\NormalTok{  anova\_p }\OtherTok{\textless{}{-}} \FunctionTok{ANOVA\_F\_aov}\NormalTok{(sim\_data)}
\NormalTok{  Welch\_p }\OtherTok{\textless{}{-}} \FunctionTok{Welch\_F}\NormalTok{(sim\_data)}
  \FunctionTok{tibble}\NormalTok{(}\AttributeTok{ANOVA =}\NormalTok{ anova\_p, }\AttributeTok{Welch =}\NormalTok{ Welch\_p)}
\NormalTok{\}}

\FunctionTok{one\_run}\NormalTok{( }\AttributeTok{mu =}\NormalTok{ mu, }\AttributeTok{sigma\_sq =}\NormalTok{ sigma\_sq, }\AttributeTok{sample\_size =}\NormalTok{ sample\_size )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1 x 2
##     ANOVA  Welch
##     <dbl>  <dbl>
## 1 0.00386 0.0266
\end{verbatim}

A single simulation trial simply does steps 2 and 3, ending with a nice dataframe or tibble that has our results for that single run.

We next call \texttt{one\_run()} over and over; see \citet{repeating_oneself} for some discussion of options.
The following uses \texttt{map\_df} to run \texttt{one\_run()} 4 times and then stack the results into a single data frame (the \texttt{\_df} tells R to do the stacking):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sim\_data }\OtherTok{\textless{}{-}} \FunctionTok{map\_df}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{,}
                  \SpecialCharTok{\textasciitilde{}} \FunctionTok{one\_run}\NormalTok{(}\AttributeTok{mu =}\NormalTok{ mu, }\AttributeTok{sigma\_sq =}\NormalTok{ sigma\_sq,}
                          \AttributeTok{sample\_size =}\NormalTok{ sample\_size) )}
\NormalTok{sim\_data}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 4 x 2
##      ANOVA   Welch
##      <dbl>   <dbl>
## 1 0.00686  0.0874 
## 2 0.0118   0.00684
## 3 0.00882  0.0332 
## 4 0.000152 0.0121
\end{verbatim}

Voila! We have simulated \(p\)-values!

\section{Analyzing the Simulation}\label{analyzing-the-simulation}

We now have all the pieces in place to reproduce the results from Brown and Forsythe (1974).
We first focus on calculating the actual type-I error rate of these tests---that is, the proportion of the time that they reject the null hypothesis of equal means when that null is actually true---for an \(\alpha\)-level of .05.
We therefore need to simulate data according to process where the population means are indeed all equal. Arbitrarily, we start with \(g = 4\) groups and set all of the means equal to zero:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mu }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{4}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

In the fifth row of Table 1, Brown and Forsythe examine performance for the following parameter values for sample size and population variance:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sample\_size }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{4}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{12}\NormalTok{)}
\NormalTok{sigma\_sq }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{)}\SpecialCharTok{\^{}}\DecValTok{2}
\end{Highlighting}
\end{Shaded}

With these parameter values, we can use our \texttt{replicate} code to simulate 10,000 \(p\)-values:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p\_vals }\OtherTok{\textless{}{-}} \FunctionTok{rerun}\NormalTok{(}\DecValTok{10000}\NormalTok{, }
\NormalTok{  sim\_data }\OtherTok{\textless{}{-}} \FunctionTok{one\_run}\NormalTok{(}\AttributeTok{mu =}\NormalTok{ mu,}
                      \AttributeTok{sigma\_sq =}\NormalTok{ sigma\_sq,}
                      \AttributeTok{sample\_size =}\NormalTok{ sample\_size) )}
\NormalTok{p\_vals }\OtherTok{\textless{}{-}} \FunctionTok{bind\_rows}\NormalTok{(p\_vals)}
\NormalTok{p\_vals}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 10,000 x 2
##     ANOVA Welch
##     <dbl> <dbl>
##  1 0.0582 0.114
##  2 0.784  0.728
##  3 0.0473 0.166
##  4 0.334  0.516
##  5 0.0182 0.284
##  6 0.498  0.650
##  7 0.0155 0.423
##  8 0.261  0.445
##  9 0.815  0.495
## 10 0.343  0.396
## # ... with 9,990 more rows
\end{verbatim}

We next use our replications to calculate the rejection rates.
The rule is that the null is rejected if the \(p\)-value is less than \(\alpha\). To get the rejection rate, calculate the proportion of replications where the null is rejected.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sum}\NormalTok{(p\_vals}\SpecialCharTok{$}\NormalTok{ANOVA }\SpecialCharTok{\textless{}} \FloatTok{0.05}\NormalTok{) }\SpecialCharTok{/} \DecValTok{10000}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.1428
\end{verbatim}

This is equivalent to taking the mean of the logical conditions:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mean}\NormalTok{(p\_vals}\SpecialCharTok{$}\NormalTok{ANOVA }\SpecialCharTok{\textless{}} \FloatTok{0.05}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.1428
\end{verbatim}

We get a rejection rate that is much larger than \(\alpha = .05\).
We have learned that the ANOVA F-test does not adequately control Type-I error under this set of conditions.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mean}\NormalTok{(p\_vals}\SpecialCharTok{$}\NormalTok{Welch }\SpecialCharTok{\textless{}} \FloatTok{0.05}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.0659
\end{verbatim}

The Welch test does much better, although it appears to be a little bit in excess of 0.05.

Note that these two numbers are quite close (though not quite identical) to the corresponding entries in Table 1 of Brown and Forsythe (1974). The difference is due to the fact that both Table 1 and are results are actually \emph{estimated} rejection rates, because we have not actually simulated an infinite number of replications. The estimation error arising from using a finite number of replications is called \emph{simulation error} (or \emph{Monte Carlo error}).
Later on, we will look more at how to estimate and control the Monte Carlo simulation error in our studies.

\section{Exercises}\label{exAnovaExercises}

The following exercises involve exploring and tweaking the above simulation code we have developed to replicate the results of Brown and Forsythe (1974).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Table 1 from Brown and Forsythe reported rejection rates for \(\alpha = .01\) and \(\alpha = .10\) in addition to \(\alpha = .05\). Calculate the rejection rates of the ANOVA F and Welch tests for all three \(\alpha\)-levels.
\item
  Try simulating the Type-I error rates for the parameter values in the first two rows of Table 1 of the original paper. Use 10,000 replications. How do your results compare to the results reported in Table 1?
\item
  Try simulating the \textbf{power levels} for a couple of sets of parameter values from Table 2. Use 10,000 replications. How do your results compare to the results reported in the Table?
\item
  One might, instead of having \texttt{one\_run} return a single row with the columns for the \(p\)-values, have multiple rows with each row being a test (so one row for ANOVA and one for Welch). E.g., it might produce results like this:
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{one\_run\_long}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 2 x 2
##   method pvalue
##   <chr>   <dbl>
## 1 ANOVA  0.0158
## 2 Welch  0.116
\end{verbatim}

Modify \texttt{one\_run()} to do this, update your simulation code, and then use \texttt{group\_by()} plus \texttt{summarise()} to calculate rejection rates in one go. The ``long'' approach is often nicer when evaluating more than two methods, or when each method returns not just a \(p\)-value but other quantities of interest.

\chapter{Data-Generating Processes (DGPs)}\label{chap_DGP}

The data generating process (DGP) is the recipe we use to create fake data that we then analyze.
Often we express our DGP as a specific model, with parameters that we can set to generate data.
The advantage of this is, generally, when we generate from a specified model we know the ``right answer,'' and can thus compare our estimates to this right answer in order to assess whether our estimation procedures worked.

The easiest way to describe a DGP is usually via a mathematical model, which is fundamentally a sequence of equations and random variables that define a series of steps.
Describing DGPs in this way is especially important for more complex DGPs, such as those for hierarchical data.
These models will often be a series of chained linear equations that use a set of parameters that we set out.
Once we have them, we can convert these equations to code by simply following these laid out steps.

In this chapter, after giving a high level overview of the DGP process, we will walk through a running example of clustered data.
In particular, we are going to focus on generating two-level data of students nested in schools.

There are several ingredients of a full mathematical model that we could use to generate data.

\textbf{COVARIATES, STRUCTURAL COVARIATES, and OUTCOMES}
Covariates are the things that we are usually given when analyzing real data, such as student demographics, or school-level characteristics such as the school's treatment assignment.
Structural covariates are covariates that we do not tend to think of as covariates per se; they are more just consequences of the data.
These are elements such as the number of observations in each school or proportion treated in each school.

In the real world statistical analysis, we rarely model covariates, but instead condition on them.
In a simulation, however, we have to decide how they come to be.

Structural covariates are also a consequence of how we decide to generate data; for example, if we are generating sites of different size, we may put a distribution on site size, and generate the sizes according to that distribution.

Outcomes are a type of covariate that are usually dependent on other covariates in our model.
In other words, some covariates depend on other covariates, which can govern the order that we can generate all of our data.

\textbf{MODEL}
This is the parametric relationship between everything, such as a specification of how the outcomes are linked to the covariates.
This includes specification of the randomness (the distribution of the residuals, etc.).
The model will usually contain equations that one might see in an analysis of data that looks like what we are trying to generate.

The full model will also have some extra parts that define how to generate the covariates and structural covariates.
For example, we might specify that site sizes are uniformly distributed between a specified minimum and maximum size, or that some covariate is normally distributed.
We might also specify an equation that connects size size to site average treatment impact, or things of that nature.

\textbf{PARAMETERS}
For a given model, parameters describe how strong a relationship there is between covariate and outcome, variance of the residuals, and so forth.
We usually estimate these \emph{from} data.
Critically, if we know them, we can \emph{generate new data}.

\textbf{DESIGN PARAMETERS}
Design parameters are the parameters that go with the parts of our model that we normally would not use when analyzing our data.
These are, e.g., the number of sites, the range of allowed site sizes.
These will control how we generate the structural covariates.
We might also have parameters governing covariate generation, such as means and variances and so forth.

For example, for the Welch data earlier we have, for observation \(i\) in group \(g\), a mathematical representation of our data of:

\[ X_{ig} = \mu_g + \epsilon_{ig} \mbox{ with } \epsilon_{ig} \sim N( 0, \sigma^2_g ) \]

These math equations would also come along with specified parameter values (the \(\mu_g\), the \(\sigma^2_g\)), and the design parameter of the sample sizes \(n_g\).

\section{A statistical model is a recipe for data generation}\label{a-statistical-model-is-a-recipe-for-data-generation}

Once we have a recipe (our mathematical model), the next step is to translate it to code.
In the real world:

\begin{itemize}
\tightlist
\item
  We obtain data, we pick a model, we estimate parameters.
\item
  The data comes with covariates and outcomes.
\item
  It also comes with sample size, sizes of the clusters, etc.
\end{itemize}

In the simulation world, by comparison:

\begin{itemize}
\tightlist
\item
  We pick a model, we decide how much data, we generate covariates, we pick the parameters, and then we generate outcomes.
\item
  We need to decide how many clusters we have, how big the clusters are, etc.
\item
  We have to specify how the covariates are made. This last piece is very different from real-world analysis.
\end{itemize}

In terms of code, a function that implements a data-generating model should have the following form:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{generate\_data }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(parameters) \{}

  \CommentTok{\# generate pseudo{-}random numbers and use those to}
  \CommentTok{\# make some data}
  
  \FunctionTok{return}\NormalTok{(sim\_data)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

The function takes a set of parameter values as input, simulates random numbers and does calculations, and produces as output a set of simulated data.
Again, there will in general be multiple parameters, and these will include not only the model parameters (e.g.~the coefficients of a regression), but also sample sizes and other study design parameters.
The output will typically be a dataframe, mimicking what data one would see in the ``real world,'' possibly augmented by some other latent (normally unobserved in the real world) values that we can use later on to assess whether the estimation procedures we are checking are close to the truth.

For example, from our Welch case study, we had the following method that generates grouped data with a single outcome.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{generate\_data }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(mu, sigma\_sq, sample\_size) \{}

\NormalTok{  N }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(sample\_size) }
\NormalTok{  g }\OtherTok{\textless{}{-}} \FunctionTok{length}\NormalTok{(sample\_size) }
  
\NormalTok{  group }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{g, }\AttributeTok{times =}\NormalTok{ sample\_size) }
\NormalTok{  mu\_long }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(mu, }\AttributeTok{times =}\NormalTok{ sample\_size)}
\NormalTok{  sigma\_long }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(}\FunctionTok{sqrt}\NormalTok{(sigma\_sq), }\AttributeTok{times =}\NormalTok{ sample\_size) }
  
\NormalTok{  x }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(N, }\AttributeTok{mean =}\NormalTok{ mu\_long, }\AttributeTok{sd =}\NormalTok{ sigma\_long)}
\NormalTok{  sim\_data }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}\AttributeTok{group =}\NormalTok{ group, }\AttributeTok{x =}\NormalTok{ x)}
    
  \FunctionTok{return}\NormalTok{(sim\_data)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Our function takes both parameters as we normally thing of them (\texttt{mu}, \texttt{sigma\_sq}), and other values that we might not think of as parameters per-se (\texttt{sample\_size}).
When simulating data, we have to specify quantities that we, when analyzing data, often have to take for granted.

\section{Checking the data-generating function}\label{checking-the-data-generating-function}

An important part of programing in R---particularly when writing functions---is finding ways to test and check the correctness of your code. Thus, after writing a data-generating function, we need to consider how to test whether the output it produces is correct. How best to do this will depend on the data-generating process being implemented.

For the heteroskedastic ANOVA problem, one basic thing we could do is check that the simulated data from each group follows a normal distribution. By generating very large samples from each group, we can effectively check characteristics of the population distribution.
In the following code, we simulate very large samples from each of the four groups, and check that the means and variances agree with the input parameters:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{check\_data }\OtherTok{\textless{}{-}} \FunctionTok{generate\_data}\NormalTok{(}\AttributeTok{mu =}\NormalTok{ mu, }\AttributeTok{sigma\_sq =}\NormalTok{ sigma\_sq,}
                            \AttributeTok{sample\_size =} \FunctionTok{rep}\NormalTok{(}\DecValTok{10000}\NormalTok{, }\DecValTok{4}\NormalTok{))}

\NormalTok{chk }\OtherTok{\textless{}{-}}\NormalTok{ check\_data }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{group\_by}\NormalTok{( group ) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{summarise}\NormalTok{( }\AttributeTok{n =} \FunctionTok{n}\NormalTok{(),}
             \AttributeTok{mean =} \FunctionTok{mean}\NormalTok{( x ),}
             \AttributeTok{var =} \FunctionTok{var}\NormalTok{( x ) ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{( }\AttributeTok{mu =}\NormalTok{ mu,}
          \AttributeTok{sigma2 =}\NormalTok{ sigma\_sq ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{relocate}\NormalTok{( group, n, mean, mu, var, sigma2 )}
\NormalTok{chk}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 4 x 6
##   group     n     mean    mu   var sigma2
##   <int> <int>    <dbl> <dbl> <dbl>  <dbl>
## 1     1 10000  0.00915     0  9.05      9
## 2     2 10000 -0.0252      0  4.04      4
## 3     3 10000 -0.0154      0  3.97      4
## 4     4 10000  0.00372     0  1.00      1
\end{verbatim}

We are recovering our parameters.

We can also make some diagnostic plots to assess whether we have normal data (using QQ plots, where we expect a straight line if the data are normal):

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{( check\_data, }\FunctionTok{aes}\NormalTok{( }\AttributeTok{sample=}\NormalTok{x ) ) }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{( }\SpecialCharTok{\textasciitilde{}}\NormalTok{ group ) }\SpecialCharTok{+}
  \FunctionTok{stat\_qq}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{Designing-Simulations-in-R_files/figure-latex/unnamed-chunk-50-1} \end{center}

We again check out.
Here, these checks may seem a bit silly, but most bugs are silly---at least once you find them!
It is easy for small things such as a sign error to happen once your model gets a bit more complex; even simple checks such as these can be quite helpful.

\section{Example: Simulating clustered data}\label{case_cluster}

Generating data with complex structure can be intimidating, but if you set out a recipe for how the data is generated it is often not to bad to build that recipe up with code.
We will next provide a more complex example with a case study of using simulation to determine best practices for analyzing data from a cluster-randomized RCT of students nested in schools.

Recent literature on multisite trials (where, for example, students are randomized to treatment or control within each of a series of sites) has explored how variation in the size of impacts across sites can affect how estimators behave, and what models we should use when there is impact variation (e.g., \citet{miratrix2021applied}, \citet{Bloom:2016um}).
We are going to extend this work to explore best practices for estimating treatment effects in cluster randomized trials.

Cluster randomized trials are randomized experiments where the unit of randomization is a group of individuals, rather than the individuals themselves.
For example, if we have a collection of schools, with students in schools, a cluster randomized trial would randomize the \emph{schools} into treatment or control, and then measure our outcome on the \emph{students} inside the schools.
We might be trying to estimate, for example, whether the average score of the treatment schools is different from the average score of the control schools.
In particular, we want to investigate what happens when the average treatment impact of a school is related to the size of the school.

Often we will design a data generating process to allow us to answer a specific question.
For our Welch example, we wanted to know how different amounts of variation in different groups impacted estimation.
We therefore needed a data generation process that allowed us to control that variation.
To figure out what we need for our clustered data example, we need to think about how we are going to use those data in our simulation study.

\subsection{A design decision: What do we want to manipulate?}\label{a-design-decision-what-do-we-want-to-manipulate}

There are a lot of ways we might generate cluster randomized trial data.
To pick between the many options, we need to think about the goals of the simulation.

Overall, our final data should be a collection of clusters with different sizes and different baseline mean outcomes.
Some of the clusters will be treated, and some not.
We can imagine our final data being individual students in schools, with each student having a school id, a treatment assignment (shared for all in the school) and an outcome.
A good starting point for building a DGP is to first write down a sketch of what the eventual data might look like on a piece of scratch paper.
In our case, for example, we might write down:

\begin{longtable}[]{@{}rrrrr@{}}
\toprule\noalign{}
schoolID & Z & size & studentID & Y \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & 1 & 24 & 1 & 3.6 \\
1 & 1 & 24 & 3 & 1.0 \\
1 & etc & etc & etc & etc \\
1 & 1 & 24 & 24 & 2.0 \\
2 & 0 & 32 & 1 & 0.5 \\
2 & 0 & 32 & 2 & 1.5 \\
2 & 0 & 32 & 3 & 1.2 \\
etc & etc & etc & etc & etc \\
\end{longtable}

We know we are planning on comparing multiple estimators, seeing how they behave differently under different conditions.
We also know that we are interested in what happens when the size of the treatment impact varies across sites, and in particular what happens when it is associated with site size.

Given these goals and beliefs, we might think:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  We figure if all the sites are the same size, then all the estimators will probably be ok. But if sites vary, then maybe we could have issues with our estimators.
\item
  Also, if site size varies, but has nothing to do with impact, then all the estimators might still be ok, at least for bias, but if size is associated with treatment impact, then maybe how our estimators end up averaging across sites is going to matter.
\end{enumerate}

Usually, when running a simulation, it is good practice to test the simple option along with the complex one.
We want to both check that something does not matter as well as verify that it does.
Given this, we land on the following points:

\begin{itemize}
\tightlist
\item
  We need a DGP that has the option to make all-same-size sites or variable size sites.
\item
  Our DGP should have some impact variation across sites.
\item
  Our DGP should allow for different sites to have different treatment impacts.
\item
  We should have the option to connect impact variation to site size.
\end{itemize}

\subsection{A model for a cluster RCT}\label{a-model-for-a-cluster-rct}

It is usually easiest to start a recipe for data generating by writing down the mathematical model.
Write down something, and then, if you do not yet know how to generate some part of what you wrote down, specify how to generate those parts that you are using, in an iterative process.

For our model, we start with a model for our student outcome:

\[ Y_{ij} = \beta_{0j} + \epsilon_{ij} \mbox{ with } \epsilon_{ij} \sim N( 0, \sigma^2_\epsilon ) \]
where \(Y_{ij}\) is the outcome for student \(i\) in site \(j\), \(\beta_{0j}\) is the average outcome in site \(j\), and \(\epsilon_{ij}\) is the residual error for student \(i\) in site \(j\).

We then need to figure out how to make the average outcome in site \(j\).
In looking at our goals, we want \(\beta_{0j}\) to depend on treatment assignment.
We might then write down:

\[ \beta_{0j} = \gamma_0 + \gamma_1 Z_j + u_j \mbox{ with } u_j \sim N( 0, \sigma^2_u )\]
saying the average outcome in site \(j\) is the average outcome in the control group (\(\gamma_0\)) plus some treatment impact (\(\gamma_1\)) if the site is treated.
We added a \(u_j\) so that our different sites can be different from each other in terms of their average outcome, even if they are not treated.
To keep things simple, we are having a common treatment impact within cluster: if we treat a cluster, everyone in the cluster is raised by some specified amount.

But we also want the size of impact to possibly vary by site size.
This suggests we also want a treatment by site size interaction term.
Instead of just using the site size, however, we are going to standardize our site sizes so they are more interpretable.
This makes it so if we double the sizes of all the sites, it does not change our size covariate: we want the size covariate to be relative size, not absolute.
To do this, we create a covariate which is the percent of the average site size that a site is:
\[ S_j = \frac{n_j - \bar{n}}{ \bar{n} } \]

where \(\bar{n}\) is the average site size. Using this coveriate, we then revise our equation for our site \(j\) to:
\[ \beta_{0j} = \gamma_{0} + \gamma_{1} Z_j + \gamma_2 Z_j S_j + u_j \]
A nice thing about \(S_j\) is that it is centered at 0, meaning the average site has an impact of just \(\gamma_1\).
If \(S_j\) was not centered at zero, then our overall average impact in our data would be a mix of the \(\gamma_1\) and the \(\gamma_2\).
By centering, we make it so the average impact is just \(\gamma_1\)--\(\gamma_1\) is our target site average treatment impact.

If we put all the above together, we see we have specified a multilevel model to describe our data:
\[
\begin{aligned}
Y_{ij} &= \beta_{0j} + \epsilon_{ij} \\
\epsilon_{ij} &\sim N( 0, \sigma^2_\epsilon ) \\
\beta_{0j} &= \gamma_{0} + \gamma_{1} Z_j + \gamma_2 Z_j S_j + u_j \\
u_j &\sim N( 0, \sigma^2_u )
\end{aligned}
\]
Our parameters are the mean outcome of control unit (\(\gamma_0\)), the average treatment impact (\(\gamma_1\)), the amount of cross site variation (\(\sigma^2_u\)), and residual variation (\(\sigma^2_\epsilon\)).
Our \(\gamma_2\) is our site-size by treatment interaction term: bigger sites will (assuming \(\gamma_2\) is positive) have larger treatment impacts.

If you prefer the reduced form, it would be:

\[ Y_{ij} = \gamma_{0} + \gamma_{1} Z_j + \gamma_2 Z_j S_j  + u_j + \epsilon_{ij}  \]
We might also include a main effect for \(S_j\).
A main effect would make larger sites systematically different than smaller sites at baseline, rather than having it only be part of our treatment variation term.
For simplicity we drop it here.

In reviewing the above, we might notice that we do not have any variation in treatment impact that is not explained by site size.
We could once again revise our model to include a term for this, but we will leave it out for now.
See the exercises at the end of the chapter.

So far we have a mathematical model analogous to what we would write if we were \emph{analyzing} the data.
To \emph{generate} data, we also need several other quantities specified.
First, we need to know the number of clusters (\(J\)) and the sizes of the clusters (\(n_j\), for \(j = 1, \ldots, J\)).
We have to provide a recipe for generating these sizes.
We might try

\[ n_j \sim unif( (1-\alpha)\bar{n}, (1+\alpha)\bar{n} ) = \bar{n} + \bar{n}\alpha \cdot unif(-1, 1) ,\]
with a fixed \(\alpha\) to control the amount of variation in cluster size.
If \(\bar{n} = 100\) and \(\alpha = 0.25\) then we would, for example, have sites ranging from 75 to 125 in size.
This specification is nice in that we can determine two parameters, \(\bar{n}\) and \(\alpha\), to get our site sizes, and both parameters are easy to comprehend: average site size and amount of site size variation.

Given how we are generating site size, look again at our treatment impact heterogeneity term:

\[ \gamma_2 Z_j S_j = \gamma_2 Z_j \left(\frac{n_j - \bar{n}}{\bar{n}}\right) = \gamma_2 Z_j \alpha U_j, \]
where \(U_j\) is the \(U_j \sim unif(-1,1)\) uniform variable used to generate \(n_j\).
Due to our standardizing by average site size, we make our covariate not change in terms of its importance as a function of site size, but rather as a function of site variation \(\alpha\).
In particular, \(\frac{n_j - \bar{n}}{\bar{n}}\) will range from \(-\alpha\) to \(\alpha\), regardless of average site size.
Carefully setting up a DGP so the ``knobs'' we use are standardized like this can make interpreting the simulation results much easier.
Consider if we did not standardize and just had \(\gamma_2 n_j\) in our equation: in this case, for a set \(\gamma_2\), the overall average impact would grow if we changed the average site size, which could make interpreting the results across scenarios very confusing.
We generally want the parameters in our DGP to change only one aspect of our simulation, if possible, to make isolating effects of different DGP characteristics easier.

We next need to define how we generate our treatment indicator, \(Z_j\).
We might specify the proportion \(p\) of clusters we will assign to treatment, and then generate \(Z_j = 1\) or \(Z_j = 0\) using a simple random sampling approach on our \(J\) clusters.
We will see code for this below.

\subsection{Converting our model to code}\label{converting-our-model-to-code}

When sketching out our DGP mathematically we worked from the students to the schools.
For actual data generation, we will now follow our final model, but go by layers in the other direction.
First, we generate the sites:

\begin{itemize}
\tightlist
\item
  Generate site sizes
\item
  Generate site-level covariates
\item
  Generate site level random effects
\end{itemize}

Then we generate the students inside the sites:

\begin{itemize}
\tightlist
\item
  Generate student covariates
\item
  Generate student residuals
\item
  Add everything up to generate student outcomes
\end{itemize}

The mathematical model gives us exactly the details we need to execute on these steps.

We start by specifying a function with all the parameters we might want to pass it, including defaults for each (see \citet{default_arguments} for more on function defaults):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gen\_dat\_model }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{( }\AttributeTok{n\_bar =} \DecValTok{10}\NormalTok{,}
                           \AttributeTok{J =} \DecValTok{30}\NormalTok{,}
                           \AttributeTok{p =} \FloatTok{0.5}\NormalTok{,}
                           \AttributeTok{gamma\_0 =} \DecValTok{0}\NormalTok{, }\AttributeTok{gamma\_1 =} \DecValTok{0}\NormalTok{, }\AttributeTok{gamma\_2 =} \DecValTok{0}\NormalTok{,}
                           \AttributeTok{sigma2\_u =} \DecValTok{0}\NormalTok{, }\AttributeTok{sigma2\_e =} \DecValTok{1}\NormalTok{,}
                           \AttributeTok{alpha =} \DecValTok{0}\NormalTok{ ) \{}
  \CommentTok{\# Code (see below) goes here.}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Note our parameters are a mix of \emph{model parameters} (\texttt{gamma\_0}, \texttt{gamma\_1}, \texttt{sigma2\_e}, etc., representing coefficients in regressions, variance terms, etc.) and \emph{design parameters} (\texttt{n\_bar}, \texttt{J}, \texttt{p}) that directly inform data generation.
We set default arguments (e.g., gamma\_0=0) so we can ignore aspects of the DGP that we do not care about later on.

Inside the model, we will have a block of code to generate the sites, and then another to generate the students.

\textbf{Make the sites.}
We make the sites first:

\begin{Shaded}
\begin{Highlighting}[]
  \CommentTok{\# generate site sizes }
\NormalTok{  n\_min }\OtherTok{=} \FunctionTok{round}\NormalTok{( n\_bar }\SpecialCharTok{*}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ alpha) )}
\NormalTok{  n\_max }\OtherTok{=} \FunctionTok{round}\NormalTok{( n\_bar }\SpecialCharTok{*}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{+}\NormalTok{ alpha) )}
\NormalTok{  nj }\OtherTok{\textless{}{-}} \FunctionTok{sample}\NormalTok{( n\_min}\SpecialCharTok{:}\NormalTok{n\_max, J, }\AttributeTok{replace=}\ConstantTok{TRUE}\NormalTok{ )}

  \CommentTok{\# Generate average control outcome and average ATE for all sites}
  \CommentTok{\# (The random effects)}
\NormalTok{  u0j }\OtherTok{=} \FunctionTok{rnorm}\NormalTok{( J, }\AttributeTok{mean=}\DecValTok{0}\NormalTok{, }\AttributeTok{sd=}\FunctionTok{sqrt}\NormalTok{( sigma2\_u ) )}
  
  \CommentTok{\# randomize units within each site (proportion p to treatment)}
\NormalTok{  Zj }\OtherTok{=} \FunctionTok{ifelse}\NormalTok{( }\FunctionTok{sample}\NormalTok{( }\DecValTok{1}\SpecialCharTok{:}\NormalTok{J ) }\SpecialCharTok{\textless{}=}\NormalTok{ J }\SpecialCharTok{*}\NormalTok{ p, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)}
  
  \CommentTok{\# Calculate site intercept for each site}
\NormalTok{  beta\_0j }\OtherTok{=}\NormalTok{ gamma\_0 }\SpecialCharTok{+}\NormalTok{ gamma\_1 }\SpecialCharTok{*}\NormalTok{ Zj }\SpecialCharTok{+}\NormalTok{ gamma\_2 }\SpecialCharTok{*}\NormalTok{ Zj }\SpecialCharTok{*}\NormalTok{ (nj}\SpecialCharTok{{-}}\NormalTok{n\_bar)}\SpecialCharTok{/}\NormalTok{n\_bar }\SpecialCharTok{+}\NormalTok{ u0j}
\end{Highlighting}
\end{Shaded}

The code is a literal translation of the math we did before.
Note the line with \texttt{sample(1:J)\ \textless{}=\ J*p}; this is a simple trick to generate a treatment and control 0/1 indicator.

There is also a serious error in the above code (serious in that the code will run and look fine in many cases, but not always do what we want); we leave it as an exercise (see below) to find and fix it.

\textbf{Make the individuals.}
We next use the site characteristics to then generate the individuals.

\begin{Shaded}
\begin{Highlighting}[]
  \CommentTok{\# Make individual site membership}
\NormalTok{  sid }\OtherTok{=} \FunctionTok{as.factor}\NormalTok{( }\FunctionTok{rep}\NormalTok{( }\DecValTok{1}\SpecialCharTok{:}\NormalTok{J, nj ) )}
\NormalTok{  dd }\OtherTok{=} \FunctionTok{data.frame}\NormalTok{( }\AttributeTok{sid =}\NormalTok{ sid )}
  
  \CommentTok{\# Make individual level tx variables}
\NormalTok{  dd}\SpecialCharTok{$}\NormalTok{Z }\OtherTok{=}\NormalTok{ Zj[ dd}\SpecialCharTok{$}\NormalTok{sid ]}
  
  \CommentTok{\# Generate the residuals }
\NormalTok{  N }\OtherTok{=} \FunctionTok{sum}\NormalTok{( nj )}
\NormalTok{  e }\OtherTok{=} \FunctionTok{rnorm}\NormalTok{( N, }\AttributeTok{mean=}\DecValTok{0}\NormalTok{, }\AttributeTok{sd=}\FunctionTok{sqrt}\NormalTok{( sigma2\_e ) )}
  
  \CommentTok{\# Bundle and send out}
\NormalTok{  dd }\OtherTok{\textless{}{-}} \FunctionTok{mutate}\NormalTok{( dd, }
                \AttributeTok{sid=}\FunctionTok{as.factor}\NormalTok{(sid),}
                \AttributeTok{Yobs =}\NormalTok{ beta\_0j[sid] }\SpecialCharTok{+}\NormalTok{ e, }
                \AttributeTok{Z =}\NormalTok{ Zj[ sid ] )}
\end{Highlighting}
\end{Shaded}

A key piece here is the \texttt{rep()} function that takes a list and repeats each element of the list a specified number of times.
In particular, \texttt{rep()} repeats each number (\(1, 2, /ldots,J\)), the corresponding number of times as listed in \texttt{nj}.

Once we put the above code in our function skeleton, we can our function as so:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dat }\OtherTok{\textless{}{-}} \FunctionTok{gen\_dat\_model}\NormalTok{( }\AttributeTok{n=}\DecValTok{5}\NormalTok{, }\AttributeTok{J=}\DecValTok{3}\NormalTok{, }\AttributeTok{p=}\FloatTok{0.5}\NormalTok{, }
                        \AttributeTok{gamma\_0=}\DecValTok{0}\NormalTok{, }\AttributeTok{gamma\_1=}\FloatTok{0.2}\NormalTok{, }\AttributeTok{gamma\_2=}\FloatTok{0.2}\NormalTok{,}
                        \AttributeTok{sigma2\_u =} \FloatTok{0.4}\NormalTok{, }\AttributeTok{sigma2\_e =} \DecValTok{1}\NormalTok{,}
                      \AttributeTok{alpha =} \FloatTok{0.5}\NormalTok{ )}

\NormalTok{dat}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    sid Z       Yobs
## 1    1 1 -0.3829168
## 2    1 1  0.5815349
## 3    1 1  0.2079695
## 4    1 1  1.3511779
## 5    2 0 -1.3234643
## 6    2 0 -0.7553822
## 7    2 0 -1.1712027
## 8    2 0  0.3153905
## 9    2 0  1.1293981
## 10   2 0 -0.5332630
## 11   2 0 -0.5165060
## 12   3 0 -0.3891640
## 13   3 0 -0.7181741
## 14   3 0  1.9895227
## 15   3 0 -0.1538450
## 16   3 0 -0.8399816
\end{verbatim}

Our data generation code is complete.
The next step is to test the code, making sure it is doing what we think it is.
See the exercises for more on this.

\section{Exercises}\label{exercises}

\subsection{\texorpdfstring{The Welch test on a shifted-and-scaled \(t\) distribution}{The Welch test on a shifted-and-scaled t distribution}}\label{ex_dgp}

The shifted-and-scaled \(t\)-distribution has parameters \(\mu\) (mean), \(\sigma\) (scale), and \(\nu\) (degrees of freedom).
If \(T\) follows a student's \(t\)-distribution with \(\nu\) degrees of freedom, then \(S = \mu + \sigma T\) follows a shifted-and-scaled \(t\)-distribution.

The following function will generate random draws from this distribution (the scaling of \((\nu-2)/\nu\) is to account for a non-scaled \(t\)-distribution having a variance of \(\nu/(\nu-2)\)).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{r\_tss }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(n, mean, sd, df) \{}
\NormalTok{  mean }\SpecialCharTok{+}\NormalTok{ sd }\SpecialCharTok{*} \FunctionTok{sqrt}\NormalTok{( (df}\DecValTok{{-}2}\NormalTok{)}\SpecialCharTok{/}\NormalTok{df ) }\SpecialCharTok{*} \FunctionTok{rt}\NormalTok{(}\AttributeTok{n =}\NormalTok{ n, }\AttributeTok{df =}\NormalTok{ df)}
\NormalTok{\}}

\FunctionTok{r\_tss}\NormalTok{(}\AttributeTok{n =} \DecValTok{8}\NormalTok{, }\AttributeTok{mean =} \DecValTok{3}\NormalTok{, }\AttributeTok{sd =} \DecValTok{2}\NormalTok{, }\AttributeTok{df =} \DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 5.311521 1.513994 5.395777 3.845034 4.679047
## [6] 9.417117 4.475893 6.454291
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Modify the Welch simulation's \texttt{simulate\_data()} function to generate data from shifted-and-scaled \(t\)-distributions rather than from normal distributions. Include the degrees of freedom as an input argument.
  Simulate a dataset with low degrees of freedom and plot it to see if you see a few outliers.
\item
  Now generate more data and calculate the means and standard deviations to see if they are correctly calibrated (generate a big dataset to ensure you get reliable mean and standard deviation estimates). Check \texttt{df} equal to 500, 5, 3, and 2.
\item
  Once you are satisfied you have a correct DGP function, re-run the Type-I error rate calculations from \hyperref[exAnovaExercises]{the prior exercises} on page \citet{exAnovaExercises} using a \(t\)-distribution with 5 degrees of freedom.
  Do the results change substantially?
\end{enumerate}

\subsection{Checking and extending the Cluster RCT DGP}\label{checking-and-extending-the-cluster-rct-dgp}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\item
  What is the variance of the outcomes generated by our model if there is no treatment effect? (Try simulating data to check!) What other quick checks can you make on your DGP to make sure it is working?
\item
  In \texttt{gen\_dat\_model()} we have the following line of code to generate the number of individuals per site.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nj }\OtherTok{\textless{}{-}} \FunctionTok{sample}\NormalTok{( n\_min}\SpecialCharTok{:}\NormalTok{n\_max, J, }
                 \AttributeTok{replace=}\ConstantTok{TRUE}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

This code has an error. Generate a variety of datasets where you vary \texttt{n\_min}, \texttt{n\_max} and \texttt{J} to discover the error. Then repair the code.
Checking your data generating process across a range of scenarios is extremely important.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{5}
\item
  The DGP allows for site-level treatment impact variation--but only if it is related to site size. How could you modify your simulation to allow for site-level treatment impact variation that is not related to site size? Implement this change and generate some data to show how it works.
\item
  Extend the data generating process to include an individual level covariate \(X\) that is predictive of outcome. In particular, you will want to adjust your level one equation to
\end{enumerate}

\[ Y_{ij} = \beta_{0j} + \beta_{1} X_{ij} + \epsilon_{ij} . \]
Keep the same \(\beta_1\) for all sites.
You will have to specify how to generate your \(X_{ij}\).
For starters, just generate it as a standard normal, and do not worry about having the mean of \(X_{ij}\) vary by sites unless you are excited to try to get that to work.

\section{Extension: Standardization in a data generating process}\label{extension-standardization-in-a-data-generating-process}

In this chapter, we made a model to generate data for a cluster randomized trial.
Given our model, we can generate data by specifying our parameters and variables of \(\gamma_{0}, \gamma_{1}, \gamma_{2}, \sigma^2_\epsilon, \sigma^2_u, \bar{n}, \alpha, J, p\).

One factor that tends to show up when working with multisite data is how much variation there is within sites vs between sites.
For example, the Intra-Class Correlation (ICC), a measure of how much of the variation in the outcome is due to differences between sites, is a major component for power calculations.
Because of this, we will likely want to manipulate the amount of within vs.~between variation in our simulations.

An easy way to do this would be to simply raise or lower the amount of variation within sites (\(\sigma^2_u\)).
This unfortunately has a side effect: if we increase \(\sigma^2_u\), our overall variation of \(Y\) will also increase.
This will make it hard to think about, e.g., power, since we have confounded within vs.~between variation with overall variation (which is itself bad for power).
It also impacts interpretation of coefficients.
A treatment effect of 0.2 on our outcome scale is ``smaller'' if there is more overall variation.

Right now, our model is
\[ Y_{ij} = \gamma_{0} + \gamma_{1} Z_j + \gamma_2 Z_j \left(\frac{n_j - \bar{n}}{\bar{n}} \right)  + u_j + \epsilon_{ij}  \]

Given our model, the variance of our control-side outcomes is
\[ 
\begin{aligned}
var( Y_{ij}(0) ) &= var( \beta_{0j} + \epsilon_{ij} ) \\
 &= var( \gamma_{0} + \gamma_{1} Z_j + \gamma_{2}Z_j \tilde{n}_j + u_j + \epsilon_{ij} ) \\
&= \sigma^2_u + \sigma^2_\epsilon
\end{aligned}
\]
We see that as we increase either within or between variation, overall variation increases.

We can improve our data generating process to allow for directly controlling the amount of within vs.~between variation without it being confounded with overall variation.
To do this we first (1) Standardize our data and then (2) reparameterize, so we have human-selected parameters that we can interpret that we then \emph{translate} to our list of data generation parameters.
In particular, we will index our DGP with the more interpretable parameter of the Intra-Class Correlation (ICC), and standardize our DGP so it is all in effect size units.

The effect size of an impact is defined as the impact over the control-side standard deviation.
(Sometimes people use the pooled standard deviation, but this is usually a bad choice if one suspects treatment variation. More treatment variation should not reduce the effect size for the same absolute average impact.)

\[ ES = \frac{\gamma_1}{SD( Y | Z_j = 0 )} = \frac{\gamma_1}{\sqrt{ \sigma^2_u + \sigma^2_\epsilon } } \]

The way we think about how ``big'' \(\gamma_1\) is depends on how much site variation and residual variation there is.
But it is also easier to detect effects when the residual variation is small.
Effect sizes ``standardize out'' these sorts of tensions. We can use that.

In particular, we will use the Intraclass Correlation Coeffiicent (ICC), defined as
\[ ICC = \frac{ \sigma^2_u }{ \sigma^2_\epsilon + \sigma^2_u } . \]
The ICC is a measure of within vs.~between variation.

What we then do is first standardized our data, meaning we ensure the control side variance equals 1.
Using the above, this means \(\sigma^2_u + \sigma^2_\epsilon = 1\).
It also gives us \(ICC = \sigma^2_u\), and \(\sigma^2_\epsilon = 1 - ICC\).

Our two model parameters are now tied together by our single ICC tuning parameter.
The core idea is we can now manipulate the aspects of the DGP we want while holding other aspects of the DGP constant.
Given our standardized scale, we have dropped a parameter from our set of parameters we might want to vary, and ensured that varying the other parameter (now the ICC) is varying only one aspect of the DGP, not both.
Before, increasing \(\sigma^2_u\) had two consequences: total variation and relative amount of variation at the school level.
Now, manipulating ICC only does the latter.

E.g., we can call our DGP function as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ICC }\OtherTok{=} \FloatTok{0.3}
\NormalTok{dat }\OtherTok{\textless{}{-}} \FunctionTok{gen\_dat\_model}\NormalTok{( }\AttributeTok{n\_bar =} \DecValTok{20}\NormalTok{, }\AttributeTok{J =} \DecValTok{30}\NormalTok{, }\AttributeTok{p =} \FloatTok{0.5}\NormalTok{,}
                      \AttributeTok{gamma\_0 =} \DecValTok{0}\NormalTok{, }\AttributeTok{gamma\_1 =} \FloatTok{0.3}\NormalTok{, }\AttributeTok{gamma\_2 =} \FloatTok{0.2}\NormalTok{,}
                      \AttributeTok{sigma2\_u =}\NormalTok{ ICC, }\AttributeTok{sigma2\_e =} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ ICC,}
                      \AttributeTok{alpha =} \FloatTok{0.5}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\chapter{Estimation procedures}\label{estimation-procedures-1}

In the abstract, a function that implements an estimation procedure should have the following form:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{estimate }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(data) \{}

  \CommentTok{\# calculations/model{-}fitting/estimation procedures}
  
  \FunctionTok{return}\NormalTok{(estimates)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

The function takes a data set as input, fits a model or otherwise calculates an estimate, possibly with associated standard errors and so forth, and produces as output these estimates.
In principle, you should be able to run your function on real data as well as simulated.

The estimates could be point-estimates of parameters, standard errors, confidence intervals, etc.
Depending on the research question, this function might involve a combination of several procedures (e.g., a diagnostic test for heteroskedasticity, followed by the conventional formula or heteroskedasticity-robust formula for standard errors).
Also depending on the research question, we might need to create \emph{several} functions that implement different estimation procedures to be compared.

In Chapter \citet{case_ANOVA}, for example, we saw different functions for some of the methods Brown and Forsythe considered for heteroskedastic ANOVA.

\section{Validating Estimation Procedures}\label{validating-estimation-procedures}

Just as with the data-generating function, it is important to verify the accuracy of the estimation functions.
For our Welch test, we can actually check our results against the built-in \texttt{oneway.test} function. Let's do that with a fresh set of data:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sim\_data }\OtherTok{\textless{}{-}} \FunctionTok{generate\_data}\NormalTok{(}\AttributeTok{mu =}\NormalTok{ mu, }\AttributeTok{sigma\_sq =}\NormalTok{ sigma\_sq,}
                          \AttributeTok{sample\_size =}\NormalTok{ sample\_size)}

\NormalTok{aov\_results }\OtherTok{\textless{}{-}} \FunctionTok{oneway.test}\NormalTok{(x }\SpecialCharTok{\textasciitilde{}} \FunctionTok{factor}\NormalTok{(group),}
                           \AttributeTok{data =}\NormalTok{ sim\_data, }
                           \AttributeTok{var.equal =} \ConstantTok{FALSE}\NormalTok{)}
\NormalTok{aov\_results}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  One-way analysis of means (not assuming
##  equal variances)
## 
## data:  x and factor(group)
## F = 9.5906, num df = 3.0000, denom df =
## 3.4245, p-value = 0.03696
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Welch\_results }\OtherTok{\textless{}{-}} \FunctionTok{Welch\_F}\NormalTok{(sim\_data)}
\FunctionTok{all.equal}\NormalTok{(aov\_results}\SpecialCharTok{$}\NormalTok{p.value, Welch\_results)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] TRUE
\end{verbatim}

We use \texttt{all.equal()} because it will check equality up to a tolerance in R, which can avoid some weird floating point errors due to rounding.

\section{Checking via simulation}\label{checking-via-simulation}

If your estimation procedure truly is new, how would you check it?
Well, one obvious answer is simulation!

In principle, for large samples and data generated under the assumptions required by your new procedure, you should have a fairly good sense that your estimation procedures should work.
It is often the case that as you design your simulation, and then start analyzing the results, you will find your estimators are really not working as planned.

Such surprises will usually be due to (at least) three factors: you did not implement your method correctly, your method is not yet a good idea in the first place, or you do not yet understand something important about how your method works.
When faced with poor performance you thus will debug your code, revise your method, and do some serious thinking.
Ideally this will eventually lead you to a deeper understanding of a method that is a better idea in general, and correctly implemented in all likelihood.

For example, in one research project Luke and other co-authors were working on a way to improve Instrumental Variable (IV) estimation using post-stratification.
The idea is to group units based on a covariate that predicts compliance status, and then estimate within each group; hopefully this would improve overall estimation.

In the first simulation, the estimates were full of NAs and odd results because we failed to properly account for what happens when the number of compliers was estimated to be zero.
That was table stakes: after repairing that, we still found odd behavior and serious and unexpected bias, which turned out to be due to failing to implement the averaging of the groups step correctly.
We fixed the estimator again and re-ran, and found that even when we had a variable that was almost perfectly predictive of compliance, gains were still surprisingly minimal.
Eventually we understood that the groups with very few compliers were actually so unstable that they ruined the overall estimate.
These results inspired us to introduce other estimators that dropped or down-weighted such strata, which gave our paper a deeper purpose and contribution.

Simulation is an iterative process.
It is to help you, the researcher, learn about your estimators so you can find a way forward with your work.
What you learn then feeds back to the prior research, and you have a cycle that you eventually step off of, if you want to finish your paper.
But do not expect it to be a single, well-defined, trajectory.

\section{Including Multiple estimation procedures}\label{including-multiple-estimation-procedures}

In \citet{chap_DGP} (\citet{case_cluster}) we introduced a case study of evaluating different procedures for estimating treatment impacts in a cluster randomized trial.
As a point of design, we generally recommend writing different functions for each estimation method one is planning on evaluating. This makes it easier to plug into play different methods as desired, and also helps generate a code base that is flexible and useful for other purposes.
It also, continuing our usual mantra, makes debugging easier: you can focus attention on one thing at a time, and worry less about how errors in one area might propagate to others.

For the cluster RCT context, we use two libraries, the \texttt{lme4} package (for multilevel modeling), the \texttt{arm} package (which gives us nice access to standard errors, with \texttt{se.fixef()}), and \texttt{lmerTest} (which gives us \(p\)-values for multilevel modeling).
We also need the \texttt{estimatr} package to get robust SEs with \texttt{lm\_robust}.
This use of different packages for different estimators is quite typical: in many simulations, many of the estimation approaches being considered are usually taken from the literature, and if you are lucky this means you can simply use a package that implements those methods.

We load our libraries at the top of our code:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{( lme4 )}
\FunctionTok{library}\NormalTok{( arm )}
\FunctionTok{library}\NormalTok{( lmerTest )}
\FunctionTok{library}\NormalTok{( estimatr )}
\end{Highlighting}
\end{Shaded}

Our three analysis functions are then Multilevel Regression (MLM):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{analysis\_MLM }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{( dat ) \{}
\NormalTok{  M1 }\OtherTok{=} \FunctionTok{lmer}\NormalTok{( Yobs }\SpecialCharTok{\textasciitilde{}} \DecValTok{1} \SpecialCharTok{+}\NormalTok{ Z }\SpecialCharTok{+}\NormalTok{ (}\DecValTok{1}\SpecialCharTok{|}\NormalTok{sid),}
             \AttributeTok{data=}\NormalTok{dat )}
\NormalTok{  est }\OtherTok{=} \FunctionTok{fixef}\NormalTok{( M1 )[[}\StringTok{"Z"}\NormalTok{]]}
\NormalTok{  se }\OtherTok{=} \FunctionTok{se.fixef}\NormalTok{( M1 )[[}\StringTok{"Z"}\NormalTok{]]}
\NormalTok{  pv }\OtherTok{=} \FunctionTok{summary}\NormalTok{(M1)}\SpecialCharTok{$}\NormalTok{coefficients[}\StringTok{"Z"}\NormalTok{,}\DecValTok{5}\NormalTok{]}
  \FunctionTok{tibble}\NormalTok{( }\AttributeTok{ATE\_hat =}\NormalTok{ est, }\AttributeTok{SE\_hat =}\NormalTok{ se, }\AttributeTok{p\_value =}\NormalTok{ pv )}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Linear Regression with Cluster-Robust Standard Errors (LM):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{analysis\_OLS }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{( dat ) \{}
\NormalTok{  M2 }\OtherTok{\textless{}{-}} \FunctionTok{lm\_robust}\NormalTok{( Yobs }\SpecialCharTok{\textasciitilde{}} \DecValTok{1} \SpecialCharTok{+}\NormalTok{ Z, }
            \AttributeTok{data=}\NormalTok{dat, }\AttributeTok{clusters=}\NormalTok{sid )}
\NormalTok{  est }\OtherTok{\textless{}{-}}\NormalTok{ M2}\SpecialCharTok{$}\NormalTok{coefficients[[}\StringTok{"Z"}\NormalTok{]]}
\NormalTok{  se  }\OtherTok{\textless{}{-}}\NormalTok{ M2}\SpecialCharTok{$}\NormalTok{std.error[[}\StringTok{"Z"}\NormalTok{]]}
\NormalTok{  pv }\OtherTok{\textless{}{-}}\NormalTok{ M2}\SpecialCharTok{$}\NormalTok{p.value[[}\StringTok{"Z"}\NormalTok{]]}
  \FunctionTok{tibble}\NormalTok{( }\AttributeTok{ATE\_hat =}\NormalTok{ est, }\AttributeTok{SE\_hat =}\NormalTok{ se, }\AttributeTok{p\_value =}\NormalTok{ pv )}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

and Aggregate data (Agg):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{analysis\_agg }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{( dat ) \{}
\NormalTok{  datagg }\OtherTok{\textless{}{-}} 
\NormalTok{    dat }\SpecialCharTok{\%\textgreater{}\%} 
    \FunctionTok{group\_by}\NormalTok{( sid, Z ) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{summarise}\NormalTok{( }
      \AttributeTok{Ybar =} \FunctionTok{mean}\NormalTok{( Yobs ),}
      \AttributeTok{n =} \FunctionTok{n}\NormalTok{() }
\NormalTok{    )}
  
  \FunctionTok{stopifnot}\NormalTok{( }\FunctionTok{nrow}\NormalTok{( datagg ) }\SpecialCharTok{==} \FunctionTok{length}\NormalTok{(}\FunctionTok{unique}\NormalTok{(dat}\SpecialCharTok{$}\NormalTok{sid) ) )}
  
\NormalTok{  M3 }\OtherTok{\textless{}{-}} \FunctionTok{lm\_robust}\NormalTok{( Ybar }\SpecialCharTok{\textasciitilde{}} \DecValTok{1} \SpecialCharTok{+}\NormalTok{ Z, }
                   \AttributeTok{data=}\NormalTok{datagg, }\AttributeTok{se\_type =} \StringTok{"HC2"}\NormalTok{ )}
\NormalTok{  est }\OtherTok{\textless{}{-}}\NormalTok{ M3}\SpecialCharTok{$}\NormalTok{coefficients[[}\StringTok{"Z"}\NormalTok{]]}
\NormalTok{  se }\OtherTok{\textless{}{-}}\NormalTok{ M3}\SpecialCharTok{$}\NormalTok{std.error[[}\StringTok{"Z"}\NormalTok{]]}
\NormalTok{  pv }\OtherTok{\textless{}{-}}\NormalTok{ M3}\SpecialCharTok{$}\NormalTok{p.value[[}\StringTok{"Z"}\NormalTok{]]}
  \FunctionTok{tibble}\NormalTok{( }\AttributeTok{ATE\_hat =}\NormalTok{ est, }\AttributeTok{SE\_hat =}\NormalTok{ se, }\AttributeTok{p\_value =}\NormalTok{ pv )}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Note the \texttt{stopifnot} command: putting \emph{assert statements} in your code like this is a good way to guarantee you are not introducing weird and hard-to-track errors in your code.
For example, R likes to recycle vectors to make them the right length; if you gave it a wrong length in error, this can be a brutal error to discover.
The \texttt{stopifnot} statements halt your code as soon as something goes wrong, rather than letting that initial wrongness flow on to further work, showing up in odd results that you don't understand later on.
See Section \citet{about_stopifnot} for more.

All of our methods give output in the similar format:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{analysis\_MLM}\NormalTok{( dat )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1 x 3
##   ATE_hat SE_hat p_value
##     <dbl>  <dbl>   <dbl>
## 1   0.140  0.199   0.487
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{analysis\_OLS}\NormalTok{( dat )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1 x 3
##   ATE_hat SE_hat p_value
##     <dbl>  <dbl>   <dbl>
## 1   0.148  0.213   0.494
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{analysis\_agg}\NormalTok{( dat )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1 x 3
##   ATE_hat SE_hat p_value
##     <dbl>  <dbl>   <dbl>
## 1   0.140  0.198   0.485
\end{verbatim}

This will allow us to make our simulation, for each iteration, call each method in turn on the same dataset, stack the results into a small table, and return that result.
This will in turn get stacked to make one giant table of results, which makes evaluating performance quite easy.
We will see this in the next chapter.

\section{Exercises}\label{exercises-1}

\subsection{More Welch and adding the BFF test}\label{more-welch-and-adding-the-bff-test}

Let's continue to explore and tweak the simulation code we have developed to replicate the results of Brown and Forsythe (1974).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Write a function that implements the Brown-Forsythe F*-test (the BFF* test!) as described on p.~130 of Brown and Forsythe (1974).
  Call it on a sample dataset to check it.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{BF\_F }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x\_bar, s\_sq, n, g) \{}
  
  \CommentTok{\# fill in the guts here}
  
  \FunctionTok{return}\NormalTok{(pval)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Try calling your \texttt{BF\_F} function on a variety of datasets of different sizes and shapes, to make sure it works. What kinds of datasets should you test out?
\end{enumerate}

\subsection{More estimators for Cluster Randomized Trials}\label{more-estimators-for-cluster-randomized-trials}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Sometimes you might want to consider two versions of an estimator. For example, in our cluster RCT code we used robust standard errors for the linear model estimator. Say we also want to include naive standard error estimates that we get out of the \texttt{lm} call.
\end{enumerate}

Extend the \texttt{OLS} call to be

\begin{verbatim}
analysis_OLS <- function( dat, robustSE = TRUE ) {
\end{verbatim}

and have the code inside calculate SEs based on the flag. Then modify the \texttt{analyze\_data()} to include both approaches (you will have to call \texttt{analysis\_OLS} twice).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Efficiency-wise, estimating the OLS twice might not be ideal, but clarity-wise it might be considered helpful. Articulate two reasons for the design choice of implementing the two OLS calls separately, and articulate two reasons for instead having the \texttt{analysis\_OLS} method generate both standard errors internally in a single call.
\end{enumerate}

\chapter{Running the Simulation}\label{running-the-simulation-1}

In the prior two chapters we saw how to write functions that generate data according to a specified model (and parameters) and functions that implement estimation procedures on simulated data.
We next put those two together and repeat a bunch of times to obtain a lot of results such as point estimates, estimated standard errors and/or confidence intervals.

We use two primary ways of doing this in this textbook.
The first is to write a function that does a single step of a simulation, and then use the \texttt{map()} function to run that single step multiple times.

For our Cluster RCT case study, for example, we would write the following that takes our simulation parameters and runs a single trial of our simulation:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{one\_run }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{( }\AttributeTok{n\_bar =} \DecValTok{30}\NormalTok{, }\AttributeTok{J=}\DecValTok{20}\NormalTok{, }
                     \AttributeTok{gamma\_1 =} \FloatTok{0.3}\NormalTok{, }\AttributeTok{gamma\_2 =} \FloatTok{0.5}\NormalTok{,}
                     \AttributeTok{sigma2\_u =} \FloatTok{0.20}\NormalTok{, }\AttributeTok{sigma2\_e =} \FloatTok{0.80}\NormalTok{,}
                     \AttributeTok{alpha =} \FloatTok{0.75}\NormalTok{ ) \{}
  
\NormalTok{  dat }\OtherTok{\textless{}{-}} \FunctionTok{gen\_dat\_model}\NormalTok{( }\AttributeTok{n\_bar =}\NormalTok{ n\_bar, }\AttributeTok{J=}\NormalTok{J, }
                        \AttributeTok{gamma\_1 =}\NormalTok{ gamma\_1, }\AttributeTok{gamma\_2 =}\NormalTok{ gamma\_2,}
                        \AttributeTok{sigma2\_u =}\NormalTok{ sigma2\_u, }\AttributeTok{sigma2\_e =}\NormalTok{ sigma2\_e,}
                        \AttributeTok{alpha =}\NormalTok{ alpha )}
\NormalTok{  MLM }\OtherTok{=} \FunctionTok{analysis\_MLM}\NormalTok{( dat )}
\NormalTok{  LR }\OtherTok{=} \FunctionTok{analysis\_OLS}\NormalTok{( dat )}
\NormalTok{  Agg }\OtherTok{=} \FunctionTok{analysis\_agg}\NormalTok{( dat )}
  
  \FunctionTok{bind\_rows}\NormalTok{( }\AttributeTok{MLM =}\NormalTok{ MLM, }\AttributeTok{LR =}\NormalTok{ LR, }\AttributeTok{Agg =}\NormalTok{ Agg,}
             \AttributeTok{.id =} \StringTok{"method"}\NormalTok{ )}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

We have added a bunch of defaults to our function, so we can easily run it without remembering all the things we can change.

When we call it, we get a nice table of results that we can evaluate:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{one\_run}\NormalTok{( }\AttributeTok{n\_bar =} \DecValTok{30}\NormalTok{, }\AttributeTok{J =} \DecValTok{20}\NormalTok{, }\AttributeTok{alpha=}\FloatTok{0.5}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 3 x 4
##   method ATE_hat SE_hat p_value
##   <chr>    <dbl>  <dbl>   <dbl>
## 1 MLM      0.137  0.273   0.623
## 2 LR       0.273  0.266   0.319
## 3 Agg      0.125  0.274   0.655
\end{verbatim}

The results for each method is a single line.
We record estimated impact, estimated standard error, and a nominal \(p\)-value.
Note how the \texttt{bind\_rows()} method can take naming on the fly, and give us a column of \texttt{method}, which will be very useful for keeping track of what estimated what.
We intentionally wrap up our results with a data frame to make later processing of data with the tidyverse package much easier.

We then use the \texttt{map()} function to run this function multiple times:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{( }\DecValTok{40404}\NormalTok{ )}
\NormalTok{R }\OtherTok{=} \DecValTok{1000}
\NormalTok{ATE }\OtherTok{=} \FloatTok{0.30}
\NormalTok{runs }\OtherTok{\textless{}{-}} 
  \FunctionTok{map\_df}\NormalTok{( }\DecValTok{1}\SpecialCharTok{:}\NormalTok{R, }\SpecialCharTok{\textasciitilde{}}\FunctionTok{one\_run}\NormalTok{( }\AttributeTok{n\_bar =} \DecValTok{30}\NormalTok{, }\AttributeTok{J=}\DecValTok{20}\NormalTok{, }\AttributeTok{gamma\_1 =}\NormalTok{ ATE ),}
          \AttributeTok{.id=}\StringTok{"runID"}\NormalTok{ ) }

\FunctionTok{saveRDS}\NormalTok{( runs, }\AttributeTok{file =} \StringTok{"results/cluster\_RCT\_simulation.rds"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

What the \texttt{map()} function is doing is first making a list from 1 to R, and then for each element in that list, it is calling \texttt{one\_run()} with the parameters \texttt{n\_bar\ =\ 30,\ J=20}.
The \texttt{\textasciitilde{}} is a shorthand way of writing a function that takes one argument, and then calls \texttt{one\_run()} with that argument; the argument is the iteration number (1, 2, 3, \ldots, R), but we are ignoring it. The \texttt{.id\ =\ "runID"} argument is a way of keeping track of which iteration number produced which result.
The \texttt{\_df} at the end of \texttt{map\_df()} is a way of telling \texttt{map()} to take the results of each iteration and bind them together into a single data frame.

Once our simulation is complete, we save our results to a file for future use; this speeds up our lives since we will not have to constantly re-run our simulation each time we want to explore the results.

We have arrived! We now have the individual results of all our methods applied to each of 1000 generated datasets.
The next step is to evaluate how well the estimators did.
Regarding our point estimate, for example, we have these primary questions:

\begin{itemize}
\tightlist
\item
  Is it biased? (bias)
\item
  Is it precise? (standard error)
\item
  Does it predict well? (RMSE)
\end{itemize}

In the next chapter, we systematically go through answering these questions for our initial scenario.

\section{Writing simulations quick with the simhelpers package}\label{writing-simulations-quick-with-the-simhelpers-package}

The \texttt{map} approach is a bit strange, with building a secret function on the fly with \texttt{\textasciitilde{}}, and also having the copy over all the parameters we pass from \texttt{one\_run()} to \texttt{gen\_dat\_model()}.
The \texttt{simhelpers} package provides a shortcut that makes this step easier.

To do it, we first need to write a single estimation procedure function that puts all of our estimators together:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{analyze\_data }\OtherTok{=} \ControlFlowTok{function}\NormalTok{( dat ) \{}
\NormalTok{  MLM }\OtherTok{=} \FunctionTok{analysis\_MLM}\NormalTok{( dat )}
\NormalTok{  LR }\OtherTok{=} \FunctionTok{analysis\_OLS}\NormalTok{( dat )}
\NormalTok{  Agg }\OtherTok{=} \FunctionTok{analysis\_agg}\NormalTok{( dat )}
  
  \FunctionTok{bind\_rows}\NormalTok{( }\AttributeTok{MLM =}\NormalTok{ MLM, }\AttributeTok{LR =}\NormalTok{ LR, }\AttributeTok{Agg =}\NormalTok{ Agg,}
             \AttributeTok{.id =} \StringTok{"method"}\NormalTok{ )}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

This is simply the \texttt{one\_run()} method from above, but without the data generating part.
When we pass a dataset to it, we get a nice table of results that we can evaluate, as we did before.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dat }\OtherTok{=} \FunctionTok{gen\_dat\_model}\NormalTok{( }\AttributeTok{n=}\DecValTok{30}\NormalTok{, }\AttributeTok{J =} \DecValTok{20}\NormalTok{, }\AttributeTok{gamma\_1 =} \FloatTok{0.30}\NormalTok{ )}
\FunctionTok{analyze\_data}\NormalTok{( dat )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## boundary (singular) fit: see help('isSingular')
\end{verbatim}

\begin{verbatim}
## # A tibble: 3 x 4
##   method ATE_hat SE_hat p_value
##   <chr>    <dbl>  <dbl>   <dbl>
## 1 MLM      0.367  0.114 0.00149
## 2 LR       0.367  0.116 0.00692
## 3 Agg      0.329  0.118 0.0120
\end{verbatim}

We can now use \texttt{simhelpers} to write us a new function for the entire simulation:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(simhelpers)}
\NormalTok{sim\_function }\OtherTok{\textless{}{-}} \FunctionTok{bundle\_sim}\NormalTok{( gen\_dat\_model, analyze\_data )}
\end{Highlighting}
\end{Shaded}

We can then use it as so:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sim\_function}\NormalTok{( }\DecValTok{2}\NormalTok{, }\AttributeTok{n\_bar =} \DecValTok{30}\NormalTok{, }\AttributeTok{J =} \DecValTok{20}\NormalTok{, }\AttributeTok{gamma\_1 =}\NormalTok{ ATE )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## boundary (singular) fit: see help('isSingular')
\end{verbatim}

\begin{verbatim}
## # A tibble: 6 x 4
##   method ATE_hat SE_hat    p_value
##   <chr>    <dbl>  <dbl>      <dbl>
## 1 MLM      0.259 0.124  0.0551    
## 2 LR       0.260 0.126  0.0587    
## 3 Agg      0.290 0.149  0.0665    
## 4 MLM      0.541 0.125  0.0000220 
## 5 LR       0.541 0.0745 0.00000693
## 6 Agg      0.604 0.196  0.00650
\end{verbatim}

The \texttt{bundle\_sim()} command takes our DGP function and our estimation procedures function and gives us back a function, which we have called \texttt{sim\_function}, that will run a simulation using whatever parameters we give it.
The \texttt{bundle\_sim()} command examines \texttt{gen\_dat\_model} function, figures out what parameters it needs, and makes sure that the newly created function is able to take those parameters from the user.

To use it for our simulation, we would then write

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rns }\OtherTok{\textless{}{-}} \FunctionTok{sim\_function}\NormalTok{( R, }\AttributeTok{n\_bar =} \DecValTok{30}\NormalTok{, }\AttributeTok{J =} \DecValTok{20}\NormalTok{, }\AttributeTok{gamma\_1 =}\NormalTok{ ATE )}
\FunctionTok{saveRDS}\NormalTok{( runs, }\AttributeTok{file =} \StringTok{"results/cluster\_RCT\_simulation.rds"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

This is a bit more elegant than the \texttt{map()} approach, and is especially useful when we have a lot of parameters to pass around.

\section{Adding Checks and Balances}\label{adding-checks-and-balances}

In the extensions of the prior DGP chapter, we discussed indexing our DGP by the ICC instead of the two variance components.
We can do this, and also translate some of the more obscure model parameters to easier to interpret parameters from within our simulation driver as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{one\_run }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{( }\AttributeTok{n\_bar =} \DecValTok{30}\NormalTok{, }\AttributeTok{J=}\DecValTok{20}\NormalTok{, }
                     \AttributeTok{ATE =} \FloatTok{0.3}\NormalTok{, }\AttributeTok{size\_coef =} \FloatTok{0.5}\NormalTok{,}
                     \AttributeTok{ICC =} \FloatTok{0.4}\NormalTok{,}
                     \AttributeTok{alpha =} \FloatTok{0.75}\NormalTok{ ) \{}
    \FunctionTok{stopifnot}\NormalTok{( ICC }\SpecialCharTok{\textgreater{}=} \DecValTok{0} \SpecialCharTok{\&\&}\NormalTok{ ICC }\SpecialCharTok{\textless{}} \DecValTok{1}\NormalTok{ )}

\NormalTok{  dat }\OtherTok{\textless{}{-}} \FunctionTok{gen\_dat\_model}\NormalTok{( }\AttributeTok{n\_bar =}\NormalTok{ n\_bar, }\AttributeTok{J=}\NormalTok{J, }
                        \AttributeTok{gamma\_1 =}\NormalTok{ ATE, }\AttributeTok{gamma\_2 =}\NormalTok{ size\_coef,}
                        \AttributeTok{sigma2\_u =}\NormalTok{ ICC, }\AttributeTok{sigma2\_e =} \DecValTok{1}\SpecialCharTok{{-}}\NormalTok{ICC,}
                        \AttributeTok{alpha =}\NormalTok{ alpha )}
\NormalTok{  MLM }\OtherTok{=} \FunctionTok{analysis\_MLM}\NormalTok{( dat )}
\NormalTok{  LR }\OtherTok{=} \FunctionTok{analysis\_OLS}\NormalTok{( dat )}
\NormalTok{  Agg }\OtherTok{=} \FunctionTok{analysis\_agg}\NormalTok{( dat )}
  
  \FunctionTok{bind\_rows}\NormalTok{( }\AttributeTok{MLM =}\NormalTok{ MLM, }\AttributeTok{LR =}\NormalTok{ LR, }\AttributeTok{Agg =}\NormalTok{ Agg,}
             \AttributeTok{.id =} \StringTok{"method"}\NormalTok{ )}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Note the \texttt{stopifnot}: it is wise to ensure our parameter transforms are all reasonable, so we do not get unexplained errors or strange results later on.
It is best if your code fails as soon as possible! Otherwise debugging can be quite hard.

In our modified \texttt{one\_run()} we are transforming our ICC parameter into specific other parameters that are used in our actual model to maintain our effect size interpretation of our simulation.
We have not even modified our \texttt{gen\_dat\_model()} DGP method: we are just specifying the constellation of parameters as a function of the parameters we want to directly control in the simulation.

Controlling how we use the foundational elements such as our data generating code is a key tool for making the higher level simulations sensible and more easily interpretable.
Here we have put our entire simulation into effect size units, and are now providing ``knobs'' to the simulation that are directly interpretable.

\section{Exercises}\label{exercises-2}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  In the prior chapter's exercises, you made a new \texttt{BF\_F} function for the Welch simulation. Now incorporate the \texttt{BF\_F} function into the \texttt{one\_run()} function, and use your revised function to generate simulation results for this additional estimator.
\end{enumerate}

\chapter{Performance criteria}\label{performance-criteria}

Once we have run our simulation, we have a pile of results to sort through.
Given these results, the question is now: how do we assess the performance of our evaluated estimation procedures?

In this chapter, we look at a variety of \textbf{performance criteria} that are commonly used to compare the relative performance of multiple estimators or measure how well an estimator works.
These performance criteria are all assessments of how the estimator behaves if you repeat the experimental process an infinite number of times.
In statistical terms, these criteria are summaries of the true sampling distribution of the estimator, given a specified data generating process.
For example, the bias of an estimator in a given scenario is how far off, on average, the estimator would be from the true parameter value if you repeated the experiment an infinite number of times.

Although we cannot observe the sampling distribution of an estimator directly (and it can only rarely be worked out in full mathematical detail), the set of estimates generated by a simulation constitute a (typically large) \emph{sample} from the sampling distribution of the studied estimator. (Say that six times fast!)
We can then use that sample to \emph{estimate} the performance criteria of interest.
For example, when we wanted to know what percent of the time we would reject the null hypothesis (for a given, specified situation) we estimated that by seeing how often we rejected in 1000 trials.

Now, because we have only a sample of trials rather than the full distribution, our estimates are merely estimates.
In other words, they can be wrong, just due to random chance.
We can describe how wrong with the \textbf{Monte Carlo standard error (MCSE)}.
The MCSE is the standard error of our estimate of performance due to the simulation only having a finite number of trials.
Just as with statistical uncertainty when analyzing data, we can estimate our MCSE and even use them to generate confidence intervals for our performance estimates.
The MCSE is \emph{not} related to the estimators being evaluated; the MCSE is a function of how much simulation we can do.
In a simulation study, we could, in theory, know \emph{exactly} how well our estimators do for a given context, if we ran an infinite number of simulations; the MCSE tells us how far we are from this ideal, given how many simulation trials we actually ran.
Given a desired MCSE, we could similarly determine how many replications were needed to ensure our performance estimates have a desired level of precision.

\section{Inference vs.~Estimation}\label{inference-vs.-estimation}

There are two general classes of analysis one typically does with data: inference and estimation.
To illustrate, we continue to reflect on the question of best practices for analyzing a cluster randomized experiment.
For this problem, we are focused on the \emph{estimand} of the site-average treatment effect, \(\gamma_1\).
The \emph{estimand} is the thing we are trying to estimate.
We can ask whether \(\gamma_1\) is non-zero (inference), and we can further ask what \(\gamma_1\) actually is (estimation).
More expanded we have:

\emph{Inference} is when we do hypothesis testing, asking whether there is evidence for some sort of effect, or asking whether there is evidence that some coefficient is greater than or less than some specified value.
In particular, for our example, to know if there is evidence that there is an average treatment effect at all we would test the null of \(H_0: \gamma_1 = 0\).

\emph{Estimation} is when we try to measure the size of an estimand such as an actual average treatment effect \(\gamma_1\).
Estimation has two major components, the point estimator and the uncertainty estimator.
We generally evaluate both the \emph{actual} properties of the point estimator and the performance of the \emph{estimated} properties of the point estimator.
For example, consider a specific estimate \(\hat{\gamma_1}\) of our average treatment effect.
We first wish to know the actual bias and true standard error (\(SE\)) of \(\hat{\gamma_1}\).
These are its actual properties.
However, for each estimated \(\hat{\gamma_1}\), we also estimate \(\widehat{SE}\), as our estimated measure of how precise our estimate is.
We need to understand the properties of \(\widehat{SE}\) as well.

Inference and estimation are clearly highly related--if we have a good estimate of the treatment effect and it is not zero, then we are willing to say that there is a treatment effect--but depending on the framing, the way you would set up a simulation to investigate the behavior of your estimators could be different.

\section{Ways of Assessing and Comparing Estimation Procedures}\label{ways-of-assessing-and-comparing-estimation-procedures}

We often have different methods for obtaining some estimate, and we often want to know which is best.
For example, comparison is the core question behind our running example of identifying which estimation strategy (aggregation, linear regression, or multilevel modeling) we should generally use when analyzing cluster randomized trial data.
The goal of a simulation comparing our estimators would be to identify whether our estimation strategies were different, whether one was superior to the other (and when), and what the salient differences were.
To fully understand the trade-offs and benefits, we would examine and compare the properties of our different approaches across a variety of circumstances, and with respect to a variety of metrics of success.

For inference, we first might ask whether our methods are valid, i.e., ask whether the methods work correctly when we test for a treatment effect when there is none.
For example, we might wonder whether using multilevel models could open the door to inference problems if we had model misspecification, such as in a scenario where the residuals had some non-normal distribution.
These sorts of questions are questions of validity.

Also for inference, we might ask which method is better for detecting an effect when there is one.
Here, we want to know how our estimators perform in circumstances with a non-zero average treatment effect.
Do they reject the null often, or rarely?
How much does using aggregation decrease (or increase?) our chances of rejection?
These are questions about power.

For estimation, we generally are concerned with two things: bias and variance.
An estimator is biased if it would generally give estimates that are systematically higher (or lower) than the parameter being estimated in a given scenario.
The variance of an estimator is a measure of how much the estimates vary from trial to trial.
The variance is the true standard error, squared.

We might also be concerned with how well we can estimate the uncertainty of our estimators (i.e., estimate our standard error).
For example, we might have an estimator that works very well, but we have no ability to estimate how well.
Continuing our example, we might want to examine how well, for example, the standard errors we get from aggregation work as compared to the standard errors we get out of our linear regression approach.

Finally, we might want to know how well confidence intervals based on our methods work.
Do the intervals capture the true estimands with the desired level of accuracy, across the simulation trials?
Are the intervals for one method generally shorter or longer than those of another?

\section{Assessing a Point Estimator}\label{assessing-a-point-estimator}

Assessing the actual properties of a point estimator is generally fairly simple.
For a given scenario, repeatedly generate data and estimate effects.
Then take summary measures such as the mean and standard deviation of these repeated trials' estimates to estimate the actual properties of the estimator via Monte Carlo.
Given sufficient simulation trials, we can obtain arbitrarily accurate measures.

The most common measures of an estimator are the bias, variance, and mean squared error.
For example, we can ask what the \emph{actual} variance (or standard error) of our estimator is.
We can ask if our estimator is biased.
We can ask what the overall \(RMSE\) (root mean squared error) of our estimator is.

To be more formal, consider an estimator \(T\) that is targeting a parameter \(\theta\).
A simulation study generates a (typically large) sample of estimates \(T_1,...,T_R\), all of the target \(\theta\).
We know \(\theta\) because we generated the data.

We can first assess whether our estimator is biased, by comparing the mean of our \(R\) estimates
\[ \bar{T} = \frac{1}{R}\sum_{r=1}^R T_r \]
to \(\theta\).
The bias of our estimator is \(bias = \bar{T} - \theta\).

We can also ask how variable our estimator is, by calculating the variance of our \(R\) estimates
\[\displaystyle{S_T^2 = \frac{1}{R - 1}\sum_{r=1}^R \left(T_r - \bar{T}\right)^2} . \]

The square root of this, \(S_T\) is the true standard error of our estimator (up to Monte Carlo simulation uncertainty).

Finally, the Root Mean Square Error (RMSE) is a combination of the above two measures:
\[ RMSE = \left\{ \frac{1}{R} \sum_{r = 1}^R \left( T_r - \theta\right)^2 \right\}^{1/2}. \]
Often people talk about the MSE (Mean Squared Error)--this is just the RMSE squared.

An important relationship connecting these three measures is
\[ RMSE^2 = bias^2 + variance = bias^2 + SE^2 .\]
It is important to clarify an important point: the \emph{true standard error} of an estimator \(\hat{\gamma_1}\) is the standard deviation of \(\hat{\gamma_1}\) across multiple datasets.
In practice, we never know this value, but in a simulation we can obtain it as the standard deviation of our simulation trial estimates.
People generally, when they say ``Standard Error'' mean \emph{estimated} Standard Error, (\(\widehat{SE}\)) which is when one uses the emperical data to estimate \(SE\).
For assessing actual properties, we have the true standard error (up to Monte Carlo simulation error).

For absolute assessments of performance, an estimator with low bias, low variance, and thus low RMSE is desired.
For comparisons of relative performance, an estimator with lower RMSE is usually preferable to an estimator with higher RMSE; if two estimators have comparable RMSE, then the estimator with lower bias would usually be preferable.

It is important to recognize that the above performance measures depend on the scale of the parameter.
For example, if our estimators are measuring a treatment impact in dollars, then our bias, SE, and RMSE are all in dollars.
The variance and MSE would be in dollars squared, which is why we take their square roots to put them back on an intepretable dollars scale.

Usually in a simulation, the scale of the outcome is irrelevant as we are comparing one estimator to the other.
To ease interpretation, we might want to assess estimators relative to the baseline variation.
To achieve this, we can generate data so the outcome has unit variance (i.e., we generate \emph{standardized data}).
Then the bias, median bias, and root mean-squared error would all be in standard deviation units.

By contrast, a nonlinear change of scale of a parameter can lead to nonlinear changes in the performance measures.
For instance, suppose that \(\theta\) is a measure of the proportion of time that a behavior occurs.
A natural way to transform this parameter would be to put it on the log-odds (logit) scale.
However, because of the nonlinear aspect of the logit,
\[\text{Bias}\left[\text{logit}(T)\right] \neq \text{logit}\left(\text{Bias}[T]\right), \qquad \text{RMSE}\left[\text{logit}(T)\right] \neq \text{logit}\left(\text{RMSE}[T]\right),\]
and so on.
This is fine, but one should be aware that this can happen and do it on purpose.

\subsection{Comparing the Performances of the Cluster RCT Estimation Procedures}\label{comparing-the-performances-of-the-cluster-rct-estimation-procedures}

Given our simulation results generated in the last chapter, we next assess the bias, standard error, and RMSE of our three different estimators of the ATE.
These performance criteria address these primary questions:

\begin{itemize}
\tightlist
\item
  Is the estimator systematically off? (bias)
\item
  Is it precise? (standard error)
\item
  Does it predict well? (RMSE)
\end{itemize}

Let us see how the three estimators compare on these criteria.

\textbf{Are the estimators biased?}
Bias is with respect to a target estimand.
Here we assess whether our estimates are systematically different from the \(\gamma_1\) parameter we used to generate the data (this is the ATE parameter, which we had set to 0.30).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{runs }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{( method ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{( }
    \AttributeTok{mean\_ATE\_hat =} \FunctionTok{mean}\NormalTok{( ATE\_hat ),}
    \AttributeTok{bias =} \FunctionTok{mean}\NormalTok{( ATE\_hat }\SpecialCharTok{{-}}\NormalTok{ ATE )  )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 3 x 3
##   method mean_ATE_hat    bias
##   <chr>         <dbl>   <dbl>
## 1 Agg           0.306 0.00561
## 2 LR            0.390 0.0899 
## 3 MLM           0.308 0.00788
\end{verbatim}

Linear regression, with a bias of about 0.09 effect size units, appears about ten times as biased as the other estimators.
There is no evidence of major bias for Agg or MLM.
This is because the linear regression is targeting the person-average average treatment effect.
Our data generating process makes larger sites have larger effects, so the person average is going to be higher since those larger sites will count more.
Our estimand, by contrast, is the site average treatment effect, i.e., the simple average of each site's true impact, which our DGP has set to 0.30.
The Agg and MLM methods, by contrast, estimate this site-average effect, putting them in line with our DGP.

If we had instead decided our target estimand was the person average effect, then we would see linear regression as unbiased, and Agg and MLM as biased; it is important to think carefully about what the estimators are targeting, and report bias with respect to a clearly articulated goal.

\textbf{Which method has the smallest standard error?}
The true Standard Error is simply how variable a point estimator is, and is calculated as the standard deviation of the point estimates for a given estimator.
The Standard Error reflects how stable our estimates are across datasets that all came from the same data generating process.
We calculate the standard error, and also the relative standard error using linear regression as a baseline:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{true\_SE }\OtherTok{\textless{}{-}}\NormalTok{ runs }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{( method ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{( }
    \AttributeTok{SE =} \FunctionTok{sd}\NormalTok{( ATE\_hat )}
\NormalTok{  )}
\NormalTok{true\_SE }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{( }\AttributeTok{per\_SE =}\NormalTok{ SE }\SpecialCharTok{/}\NormalTok{ SE[method}\SpecialCharTok{==}\StringTok{"LR"}\NormalTok{] )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 3 x 3
##   method    SE per_SE
##   <chr>  <dbl>  <dbl>
## 1 Agg    0.168  0.916
## 2 LR     0.183  1    
## 3 MLM    0.168  0.916
\end{verbatim}

These standard errors are all what we would be trying to estimate with a standard error estimator in a normal data analysis.
The other methods appear to have SEs about 8\% smaller than Linear Regression.

\textbf{Which method has the smallest Root Mean Squared Error?}

So far linear regression is not doing well: it has more bias and a larger standard error than the other two.
We can assess overall performance by combining these two quantities with the RMSE:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{runs }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{( method ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{( }
    \AttributeTok{bias =} \FunctionTok{mean}\NormalTok{( ATE\_hat }\SpecialCharTok{{-}}\NormalTok{ ATE ),}
    \AttributeTok{SE =} \FunctionTok{sd}\NormalTok{( ATE\_hat ),}
    \AttributeTok{RMSE =} \FunctionTok{sqrt}\NormalTok{( }\FunctionTok{mean}\NormalTok{( (ATE\_hat }\SpecialCharTok{{-}}\NormalTok{ ATE)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{ ) )}
\NormalTok{  ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{( }\AttributeTok{per\_RMSE =}\NormalTok{ RMSE }\SpecialCharTok{/}\NormalTok{ RMSE[method}\SpecialCharTok{==}\StringTok{"LR"}\NormalTok{] )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 3 x 5
##   method    bias    SE  RMSE per_RMSE
##   <chr>    <dbl> <dbl> <dbl>    <dbl>
## 1 Agg    0.00561 0.168 0.168    0.823
## 2 LR     0.0899  0.183 0.204    1    
## 3 MLM    0.00788 0.168 0.168    0.823
\end{verbatim}

We also include SE and bias as reference.

RMSE is a way of taking both bias and variance into account, all at once.
For Agg and MLM, the RMSE is basically the standard error; this makes sense as they are not biased.
For LR, the combination of bias plus increased variability gives a higher RMSE.
That said, clearly the standard error dominates the bias term (note how RMSE and SE are more similar than RMSE and bias).
This is especially the case as RMSE is the square root of the bias and standard errors \emph{squared}; this makes difference between them even more extreme.
Overall, Agg and MLM have RMSEs around 16\% smaller than LR--this seems notable.

\subsection{Handling Estimands Not Represented By a Parameter}\label{handling-estimands-not-represented-by-a-parameter}

In our Cluster RCT example, we have focused on an estimand of the ATE as captured by our model parameter \(\gamma_1\).
But say we were interested in the person-average effect.
This is not represented by any number, so we would have to calculate it and then compare all of our estimates to it.

We offer two ways of doing this.
The first is to simply generate a massive dataset, and then average across it to get a good estimate of the true person-average effect.
If our dataset is big enough, then the uncertainty in this estimate will be neglidgable compared to the uncertainty in our simulation.

Here we try this:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dat }\OtherTok{=} \FunctionTok{gen\_dat\_model}\NormalTok{( }\AttributeTok{n\_bar =} \DecValTok{30}\NormalTok{, }\AttributeTok{J=}\DecValTok{100000}\NormalTok{, }
                     \AttributeTok{gamma\_1 =} \FloatTok{0.3}\NormalTok{, }\AttributeTok{gamma\_2 =} \FloatTok{0.5}\NormalTok{,}
                     \AttributeTok{sigma2\_u =} \FloatTok{0.20}\NormalTok{, }\AttributeTok{sigma2\_e =} \FloatTok{0.80}\NormalTok{,}
                     \AttributeTok{alpha =} \FloatTok{0.75}\NormalTok{  )}
\NormalTok{ATE\_person }\OtherTok{=} \FunctionTok{mean}\NormalTok{( dat}\SpecialCharTok{$}\NormalTok{Yobs[dat}\SpecialCharTok{$}\NormalTok{Z}\SpecialCharTok{==}\DecValTok{1}\NormalTok{] ) }\SpecialCharTok{{-}} \FunctionTok{mean}\NormalTok{( dat}\SpecialCharTok{$}\NormalTok{Yobs[dat}\SpecialCharTok{$}\NormalTok{Z}\SpecialCharTok{==}\DecValTok{0}\NormalTok{] )}
\NormalTok{ATE\_person}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.3881385
\end{verbatim}

Note our estimate of the person-average effect of 0 is about what we would expect given the bias of the linear model!

Note how bias and RMSE have shifted, but SE is the same, when we compare to \texttt{ATE\_person}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{runs }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{( method ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{( }
    \AttributeTok{bias =} \FunctionTok{mean}\NormalTok{( ATE\_hat }\SpecialCharTok{{-}}\NormalTok{ ATE\_person ),}
    \AttributeTok{SE =} \FunctionTok{sd}\NormalTok{( ATE\_hat ),}
    \AttributeTok{RMSE =} \FunctionTok{sqrt}\NormalTok{( }\FunctionTok{mean}\NormalTok{( (ATE\_hat }\SpecialCharTok{{-}}\NormalTok{ ATE\_person)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{ ) )}
\NormalTok{  ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{( }\AttributeTok{per\_RMSE =}\NormalTok{ RMSE }\SpecialCharTok{/}\NormalTok{ RMSE[method}\SpecialCharTok{==}\StringTok{"LR"}\NormalTok{] )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 3 x 5
##   method     bias    SE  RMSE per_RMSE
##   <chr>     <dbl> <dbl> <dbl>    <dbl>
## 1 Agg    -0.0825  0.168 0.187     1.02
## 2 LR      0.00173 0.183 0.183     1   
## 3 MLM    -0.0803  0.168 0.186     1.02
\end{verbatim}

We see Agg and MLM are now biased, and LR is unbiased.
RMSE is now a tension between bias and reduced variance.
Overall, Agg and MLM are 4\% worse than LR in terms of RMSE, because they have lower SEs but more bias.

The second method of calculating \texttt{ATE\_person} would be to record the true person average effect of the dataset of each simulation iteration, and then average those at the end.
To do this we would need to modify our \texttt{gen\_dat\_model()} DGP code to track this additional information.
We might have, for example

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tx\_effect }\OtherTok{=}\NormalTok{ gamma\_1 }\SpecialCharTok{+}\NormalTok{ gamma\_2 }\SpecialCharTok{*}\NormalTok{ (nj}\SpecialCharTok{{-}}\NormalTok{n\_bar)}\SpecialCharTok{/}\NormalTok{n\_bar}
\NormalTok{beta\_0j }\OtherTok{=}\NormalTok{ gamma\_0 }\SpecialCharTok{+}\NormalTok{ Zj }\SpecialCharTok{*}\NormalTok{ tx\_effect }\SpecialCharTok{+}\NormalTok{ u0j}
\end{Highlighting}
\end{Shaded}

and then we would return \texttt{tx\_effect} as well as \texttt{Yobs} and \texttt{Z} as a column in our dataset.
This is similar to directly calculating \emph{potential outcomes}, as discussed in Chapter \citet{ref}(chap\_potential\_outcomes).

Once we modified our DGP code, we \emph{also} need to modify our analysis functions to record this information.
We might have, for example:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{analyze\_data }\OtherTok{=} \ControlFlowTok{function}\NormalTok{( dat ) \{}
\NormalTok{  MLM }\OtherTok{=} \FunctionTok{analysis\_MLM}\NormalTok{( dat )}
\NormalTok{  LR }\OtherTok{=} \FunctionTok{analysis\_OLS}\NormalTok{( dat )}
\NormalTok{  Agg }\OtherTok{=} \FunctionTok{analysis\_agg}\NormalTok{( dat )}
\NormalTok{  res }\OtherTok{\textless{}{-}} \FunctionTok{bind\_rows}\NormalTok{( }\AttributeTok{MLM =}\NormalTok{ MLM, }\AttributeTok{LR =}\NormalTok{ LR, }\AttributeTok{Agg =}\NormalTok{ Agg,}
             \AttributeTok{.id =} \StringTok{"method"}\NormalTok{ )}
\NormalTok{  res}\SpecialCharTok{$}\NormalTok{ATE\_person }\OtherTok{=} \FunctionTok{mean}\NormalTok{( dat}\SpecialCharTok{$}\NormalTok{tx\_effect )}
  \FunctionTok{return}\NormalTok{( res )}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Now when we run our simulation, we would have a column which is the true person average treatment effect for each dataset.
We could then take the average of those across our datasets to estimate the true person average treatment effect in the population, and then compare our point estimators to that value.

Clearly, an estimand that is not represented by a parameter is more difficult to work with, but it is not impossible.
As always, be clear as to what you are trying to estimate.

\section{Assessing a Standard Error Estimator}\label{assessing-a-standard-error-estimator}

Statistics is perhaps more about assessing how good an estimate is than making an estimate in the first place.
This translates to simulation studies: in our simulation we can know an estimator's actual properties, but if we were to use this estimator in practice we would have to also estimate its associated standard error, and generate confidence intervals and so forth using this standard error estimate.
To understand if this would work in practice, we would need to evaluate not only the behavior of the estimator itself, but the behavior of these associated things.
In other words, we generally not only want to know whether our point estimator is doing a good job, but we usually want to know whether we are able to get a good standard error for that point estimator as well.

To do this we first compare the expected value of \(\widehat{SE}\) (estimated with the average \(\widehat{SE}\) across our simulation trials) to the actual \(SE\).
This tells us whether our uncertainty estimates are \emph{biased}.
We could also examine the standard deviation of \(\widehat{SE}\) across trials, which tells us whether our estimates of uncertainty are relatively stable.
We finally could examine whether there is correlation between \(\widehat{SE}\) and the actual error (e.g., \(\left|T - \theta \right|\)).
Good estimates of uncertainty should predict error in a given context (especially if calculating conditional estimates); see \citet{sundberg2003conditional}.

For the first assessment, we usually assess the quality of a standard error estimator with a relative performance criteria, rather than an absolute one, meaning we compare the estimated standard error to the true standard error as a ratio.

For an example, suppose that in our simulation we are examining the performance of a point-estimator \(T\) for a parameter \(\theta\) along with an estimator \(\widehat{SE}\) for the standard error of \(T\).
In this case, we likely do not know the true standard error of \(T\), for our simulation context, prior to the simulation.
However, we can use the variance of \(T\) across the replications (\(S_T^2\)) to directly estimate the true sampling variance \(\text{Var}(T) = SE^2(T)\).
The \emph{relative bias} of \(\widehat{SE}^2\) would then be estimated by \(RB = \bar{V} / S_T^2\), where \(\bar{V}\) is the average of \(\widehat{SE}^2\) across simulation runs.
Note that a value of 1 for relative bias corresponds to exact unbiasedness of the variance estimator.
The relative bias measure is a measure of \emph{proportionate} under- or over-estimation.
For example, a relative bias of 1.12 would mean the standard error was, on average, 12\% too large.
We discuss relative performance measures further in Section \citet{sec-relative-performance}.

\subsection{Why Not Assess the Estimated SE directly?}\label{why-not-assess-the-estimated-se-directly}

We typically see assessment of \(\widehat{SE}^2\), not \(\widehat{SE}\).
In other words, we typically work with assessing whether the variance estimator is unbiased, etc., rather than the standard error estimator.
This comes out of a few reasons.
First, in practice, so-called unbiased standard errors usually are not in fact actually unbiased (see the delightfully titled section 11.5, ``The Joke Is on Us: The Standard Deviation Estimator is Biased after All,'' in \citet{westfall2013understanding} for further discussion).
For linear regression, for example, the classic standard error estimator is an unbiased \emph{variance} estimator, meaning that we have a small amount of bias due to the square-rooting because:

\[ E[ \sqrt{ V } ] \neq \sqrt{ E[ V ] } . \]

Variance is also the component that gives us the classic bias-variance breakdown of \(MSE = Variance + Bias^2\), so if we are trying to assign whether an overall MSE is due to instability or systematic bias, operating in this squared space may be preferable.

That being said, to put things in terms of performance criteria humans understand, it is usually nicer to put final evaluation metrics back into standard error units.
For example, saying there is a 10\% reduction in the standard error is more meaningful (even if less impressive sounding) than saying there is a 19\% reduction in the variance.

\subsection{Assessing SEs for Our Cluster RCT Simulation}\label{assessing-ses-for-our-cluster-rct-simulation}

To assess whether our estimated SEs are about right, we can look at the average \emph{estimated} (squared) standard error and compare it to the true standard error.
Our standard errors are \emph{inflated} if they are systematically larger than they should be, across the simulation runs.
We can also look at how stable our standard error estimates are, by taking the standard deviation of our standard error estimates.
We interpret this quantity relative to the actual standard error to get how far off, as a percent of the actual standard error, we tend to be.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{runs }\SpecialCharTok{\%\textgreater{}\%}  \FunctionTok{group\_by}\NormalTok{( method ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{( }
    \AttributeTok{SE =} \FunctionTok{sd}\NormalTok{( ATE\_hat ),}
    \AttributeTok{mean\_SEhat =} \FunctionTok{sqrt}\NormalTok{( }\FunctionTok{mean}\NormalTok{( SE\_hat}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{ ) ),}
    \AttributeTok{infl =} \DecValTok{100} \SpecialCharTok{*}\NormalTok{ mean\_SEhat }\SpecialCharTok{/}\NormalTok{ SE,}
    \AttributeTok{sd\_SEhat =} \FunctionTok{sd}\NormalTok{( SE\_hat ),}
    \AttributeTok{stability =} \DecValTok{100} \SpecialCharTok{*}\NormalTok{ sd\_SEhat }\SpecialCharTok{/}\NormalTok{ SE )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 3 x 6
##   method    SE mean_SEhat  infl sd_SEhat stability
##   <chr>  <dbl>      <dbl> <dbl>    <dbl>     <dbl>
## 1 Agg    0.168      0.174  104.   0.0232      13.8
## 2 LR     0.183      0.185  101.   0.0309      16.8
## 3 MLM    0.168      0.174  104.   0.0232      13.8
\end{verbatim}

The SEs for Agg and MLM appear to be a bit conservative on average. (3 or 4 percentage points too big).

The last column (\texttt{stability}) shows how variable the standard error estimates are relative to the true standard error.
50\% would mean the standard error estimates can easily be off by 50\% of the truth, which would not be particularly good.
Here we see the linear regression is more unstable than the other methods (cluster-robust standard errors are generally known to be a bit unstable, so this is not too surprising).
It is a bad day for linear regression.

\section{Assessing an Inferential Procedure (Hypothesis Testing)}\label{assessing-an-inferential-procedure-hypothesis-testing}

When hypothesis tests are used in practice, the researcher specifies a null (e.g., no treatment effect), collects data, and generates a \(p\)-value, which is a measure of how extreme the observed data are from what we would expect to naturally occur, if the null were true.
When we assess a method for hypothesis testing, we are therefore typically concerned with two aspects: \emph{validity} and \emph{power}.

\subsection{Validity}\label{validity}

Validity revolves around whether we erroneously reject a true null more than we should.
Put another way, we say an inference method is valid if it has no more than an \(\alpha\) chance of rejecting the null, when it is true, when we are testing at the \(\alpha\) level.
This means if we used this method 1000 times, where the null was true for all of those 1000 times, we should not see more than about \(1000 \alpha\) rejections (so, 50, if we were using the classic \(\alpha = 0.05\) rule).

To assess validity we would therefore specify a data generating process where the null is in fact true.
We then, for a series of such data sets with a true null, conduct our inferential processes on the data, record the \(p\)-value, and score whether we reject the null hypothesis or not.

We might then test our methods by exploring more extreme data generation processes, where the null is true but other aspects of the data (such as outliers or heavy skew) make estimation difficult.
This allows us to understand if our methods are robust to strange data patterns in finite sample contexts.

The key concept for validity is that the date we generate, no matter how we do it, must be data with a true null.
The check is always then to see if we reject the null more than we should.

\subsection{Power}\label{power}

Power is, loosely speaking, how often we notice an effect when one is there.
Power is a much more nebulous concept than validity, because some effects (e.g.~large effects) are clearly easier to notice than others.
If we are comparing estimators to each other, the overall chance of noticing is less of a concern, because we are typically interested in relative performance.
That being said, in order to generate data for a power evaluation, we have to generate data where there is something to detect.
In other words, we need to commit to what the alternative is, and this can be a tricky business.

Typically, we think of power as a function of sample size or effect size. Therefore, we will typically examine a sequence of scenarios with steadily increasing sample size or effect size, estimating the power for each scenario in the sequence.

We then, for each sample in our series, estimate the power by the same process as for validity, above.
When assessing validity, we want rejection rates to be low, below \(\alpha\), and when assessing power we want them to be as high as possible. But the simulation process itself, other than the data generating process, is exactly the same.

\subsection{The Rejection Rate}\label{the-rejection-rate}

To put some technical terms to this framing, for both validity and power assessment the main performance criterion is the \textbf{rejection rate} of the hypothesis test. When the data are simulated from a model in which the null hypothesis being tested is true, then the rejection rate is equivalent to the \textbf{Type-I error rate} of the test. When the data are simulated from a model in which the null hypothesis is false, then the rejection rate is equivalent to the \textbf{power} of the test (for the given alternate hypothesis represented by the DGP).
Ideally, a testing procedure should have actual Type-I error equal to the nominal level \(\alpha\) (this is the definition of validity), but such exact tests are rare.

There are some different perspectives on how close the actual Type-I error rate should be in order to qualify as suitable for use in practice. Following a strict statistical definition, a hypothesis testing procedure is said to be \textbf{level-\(\alpha\)} if its actual Type-I error rate is \emph{always} less than or equal to \(\alpha\).
Among a set of level-\(\alpha\) tests, the test with highest power would be preferred.
If looking only at null rejection rates, then the test with Type-I error closest to \(\alpha\) would usually be preferred.
A less stringent criteria is sometimes used instead, where type I error would be considered acceptable if it is within 50\% of the desired \(\alpha\).

Often, it is of interest to evaluate the performance of the test at several different \(\alpha\) levels.
A convenient way to calculate a set of different rejection rates is to record the simulated \(p\)-values and then calculate from those.
To illustrate, suppose that \(P_r\) is the \(p\)-value from simulation replication \(k\), for \(k = 1,...,R\).
Then the rejection rate for a level-\(\alpha\) test is defined as \(\rho_\alpha = \text{Pr}\left(P_r < \alpha\right)\) and estimated as, using the recorded \(p\)-values,
\[r_\alpha = \frac{1}{R} \sum_{r=1}^R I(P_r < \alpha).\]

For a null DGP, one can also plot the emperical cumulative density function of the \(p\)-values; a valid test should give a \(45^\circ\) line as the \(p\)-values should be standard uniform in distribution.

\subsection{Inference in our Cluster RCT Simulation}\label{inference-in-our-cluster-rct-simulation}

For our scenario, we generated data with an actual treatment effect.
Without further simulation, we therefore could only assess power, not validity.
This is easily solved!
We simply rerun our simulation code that we made last chapter with \texttt{simhelpers}, but with setting \texttt{ATE\ =\ 0}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{( }\DecValTok{404044}\NormalTok{ )}
\NormalTok{runs\_val }\OtherTok{\textless{}{-}} \FunctionTok{sim\_function}\NormalTok{( R, }\AttributeTok{n\_bar =} \DecValTok{30}\NormalTok{, }\AttributeTok{J =} \DecValTok{20}\NormalTok{, }\AttributeTok{gamma\_1 =} \DecValTok{0}\NormalTok{ )}
\FunctionTok{saveRDS}\NormalTok{( runs\_val, }\AttributeTok{file =} \StringTok{"results/cluster\_RCT\_simulation\_validity.rds"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

Assessing power and validity is exactly the same calculation: we see how often we have a \(p\)-value less than 0.05.
For power we have:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{runs }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{group\_by}\NormalTok{( method ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{( }\AttributeTok{power =} \FunctionTok{mean}\NormalTok{( p\_value }\SpecialCharTok{\textless{}=} \FloatTok{0.05}\NormalTok{ ) )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 3 x 2
##   method power
##   <chr>  <dbl>
## 1 Agg    0.376
## 2 LR     0.503
## 3 MLM    0.383
\end{verbatim}

For validity:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{runs\_val }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{group\_by}\NormalTok{( method ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{( }\AttributeTok{power =} \FunctionTok{mean}\NormalTok{( p\_value }\SpecialCharTok{\textless{}=} \FloatTok{0.05}\NormalTok{ ) )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 3 x 2
##   method power
##   <chr>  <dbl>
## 1 Agg    0.051
## 2 LR     0.059
## 3 MLM    0.048
\end{verbatim}

The power when there is an effect (for this specific scenario) is not particularly high, and the validity is around 0.05, as desired.

Linear regression has notabily higher power\ldots{} but this may be in part due to the invalidity of the test (note the rejection rate is around 6\%, rather than the target of 5\%).
The elevated power is also likely due to the upward bias in estimation.
As discussed above, LR is targeting the person-average impact which, in this case, is not 0 even under our null because we have kept our impact heterogeniety parameter to its default of \(\gamma_2=0.2\), meaning we have treatment variation around 0.
We could run our simulation with truly null effects to see if the false rejection rate goes down.

\section{Assessing Confidence Intervals}\label{assessing-confidence-intervals}

Some estimation procedures result in confidence intervals (or sets) which are ranges of values that should contain the true answer with some specified degree of confidence.
For example, a normal-based confidence interval is a combination of an estimator and its estimated uncertainty.

We typically score a confidence interval along two dimensions, \textbf{coverage rate} and \textbf{average length}.
To calculate coverage rate, we score whether each interval ``captured'' the true parameter.
A success is if the true parameter is inside the interval.
To calculate average length, we record each confidence interval's length, and then average across simulation runs.
We say an estimator has good properties if it has good coverage, i.e.~it is capturing the true value at least \(1-\alpha\) of the time, and if it is generally short (i.e., the average length of the interval is less than the average length for other methods).

Confidence interval coverage is simultaneously evaluating the estimators in terms of how well they estimate (precision) and their inferential properties.
We have combined inference and estimation.

Suppose that the confidence intervals are for the target parameter \(\theta\) and have coverage level \(\beta\).
Let \(A_r\) and \(B_r\) denote the lower and upper end-points of the confidence interval from simulation replication \(r\), and let \(W_r = B_r - A_r\), all for \(r = 1,...,R\).
The coverage rate \(\omega_\beta\) and average length \(\text{E}(W)\) criteria are then as defined in the table below.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4265}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3235}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Criterion
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Definition
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Estimate
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Coverage & \(\omega_\beta = \text{Pr}(A \leq \theta \leq B)\) & \(\frac{1}{R}\sum_{r=1}^R I(A_r \leq \theta \leq B_r)\) \\
Expected length & \(\text{E}(W) = \text{E}(B - A)\) & \(\bar{W} = \bar{B} - \bar{A}\) \\
\end{longtable}

Just as with hypothesis testing, a strict statistical interpretation would deem a hypothesis testing procedure acceptable if it has actual coverage rate greater than or equal to \(\beta\).
If multiple tests satisfy this criterion, then the test with the lowest expected length would be preferable. Some analysts prefer to look at lower and upper coverage separately, where lower coverage is \(\text{Pr}(A \leq \theta)\) and upper coverage is \(\text{Pr}(\theta \leq B)\).

\subsection{Confidence Intervals in our Cluster RCT Example}\label{confidence-intervals-in-our-cluster-rct-example}

For our CRT simulation, we first have to calculate confidence intervals, and then assess coverage.
We could have used methods such as \texttt{confint()} in the estimation approaches; this would be preferred if we wanted more accurately calculated confidence intervals that used \(t\)-distributions and so forth to account for the moderate number of clusters.

But if we want to use normal assumption confidence intervals we can calculate them post-hoc:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{runs }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{mutate}\NormalTok{( }\AttributeTok{CI\_l =}\NormalTok{ ATE\_hat }\SpecialCharTok{{-}} \FloatTok{1.96}\SpecialCharTok{*}\NormalTok{SE\_hat,}
                 \AttributeTok{CI\_h =}\NormalTok{ ATE\_hat }\SpecialCharTok{+} \FloatTok{1.96}\SpecialCharTok{*}\NormalTok{SE\_hat,}
                 \AttributeTok{covered =}\NormalTok{ CI\_l }\SpecialCharTok{\textless{}=}\NormalTok{ ATE }\SpecialCharTok{\&}\NormalTok{ ATE }\SpecialCharTok{\textless{}=}\NormalTok{ CI\_h,}
                 \AttributeTok{width =}\NormalTok{ CI\_h }\SpecialCharTok{{-}}\NormalTok{ CI\_l ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{( method ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{( }\AttributeTok{coverage =} \FunctionTok{mean}\NormalTok{( covered ),}
             \AttributeTok{width =} \FunctionTok{mean}\NormalTok{( width ))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 3 x 3
##   method coverage width
##   <chr>     <dbl> <dbl>
## 1 Agg       0.942 0.677
## 2 LR        0.908 0.717
## 3 MLM       0.943 0.677
\end{verbatim}

Our coverage is about right for Agg and MLM, and around 5 percentage points too low for LR.
Linear regression is taking a hit from the bias term.
The CIs of LR are a bit wider than the other methods due to the estimated SEs being slightly larger.

\section{Additional Thoughts on Measuring Performance}\label{additional-thoughts-on-measuring-performance}

In this section we provide some additional thoughts on performance measures.
We first discuss relative vs.~absolute criteria some more, then touch on robust measures of performance.
We finally summarize the measures we discuss in this chapter.

\subsection{Selecting Relative vs.~Absolute Criteria}\label{sec-relative-performance}

We have primarily examined performance estimators for point estimators using absolute criteria, focusing on measures like bias directly on the scale of the outcome.
In contrast, for evaluation things such as estimated standard errors, which are always positive and scale-dependent, it often makes sense to use relative criteria, i.e., criteria calculated as proportions of the target parameter (\(T/\theta\)) rather than as differences (\(T - \theta\)).
We typically apply absolute criteria to point estimators and relative criteria to standard error estimators (we are setting aside, for the moment, the relative criteria of a measure from one estimation procedure to another, as we saw earlier when we compared the SEs to a baseline SE of linear regression for the cluster randomized trial simulation.
So how do we select when to use what?

As a first piece of guidance, establish whether we expect the performance (e.g., bias, standard error, or RMSE) of a point estimate to depend on the magnitude of the estimand.
For example, if we are estimating some mean \(\theta\), and we generate data where \(\theta = 100\) vs where \(\theta = 1000\) (or any other arbitrary number), we would not generally expect the value of \(\theta\) to change the magnitude of bias, variance, or MSE.
On the other hand, these different \(\theta\)s would have a large impact on the \emph{relative} bias and \emph{relative} MSE.
(Want smaller relative bias? Just add a million to the parameter!)
For these sorts of ``location parameters'' we generally use absolute measures of performance.

That being said, a more principled approach for determining whether to use absolute or relative performance criteria depends on assessing performance for \emph{multiple} values of the parameter.
In many simulation studies, replications are generated and performance criteria are calculated for several different values of a parameter, say \(\theta = \theta_1,...,\theta_p\).
Let's focus on bias for now, and say that we've estimated (from a large number of replications) the bias at each parameter value.
We present two hypothetical scenarios, A and B, in the figures below.

\begin{center}\includegraphics[width=0.75\linewidth]{Designing-Simulations-in-R_files/figure-latex/unnamed-chunk-85-1} \end{center}

If the absolute bias is roughly the same for all values of \(\theta\) (as in Scenario A), then it makes sense to report absolute bias as the summary performance criterion.
On the other hand, if the bias grows roughly in proportion to \(\theta\) (as in Scenario B), then relative bias might be a better summary criterion.

\textbf{Performance relative to a baseline estimator.}

Another relative measure, as we saw earlier, is to calculate performance relative to some baseline.
For example, if one of the estimators is the ``generic method,'' we could calculate ratios of the RMSE of our estimators to the baseline RMSE.
This can provide a way of standardizing across simulation scenarios where the overall scale of the RMSE changes radically.
This could be critical to, for example, examining trends across simulations that have different sample sizes, where we would expect all estimators' performance measures to improve as sample size grows.
This kind of relative standardization allows us to make statements such as ``Aggregation has standard errors around 8\% smaller than linear regression''--which is very interpretable, more interpretable than saying ``Aggregation has standard errors around 0.01 smaller than linear regression.''
In the latter case, we do not know if that is big or small.

While a powerful tool, standardization is not without risks: if you scale relative to something, then higher or lower ratios can either be due to the primary method of interest (the numerator) or due to the behavior of the reference method in the denominator.
These relative ratios can end up being confusing to interpret due to this tension.

They can also break when everything is on a constrained scale, like power.
If we have a power of 0.05, and we improve it to 0.10, we have doubled our power, but if it is 0.10 and we increase to 0.15, we have only increased by 50\%.
Ratios when near zero can be very deceiving.

\subsection{Robust Measures of Performance}\label{robust-measures-of-performance}

Depending on the model and estimation procedures being examined, a range of different criteria might be used to assess estimator performance.
For point estimation, we have seen bias, variance and MSE as the three core measures of performance.
Other criteria exist, such as the median bias and the median absolute deviation of \(T\), where we use the median \(\tilde{T}\) of our estimates rather than the mean \(\bar{T}\).

The usual bias, variance and MSE measures can be sensitive to outliers.
If an estimator generally does well, except for an occasional large mistake, these classic measures can return very poor overall performance.
Instead, we might turn to quantities such as the median bias (sort all the estimation errors across the simulation scenarios, and take the middle), or the Median Absolute Distance (MAD, where you take the median of the absolute values of the errors, which is an alternative to RMSE) as a measure of performance.

Other robust measures are also possible, such as simply truncating all errors to a maximum size (this is called Windsorizing).
This is a way of saying ``I don't care if you are off by 1000, I am only going to count it as 10.''

\subsection{Summary of Peformance Measures}\label{summary-of-peformance-measures}

We list most of the performance criteria we saw in this chapter in the table below, for reference:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3833}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3667}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Criterion
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Definition
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Estimate
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Bias & \(\text{E}(T) - \theta\) & \(\bar{T} - \theta\) \\
Median bias & \(\text{M}(T) - \theta\) & \(\tilde{T} - \theta\) \\
Variance & \(\text{E}\left[\left(T - \text{E}(T)\right)^2\right]\) & \(S_T^2\) \\
MSE & \(\text{E}\left[\left(T - \theta\right)^2\right]\) & \(\left(\bar{T} - \theta\right)^2 + S_T^2\) \\
MAD & \(\text{M}\left[\left|T - \theta\right|\right]\) & \(\left[\left|T - \theta\right|\right]_{R/2}\) \\
Relative bias & \(\text{E}(T) / \theta\) & \(\bar{T} / \theta\) \\
Relative median bias & \(\text{M}(T) / \theta\) & \(\tilde{T} / \theta\) \\
Relative MSE & \(\text{E}\left[\left(T - \theta\right)^2\right] / \theta^2\) & \(\frac{\left(\bar{T} - \theta\right)^2 + S_T^2}{\theta^2}\) \\
\end{longtable}

\begin{itemize}
\tightlist
\item
  Bias and median bias are measures of whether the estimator is systematically higher or lower than the target parameter.
\item
  Variance is a measure of the \textbf{precision} of the estimator---that is, how far it deviates \emph{from its average}. We might look at the square root of this, to assess the precision in the units of the original measure. This is the true SE of the estimator.
\item
  Mean-squared error is a measure of \textbf{overall accuracy}, i.e.~is a measure how far we typically are from the truth. We more frequently use the root mean-squared error, or RMSE, which is just the square root of the MSE.
\item
  The median absolute deviation (MAD) is another measure of overall accuracy that is less sensitive to outlier estimates. The RMSE can be driven up by a single bad egg. The MAD is less sensitive to this.
\end{itemize}

\section{Uncertainty in Performance Estimates (the MCSE)}\label{uncertainty-in-performance-estimates-the-mcse}

Our performance criteria are defined as average performance across an infinite number of trials.
Of course, in our simulations we only run a finite number of trials, and estimate the performance criteria with the sample of trials we generate.
For example, if we are assessing coverage across 100 trials, we can calculate what fraction rejected the null for that 100.
This is an \emph{estimate} of the true coverage rate.
Due to random chance, we might see a higher, or lower, proportion rejected than what we would see if we ran the simulation forever.

To account for estimation uncertainty we want associated uncertainty estimates to go with our point estimates of performance.
We want to, in other words, treat our simulation results as a dataset in its own right.
(And yes, this is quite meta!)

Once we frame the problem in these terms, it is relatively straightforward to calculate standard errors for most of the performance critera because we have an independent and identically distributed set of measurements.
We call these standard errors Monte Carlo Simulation Errors, or MCSEs.
For some of the performance criteria we have to be a bit more clever, as we will discuss below.

We list MCSE expressions for many of our straightforward performance measures on the following table.
In reading the table, recall that, for an estimator \(T\), we have \(S_T\) being the standard deviation of \(T\) across our simulation runs (i.e., our estimated true Standard Error).
We also have

\begin{itemize}
\tightlist
\item
  Sample skewness (standardized): \(\displaystyle{g_T = \frac{1}{R S_T^3}\sum_{r=1}^R \left(T_r - \bar{T}\right)^3}\)
\item
  Sample kurtosis (standardized): \(\displaystyle{k_T = \frac{1}{R S_T^4} \sum_{r=1}^R \left(T_r - \bar{T}\right)^4}\)
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.6667}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.3333}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Criterion for T
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
MCSE
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Bias (\(T-\theta\)) & \(\sqrt{S_T^2/ R}\) \\
Variance (\(S_T^2\)) & \(\displaystyle{S_T^2 \sqrt{\frac{k_T - 1}{R}}}\) \\
MSE & see below \\
MAD & - \\
Power \& Validity (\(r_\alpha\)) & \(\sqrt{ r_\alpha \left(1 - r_\alpha\right) / R}\) \\
Coverage (\(\omega_\beta\)) & \(\sqrt{\omega_\beta \left(1 - \omega_\beta\right) / R}\) \\
Average length (\(\text{E}(W)\)) & \(\sqrt{S_W^2 / R}\) \\
\end{longtable}

The MCSE for the MSE is a bit more complicated, and does not quite fit on our table:
\[ \widehat{MCSE}( \widehat{MSE} ) = \displaystyle{\sqrt{\frac{1}{R}\left[S_T^4 (k_T - 1) + 4 S_T^3 g_T\left(\bar{T} - \theta\right) + 4 S_T^2 \left(\bar{T} - \theta\right)^2\right]}} .\]

For relative quantities with respect to an estimand, simply divide the criterion by the target estimand.
E.g., for relative bias \(T / \theta\), the standard error would be
\[ SE\left( \frac{T}{\theta} \right) = \frac{1}{\theta} SE(T) = \sqrt{\frac{S_T^2}{R\theta^2}} .\]

For square rooted quantities, such as the SE for the true SE (square root of the Variance) or the RMSE (square root of MSE) we can use the Delta method.
The Delta method says (with some conditions), that if we assume \(X \sim N( \phi, V )\), then we can approximate the distribution of \(g(X)\) for some continuous function \(g(\cdot)\) as
\[ g(X) \sim N\left( g(\phi), \;\; g'(\phi)^2\cdot V \right) , \]
where \(g'(\phi)\) is the derivative of \(g(\cdot)\) evaluated at \(\phi\).
In other words,
\[ SE( g(\hat{X}) ) \approx g'(\theta)  \times SE(\hat{X}) .\]
For estimation, we plug in \(\hat{\theta}\) and our estimate of \(SE(\hat{X})\) into the above.
Back to the square root, we have \(g(x) = \sqrt(x)\) and \(g'(x) = 1/2\sqrt(x)\).
This gives, for example, the estimated MCSE of the SE as
\[ \widehat{SE}( \widehat{SE} ) = \widehat{SE}( S^2_T ) = \frac{1}{2S^2_T} \widehat{SE}( S^2_T ) = \frac{1}{2S^2_T} S_T^2 \sqrt{\frac{k_T - 1}{R}} = \frac{1}{2} \sqrt{\frac{k_T - 1}{R}} .\]

\subsection{MCSE for Relative Variance Estimators}\label{mcse-for-relative-variance-estimators}

Estimating the MCSE of the relative bias or relative MSE of a (squared) standard error estimator, i.e., of \(E( \widehat{SE^2} - SE^2 ) / SE^2 )\) or \(\widehat{MSE} / MSE\), is complicated by the appearance of an estimated quantity, \(SE^2\) or \(MSE\), in the denominator of the ratio.
This renders the simple division approach from above unusable, technically speaking.
The problem is we cannot use our clean expressions for MCSEs of relative performance measures since we are not taking the uncertainty of our denominator into account.

To properly assess the overall MCSE, we need to do something else.
One approach is to use the \emph{jackknife} technique.
Let \(\bar{V}_{(j)}\) and \(S_{T(j)}^2\) be the average squared standard error estimate and the true variance estimate calculated from the set of replicates \textbf{\emph{that excludes replicate \(j\)}}, for \(j = 1,...,R\).
The relative bias estimate, excluding replicate \(j\) would then be \(\bar{V}_{(j)} / S_{T(j)}^2\).
Calculating all \(R\) versions of this relative bias estimate and taking the variance of these \(R\) versions yields the jackknife variance estimator:

\[
MCSE\left( \frac{ \widehat{SE}^2 }{SE^2} \right) = \frac{1}{R} \sum_{j=1}^R \left(\frac{\bar{V}_{(j)}}{S_{T(j)}^2} - \frac{\bar{V}}{S_T^2}\right)^2.
\]

This would be quite time-consuming to compute if we did it by brute force. However, a few algebra tricks provide a much quicker way. The tricks come from observing that

\[
\begin{aligned}
\bar{V}_{(j)} &= \frac{1}{R - 1}\left(R \bar{V} - V_j\right) \\
S_{T(j)}^2 &= \frac{1}{R - 2} \left[(R - 1) S_T^2 - \frac{R}{R - 1}\left(T_j - \bar{T}\right)^2\right]
\end{aligned}
\]
These formulas can be used to avoid re-computing the mean and sample variance from every subsample.
Instead, you calculate the overall mean and overall variance, and then do a small adjustment with each jackknife iteration.
You can even implement this with vector processing in R!

\subsection{\texorpdfstring{Calculating MCSEs With the \texttt{simhelpers} Package}{Calculating MCSEs With the simhelpers Package}}\label{calculating-mcses-with-the-simhelpers-package}

The \texttt{simhelper} package is designed to calculate MCSEs (and the performance metrics themselves) for you.
It is easy to use: take this set of simulation runs on the Welch dataset:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{( simhelpers )}
\FunctionTok{data}\NormalTok{( welch\_res )}
\NormalTok{welch }\OtherTok{\textless{}{-}}\NormalTok{ welch\_res }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{( method }\SpecialCharTok{==} \StringTok{"t{-}test"}\NormalTok{ ) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{( }\SpecialCharTok{{-}}\NormalTok{method, }\SpecialCharTok{{-}}\NormalTok{seed, }\SpecialCharTok{{-}}\NormalTok{iterations )}

\NormalTok{welch}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 8,000 x 8
##       n1    n2 mean_diff      est    var p_val
##    <dbl> <dbl>     <dbl>    <dbl>  <dbl> <dbl>
##  1    50    50         0  0.0258  0.0954 0.934
##  2    50    50         0  0.00516 0.0848 0.986
##  3    50    50         0 -0.0798  0.0818 0.781
##  4    50    50         0 -0.0589  0.102  0.854
##  5    50    50         0  0.0251  0.118  0.942
##  6    50    50         0 -0.115   0.106  0.725
##  7    50    50         0  0.157   0.115  0.645
##  8    50    50         0 -0.213   0.121  0.543
##  9    50    50         0  0.509   0.117  0.139
## 10    50    50         0 -0.354   0.0774 0.206
## # i 7,990 more rows
## # i 2 more variables: lower_bound <dbl>,
## #   upper_bound <dbl>
\end{verbatim}

We can calculate performance metrics across all the range of scenarios.
Here is the rejection rate:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{welch\_sub }\OtherTok{=} \FunctionTok{filter}\NormalTok{( welch, n1 }\SpecialCharTok{==} \DecValTok{50}\NormalTok{, n2 }\SpecialCharTok{==} \DecValTok{50}\NormalTok{, mean\_diff}\SpecialCharTok{==}\DecValTok{0}\NormalTok{ )}
\FunctionTok{calc\_rejection}\NormalTok{(welch\_sub, p\_val)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   K_rejection rej_rate rej_rate_mcse
## 1        1000    0.048   0.006759882
\end{verbatim}

And coverage:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{calc\_coverage}\NormalTok{(welch\_sub, lower\_bound, upper\_bound, mean\_diff)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1 x 5
##   K_coverage coverage coverage_mcse width
##        <int>    <dbl>         <dbl> <dbl>
## 1       1000    0.952       0.00676  1.25
## # i 1 more variable: width_mcse <dbl>
\end{verbatim}

Using \texttt{tidyverse} it is easy to process across scenarios (more on experimental design and multiple scenarios later):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{welch }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{group\_by}\NormalTok{(n1,n2,mean\_diff) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{( }\FunctionTok{calc\_rejection}\NormalTok{( }\AttributeTok{p\_values =}\NormalTok{ p\_val ) )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 8 x 6
## # Groups:   n1, n2 [2]
##      n1    n2 mean_diff K_rejection rej_rate
##   <dbl> <dbl>     <dbl>       <int>    <dbl>
## 1    50    50       0          1000    0.048
## 2    50    50       0.5        1000    0.34 
## 3    50    50       1          1000    0.876
## 4    50    50       2          1000    1    
## 5    50    70       0          1000    0.027
## 6    50    70       0.5        1000    0.341
## 7    50    70       1          1000    0.904
## 8    50    70       2          1000    1    
## # i 1 more variable: rej_rate_mcse <dbl>
\end{verbatim}

\subsection{MCSE Calculation in our Cluster RCT Example}\label{mcse-calculation-in-our-cluster-rct-example}

We can check our MCSEs for our performance measures to see if we have enough simulation trials to give us precise enough estimates to believe the differences we reported earlier.
In particular, we have:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{( simhelpers )}
\NormalTok{runs}\SpecialCharTok{$}\NormalTok{ATE }\OtherTok{=}\NormalTok{ ATE}
\NormalTok{runs }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarise}\NormalTok{( }\FunctionTok{calc\_absolute}\NormalTok{( }\AttributeTok{estimates =}\NormalTok{ ATE\_hat,}
                            \AttributeTok{true\_param =}\NormalTok{ ATE,}
                            \AttributeTok{criteria =} \FunctionTok{c}\NormalTok{(}\StringTok{"bias"}\NormalTok{,}\StringTok{"stddev"}\NormalTok{, }\StringTok{"rmse"}\NormalTok{)) ) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{( }\SpecialCharTok{{-}}\NormalTok{K\_absolute ) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\AttributeTok{digits=}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{r|r|r|r|r|r}
\hline
bias & bias\_mcse & stddev & stddev\_mcse & rmse & rmse\_mcse\\
\hline
0.034 & 0.003 & 0.178 & 0.002 & 0.181 & 0.003\\
\hline
\end{tabular}

We see the MCSEs are quite small relative to the linear regression bias term and all the SEs (\texttt{stddev}) and RMSEs: we have simulated enough runs to see the gross trends identified.
We have \emph{not} simulated enough to for sure know if MLM and Agg are not slightly biased. Given our MCSEs, they could have true bias of around 0.01 (two MCSEs).

\section{Exercises}\label{exercises-3}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Continuing the exercises from the prior chapters, estimate rejection rates of the BFF* test for the parameter values in the fifth line of Table 1 of Brown and Forsythe (1974).
\item
  Implement the jackknife as described above in code. Check your answers against the \texttt{simhelpers} package for the built-in \texttt{t\_res} dataset:
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{( simhelpers )}
\FunctionTok{calc\_relative}\NormalTok{(}\AttributeTok{data =}\NormalTok{ t\_res, }\AttributeTok{estimates =}\NormalTok{ est, }\AttributeTok{true\_param =}\NormalTok{ true\_param)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1 x 7
##   K_relative rel_bias rel_bias_mcse rel_mse
##        <int>    <dbl>         <dbl>   <dbl>
## 1       1000     1.00        0.0128   0.163
## # i 3 more variables: rel_mse_mcse <dbl>,
## #   rel_rmse <dbl>, rel_rmse_mcse <dbl>
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  As foreground to the following chapters, can you explore multiple scenarios for the cluster RCT example to see if the trends are common? First write a function that takes a parameter, runs the entire simulation, and returns the results as a small table. You pick which parameter, e.g., average treatment effect, \texttt{alpha}, or whatever you like), that you wish to vary. Here is a skeleton for the function:
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_simulation }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{( my\_param ) \{}
  \CommentTok{\# call the sim\_function() simulation function from the end of last}
  \CommentTok{\# chapter, setting the parameter you want to vary to my\_param}
  
  \CommentTok{\# Analyze the results, generating a table of performance metrics,}
  \CommentTok{\# e.g., bias or coverage. Make sure your analysis is a data frame,}
  \CommentTok{\# like we saw earlier this chapter.}
  
  \CommentTok{\# Return results}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Then use code like the following to generate a set of results measured as a function of a varying parameter:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{vals }\OtherTok{=} \FunctionTok{seq}\NormalTok{( start, stop, }\AttributeTok{length.out =} \DecValTok{5}\NormalTok{ )}
\NormalTok{res }\OtherTok{=} \FunctionTok{map\_df}\NormalTok{( vals, my\_simulation ) }
\end{Highlighting}
\end{Shaded}

The above code will give you a data frame of results, one column for each performance measure.
Finally, you can use this table and plot the performance measure as a function of the varying parameter.

\chapter{Project: Cronbach Alpha}\label{case_Cronbach}

In this section we walk through a case study of Cronbach Alpha to give an extended ``project,'' or series of exercises, that illustrate writing a complete simulation generated by the filling out of the code skeleton we get from \texttt{simhelpers}'s \texttt{create\_skeleton()} package.

\section{Background}\label{background}

Cronbach's \(\alpha\) coefficient is commonly reported as a measure of the internal consistency among a set of test items. Consider a set of \(p\) test items with population variance-covariance matrix \(\boldsymbol\Phi = \left[\phi_{ij}\right]_{i,j=1}^p\), where \(\phi_{ij}\) is the covariance of item \(i\) and item \(j\) on the test, across all students taking the test.
This population variance-covariance matrix describes how our \(p\) test items co-vary.

Cronback's \(\alpha\) is, under this model, defined as
\[
\alpha = \frac{p}{p - 1}\left(1 - \frac{\sum_{i=1}^p \phi_{ii}}{\sum_{i=1}^p \sum_{j=1}^p \phi_{ij}}\right).
\]

Given a sample of size \(n\), the usual estimate of \(\alpha\) is obtained by replacing the population variances and covariances with corresponding sample estimates. Letting \(s_{ij}\) denote the sample covariance of items \(i\) and \(j\)

\[
A = \frac{p}{p - 1}\left(1 - \frac{\sum_{i=1}^p s_{ii}}{\sum_{i=1}^p \sum_{j=1}^p s_{ij}}\right).
\]

If we assume that the items follow a multivariate normal distribution, then \(A\) corresponds to the maximum likelihood estimator of \(\alpha\).

Our goal is to examine the properties of \(A\) when the set of \(P\) items is \emph{not} multi-variate normal, but rather follows a multivariate \(t\) distribution with \(v\) degrees of freedom.
For simplicity, we shall assume that the items have common variance and have a \textbf{compound symmetric} covariance matrix, such that \(\phi_{11} = \phi_{22} = \cdots = \phi_{pp} = \phi\) and \(\phi_{ij} = \rho \phi\). In this case we can simplify our expression for \(\alpha\) to

\[
\alpha = \frac{p \rho}{1 + \rho (p - 1)}.
\]

\section{Getting started}\label{getting-started}

First create the skeleton of our simulation.
We will then walk through filling in all the pieces.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{( simhelpers )}
\FunctionTok{create\_skeleton}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\section{The data-generating function}\label{the-data-generating-function}

The first two sections in the skeleton are about the data-generating model:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{rm}\NormalTok{(}\AttributeTok{list =} \FunctionTok{ls}\NormalTok{())}

\CommentTok{\#{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\# Set development values for simulation parameters}
\CommentTok{\#{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}

\CommentTok{\# What are your model parameters?}
\CommentTok{\# What are your design parameters?}

\CommentTok{\#{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\# Data Generating Model}
\CommentTok{\#{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}

\NormalTok{dgm }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(model\_params) \{}

  \FunctionTok{return}\NormalTok{(dat)}
\NormalTok{\}}

\CommentTok{\# Test the data{-}generating model {-} How can you verify that it is correct?}
\end{Highlighting}
\end{Shaded}

We need to create and test a function that takes model parameters (and sample sizes and such) as inputs, and produces a simulated dataset.
The following function generates a sample of \(n\) observations of \(p\) items from a multivariate \(t\)-distribution with a compound symmetric covariance matrix, intra-class correlation \(\rho\), and \(v\) degrees of freedom:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# model parameters}
\NormalTok{alpha }\OtherTok{\textless{}{-}} \FloatTok{0.73} \CommentTok{\# true alpha}
\NormalTok{df }\OtherTok{\textless{}{-}} \DecValTok{12} \CommentTok{\# degrees of freedom}

\CommentTok{\# design parameters}
\NormalTok{n }\OtherTok{\textless{}{-}} \DecValTok{50} \CommentTok{\# sample size}
\NormalTok{p }\OtherTok{\textless{}{-}} \DecValTok{6} \CommentTok{\# number of items}


\FunctionTok{library}\NormalTok{(mvtnorm)}

\NormalTok{r\_mvt\_items }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(n, p, alpha, df) \{}
\NormalTok{  icc }\OtherTok{\textless{}{-}}\NormalTok{ alpha }\SpecialCharTok{/}\NormalTok{ (p }\SpecialCharTok{{-}}\NormalTok{ alpha }\SpecialCharTok{*}\NormalTok{ (p }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{))}
\NormalTok{  V\_mat }\OtherTok{\textless{}{-}}\NormalTok{ icc }\SpecialCharTok{+} \FunctionTok{diag}\NormalTok{(}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ icc, }\AttributeTok{nrow =}\NormalTok{ p)}
\NormalTok{  X }\OtherTok{\textless{}{-}} \FunctionTok{rmvt}\NormalTok{(}\AttributeTok{n =}\NormalTok{ n, }\AttributeTok{sigma =}\NormalTok{ V\_mat, }\AttributeTok{df =}\NormalTok{ df)}
  \FunctionTok{colnames}\NormalTok{(X) }\OtherTok{\textless{}{-}}\NormalTok{ LETTERS[}\DecValTok{1}\SpecialCharTok{:}\NormalTok{p]}
\NormalTok{  X}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Note how we translate the target \(\alpha\) to \(ICC\) for our DGP; we will see this type of translation more later on.

We check our method first to see if we get the right kind of data:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{small\_sample }\OtherTok{\textless{}{-}} \FunctionTok{r\_mvt\_items}\NormalTok{(}\AttributeTok{n =} \DecValTok{8}\NormalTok{, }\AttributeTok{p =} \DecValTok{3}\NormalTok{, }\AttributeTok{alpha =} \FloatTok{0.73}\NormalTok{, }\AttributeTok{df =} \DecValTok{5}\NormalTok{)}
\NormalTok{small\_sample}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##               A           B          C
## [1,]  0.8936985  0.03730282  2.1568676
## [2,]  0.0211971  1.38423505  0.9825382
## [3,] -0.8850946 -0.23581614 -1.2779934
## [4,] -0.5871447  1.14479160  0.1826922
## [5,]  0.5331295  0.60985172  1.2881116
## [6,] -3.2875992  0.66925687 -0.3486387
## [7,]  0.1083000  0.38639687 -0.7700505
## [8,]  1.3067774  2.43909678  9.3908235
\end{verbatim}

It looks like we have 8 observations of 3 items, as desired.

To check that the function is indeed simulating data following the intended distribution, let's next generate a very large sample of items. We can then verify that the correlation matrix of the items is compound-symmetric and that the marginal distributions of the items follow \(t\)-distributions with specified degrees of freedom.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{big\_sample }\OtherTok{\textless{}{-}} \FunctionTok{r\_mvt\_items}\NormalTok{(}\AttributeTok{n =} \DecValTok{100000}\NormalTok{, }\AttributeTok{p =} \DecValTok{4}\NormalTok{, }\AttributeTok{alpha =} \FloatTok{0.73}\NormalTok{, }\AttributeTok{df =} \DecValTok{5}\NormalTok{)}
\FunctionTok{round}\NormalTok{(}\FunctionTok{cor}\NormalTok{(big\_sample), }\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##       A     B     C     D
## A 1.000 0.403 0.400 0.402
## B 0.403 1.000 0.399 0.408
## C 0.400 0.399 1.000 0.403
## D 0.402 0.408 0.403 1.000
\end{verbatim}

Is this what it should look like?

We can also check normality:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{qqplot}\NormalTok{(}\FunctionTok{qt}\NormalTok{(}\FunctionTok{ppoints}\NormalTok{(}\DecValTok{200}\NormalTok{), }\AttributeTok{df =} \DecValTok{5}\NormalTok{), big\_sample[,}\DecValTok{2}\NormalTok{], }\AttributeTok{ylim =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{4}\NormalTok{,}\DecValTok{4}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{Designing-Simulations-in-R_files/figure-latex/unnamed-chunk-99-1} \end{center}

Looks good! A nice straight line.

\section{The estimation function}\label{the-estimation-function}

The next section of the template looks like this:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\# Model{-}fitting/estimation/testing functions}
\CommentTok{\#{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}

\NormalTok{estimate }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(dat, design\_params) \{}

  \FunctionTok{return}\NormalTok{(result)}
\NormalTok{\}}

\CommentTok{\# Test the estimation function}
\end{Highlighting}
\end{Shaded}

van Zyl, Neudecker, and Nel (2000) demonstrate that, if the items have a compound-symmetric covariance matrix, then the asymptotic variance of \(A\) is
\[
\text{Var}(A) \approx \frac{2p(1 - \alpha)^2}{(p - 1) n}.
\]
Substituting \(A\) in place of \(\alpha\) on the right hand side gives an estimate of the variance of \(A\). The following function calculates \(A\) and its variance estimator from a sample of data:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{estimate\_alpha }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(dat) \{}
\NormalTok{  V }\OtherTok{\textless{}{-}} \FunctionTok{cov}\NormalTok{(dat)}
\NormalTok{  p }\OtherTok{\textless{}{-}} \FunctionTok{ncol}\NormalTok{(dat)}
\NormalTok{  n }\OtherTok{\textless{}{-}} \FunctionTok{nrow}\NormalTok{(dat)}
  
  \CommentTok{\# Calculate A with our formula}
\NormalTok{  A }\OtherTok{\textless{}{-}}\NormalTok{ p }\SpecialCharTok{/}\NormalTok{ (p }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{) }\SpecialCharTok{*}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}} \FunctionTok{sum}\NormalTok{(}\FunctionTok{diag}\NormalTok{(V)) }\SpecialCharTok{/} \FunctionTok{sum}\NormalTok{(V))}
  
  \CommentTok{\# Calculate our estimate of the variance (SE\^{}2) of A}
\NormalTok{  Var\_A }\OtherTok{\textless{}{-}} \DecValTok{2} \SpecialCharTok{*}\NormalTok{ p }\SpecialCharTok{*}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ A)}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{/}\NormalTok{ ((p }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{) }\SpecialCharTok{*}\NormalTok{ n)}
  
  \CommentTok{\# Pack up our results}
  \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{A =}\NormalTok{ A, }\AttributeTok{Var =}\NormalTok{ Var\_A)}
\NormalTok{\}}

\FunctionTok{estimate\_alpha}\NormalTok{(small\_sample)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           A       Var
## 1 0.6465254 0.0468541
\end{verbatim}

The \texttt{psych} package provides a function for calculating \(\alpha\), which can be used to verify that the calculation of \(A\) in \texttt{estimate\_alpha} is correct:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(psych)}
\FunctionTok{summary}\NormalTok{(}\FunctionTok{alpha}\NormalTok{(}\AttributeTok{x =}\NormalTok{ small\_sample))}\SpecialCharTok{$}\NormalTok{raw\_alpha}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Number of categories should be increased  in order to count frequencies.
\end{verbatim}

\begin{verbatim}
## 
## Reliability analysis   
##  raw_alpha std.alpha G6(smc) average_r S/N ase
##       0.65      0.79    0.81      0.55 3.7 0.1
##  mean  sd median_r
##  0.67 1.7     0.58
\end{verbatim}

\begin{verbatim}
## NULL
\end{verbatim}

The next step is to evaluate these individual estimates and see how well our estimator \(A\) performs.

\subsection{Exercices (Naive confidence intervals)}\label{exercices-naive-confidence-intervals}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  One way to obtain an approximate confidence interval for \(\alpha\) would be to take \(A \pm z \sqrt{\text{Var}(A)}\), where \(\text{Var}(A)\) is estimated as described above and \(z\) is a standard normal critical value at the appropriate level (i.e., \(z = 1.96\) for a 95\% CI).
  Extend your simulation to calculate a confidence interval for each simulation round (put this code inside \texttt{estimate\_alpha()}) and then calculate confidence interval coverage.
\end{enumerate}

Your \texttt{estimate\_alpha} would then give a result like this:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{40200}\NormalTok{)}
\NormalTok{dat }\OtherTok{=} \FunctionTok{r\_mvt\_items}\NormalTok{(}\AttributeTok{n =} \DecValTok{50}\NormalTok{, }\AttributeTok{p =} \DecValTok{5}\NormalTok{, }\AttributeTok{alpha =} \FloatTok{0.9}\NormalTok{, }\AttributeTok{df =} \DecValTok{3}\NormalTok{ )}
\FunctionTok{estimate\_alpha}\NormalTok{(dat)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           A          Var   CI_low   CI_high
## 1 0.9425904 0.0001647933 0.916916 0.9682647
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  You can calculate confidence intervals with coverage other than 95\% by calculating an appropriate number of standard errors, \(z\) (usually just taken as 2, as above, for a nominal 95\%), with
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{coverage }\OtherTok{=} \FloatTok{0.95}
\NormalTok{z }\OtherTok{=} \FunctionTok{qnorm}\NormalTok{( (}\DecValTok{1}\SpecialCharTok{{-}}\NormalTok{coverage) }\SpecialCharTok{/} \DecValTok{2}\NormalTok{, }\AttributeTok{lower.tail =} \ConstantTok{FALSE}\NormalTok{ )}
\NormalTok{z}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1.959964
\end{verbatim}

Extend \texttt{estimate\_alpha()} to allow for a specified coverage by adding a parameter, \texttt{coverage}, along with a default of 0.95. Revise the body of \texttt{estimate\_alpha} to calculate a confidence interval with the specified coverage rate.

\section{Estimator performance}\label{estimator-performance}

The next section of the template deals with performance calculations.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\# Calculate performance measures}
\CommentTok{\# (For some simulations, it may make more sense}
\CommentTok{\# to do this as part of the simulation driver.)}
\CommentTok{\#{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}

\NormalTok{performance }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(results, model\_params) \{}

  \FunctionTok{return}\NormalTok{(performance\_measures)}
\NormalTok{\}}

\CommentTok{\# Check performance calculations}
\end{Highlighting}
\end{Shaded}

The \texttt{performance()} function takes as input a bunch of simulated data (which we might call \texttt{results}) and the true values of the model parameters (\texttt{model\_params}) and returns as output a set of summary performance measures. As noted in the comments above, for simple simulations it might not be necessary to write a separate function to do these calculations. For more complex simulations, though, it can be helpful to break these calculations out in a function.

To start to get the code working that we would put into this function, it is useful to start with some simulation replicates to practice on.
We can generate 1000 replicates using samples of size \(n = 40\), \(p = 6\) items, a true \(\alpha = 0.8\), and \(v = 5\) degrees of freedom:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{one\_run }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{( n, p, alpha, df ) \{}
\NormalTok{    dat }\OtherTok{\textless{}{-}} \FunctionTok{r\_mvt\_items}\NormalTok{(}\AttributeTok{n =}\NormalTok{ n, }\AttributeTok{p =}\NormalTok{ p, }\AttributeTok{alpha =}\NormalTok{ alpha, }\AttributeTok{df =}\NormalTok{ df)}
    \FunctionTok{estimate\_alpha}\NormalTok{(dat)}
\NormalTok{\}}
\NormalTok{true\_alpha }\OtherTok{=} \FloatTok{0.7}
\NormalTok{results }\OtherTok{=} \FunctionTok{rerun}\NormalTok{( }\DecValTok{1000}\NormalTok{, }\FunctionTok{one\_run}\NormalTok{(}\DecValTok{40}\NormalTok{, }\DecValTok{6}\NormalTok{, }\AttributeTok{alpha=}\NormalTok{true\_alpha, }\AttributeTok{df=}\DecValTok{5}\NormalTok{) ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{bind\_rows}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: `rerun()` was deprecated in purrr 1.0.0.
## i Please use `map()` instead.
##   # Previously
## rerun(1000, one_run(40, 6, alpha = true_alpha,
## df = 5))
## 
##   # Now
## map(1:1000, ~ one_run(40, 6, alpha = true_alpha,
## df = 5))
## This warning is displayed once every 8 hours.
## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated.
\end{verbatim}

\subsection{Exercises (Calculating Performance)}\label{exercises-calculating-performance}

For the Cronbach alpha simulation, we might want to calculate the following performance measures:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  With the parameters specified above, calculate the bias of \(A\). Also calculate the Monte Carlo standard error (MCSE) of the bias estimate.
\item
  Estimate the true Standard Error of \(A\).
\item
  Calculate the mean squared error of \(A\).
\item
  Calculate the relative bias of the asymptotic variance estimator.
\item
  Using the work from above, wrap your code in an \texttt{alpha\_performance()} function that takes the results of uses \texttt{run\_alpha\_sim} and returns a one-row data frame with columns corresponding to the bias, mean squared error, relative bias of the asymptotic variance estimator.
\end{enumerate}

E.g.,

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{alpha\_performance}\NormalTok{(results, true\_alpha)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##          bias     bias_SE        SE       MSE
## 1 -0.02329445 0.003741931 0.1183302 0.1205433
##    bias_Var
## 1 0.5078144
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{5}
\item
  Extend your function to add in the MCSEs for the SE and MSE. Code up the skewness and kurtosis values by hand, using the formula in the MCSE section of the performance measure chapter.
\item
  \textbf{(Challenge problem)} Code up a jackknife MCSE function to calculate the MCSE for the relative bias of the asymptotic variance estimator.
  Use the following template that takes a vector of point estimates and associated standard errors.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{jackknife\_MCSE }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{( estimates, SEs ) \{}
  \CommentTok{\# code}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

You would use this function as:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{jackknife\_MCSE}\NormalTok{( alpha\_reps}\SpecialCharTok{$}\NormalTok{A, }\FunctionTok{sqrt}\NormalTok{( alpha\_reps}\SpecialCharTok{$}\NormalTok{Var ) )}
\end{Highlighting}
\end{Shaded}

\section{Replication (and the simulation)}\label{replication-and-the-simulation}

We now have all the components we need to get simulation results, given a set of parameter values.
In the next section of the template, we put all these pieces together in a function---which we might call the \emph{simulation driver}---that takes as input 1) parameter values, 2) the desired number of replications, and 3) optionally, a seed value (this allows for reproducability, see Chapter \citet{sec_reproducability}). The function produces as output a single set of performance estimates. Generically, the function looks like this:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\# Simulation Driver {-} should return a data.frame or tibble}
\CommentTok{\#{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}

\NormalTok{runSim }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(iterations, model\_params, design\_params, }\AttributeTok{seed =} \ConstantTok{NULL}\NormalTok{) \{}
  \ControlFlowTok{if}\NormalTok{ (}\SpecialCharTok{!}\FunctionTok{is.null}\NormalTok{(seed)) }\FunctionTok{set.seed}\NormalTok{(seed)}

\NormalTok{  results }\OtherTok{\textless{}{-}} \FunctionTok{rerun}\NormalTok{(iterations, \{}
\NormalTok{                dat }\OtherTok{\textless{}{-}} \FunctionTok{dgm}\NormalTok{(model\_params)}
                \FunctionTok{estimate}\NormalTok{(dat, design\_params)}
\NormalTok{              \}) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{bind\_rows}\NormalTok{()}
  

  \FunctionTok{performance}\NormalTok{(results, model\_params)}
\NormalTok{\}}

\CommentTok{\# demonstrate the simulation driver}
\end{Highlighting}
\end{Shaded}

The \texttt{runSim} function should require very little modification for a new simulation. Essentially, all we need to change is the names of the functions that are called, so that they line up with the functions we have designed for our simulation. Here's what this looks like for the Cronbach alpha simulation (we pull out the code to replicate into its own method, \texttt{one\_run()}, which helps with debugging):

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\# Simulation Driver {-} should return a data.frame or tibble}
\CommentTok{\#{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}


\NormalTok{one\_run }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{( n, p, alpha, df ) \{}
\NormalTok{    dat }\OtherTok{\textless{}{-}} \FunctionTok{r\_mvt\_items}\NormalTok{(}\AttributeTok{n =}\NormalTok{ n, }\AttributeTok{p =}\NormalTok{ p, }\AttributeTok{alpha =}\NormalTok{ alpha, }\AttributeTok{df =}\NormalTok{ df)}
    \FunctionTok{estimate\_alpha}\NormalTok{(dat)}
\NormalTok{\}}


\NormalTok{run\_alpha\_sim }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(iterations, n, p, alpha, df, }\AttributeTok{seed =} \ConstantTok{NULL}\NormalTok{) \{}
  
  \ControlFlowTok{if}\NormalTok{ (}\SpecialCharTok{!}\FunctionTok{is.null}\NormalTok{(seed)) }\FunctionTok{set.seed}\NormalTok{(seed)}

\NormalTok{  results }\OtherTok{\textless{}{-}} 
    \FunctionTok{rerun}\NormalTok{(iterations, }\FunctionTok{one\_run}\NormalTok{(n, p, alpha, df) ) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{bind\_rows}\NormalTok{()}
  
  \FunctionTok{alpha\_performance}\NormalTok{(results, }\AttributeTok{alpha =}\NormalTok{ alpha)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\section{Extension: Confidence interval coverage}\label{extension-confidence-interval-coverage}

However, van Zyl, Neudecker, and Nel (2000) suggest that a better approximation involves first applying a transformation to \(A\) (to make it more normal in shape), then calculating a confidence interval, then back-transforming to the original scale (this is very similar to the procedure for calculating confidence intervals for correlation coefficients, using Fisher's \(z\) transformation). Let our transformed parameter and estimator be

\[
\begin{aligned}
\beta &= \frac{1}{2} \ln\left(1 - \alpha\right) \\
B &= \frac{1}{2} \ln\left(1 - A\right)
\end{aligned}
\]

and our transformed variance estimator be

\[
V^B = \frac{p}{2 n (p - 1)}.
\]
(This expression comes from a Delta method expansion on \(A\).)

An approximate confidence interval for \(\beta\) is given by \([B_L, B_U]\), where

\[
B_L = B - z \sqrt{V^B}, \qquad B_U = B + z \sqrt{V^B}.
\]

Applying the inverse of the transformation gives a confidence interval for \(\alpha\):

\[
\left[1 - \exp(2B_U), \ 1 - \exp(2 B_L)\right].
\]

\section{A taste of multiple scenarios}\label{a-taste-of-multiple-scenarios}

In the previous sections, we've created code that will generate a set of performance estimates, given a set of parameter values. We can created a dataset that represents every combination of parameter values that we want to examine. How do we put the pieces together?

If we only had a couple of parameter combinations, it would be easy enough to just call our \texttt{run\_alpha\_sim} function a couple of times:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{run\_alpha\_sim}\NormalTok{(}\AttributeTok{iterations =} \DecValTok{100}\NormalTok{, }\AttributeTok{n =} \DecValTok{50}\NormalTok{, }\AttributeTok{p =} \DecValTok{4}\NormalTok{, }\AttributeTok{alpha =} \FloatTok{0.7}\NormalTok{, }\AttributeTok{df =} \DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: `rerun()` was deprecated in purrr 1.0.0.
## i Please use `map()` instead.
##   # Previously
##   rerun(100, one_run(n, p, alpha, df))
## 
##   # Now
##   map(1:100, ~ one_run(n, p, alpha, df))
## This warning is displayed once every 8 hours.
## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated.
\end{verbatim}

\begin{verbatim}
##          bias    bias_SE        SE       MSE
## 1 -0.01211522 0.01088426 0.1088426 0.1089725
##    bias_Var
## 1 0.4913622
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{run\_alpha\_sim}\NormalTok{(}\AttributeTok{iterations =} \DecValTok{100}\NormalTok{, }\AttributeTok{n =} \DecValTok{100}\NormalTok{, }\AttributeTok{p =} \DecValTok{4}\NormalTok{, }\AttributeTok{alpha =} \FloatTok{0.7}\NormalTok{, }\AttributeTok{df =} \DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##         bias    bias_SE        SE        MSE
## 1 -0.0117337 0.00761985 0.0761985 0.07671916
##    bias_Var
## 1 0.4727169
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{run\_alpha\_sim}\NormalTok{(}\AttributeTok{iterations =} \DecValTok{100}\NormalTok{, }\AttributeTok{n =} \DecValTok{50}\NormalTok{, }\AttributeTok{p =} \DecValTok{8}\NormalTok{, }\AttributeTok{alpha =} \FloatTok{0.7}\NormalTok{, }\AttributeTok{df =} \DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##          bias    bias_SE        SE       MSE
## 1 -0.02601087 0.01120983 0.1120983 0.1145292
##    bias_Var
## 1 0.4319068
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{run\_alpha\_sim}\NormalTok{(}\AttributeTok{iterations =} \DecValTok{100}\NormalTok{, }\AttributeTok{n =} \DecValTok{100}\NormalTok{, }\AttributeTok{p =} \DecValTok{8}\NormalTok{, }\AttributeTok{alpha =} \FloatTok{0.7}\NormalTok{, }\AttributeTok{df =} \DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##          bias     bias_SE         SE        MSE
## 1 0.007758062 0.006203414 0.06203414 0.06220884
##    bias_Var
## 1 0.5299059
\end{verbatim}

But in an actual simulation we will probably have too many different combinations to do this ``by hand.''
The final sections of the simulation template demonstrate two different approaches to doing the calculations for \emph{every} combination of parameter values, given a set of parameter values one wants to explore.

This is discussed further in Chapter @ref(exp\_design), but let's get a small taste of doing this now.
In particular, the following code will evaluate the performance of \(A\) for true values of \(\alpha\) ranging from 0.5 to 0.9 (i.e., \texttt{alpha\_true\_seq\ \textless{}-\ seq(0.5,\ 0.9,\ 0.1)}) via \texttt{map\_df()}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{alpha\_true\_seq }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\FloatTok{0.5}\NormalTok{, }\FloatTok{0.9}\NormalTok{, }\FloatTok{0.1}\NormalTok{)}
\NormalTok{results }\OtherTok{\textless{}{-}} \FunctionTok{map\_df}\NormalTok{( alpha\_true\_seq, }
\NormalTok{                   run\_simulation,}
                   \AttributeTok{R =} \DecValTok{100}\NormalTok{,}
                   \AttributeTok{n =} \DecValTok{50}\NormalTok{, }\AttributeTok{p =} \DecValTok{5}\NormalTok{, }\AttributeTok{df =} \DecValTok{5}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

How does coverage change for different values of \(A\)?

\subsection{Exercises}\label{exercises-4}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Show the inverse transform of \(B = g(A)\) gives the above expression.
\item
  Make a new function, \texttt{estimate\_alpha\_xform()} that, given a dataset, calculates a confidence interval for \(\alpha\) following the method described above.
\item
  Using the modified \texttt{estimate\_alpha\_xform()}, generate 1000 replicated confidence intervals for \(n = 40\), \(p = 6\) items, a true \(\alpha = 0.8\), and \(v = 5\) degrees of freedom. Using these replicates, calculate the true coverage rate of the confidence interval. Also calculate the Monte Carlo standard error (MCSE) of this coverage rate.
\item
  Calculate the average length of the confidence interval for \(\alpha\), along with its MCSE.
\item
  Compare the results of this approach to the more naive approach. Are there gains in performance?
\item
  \emph{Challenge} Derive the variance expression for the transformed estimator using the Delta method on the variance expression for \(A\) coupled with the transform. The Delta method says that:
\end{enumerate}

\[ var( f(A) ) \approx \frac{1}{f'(\alpha)} (A - \alpha)^2 . \]

\chapter{Case study: Attrition in a simple randomized experiment}\label{case-study-attrition-in-a-simple-randomized-experiment}

Missingness of the outcome variable is a common problem in randomized experiments conducted with human participants. For experiments where the focus is on estimating average causal effects, many different strategies for handling attrition have been proposed. Gerber and Green (2012) describe several strategies, including:

\begin{itemize}
\tightlist
\item
  Complete-case analysis, in which observations with missing outcome data are simply dropped from analysis;
\item
  Regression estimation under the assumption that missingness is independent of potential outcomes, conditional on a set of pre-treatment covariates; and
\item
  Monotone treatment response bounds, which provides bounds on the average treatment effect among participants who would provide outcome data under any treatment condition, under the assumption that that the effect of treatment on providing outcome data is strictly non-negative.
\end{itemize}

In this case study, we will develop a data-generating process that allows us to study these different estimation strategies. To make things interesting, we will examine a scenario in which the data include covariates that influence the probability of providing outcome data under treatment and under control, as well as being predictive of the outcome.

To formalize these notions, let us define the following variables:

\begin{itemize}
\tightlist
\item
  \(X\): a continuous covariate
\item
  \(C\): a binary covariate
\item
  \(A\): an indicator for whether a participant provides outcome data if assigned to treatment (latent)
\item
  \(R\): an indicator for whether a participant provides outcome data if assigned to control, given that \(A = 1\) (latent)
\item
  \(Z\): a randomized treatment indicator
\item
  \(Y^0, Y^1\): potential outcomes (latent)
\item
  \(Y^F\): the outcome that would be observed if all participants were to respond (latent)
\item
  \(Y\): the measured outcome, which is only observed for some participants
\item
  \(O\): an indicator for whether the outcome \(Y\) is observed
\end{itemize}

We will posit that these variables are related as follows:

\textbf{TODO / NOTE: Dag code broken -- need to fix}

Now, let's lay out a more specific distributional model:
\[
\begin{aligned}
X &\sim N(0,1) \\
C &\sim Bern(\kappa) - \kappa\\
Z &\sim Bern(0.5) \\
A &\sim Bern\left(\pi_A(C,X)\right) \\
R &\sim Bern\left(\pi_R(C,X)\right)
\end{aligned}
\]
where the response indicators follow the models
\[
\begin{aligned}
\text{logit} \ \pi_A(C,X) &= \alpha_{A0} + \alpha_{A1} C + \alpha_{A2} X \\
\text{logit} \ \pi_R(C,X) &= \alpha_{R0} + \alpha_{R1} C + \alpha_{R2} X.
\end{aligned}
\]
Next, let \(B\) denote \(\text{E}(Y^0 | A, R, C, X)\) and suppose that
\[
B = \beta_0 + \beta_1 C + \beta_2 X + \beta_3 R + \beta_4 C \times R + \beta_5 X \times R.
\]
Let \(D\) denote the the treatment effect surface, \(\text{E}(Y^1 - Y^0 | A, R, C, X)\), and suppose that
\[
D = \delta_0 + \delta_1 C + \delta_2 X + \delta_3 R + \delta_4 C \times R + \delta_5 X \times R.
\]
Note that these models do not differentiate between the never-responders (who have \(A = 0\)) and those who respond only if assigned to treatment (who have \(A = 1, R = 0\)) because the outcome will never be observed for never-responders.

With these functions, we define the potential outcomes as
\[
\begin{aligned}
Y^0 &= B + e_0 \\
Y^1 &= B + D + e_1,
\end{aligned}
\]
where \((e_0, e_1)\) are bivariate normal with means of zero, standard deviations \(\sigma_0, \sigma_1\), and correlation \(\rho\).

The remaining variables in the model are structurally related to those previously defined:
\[
\begin{aligned}
O &= (1 - Z) R A + Z A \\
Y^F &= (1 - Z) Y^0 + Z Y^1 \\
Y &= \begin{cases} Y^F & \text{if} \quad O = 1 \\ 
\cdot & \text{if} \quad O = 0. \end{cases}
\end{aligned}
\]
The observed data include the variables \(X, C, Z, O\), and \(Y\).

\section{The data generating process}\label{the-data-generating-process}

Let's write a function to generate data based on this model. Here's a skeleton to get started:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sim\_attrition\_data }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(}
\NormalTok{    N,                      }\CommentTok{\# sample size}
\NormalTok{    kappa,                  }\CommentTok{\# probability of C}
\NormalTok{    alpha\_A,                }\CommentTok{\# regression for response under treatment}
\NormalTok{    alpha\_R,                }\CommentTok{\# regression for response under control}
\NormalTok{    beta,                   }\CommentTok{\# regression parameters for Y0}
\NormalTok{    delta,                  }\CommentTok{\# regression parameters for treatment response}
    \AttributeTok{sigma0 =} \DecValTok{1}\NormalTok{, }\AttributeTok{sigma1 =} \DecValTok{1}\NormalTok{, }\CommentTok{\# conditional standard deviation of potential outcomes}
\NormalTok{    rho                     }\CommentTok{\# conditional correlation between potential outcomes}
\NormalTok{) \{}
  \CommentTok{\# generate data frame}
  \FunctionTok{return}\NormalTok{(df)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\section{Estimators}\label{estimators}

What estimators might we use with these data?

\section{Performance criteria}\label{performance-criteria-1}

What performance criteria should we look at?

\chapter{Designing the multifactor simulation experiment}\label{exp_design}

So far, we've created code that will give us results for a single combination of parameter values.
In practice, simulation studies typically examine a range of different values, including varying the level of the true parameter values and perhaps also varying sample sizes, to explore a range of different scenarios.
We either want reassurance that our findings are general, or we want to understand what aspects of the context lead to our found results.
A single simulation gives us no hint as to either of these questions.
It is only by looking across a range of settings that we can fully understand trade-offs, general rules, and limits.
Let's now look at the remaining piece of the simulation puzzle: the study's experimental design.

Simulation studies often take the form of \textbf{full factorial} designed experiments. In full factorials, each factor (a particular knob a researcher might turn to change the simulation conditions) is varied across multiple levels, and the design includes \emph{every} possible combination of the levels of every factor. One way to represent such a design is as a list of factors and levels.

For example, for the Cronbach alpha simulation, we might want to vary:

\begin{itemize}
\tightlist
\item
  the sample size, with values of 50 or 100; and
\item
  the number of items, with values of 4 or 8.
\item
  the true value of alpha, with values ranging from 0.1 to 0.9;
\item
  the degrees of freedom of the multivariate \(t\) distribution, with values of 5, 10, 20, or 100;
\end{itemize}

We first express the simulation parameters as a list of factors, each factor having a list of values to explore.
We will then run a simulation for every possible combination of these values.
We call this a \(2 \times 2 \times 9 \times 4\) factorial design, where each element is the number of options for that factor.
Here is code that generates all the scenarios we will run given the above design, storing these combinations in a data frame, \texttt{params}, that represents the full experimental design:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{design\_factors }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}
  \AttributeTok{n =} \FunctionTok{c}\NormalTok{(}\DecValTok{50}\NormalTok{, }\DecValTok{100}\NormalTok{),}
  \AttributeTok{p =} \FunctionTok{c}\NormalTok{(}\DecValTok{4}\NormalTok{, }\DecValTok{8}\NormalTok{),}
  \AttributeTok{alpha =} \FunctionTok{seq}\NormalTok{(}\FloatTok{0.1}\NormalTok{, }\FloatTok{0.9}\NormalTok{, }\FloatTok{0.1}\NormalTok{),}
  \AttributeTok{df =} \FunctionTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{20}\NormalTok{, }\DecValTok{100}\NormalTok{)}
\NormalTok{)}

\NormalTok{params }\OtherTok{\textless{}{-}} \FunctionTok{cross\_df}\NormalTok{(design\_factors)}
\NormalTok{params}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 144 x 4
##        n     p alpha    df
##    <dbl> <dbl> <dbl> <dbl>
##  1    50     4   0.1     5
##  2   100     4   0.1     5
##  3    50     8   0.1     5
##  4   100     8   0.1     5
##  5    50     4   0.2     5
##  6   100     4   0.2     5
##  7    50     8   0.2     5
##  8   100     8   0.2     5
##  9    50     4   0.3     5
## 10   100     4   0.3     5
## # i 134 more rows
\end{verbatim}

See what we get?
The parameters we would pass to \texttt{run.experiment()} correspond to the
columns of our dataset.
We have a total of \(2 \times 2 \times 9 \times 4 = 144\) rows, each row corresponding to a simulation scenario to explore.
With multifactor experiments, it is easy to end up running a lot of experiments!

\section{Choosing parameter combinations}\label{choosing-parameter-combinations}

How do we go about choosing parameter values to examine?
Choosing which parameters to use is a central part of good simulation design because the primary limitation of simulation studies is always their \emph{generalizability}.
On the one hand, it's difficult to extrapolate findings from a simulation study beyond the set of simulation conditions that were examined. On the other hand, it's often difficult or impossible to examine the full space of all possible parameter values, except for very simple problems. Even in the Cronbach alpha simulation, we've got four factors, and the last three could each take an infinite number of different levels, in theory. How can we come up with a defensible set of levels to examine?

The choice of simulation conditions needs to be made in the context of the problem or model that you're studying, so it's a bit difficult to offer valid, decontextualized advice.
We can provide a few observations all the same:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  For research simulations, it often is important to be able to relate your findings to previous research. This suggests that you should select parameter levels to make this possible, such as by looking at sample sizes similar to those examined in previous studies. That said, previous simulation studies are not always perfect (actually, there's a lot of really crummy ones out there!), and so prior work should not geneally be your sole guide or justification.
\item
  Generally, it is better to err on the side of being more comprehensive. You learn more by looking at a broader range of conditions, and you can always boil down your results to a more limited set of conditions for purposes of presentation.
\item
  It is also important to explore breakdown points (e.g., what sample size is too small for a method to work?) rather than focusing only on conditions where a method might be expected to work well. Pushing the boundaries and identifying conditions where estimation methods break will help you to provide better guidance for how the methods should be used in practice.
\end{enumerate}

An important point regarding (2) is that you can be more comprehensive and then have fewer replications per scenario.
For example, say you were planning on doing 1000 simulations per scenario, but then you realize there is some new factor that you don't think matters, but that you believe other researchers will worry about.
You could add in that factor, say with four levels, and then do 250 simulations per scenario.
The total work remains the same.

When analyzing the final simulation you can then first verify you do not see trends along this new factor, and then marganalize out the factor in your summaries of results.
Marginalizing out a factor (i.e., averaging your performance metrics across the additional factor) is a powerful technique of making a claim about how your methods work \emph{on average} across a \emph{range} of scenarios, rather than for a specific scenario.

Overall, you generally want to vary parameters that you believe matter, or that you think other people will believe matter.
The first is so you can learn.
The second is to build your case.

Once you have identified your parameters you then have to decide on the levels of the parameter you will include in the simulation.
There are three strategies you might take:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Vary a parameter over its entire range (or nearly so).
\item
  Choose parameter levels to represent realistic practical range.

  \begin{itemize}
  \tightlist
  \item
    Empirical justification based on systematic reviews of applications
  \item
    Or at least informal impressions of what's realistic in practice
  \end{itemize}
\item
  Choose parameters to emulate one important application.
\end{enumerate}

In the above (1) is the most general---but also the most computationally intensive.
(2) will focus attention, ideally, on what is of practical relevance to a practitioner.
(3) is usually coupled with a subsequent applied data analysis, and in this case the simulation is often used to enrich that analysis.
For example, if the simulation shows the methods work for data with the given form of the target application, people may be more willing to believe the application's findings.

Regardless of how you select your primary parameters, you should also vary nuisance parameters (at least a little) to test sensitivity of results.
While simulations will (generally) never be fully generalizable, you can certainly make them so they avoid the obvious things a critic might identify as an easy dismissal of your findings.

To recap, as you think about your parameter selection, always keep the following design principles and acknowledgements:

\begin{itemize}
\tightlist
\item
  The primary limitation of simulation studies is \textbf{generalizability}.
\item
  Choose conditions that allow you to relate findings to previous work.
\item
  Err towards being comprehensive.

  \begin{itemize}
  \tightlist
  \item
    The goal should be to build an understanding of the major moving parts.
  \item
    Presentation of results can always be tailored to illustrate trends.
  \end{itemize}
\item
  Explore breakdown points (e.g., what sample size is too small for applying a given method?).
\end{itemize}

And fully expect to add and subtract from your set of parameters as you get your initial simulation results! No one ever runs just a single simulation.

\subsection{Choosing parameters for the Clustered RCT}\label{choosing-parameters-for-the-clustered-rct}

Extending our case study presented in Section \citet{case_cluster} to a multifactor simulation, let's think about how to design our full experiment.

So far, we have only investigated a single scenario at a time, although our modular approach does make exploring a range of scenarios by re-calling our simulation function relatively straightforward.
But how do our findings generalize? When are the different methods differently appropriate?
To answer this, we need to extend to a multifactor simulation to \emph{systematically} explore trends across contexts for our three estimators.
We begin by identifying some questions we might have, given our preliminary results.

Regarding bias, in our initial simulation, we noticed that Linear Regression is estimating a person-weighted quantity, and so would be considered biased for the site-average ATE.
We might next ask, how much does bias change if we change the site-size by impact relationship?

For precision, we also saw that Linear Regression has a higher standard error.
But is this a general finding? When does this occur?
Are there contexts where linear regression will do better than the others?
Originally we thought aggregation would lose information becuase little sites will have the same weight as big sites, but be more imprecisely estimated.
Were we wrong? Or perhaps if site size was even more variable, Agg might do worse and worse.

Finally, the estimated SEs all appeared to be good, although they were rather variable, relative to the true SE.
We might then ask, is this always the case? Will the estimated SEs fall apart (e.g., be way too large or way too small, in general) in different contexts?

To answer these questions we need to more systematically explore the space of models. But we have a lot of knobs to turn.
In our simulation, we can generate fake cluster randomized data with the following features:

\begin{itemize}
\item
  The treatment impact of the site can vary, and vary with the site size
\item
  We can have sites of different sizes if we want
\item
  We can also vary:

  \begin{itemize}
  \tightlist
  \item
    the site intercept variance
  \item
    the residual variance,
  \item
    the treatment impact
  \item
    the site size
  \item
    the number of sites, \ldots{}
  \end{itemize}
\end{itemize}

We cannot easily vary all of these.
We instead reflect on our research questions, speculate as to what is likely to matter, and then consider varying the following:

\begin{itemize}
\tightlist
\item
  Average site size: Does the number of students/site matter?
\item
  Number of sites: Do cluster-robust SEs work with fewer sites?
\item
  Variation in site size: Varying site sizes cause bias or break things?
\item
  Correlation of site size and site impact: Will correlation cause bias?
\item
  Cross site variation: Does the amount of site variation matter?
\end{itemize}

When designing the final factors, it is important to ensure those factors are isolated, in that changing one of them is not changing a host of other things that might impact performance.
For example, in our case, if we simply added more cross site variation by directly increasing the random effects for the clusters, our total variation will increase.
If we see that methods deteriorate, we then have a confound: is it the cross site variation causing the problem, or is it the total variation?
We therefore want to vary site variation while controlling total variation; this is why we use the ICC knob discussed in the section on the data generation process.

We might thus end up with the following for our factors:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{crt\_design\_factors }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}
  \AttributeTok{n\_bar =} \FunctionTok{c}\NormalTok{( }\DecValTok{20}\NormalTok{, }\DecValTok{80}\NormalTok{, }\DecValTok{320}\NormalTok{ ),}
  \AttributeTok{J =} \FunctionTok{c}\NormalTok{( }\DecValTok{5}\NormalTok{, }\DecValTok{20}\NormalTok{, }\DecValTok{80}\NormalTok{ ),}
  \AttributeTok{ATE =} \FunctionTok{c}\NormalTok{( }\FloatTok{0.2}\NormalTok{ ),}
  \AttributeTok{size\_coef =} \FunctionTok{c}\NormalTok{( }\DecValTok{0}\NormalTok{, }\FloatTok{0.2}\NormalTok{ ),}
  \AttributeTok{ICC =} \FunctionTok{c}\NormalTok{( }\DecValTok{0}\NormalTok{, }\FloatTok{0.2}\NormalTok{, }\FloatTok{0.4}\NormalTok{, }\FloatTok{0.6}\NormalTok{, }\FloatTok{0.8}\NormalTok{ ),}
  \AttributeTok{alpha =} \FunctionTok{c}\NormalTok{( }\DecValTok{0}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\FloatTok{0.8}\NormalTok{ )}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\section{Using pmap to run multifactor simulations}\label{using-pmap-to-run-multifactor-simulations}

To run simulations across all of our factor combinations, we are going to use a very useful method in the \texttt{purrr} package called \texttt{pmap()}.
\texttt{pmap()} marches down a set of lists, running a function on each \(p\)-tuple of elements, taking the \(i^{th}\) element from each list for iteration \(i\), and passing them as parameters to the specified function.
\texttt{pmap()} then returns the results of this sequence of function calls as a list of results.

Here is a small illustration:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_function }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{( a, b, theta, scale ) \{}
\NormalTok{    scale }\SpecialCharTok{*}\NormalTok{ (a }\SpecialCharTok{+}\NormalTok{ theta}\SpecialCharTok{*}\NormalTok{(b}\SpecialCharTok{{-}}\NormalTok{a))}
\NormalTok{\}}

\NormalTok{args }\OtherTok{=} \FunctionTok{list}\NormalTok{( }\AttributeTok{a =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{, }
             \AttributeTok{b =} \DecValTok{5}\SpecialCharTok{:}\DecValTok{7}\NormalTok{, }
             \AttributeTok{theta =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.2}\NormalTok{, }\FloatTok{0.3}\NormalTok{, }\FloatTok{0.7}\NormalTok{) )}
\NormalTok{purrr}\SpecialCharTok{::}\FunctionTok{pmap\_dbl}\NormalTok{(  args, my\_function, }\AttributeTok{scale =} \DecValTok{10}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 18 32 58
\end{verbatim}

One important note is the variable names for the lists being iterated over must correspond exactly to function arguments of the called function.
Extra parameters can be passed after the function name; these will be held constant, and passed to each function call.
See how \texttt{scale} is the same for all calls.

As we see above, \texttt{pmap()} has variants such as \texttt{\_dbl} or \texttt{\_df} just like the \texttt{map()} and \texttt{map2()} methods.
These variants will automatically stack or convert the list of things returned into a tidier collection (for \texttt{\_dbl} it will convert to a vector of numbers, for \texttt{\_df} it will stack the results to make a large dataframe, assuming each thing returned is a little dataframe).

So far, this is great, but it does not quite look like what we want: our factors are stored as a dataframe, not three lists.
This is where R gets interesting: in R, the columns of a dataframe are stored as a list of vectors or lists (with each of the vectors or lists having the exact same length).
This works beautifully with \texttt{pmap()}.
Witness:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{args[[}\DecValTok{2}\NormalTok{]]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 5 6 7
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{a\_df }\OtherTok{=} \FunctionTok{as.data.frame}\NormalTok{(args)}
\NormalTok{a\_df}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   a b theta
## 1 1 5   0.2
## 2 2 6   0.3
## 3 3 7   0.7
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{a\_df[[}\DecValTok{2}\NormalTok{]]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 5 6 7
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{purrr}\SpecialCharTok{::}\FunctionTok{pmap\_dbl}\NormalTok{( a\_df, my\_function, }\AttributeTok{scale =} \DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 18 32 58
\end{verbatim}

We can pass \texttt{a\_df} to \texttt{pmap}, and \texttt{pmap} takes it as a list of lists, and therefore does exactly what it did before.

All of this means \texttt{pmap()} can run a specified function on each row of a dataset.
Continuing the Cronbach Alpha simulation from above, we would have the following:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{params}\SpecialCharTok{$}\NormalTok{iterations }\OtherTok{\textless{}{-}} \DecValTok{500}
\NormalTok{sim\_results }\OtherTok{\textless{}{-}}\NormalTok{  params }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{res =} \FunctionTok{pmap}\NormalTok{(., run\_alpha\_sim ) )}
\end{Highlighting}
\end{Shaded}

We add a column to \texttt{params} to record the desired 500 replications per condition.
The above code calls our \texttt{run\_alpha\_sim()} method for each row of our list of scenarios we want to explore.
Even better, we are storing the results \textbf{as a new variable in the same dataset}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sim\_results}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 144 x 6
##        n     p alpha    df iterations res  
##    <dbl> <dbl> <dbl> <dbl>      <dbl> <lgl>
##  1    50     4   0.1     5        500 NA   
##  2   100     4   0.1     5        500 NA   
##  3    50     8   0.1     5        500 NA   
##  4   100     8   0.1     5        500 NA   
##  5    50     4   0.2     5        500 NA   
##  6   100     4   0.2     5        500 NA   
##  7    50     8   0.2     5        500 NA   
##  8   100     8   0.2     5        500 NA   
##  9    50     4   0.3     5        500 NA   
## 10   100     4   0.3     5        500 NA   
## # i 134 more rows
\end{verbatim}

We are creating a \textbf{list-column}, where each element in our list column is the little summary of our simulation results for that scenario.
Here is the third scenario, for example:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sim\_results}\SpecialCharTok{$}\NormalTok{res[[}\DecValTok{3}\NormalTok{]]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] NA
\end{verbatim}

We finally use \texttt{unnest()} to expand the \texttt{res} variable, replicating the values of the main variables once for each row in the nested dataset:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyr)}
\NormalTok{sim\_results }\OtherTok{\textless{}{-}} \FunctionTok{unnest}\NormalTok{(sim\_results, }\AttributeTok{cols =}\NormalTok{ res) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{( }\SpecialCharTok{{-}}\NormalTok{iterations )}
\NormalTok{sim\_results}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 144 x 5
##        n     p alpha    df res  
##    <dbl> <dbl> <dbl> <dbl> <lgl>
##  1    50     4   0.1     5 NA   
##  2   100     4   0.1     5 NA   
##  3    50     8   0.1     5 NA   
##  4   100     8   0.1     5 NA   
##  5    50     4   0.2     5 NA   
##  6   100     4   0.2     5 NA   
##  7    50     8   0.2     5 NA   
##  8   100     8   0.2     5 NA   
##  9    50     4   0.3     5 NA   
## 10   100     4   0.3     5 NA   
## # i 134 more rows
\end{verbatim}

We can put all of this together in a a tidy workflow as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sim\_results }\OtherTok{\textless{}{-}} 
\NormalTok{  params }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{res =} \FunctionTok{pmap}\NormalTok{(., }\AttributeTok{.f =}\NormalTok{ run\_alpha\_sim)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{unnest}\NormalTok{(}\AttributeTok{cols =}\NormalTok{ res)}
\end{Highlighting}
\end{Shaded}

If we wanted to use parallel processing (more on this later) we can also simply use the \texttt{simhelpers} package (the following code is auto-generated by the \texttt{create\_skeleton()} method as well):

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plan}\NormalTok{(multisession) }\CommentTok{\# choose an appropriate plan from the future package}
\FunctionTok{evaluate\_by\_row}\NormalTok{(params, run\_alpha\_sim)}
\end{Highlighting}
\end{Shaded}

We finally save our results using tidyverse's \texttt{write\_csv()}; see ``R for Data Science'' textbook, 11.5.
We can ensure we have a directory by making one via \texttt{dir.create()} (see Section \citet{saving_files} for more on files):

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{dir.create}\NormalTok{(}\StringTok{"results"}\NormalTok{, }\AttributeTok{showWarnings =} \ConstantTok{FALSE}\NormalTok{ )}
\FunctionTok{write\_csv}\NormalTok{( sim\_results, }\AttributeTok{file =} \StringTok{"results/cronbach\_results.csv"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\section{Ways of repeating oneself}\label{ways-of-repeating-oneself}

We have three core elements in our simulation:
- Generate data
- Analyze data
- Assess performance

In arranging these elements, we have a choice: do we compute performance measures for each simulation scenario as we go (inside) vs.~computing after we get all of our individual results (outside)?

\emph{INSIDE (aggregate as you simulate):}
In this approach, illustrated above, we, for each scenario defined by a specific combination of factors, run our simulation for that scenario, assess the performance, and then return a nice summary table of how well our methods did.
This is the most straightforward, given what we have done so far: we have a method to run a simulation for a scenario, and we simply run that method for a bunch of scenarios and collate.

After the \texttt{pmap()} call, we end up with a dataframe with all our simulations, one simulation context per row (or maybe bundles, one for each method), with
our measured performance outcomes. This is ideally all we need to analyze.

We have less data to store, and it is easier to compartmentalize.
On the cons side, we have no ability to add new performance measures on the fly.

Overall, this seems pretty good.
That being said, sometimes we might want to use a lot of disk space and keep
much more. In particular, each row of the above corresponds to the summary
of a whole collection of individual runs. We might instead store all of
these runs, which brings us to the other approach.

\emph{OUTSIDE (keep all simulation runs):}
In this approach we do not aggregate, but instead, for each scenario, return the entire set of individual estimates.
The benefit of this is, given the raw estimates, you can dynamically add or change how you calculate performance measures.
You do, however, end up with massive amounts of data to store and manipulate.

To move from inside to outside, we just take the summarizing step out of \texttt{run\_alpha\_sim()}.
E.g.,:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{run\_alpha\_sim\_raw }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(iterations, n, p, alpha, df, }\AttributeTok{coverage =} \FloatTok{0.95}\NormalTok{, }\AttributeTok{seed =} \ConstantTok{NULL}\NormalTok{) \{}
  
  \ControlFlowTok{if}\NormalTok{ (}\SpecialCharTok{!}\FunctionTok{is.null}\NormalTok{(seed)) }\FunctionTok{set.seed}\NormalTok{(seed)}
  
\NormalTok{  results }\OtherTok{\textless{}{-}} 
    \FunctionTok{replicate}\NormalTok{(}\AttributeTok{n =}\NormalTok{ iterations, \{}
\NormalTok{      dat }\OtherTok{\textless{}{-}} \FunctionTok{r\_mvt\_items}\NormalTok{(}\AttributeTok{n =}\NormalTok{ n, }\AttributeTok{p =}\NormalTok{ p, }\AttributeTok{alpha =}\NormalTok{ alpha, }\AttributeTok{df =}\NormalTok{ df)}
      \FunctionTok{estimate\_alpha}\NormalTok{(dat, }\AttributeTok{coverage =}\NormalTok{ coverage)}
\NormalTok{    \}, }\AttributeTok{simplify =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{bind\_rows}\NormalTok{()}
  
\NormalTok{  results}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Each call to \texttt{run\_alpha\_sim\_raw()} now gives one row per simulation trial.
We replicate our simulation parameters for each row.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{run\_alpha\_sim\_raw}\NormalTok{( }\DecValTok{4}\NormalTok{, }\DecValTok{50}\NormalTok{, }\DecValTok{6}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\DecValTok{3}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           A      Var_A       CI_L      CI_U
## 1 0.5402995 0.01014358 0.29374115 0.7007831
## 2 0.3867717 0.01805035 0.05786946 0.6008526
## 3 0.4617342 0.01390704 0.17303773 0.6496454
## 4 0.4810237 0.01292815 0.20267301 0.6622008
\end{verbatim}

The primary advantage of this is we can then generate new performance measures, as they
occur to us, later on. The disadvantage is this result file will be \(R\)
times as many rows as the older file, which can get quite, quite large.

But disk space is cheap!
Here we run the same experiment with our more
complete storage. Note how the \texttt{pmap\_df} stacks the
multiple rows from each run, giving us everything nicely bundled up:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{params}\SpecialCharTok{$}\NormalTok{res }\OtherTok{\textless{}{-}}\NormalTok{ params }\SpecialCharTok{\%\textgreater{}\%} 
    \FunctionTok{pmap}\NormalTok{( run\_alpha\_sim\_raw, }\AttributeTok{iterations =} \DecValTok{500}\NormalTok{ )}
\NormalTok{sim\_results\_full }\OtherTok{\textless{}{-}} \FunctionTok{unnest}\NormalTok{( params,}
                            \AttributeTok{cols =}\NormalTok{ res ) }
\FunctionTok{write\_csv}\NormalTok{( sim\_results\_full, }\StringTok{"results/cronbach\_results\_full.csv"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

We end up with a lot more rows:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{nrow}\NormalTok{( sim\_results\_full )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 72000
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{nrow}\NormalTok{( sim\_results )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 144
\end{verbatim}

Compare the file sizes: one is several k, the other is around 20 megabytes.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{file.size}\NormalTok{(}\StringTok{"results/cronbach\_results.csv"}\NormalTok{) }\SpecialCharTok{/} \DecValTok{1024}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 3.961914
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{file.size}\NormalTok{(}\StringTok{"results/cronbach\_results\_full.csv"}\NormalTok{) }\SpecialCharTok{/} \DecValTok{1024}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 7445.908
\end{verbatim}

\subsection{Getting raw results ready for analysis}\label{getting-raw-results-ready-for-analysis}

If we generated raw results then we need to collapse them by experimental run
before calculating performance measures so we can explore the trends across the
experiments.
We do this by grouping our data and calling \texttt{alpha\_performance()} for each group:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{results }\OtherTok{\textless{}{-}}\NormalTok{ sim\_results\_full }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{nest\_by}\NormalTok{( n, p, alpha, df, }\AttributeTok{.key =} \StringTok{"alpha\_sims"}\NormalTok{ )}
\NormalTok{results}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 144 x 5
## # Rowwise:  n, p, alpha, df
##        n     p alpha    df         alpha_sims
##    <dbl> <dbl> <dbl> <dbl> <list<tibble[,6]>>
##  1    50     4   0.1     5          [500 x 6]
##  2    50     4   0.1    10          [500 x 6]
##  3    50     4   0.1    20          [500 x 6]
##  4    50     4   0.1   100          [500 x 6]
##  5    50     4   0.2     5          [500 x 6]
##  6    50     4   0.2    10          [500 x 6]
##  7    50     4   0.2    20          [500 x 6]
##  8    50     4   0.2   100          [500 x 6]
##  9    50     4   0.3     5          [500 x 6]
## 10    50     4   0.3    10          [500 x 6]
## # i 134 more rows
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{results}\SpecialCharTok{$}\NormalTok{performance }\OtherTok{=} \FunctionTok{map2}\NormalTok{( results}\SpecialCharTok{$}\NormalTok{alpha\_sims, }
\NormalTok{                               results}\SpecialCharTok{$}\NormalTok{alpha, alpha\_performance )}
\NormalTok{results }\OtherTok{\textless{}{-}}\NormalTok{ results }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{( }\SpecialCharTok{{-}}\NormalTok{alpha\_sims ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{unnest}\NormalTok{( }\AttributeTok{cols=}\StringTok{"performance"}\NormalTok{ ) }
\NormalTok{results}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 576 x 7
## # Groups:   n, p, alpha, df [144]
##        n     p alpha    df criterion           est
##    <dbl> <dbl> <dbl> <dbl> <chr>             <dbl>
##  1    50     4   0.1     5 alpha bias      -0.103 
##  2    50     4   0.1     5 alpha RMSE       0.388 
##  3    50     4   0.1     5 V relative bias  0.436 
##  4    50     4   0.1     5 coverage         0.826 
##  5    50     4   0.1    10 alpha bias      -0.0527
##  6    50     4   0.1    10 alpha RMSE       0.263 
##  7    50     4   0.1    10 V relative bias  0.780 
##  8    50     4   0.1    10 coverage         0.908 
##  9    50     4   0.1    20 alpha bias      -0.0190
## 10    50     4   0.1    20 alpha RMSE       0.221 
## # i 566 more rows
## # i 1 more variable: MCSE <dbl>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# }\AlertTok{NOTE}\CommentTok{: This is HORRIBLE.  There has to be a better way.}
\end{Highlighting}
\end{Shaded}

Now, if we want to add a performance metric, we can simply change \texttt{alpha\_performance} and recalculate, without running the time-intensive simulations.
Being able to re-analyze your results is generally a far easier fix than running all the simulations again
after changing the \texttt{run\_alpha\_sim()} method.

The results of summarizing during the simulation vs.~after as we just did
leads to essentially the same place, however, although our old results are in long format (with one row per simulation metric vs.~the metrics being columns).

\section{Running the Cluster RCT multifactor experiment}\label{running-the-cluster-rct-multifactor-experiment}

Running our cluster RCT simulation is the exact same code as we have used before.
Simulations take awhile to run so we save them so we can analyze at our leisure.
Because we are not exactly sure what performance metrics we want, we will save our individual results, and calculate performance metrics on the full data.
I.e., we are storing the individual runs, not the analyzed results!

The code is as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{params }\OtherTok{\textless{}{-}} 
  \FunctionTok{cross\_df}\NormalTok{(crt\_design\_factors) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}
    \AttributeTok{reps =} \DecValTok{100}\NormalTok{,}
    \AttributeTok{seed =} \DecValTok{20200320} \SpecialCharTok{+} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{n}\NormalTok{()}
\NormalTok{  )}
\NormalTok{params}\SpecialCharTok{$}\NormalTok{res }\OtherTok{=} \FunctionTok{pmap}\NormalTok{(params, }\AttributeTok{.f =}\NormalTok{ run\_CRT\_sim )}
\NormalTok{res }\OtherTok{=}\NormalTok{ params }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{unnest}\NormalTok{( }\AttributeTok{cols=}\FunctionTok{c}\NormalTok{(data) )}
\FunctionTok{saveRDS}\NormalTok{( res, }\AttributeTok{file =} \StringTok{"results/simulation\_CRT.rds"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

The seed is for reproducibility; we discuss this more later on.

We then group by our simulation factors and calculate all our performance metrics at once directly.
For example, here is the code for calculating performance measures across our simulation for the cluster randomized experiments example:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{res }\OtherTok{\textless{}{-}} \FunctionTok{readRDS}\NormalTok{( }\AttributeTok{file =} \StringTok{"results/simulation\_CRT.rds"}\NormalTok{ )}

\NormalTok{sres }\OtherTok{\textless{}{-}} 
\NormalTok{  res }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{( n\_bar, J, ATE, size\_coef, ICC, alpha, method ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{( }
    \AttributeTok{bias =} \FunctionTok{mean}\NormalTok{(ATE\_hat }\SpecialCharTok{{-}}\NormalTok{ ATE),}
    \AttributeTok{SE =} \FunctionTok{sd}\NormalTok{( ATE\_hat ),}
    \AttributeTok{RMSE =} \FunctionTok{sqrt}\NormalTok{( }\FunctionTok{mean}\NormalTok{( (ATE\_hat }\SpecialCharTok{{-}}\NormalTok{ ATE )}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{ ) ),}
    \AttributeTok{ESE\_hat =} \FunctionTok{sqrt}\NormalTok{( }\FunctionTok{mean}\NormalTok{( SE\_hat}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{ ) ),}
    \AttributeTok{SD\_SE\_hat =} \FunctionTok{sqrt}\NormalTok{( }\FunctionTok{sd}\NormalTok{( SE\_hat}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{ ) ),}
    \AttributeTok{power =} \FunctionTok{mean}\NormalTok{( p\_value }\SpecialCharTok{\textless{}=} \FloatTok{0.05}\NormalTok{ ),}
    \AttributeTok{R =} \FunctionTok{n}\NormalTok{(),}
    \AttributeTok{.groups =} \StringTok{"drop"}
\NormalTok{  )}
\NormalTok{sres}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 810 x 14
##    n_bar     J   ATE size_coef   ICC alpha method
##    <dbl> <dbl> <dbl>     <dbl> <dbl> <dbl> <chr> 
##  1    20     5   0.2         0   0     0   Agg   
##  2    20     5   0.2         0   0     0   LR    
##  3    20     5   0.2         0   0     0   MLM   
##  4    20     5   0.2         0   0     0.5 Agg   
##  5    20     5   0.2         0   0     0.5 LR    
##  6    20     5   0.2         0   0     0.5 MLM   
##  7    20     5   0.2         0   0     0.8 Agg   
##  8    20     5   0.2         0   0     0.8 LR    
##  9    20     5   0.2         0   0     0.8 MLM   
## 10    20     5   0.2         0   0.2   0   Agg   
## # i 800 more rows
## # i 7 more variables: bias <dbl>, SE <dbl>,
## #   RMSE <dbl>, ESE_hat <dbl>, SD_SE_hat <dbl>,
## #   power <dbl>, R <int>
\end{verbatim}

\subsection{Making analyze\_data() quiet}\label{making-analyze_data-quiet}

If we run our simulation when there is little cluster variation, we start getting a lot of messages and warnings from our MLM estimator.
For example, from a single call we get:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{res }\OtherTok{\textless{}{-}} \FunctionTok{analyze\_data}\NormalTok{(dat)}
\end{Highlighting}
\end{Shaded}

When we scale up to our full simulations, these warnings can become a nuisance.
Furthermore, we have found that the \texttt{lmer} command can sometimes just fail (we believe there is some bug in the optimizer that fails if things are just perfectly wrong).
If this was on simulation run 944 out of 1000, we would lose everything!
To protect ourselves, we trap messages and warnings as so (see Chapter @(\#safe\_code) for more on this):

\begin{verbatim}
quiet_lmer = quietly( lmer )
analyze_data <- function( dat ) {
    
    # MLM
    M1 <- quiet_lmer( Yobs ~ 1 + Z + (1|sid), data=dat )
    message1 = ifelse( length( M1$message ) > 0, 1, 0 )
    warning1 = ifelse( length( M1$warning ) > 0, 1, 0 )

   ...

    # Compile our results
    tibble( 
      method = c( "MLM", "LR", "Agg" ),
      ATE_hat = c( est1, est2, est3 ),
      SE_hat = c( se1, se2, se3 ),
      p_value = c( pv1, pv2, pv3 ),
      message = c( message1, 0, 0 ),
      warning = c( warning1, 0, 0 )
    )
}
\end{verbatim}

We now get a note about the message regarding convergence saved in our results:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{res }\OtherTok{\textless{}{-}} \FunctionTok{analyze\_data}\NormalTok{(dat)}
\NormalTok{res}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 3 x 6
##   method ATE_hat SE_hat p_value message warning
##   <chr>    <dbl>  <dbl>   <dbl>   <dbl>   <dbl>
## 1 MLM    -0.376   1.44    0.842       0       0
## 2 LR     -0.0973  0.784   0.921       0       0
## 3 Agg    -0.440   0.856   0.698       0       0
\end{verbatim}

See? No more warnings, but we see the message as a variable in our results.

\chapter{Analyzing the multifactor experiment}\label{analyzing-the-multifactor-experiment}

Once we have performance measures for all our simulation scenarios, how do we explore them?
For our Cluster RCT simulation, we have 270 different simulation runs across our factors (with three rows per simulation run, one for each method).
How can we visualize and understand trends across this complex domain?

There are several techniques for summarizing across the data that one might use.

\section{Bundling}\label{bundling}

As a first step, we might bundle the simulations by the primary factors of interest.
We would then plot these bundles as box plots to see central tendency along with variation.
With bundling, we would need a good number of simulation runs per scenario, so that the MCSE in the performance measures does not make our boxplots look substantially more variable than the truth.

For example, as a first step to understanding bias, we might bundle our results by ICC.
In this code we are making groups of method by ICC level so we get side-by-side boxplots for each ICC level considered:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{res }\OtherTok{\textless{}{-}} \FunctionTok{readRDS}\NormalTok{( }\StringTok{"results/simulation\_CRT.rds"}\NormalTok{ )}
\FunctionTok{ggplot}\NormalTok{( sres, }\FunctionTok{aes}\NormalTok{( ICC, bias, }\AttributeTok{col=}\NormalTok{method, }\AttributeTok{group=}\FunctionTok{paste0}\NormalTok{(ICC,method) ) ) }\SpecialCharTok{+}
  \FunctionTok{facet\_grid}\NormalTok{( alpha }\SpecialCharTok{\textasciitilde{}}\NormalTok{ size\_coef, }\AttributeTok{labeller =}\NormalTok{ label\_both ) }\SpecialCharTok{+}
  \FunctionTok{geom\_boxplot}\NormalTok{(}\AttributeTok{coef =} \ConstantTok{Inf}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_hline}\NormalTok{( }\AttributeTok{yintercept =} \DecValTok{0}\NormalTok{ ) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{scale\_x\_continuous}\NormalTok{( }\AttributeTok{breaks =} \FunctionTok{unique}\NormalTok{( sres}\SpecialCharTok{$}\NormalTok{ICC) )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{Designing-Simulations-in-R_files/figure-latex/clusterRCT_plot_bias_v1-1} \end{center}

Each box is a collection of simulation trials. E.g., for \texttt{ICC\ =\ 0.6}, \texttt{size\_coef\ =\ 0.2}, and \texttt{alpha\ =\ 0.8} we have 9 scenarios representing the varying level 1 and level 2 sample sizes:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{filter}\NormalTok{( sres, ICC }\SpecialCharTok{==} \FloatTok{0.6}\NormalTok{, size\_coef }\SpecialCharTok{==} \FloatTok{0.2}\NormalTok{,}
\NormalTok{        alpha }\SpecialCharTok{==} \FloatTok{0.8}\NormalTok{, method}\SpecialCharTok{==}\StringTok{"Agg"}\NormalTok{ ) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{( n\_bar}\SpecialCharTok{:}\NormalTok{alpha, bias )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 9 x 7
##   n_bar     J   ATE size_coef   ICC alpha     bias
##   <dbl> <dbl> <dbl>     <dbl> <dbl> <dbl>    <dbl>
## 1    20     5   0.2       0.2   0.6   0.8  0.00799
## 2    20    20   0.2       0.2   0.6   0.8 -0.0445 
## 3    20    80   0.2       0.2   0.6   0.8  0.0206 
## 4    80     5   0.2       0.2   0.6   0.8  0.111  
## 5    80    20   0.2       0.2   0.6   0.8 -0.0144 
## 6    80    80   0.2       0.2   0.6   0.8  0.0132 
## 7   320     5   0.2       0.2   0.6   0.8  0.151  
## 8   320    20   0.2       0.2   0.6   0.8  0.0361 
## 9   320    80   0.2       0.2   0.6   0.8  0.0394
\end{verbatim}

We are seeing a few outliers for some of the boxplots, suggesting that there are other factors driving bias. We could try bundling along different aspects to see:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{( sres, }\FunctionTok{aes}\NormalTok{( }\FunctionTok{as.factor}\NormalTok{(n\_bar), bias, }\AttributeTok{col=}\NormalTok{method, }\AttributeTok{group=}\FunctionTok{paste0}\NormalTok{(n\_bar,method) ) ) }\SpecialCharTok{+}
  \FunctionTok{facet\_grid}\NormalTok{( alpha }\SpecialCharTok{\textasciitilde{}}\NormalTok{  size\_coef, }\AttributeTok{labeller =}\NormalTok{ label\_both ) }\SpecialCharTok{+}
  \FunctionTok{geom\_boxplot}\NormalTok{(}\AttributeTok{coef =} \ConstantTok{Inf}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_hline}\NormalTok{( }\AttributeTok{yintercept =} \DecValTok{0}\NormalTok{ ) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{Designing-Simulations-in-R_files/figure-latex/clusterRCT_plot_bias_v2-1} \end{center}

No progress there. Perhaps it is instability or MCSE.
We make a note to investigate further, later on.

\section{Aggregation}\label{aggregation}

The boxplots are hard for seeing trends.
Instead of bundling, we can therefore aggregate, to look at overall trends rather than individual simulation variation.
This is especially important if the number of replicates within each scenario is small, because then each scenario's performance is measured with a lot of error.

With aggregation, we average over some of the factors, collapsing our simulation results down to fewer moving parts.
This is better than having not had those factors in the first place!
Averaging over a factor is a more general answer than having not varied the factor at all.

For example, if we average across ICC and site variation, and see how the methods change performance as a function of \(J\), we would know that this is a general trend across a range of scenarios defined by different ICC and site variation levels.
Our conclusions would then be more general than if we picked a single ICC and amount of site variation: in this latter case we would not know if we would see our trend more broadly.

Also, with aggregation, we can have a smaller number of replications per factor combination.
The averaging will, in effect, give a lot more reps per aggregated performance measure.

A caution with aggregation is that it can be deceitful if you have scaling issues or extreme outliers.
With bias, our scale is fairly well set, so we are good!
But if we were aggregating standard errors over sample size, then the larger standard errors of the smaller sample size simulations (and the greater variability in estimating those standard errors) would swamp the standard errors of the larger sample sizes.
Usually, with aggregation, we want to average over something we believe will not change massively over the marginalized-out factors.
Alternatively, we can average over a relative measure, which tend to be more invariant and comparable across scenarios.

For our cluster RCT, we might aggregate as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ssres }\OtherTok{\textless{}{-}} 
\NormalTok{  sres }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{( ICC, method, alpha, size\_coef ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{( }\AttributeTok{bias =} \FunctionTok{mean}\NormalTok{( bias ) )}

\FunctionTok{ggplot}\NormalTok{( ssres, }\FunctionTok{aes}\NormalTok{( ICC, bias, }\AttributeTok{col=}\NormalTok{method ) ) }\SpecialCharTok{+}
  \FunctionTok{facet\_grid}\NormalTok{( alpha }\SpecialCharTok{\textasciitilde{}}\NormalTok{  size\_coef, }\AttributeTok{labeller =}\NormalTok{ label\_both ) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{( }\AttributeTok{alpha=}\FloatTok{0.75}\NormalTok{ ) }\SpecialCharTok{+} 
  \FunctionTok{geom\_line}\NormalTok{( }\AttributeTok{alpha=}\FloatTok{0.75}\NormalTok{ ) }\SpecialCharTok{+}
  \FunctionTok{geom\_hline}\NormalTok{( }\AttributeTok{yintercept =} \DecValTok{0}\NormalTok{ ) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{Designing-Simulations-in-R_files/figure-latex/unnamed-chunk-134-1} \end{center}

This shows that site variation leads to greater bias, but only if the coefficient for size is nonzero.
We also see that all the estimators must be the same if site variation is 0, with the overplotted lines on the top row of the figure.

\section{Regression Summarization}\label{regression-summarization}

One can treat the simulation results as a dataset in its own right.
In this case we can regress a performance measure against the methods and various factor levels to get ``main effects'' of how the different levels impact performance holding the other levels constant.
The main effect of the method will tell us if a method is, on average, higher or lower than the baseline method.
The main effect of the factors will tell us if that factor impacts the performance measure.

These regressions can also include interactions between method and factor, to see if some factors impact different methods differently.
They can also include interactions between factors, which allows us to explore how the impact of a factor can matter more or less, depending on other aspects of the context.

For our cluster RCT, we might have, for example:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sres\_f }\OtherTok{=}\NormalTok{ sres }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{( }\FunctionTok{across}\NormalTok{( }\FunctionTok{c}\NormalTok{( n\_bar, J, size\_coef, ICC, alpha ), factor ) )}

\NormalTok{M }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{( bias }\SpecialCharTok{\textasciitilde{}}\NormalTok{ (n\_bar }\SpecialCharTok{+}\NormalTok{ J }\SpecialCharTok{+}\NormalTok{ size\_coef }\SpecialCharTok{+}\NormalTok{ ICC }\SpecialCharTok{+}\NormalTok{ alpha) }\SpecialCharTok{*}\NormalTok{ method, }
         \AttributeTok{data =}\NormalTok{ sres\_f )}
\NormalTok{stargazer}\SpecialCharTok{::}\FunctionTok{stargazer}\NormalTok{(M, }\AttributeTok{type =} \StringTok{"text"}\NormalTok{,}
                     \AttributeTok{single.row =} \ConstantTok{TRUE}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## ==================================================
##                            Dependent variable:    
##                        ---------------------------
##                                   bias            
## --------------------------------------------------
## n_bar80                      -0.009 (0.007)       
## n_bar320                     -0.006 (0.007)       
## J20                           0.004 (0.007)       
## J80                           0.006 (0.007)       
## size_coef0.2                -0.035*** (0.006)     
## ICC0.2                       -0.004 (0.009)       
## ICC0.4                       -0.002 (0.009)       
## ICC0.6                       -0.003 (0.009)       
## ICC0.8                        0.001 (0.009)       
## alpha0.5                    0.055*** (0.007)      
## alpha0.8                    0.053*** (0.007)      
## methodLR                     -0.007 (0.014)       
## methodMLM                     0.006 (0.014)       
## n_bar80:methodLR             -0.001 (0.010)       
## n_bar320:methodLR             0.001 (0.010)       
## n_bar80:methodMLM            -0.001 (0.010)       
## n_bar320:methodMLM           -0.002 (0.010)       
## J20:methodLR                  0.006 (0.010)       
## J80:methodLR                  0.005 (0.010)       
## J20:methodMLM                 0.002 (0.010)       
## J80:methodMLM                 0.002 (0.010)       
## size_coef0.2:methodLR       0.030*** (0.008)      
## size_coef0.2:methodMLM        0.006 (0.008)       
## ICC0.2:methodLR               0.003 (0.013)       
## ICC0.4:methodLR              0.0004 (0.013)       
## ICC0.6:methodLR               0.005 (0.013)       
## ICC0.8:methodLR               0.002 (0.013)       
## ICC0.2:methodMLM             -0.006 (0.013)       
## ICC0.4:methodMLM             -0.008 (0.013)       
## ICC0.6:methodMLM             -0.008 (0.013)       
## ICC0.8:methodMLM             -0.008 (0.013)       
## alpha0.5:methodLR            -0.007 (0.010)       
## alpha0.8:methodLR             0.004 (0.010)       
## alpha0.5:methodMLM           -0.002 (0.010)       
## alpha0.8:methodMLM           -0.0003 (0.010)      
## Constant                    -0.034*** (0.010)     
## --------------------------------------------------
## Observations                       810            
## R2                                0.302           
## Adjusted R2                       0.270           
## Residual Std. Error         0.046 (df = 774)      
## F Statistic              9.547*** (df = 35; 774)  
## ==================================================
## Note:                  *p<0.1; **p<0.05; ***p<0.01
\end{verbatim}

We can quickly get a lot of features, and this approach can be hard to interpret. But picking out the significant coefficents does provide a lot of clues, rather rapidly.
E.g., many features interact with the LR method for bias. The other methods seem less impacted.

\section{Focus on subset, kick rest to supplement}\label{focus-on-subset-kick-rest-to-supplement}

Frequently researchers might simply filter the simulation results to a single factor level for some nuisance parameter.
For example, we might examine ICC of 0.20 only, as this is a ``reasonable'' value given substance matter knowledge.
We would then consider the other levels as a ``sensitivity'' analysis vaguely alluded to in our main report and placed elsewhere, such as an online supplemental appendix.

It would be our job, in this case, to verify that our reported findings on the main results indeed were echoed in our other, set-aside, simulation runs.

\section{Analyzing results when some trials have failed}\label{analyzing-results-when-some-trials-have-failed}

If methods fail, then this is something to investigate in its own right.
Ideally, failure is not too common, so we can drop those trials, or keep them, without really impacting our overall results.
But one should at least know what one is ignoring.

For example, in our cluster RCT, we know we have, at least sometimes, convergence issues.
We know that ICC is an important feature, so we can explore how often we get a convergence message by ICC level:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{res }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{( method, ICC ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{( }\AttributeTok{message =} \FunctionTok{mean}\NormalTok{( message ) ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{pivot\_wider}\NormalTok{( }\AttributeTok{names\_from =} \StringTok{"method"}\NormalTok{, }\AttributeTok{values\_from=}\StringTok{"message"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: There were 15 warnings in `summarise()`.
## The first warning was:
## i In argument: `message = mean(message)`.
## i In group 1: `method = "Agg"` and `ICC = 0`.
## Caused by warning in `mean.default()`:
## ! argument is not numeric or logical: returning NA
## i Run `dplyr::last_dplyr_warnings()` to see the 14 remaining warnings.
\end{verbatim}

\begin{verbatim}
## # A tibble: 5 x 4
##     ICC   Agg    LR   MLM
##   <dbl> <dbl> <dbl> <dbl>
## 1   0      NA    NA    NA
## 2   0.2    NA    NA    NA
## 3   0.4    NA    NA    NA
## 4   0.6    NA    NA    NA
## 5   0.8    NA    NA    NA
\end{verbatim}

We see that when the ICC is 0 we get a lot of convergence issues, but as soon as we pull away from 0 it drops off considerably.
At this point we might decide to drop those runs with a message or keep them.
In this case, we decide to keep.
It shouldn't matter much in any case except the ICC = 0 case, and we know the convergence is due to trying to estimate a 0 variance, and thus is in some sense expected.
Furthermore, we know people using these methods would likely ignore these messages, and thus we are faithfully capturing how these methods would be used in practice.
We might eventually, however, want to do a separate analysis of the ICC = 0 context to see if the MLM approach actually falls apart, or if it is just throwing error messages.

\section{A demonstration of visualization}\label{a-demonstration-of-visualization}

We next explore a case study comparing different visualizations of the same performance metric (in this case, power).
The goal is to see how to examine a metric from several perspectives, and to see how to explore simulation results across scenarios.

For this example, we are going to look at a randomized experiment.
We will generate control potential outcomes, and then add a treatment effect to the treated units.
We assume the control group is normally distributed.
We will generate a random data set, estimate the
treatment effect by taking the difference in means and calculating the
associated standard error, and generating a \(p\)-value using the normal
approximation. (As we will see, this is not a good idea for small sample
size since we should be using a \(t\)-test style approach.)

Violating our usual modular approach, we ae going to have a single function that does an entire step: it generates two groups of the given sizes, one treatment and
one control, and then calculates the difference in means. It will then test
this difference using the normal approximation.

The function also calculates and returns the true effect size of the DGP as the true treatment
effect divided by the control standard deviation (useful for understanding
power, shown later on).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{run.one }\OtherTok{=} \ControlFlowTok{function}\NormalTok{( nC, nT, sdC, tau, }\AttributeTok{mu =} \DecValTok{5}\NormalTok{, }\AttributeTok{sdTau =} \DecValTok{0}\NormalTok{ ) \{}
\NormalTok{  Y0 }\OtherTok{=}\NormalTok{ mu }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{( nC, }\AttributeTok{sd=}\NormalTok{sdC )}
\NormalTok{  Y1 }\OtherTok{=}\NormalTok{ mu }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{( nT, }\AttributeTok{sd=}\NormalTok{sdC ) }\SpecialCharTok{+}\NormalTok{ tau }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{( nT, }\AttributeTok{sd=}\NormalTok{sdTau )}

\NormalTok{  tau.hat }\OtherTok{=} \FunctionTok{mean}\NormalTok{( Y1 ) }\SpecialCharTok{{-}} \FunctionTok{mean}\NormalTok{( Y0 )}
\NormalTok{  SE.hat }\OtherTok{=} \FunctionTok{sqrt}\NormalTok{( }\FunctionTok{var}\NormalTok{( Y0 ) }\SpecialCharTok{/}\NormalTok{ ( nC ) }\SpecialCharTok{+} \FunctionTok{var}\NormalTok{( Y1 ) }\SpecialCharTok{/}\NormalTok{ ( nT ) )}

\NormalTok{  z }\OtherTok{=}\NormalTok{ tau.hat }\SpecialCharTok{/}\NormalTok{ SE.hat}
\NormalTok{  pv }\OtherTok{=} \DecValTok{2} \SpecialCharTok{*}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}} \FunctionTok{pnorm}\NormalTok{( }\FunctionTok{abs}\NormalTok{( z ) ))}

  \FunctionTok{data.frame}\NormalTok{( }\AttributeTok{tau.hat =}\NormalTok{ tau.hat, }\AttributeTok{SE.hat =}\NormalTok{ SE.hat, }\AttributeTok{z=}\NormalTok{z, }\AttributeTok{p.value=}\NormalTok{pv )}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Our function generates a data set, analyzes it, and give us back a variety
of results as a one-row dataframe, as per usual:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{run.one}\NormalTok{( }\AttributeTok{nT=}\DecValTok{5}\NormalTok{, }\AttributeTok{nC=}\DecValTok{10}\NormalTok{, }\AttributeTok{sdC=}\DecValTok{1}\NormalTok{, }\AttributeTok{tau=}\FloatTok{0.5}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     tau.hat    SE.hat          z   p.value
## 1 -0.246767 0.6607213 -0.3734812 0.7087903
\end{verbatim}

In this case, our results are a mix of the parameters and estimated quantities.

We then write a function that runs our single trial multiple times and
summarizes the results:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{run.experiment }\OtherTok{=} \ControlFlowTok{function}\NormalTok{( nC, nT, sdC, tau, }\AttributeTok{mu =} \DecValTok{5}\NormalTok{, }\AttributeTok{sdTau =} \DecValTok{0}\NormalTok{, }\AttributeTok{R =} \DecValTok{500}\NormalTok{ ) \{}

\NormalTok{  eres }\OtherTok{=} \FunctionTok{replicate}\NormalTok{( R, }\FunctionTok{run.one}\NormalTok{( nC, nT, sdC, tau, }\AttributeTok{sdTau=}\NormalTok{sdTau, }\AttributeTok{mu=}\NormalTok{mu ), }
                    \AttributeTok{simplify=}\ConstantTok{FALSE}\NormalTok{ )}
\NormalTok{  eres }\OtherTok{=} \FunctionTok{bind\_rows}\NormalTok{( eres )}
\NormalTok{  eres }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{summarise}\NormalTok{( }\AttributeTok{E.tau.hat =} \FunctionTok{mean}\NormalTok{( tau.hat ),}
                      \AttributeTok{E.SE.hat =} \FunctionTok{mean}\NormalTok{( SE.hat ),}
                      \AttributeTok{power =} \FunctionTok{mean}\NormalTok{( p.value }\SpecialCharTok{\textless{}=} \FloatTok{0.05}\NormalTok{ ) ) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{mutate}\NormalTok{( }\AttributeTok{nC=}\NormalTok{nC, }\AttributeTok{nT=}\NormalTok{nT, }\AttributeTok{sdC=}\NormalTok{sdC, }\AttributeTok{tau=}\NormalTok{tau, }\AttributeTok{mu=}\NormalTok{mu, }\AttributeTok{sdTau=}\NormalTok{sdTau, }\AttributeTok{R=}\NormalTok{R )}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

For performance, we have the average average treatment effect estimate \texttt{E.tau.hat},
the average Standard Error estimate \texttt{E.SE.hat},
and the power \texttt{power} (defined as the percent of time we reject at
alpha=0.05, i.e., the percent of times our \(p\)-value was less than our 0.05
threshold):

Our function also adds in the details of the simulation (the parameters we passed
to the \texttt{run.one()} call). This is an easy way to keep track of things.

We test our function to see what we get:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{run.experiment}\NormalTok{( }\DecValTok{10}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{1}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\DecValTok{5}\NormalTok{, }\FloatTok{0.2}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   E.tau.hat  E.SE.hat power nC nT sdC tau mu
## 1  0.497077 0.6284977 0.218 10  3   1 0.5  5
##   sdTau   R
## 1   0.2 500
\end{verbatim}

We next use the above to run a \emph{multi-factor simulation experiment}. We are
going to vary four factors: control group size, treatment group size,
standard deviation of the units, and the treatment effect.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nC }\OtherTok{=} \FunctionTok{c}\NormalTok{( }\DecValTok{2}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{7}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{50}\NormalTok{, }\DecValTok{500}\NormalTok{ )}
\NormalTok{nT }\OtherTok{=} \FunctionTok{c}\NormalTok{( }\DecValTok{2}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{7}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{50}\NormalTok{, }\DecValTok{500}\NormalTok{ )}
\NormalTok{sdC }\OtherTok{=} \FunctionTok{c}\NormalTok{( }\DecValTok{1}\NormalTok{ )}
\NormalTok{tau }\OtherTok{=} \FunctionTok{c}\NormalTok{( }\DecValTok{0}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\DecValTok{1}\NormalTok{ )}
\NormalTok{sdTau }\OtherTok{=} \FunctionTok{c}\NormalTok{( }\DecValTok{0}\NormalTok{, }\FloatTok{0.5}\NormalTok{ )}

\NormalTok{experiments }\OtherTok{=} \FunctionTok{expand\_grid}\NormalTok{( }\AttributeTok{nC=}\NormalTok{nC, }\AttributeTok{nT=}\NormalTok{nT, }\AttributeTok{sdC=}\NormalTok{sdC, }\AttributeTok{tau=}\NormalTok{tau, }\AttributeTok{sdTau =}\NormalTok{ sdTau )}
\NormalTok{experiments}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 216 x 5
##       nC    nT   sdC   tau sdTau
##    <dbl> <dbl> <dbl> <dbl> <dbl>
##  1     2     2     1   0     0  
##  2     2     2     1   0     0.5
##  3     2     2     1   0.5   0  
##  4     2     2     1   0.5   0.5
##  5     2     2     1   1     0  
##  6     2     2     1   1     0.5
##  7     2     4     1   0     0  
##  8     2     4     1   0     0.5
##  9     2     4     1   0.5   0  
## 10     2     4     1   0.5   0.5
## # i 206 more rows
\end{verbatim}

We next run an experiment for each row of our dataframe of experiment factor
combinations, and save the results.
Note our method of adding the design parametes into our results makes this step arguably moe clean than some of the other templates we have seen.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{exp.res }\OtherTok{\textless{}{-}}\NormalTok{ experiments }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{pmap\_df}\NormalTok{( run.experiment, }\AttributeTok{R=}\DecValTok{2000}\NormalTok{ )}
\FunctionTok{dir.create}\NormalTok{(}\StringTok{"results"}\NormalTok{, }\AttributeTok{showWarnings =} \ConstantTok{FALSE}\NormalTok{ )}
\FunctionTok{saveRDS}\NormalTok{( exp.res, }\AttributeTok{file=}\StringTok{"results/Neyman\_RCT\_results.rds"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

The \texttt{R=500} after \texttt{run.experiment} passes the \emph{same} parameter of \(R=500\) to each
run (we run the same number of trials for each experiment).
We can put it there rather than have it be a column in our list of factors to run.

Here is a peek at our results:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{( exp.res )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      E.tau.hat  E.SE.hat  power nC nT sdC tau mu
## 1 -0.001525155 0.8818917 0.1975  2  2   1 0.0  5
## 2 -0.026286662 0.9307269 0.1915  2  2   1 0.0  5
## 3  0.485430674 0.8861287 0.2185  2  2   1 0.5  5
## 4  0.452375661 0.9363909 0.2095  2  2   1 0.5  5
## 5  1.029038283 0.8915091 0.3120  2  2   1 1.0  5
## 6  0.957434064 0.9347139 0.3025  2  2   1 1.0  5
##   sdTau    R
## 1   0.0 2000
## 2   0.5 2000
## 3   0.0 2000
## 4   0.5 2000
## 5   0.0 2000
## 6   0.5 2000
\end{verbatim}

\subsection{The initial analysis}\label{the-initial-analysis}

We are ready to analyze.

We start with plotting.
Plotting is always a good way to vizualize simulation results.
We first make
our tau into a factor, so \texttt{ggplot} behaves, and then plot all our
experiments as two rows based on one factor (\texttt{sdTau}) with the columns being
another (\texttt{nT}). (This style of plotting a bunch of small plots is called
``many multiples'' and is beloved by Tufte.) Within each plot we have the
x-axis for one factor (\texttt{nC}) and multiple lines for the final factor (\texttt{tau}).
The \(y\)-axis is our outcome of interest, power. We add a 0.05 line to show
when we are rejecting at rates above our nominal \(\alpha\). This plot shows
the relationship of 5 variables.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{exp.res }\OtherTok{=}\NormalTok{ exp.res }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{mutate}\NormalTok{( }\AttributeTok{tau =} \FunctionTok{as.factor}\NormalTok{( tau ) )}
\FunctionTok{ggplot}\NormalTok{( exp.res, }\FunctionTok{aes}\NormalTok{( }\AttributeTok{x=}\NormalTok{nC, }\AttributeTok{y=}\NormalTok{power, }\AttributeTok{group=}\NormalTok{tau, }\AttributeTok{col=}\NormalTok{tau ) ) }\SpecialCharTok{+}
  \FunctionTok{facet\_grid}\NormalTok{( sdTau }\SpecialCharTok{\textasciitilde{}}\NormalTok{ nT, }\AttributeTok{labeller=}\NormalTok{label\_both ) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{scale\_x\_log10}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_hline}\NormalTok{( }\AttributeTok{yintercept=}\FloatTok{0.05}\NormalTok{, }\AttributeTok{col=}\StringTok{"black"}\NormalTok{, }\AttributeTok{lty=}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{Designing-Simulations-in-R_files/figure-latex/unnamed-chunk-142-1} \end{center}

We ae looking at power for different control and treatment group sizes. The tau is our treatment effect, and so for \(\tau = 0\) we are looking at validity (false rejection of the null) and for the other \(\tau\) power (noticing an effect when it is there).
Notice that we are seeing elevated rejection rates under the null for small and even moderate sample sizes!

\subsection{Focusing on validity}\label{focusing-on-validity}

We can zoom in on specific simulations run, to get some more detail
such as estimated power under the null for larger groups. Here we check
and we are seeing rejection rates (\texttt{power}) of around 0.05, which is what we want.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{filter}\NormalTok{( exp.res, tau}\SpecialCharTok{==}\DecValTok{0}\NormalTok{, nT }\SpecialCharTok{\textgreater{}=} \DecValTok{50}\NormalTok{, nC }\SpecialCharTok{\textgreater{}=} \DecValTok{50}\NormalTok{ ) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\AttributeTok{digits=}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{r|r|r|r|r|r|l|r|r|r}
\hline
E.tau.hat & E.SE.hat & power & nC & nT & sdC & tau & mu & sdTau & R\\
\hline
0 & 0.20 & 0.06 & 50 & 50 & 1 & 0 & 5 & 0.0 & 2000\\
\hline
0 & 0.21 & 0.05 & 50 & 50 & 1 & 0 & 5 & 0.5 & 2000\\
\hline
0 & 0.15 & 0.06 & 50 & 500 & 1 & 0 & 5 & 0.0 & 2000\\
\hline
0 & 0.15 & 0.05 & 50 & 500 & 1 & 0 & 5 & 0.5 & 2000\\
\hline
0 & 0.15 & 0.05 & 500 & 50 & 1 & 0 & 5 & 0.0 & 2000\\
\hline
0 & 0.16 & 0.06 & 500 & 50 & 1 & 0 & 5 & 0.5 & 2000\\
\hline
0 & 0.06 & 0.04 & 500 & 500 & 1 & 0 & 5 & 0.0 & 2000\\
\hline
0 & 0.07 & 0.06 & 500 & 500 & 1 & 0 & 5 & 0.5 & 2000\\
\hline
\end{tabular}

We can get fancy and look at rejection rate (power under \texttt{tau\ =\ 0}) as a
function of both nC and nT using an interaction-style plot where we average over the other variables:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{exp.res.rej }\OtherTok{\textless{}{-}}\NormalTok{ exp.res }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{filter}\NormalTok{( tau }\SpecialCharTok{==} \DecValTok{0}\NormalTok{ ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{( nC, nT ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarize}\NormalTok{( }\AttributeTok{power =} \FunctionTok{mean}\NormalTok{( power ) )}

\NormalTok{exp.res.rej }\OtherTok{=} \FunctionTok{mutate}\NormalTok{( exp.res.rej, }\AttributeTok{power =} \FunctionTok{round}\NormalTok{( power }\SpecialCharTok{*} \DecValTok{100}\NormalTok{ ) )}

\FunctionTok{ggplot}\NormalTok{( exp.res.rej, }\FunctionTok{aes}\NormalTok{( }\AttributeTok{x=}\NormalTok{nC, }\AttributeTok{y=}\NormalTok{power, }\AttributeTok{group=}\NormalTok{nT, }\AttributeTok{col=}\FunctionTok{as.factor}\NormalTok{(nT) ) ) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_hline}\NormalTok{( }\AttributeTok{yintercept =} \DecValTok{5}\NormalTok{ ) }\SpecialCharTok{+}
  \FunctionTok{scale\_y\_continuous}\NormalTok{( }\AttributeTok{limits =} \FunctionTok{c}\NormalTok{( }\DecValTok{0}\NormalTok{, }\DecValTok{40}\NormalTok{ ) ) }\SpecialCharTok{+}
  \FunctionTok{scale\_x\_log10}\NormalTok{( }\AttributeTok{breaks =} \FunctionTok{unique}\NormalTok{( exp.res.rej}\SpecialCharTok{$}\NormalTok{nC ) ) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{( }\AttributeTok{x =} \StringTok{"\# C"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Power"}\NormalTok{, }\AttributeTok{colour =} \StringTok{"\# T"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{Designing-Simulations-in-R_files/figure-latex/plot_interaction_Results-1} \end{center}

This plot focuses on the validity of our test.
It shows that we have massively elevated rates when either the number of treated or control units is small (10 or below).
It also shows that as the size of one group increases, if the other is small our rejection rates climb! Note how for 4 control units, the \(n_T = 500\) line is above the others (except for the \(n_T = 2\) line).

\subsection{Looking at main effects}\label{looking-at-main-effects}

We can ignore all the other factors while we look at one specific factor of interest.
This is looking at the \textbf{main effect} or \textbf{marginal effect} of the factor.

The easy way to do this is to let \texttt{ggplot} smooth our individual points on a
plot. Be sure to also plot the individual points to see variation, however.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{( exp.res, }\FunctionTok{aes}\NormalTok{( }\AttributeTok{x=}\NormalTok{nC, }\AttributeTok{y=}\NormalTok{power, }\AttributeTok{group=}\NormalTok{tau, }\AttributeTok{col=}\NormalTok{tau ) ) }\SpecialCharTok{+}
  \FunctionTok{geom\_jitter}\NormalTok{( }\AttributeTok{width=}\FloatTok{0.02}\NormalTok{, }\AttributeTok{height=}\DecValTok{0}\NormalTok{, }\AttributeTok{alpha=}\FloatTok{0.5}\NormalTok{ ) }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{( }\AttributeTok{se =} \ConstantTok{FALSE}\NormalTok{ ) }\SpecialCharTok{+}
  \FunctionTok{scale\_x\_log10}\NormalTok{( }\AttributeTok{breaks=}\NormalTok{nC) }\SpecialCharTok{+}
  \FunctionTok{geom\_hline}\NormalTok{( }\AttributeTok{yintercept=}\FloatTok{0.05}\NormalTok{, }\AttributeTok{col=}\StringTok{"black"}\NormalTok{, }\AttributeTok{lty=}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{Designing-Simulations-in-R_files/figure-latex/plotPool-1} \end{center}

Note how we see our individual runs that we marginalize over as the dots.

To look at our main effects we can also summarize our results, averaging our
experimental runs across other factor levels. For example, in the code below
we average over the different treatment group sizes and standard deviations,
and plot the marginalized results.

To marginalize, we group by the things we want to keep. \texttt{summarise()} then
averages over the things we want to get rid of.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{exp.res.sum }\OtherTok{=}\NormalTok{ exp.res }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{group\_by}\NormalTok{( nC, tau ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{( }\AttributeTok{power =} \FunctionTok{mean}\NormalTok{( power ) )}
\FunctionTok{head}\NormalTok{( exp.res.sum )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 3
## # Groups:   nC [2]
##      nC tau   power
##   <dbl> <fct> <dbl>
## 1     2 0     0.205
## 2     2 0.5   0.254
## 3     2 1     0.395
## 4     4 0     0.114
## 5     4 0.5   0.204
## 6     4 1     0.427
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{( exp.res.sum, }\FunctionTok{aes}\NormalTok{( }\AttributeTok{x=}\NormalTok{nC, }\AttributeTok{y=}\NormalTok{power, }\AttributeTok{group=}\NormalTok{tau, }\AttributeTok{col=}\NormalTok{tau ) ) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{scale\_x\_log10}\NormalTok{( }\AttributeTok{breaks=}\NormalTok{nC) }\SpecialCharTok{+}
  \FunctionTok{geom\_hline}\NormalTok{( }\AttributeTok{yintercept=}\FloatTok{0.05}\NormalTok{, }\AttributeTok{col=}\StringTok{"black"}\NormalTok{, }\AttributeTok{lty=}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{Designing-Simulations-in-R_files/figure-latex/plotCollapse-1} \end{center}

We can try to get clever and look at other aspects of our experimental runs.
The above suggests that the smaller of the two groups is dictating things
going awry, in terms of elevated rejection rates under the null. We can also
look at things in terms of some other more easily interpretable parameter
(here we switch to effect size instead of raw treatment effect).

Given this, we might decide to look at total sample size or the smaller of
the two groups sample size and make plots that way (we are also subsetting to
just the \texttt{sd=1} cases as there is nothing really different about the two
options; we probably should average across but this could reduce clarity of
the presentation of results):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{exp.res }\OtherTok{\textless{}{-}}\NormalTok{ exp.res }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{mutate}\NormalTok{( }\AttributeTok{n =}\NormalTok{ nC }\SpecialCharTok{+}\NormalTok{ nT,}
                               \AttributeTok{n.min =} \FunctionTok{pmin}\NormalTok{( nC, nT ) )}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{( exp.res, }\FunctionTok{aes}\NormalTok{( }\AttributeTok{x=}\NormalTok{n, }\AttributeTok{y=}\NormalTok{power, }\AttributeTok{group=}\NormalTok{tau, }\AttributeTok{col=}\NormalTok{tau ) ) }\SpecialCharTok{+}
  \FunctionTok{geom\_jitter}\NormalTok{( }\AttributeTok{width=}\FloatTok{0.05}\NormalTok{, }\AttributeTok{height=}\DecValTok{0}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{( }\AttributeTok{se =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{span =} \DecValTok{1}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_x\_log10}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_hline}\NormalTok{( }\AttributeTok{yintercept=}\FloatTok{0.05}\NormalTok{, }\AttributeTok{col=}\StringTok{"black"}\NormalTok{, }\AttributeTok{lty=}\DecValTok{2}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{( }\AttributeTok{title =} \StringTok{"Total sample size"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{Designing-Simulations-in-R_files/figure-latex/plotA-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{( exp.res, }\FunctionTok{aes}\NormalTok{( }\AttributeTok{x=}\NormalTok{n.min, }\AttributeTok{y=}\NormalTok{power, }\AttributeTok{group=}\NormalTok{tau, }\AttributeTok{col=}\NormalTok{tau ) ) }\SpecialCharTok{+}
  \FunctionTok{geom\_jitter}\NormalTok{( }\AttributeTok{width=}\FloatTok{0.05}\NormalTok{, }\AttributeTok{height=}\DecValTok{0}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{( }\AttributeTok{se =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{span =} \DecValTok{1}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_x\_log10}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_hline}\NormalTok{( }\AttributeTok{yintercept=}\FloatTok{0.05}\NormalTok{, }\AttributeTok{col=}\StringTok{"black"}\NormalTok{, }\AttributeTok{lty=}\DecValTok{2}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{( }\AttributeTok{title =} \StringTok{"Minimum group size"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{Designing-Simulations-in-R_files/figure-latex/plotB-1} \end{center}

Note the few observations out in the high \texttt{n.min} region for the second
plot---this plot is a bit strange in that the different levels along the
x-axis are assymetric with respect to each other. It is not balanced.

\subsection{Recap}\label{recap}

Overall, this exploration demonstrates the process of looking at a single performance metric (power) and refining a series of plots to get a sense of what the simulation is taking us.
There are many different plots we might choose, and this depends on the messages we are trying to convey.

The key is to explore, and see what you can learn!

\section{Exercises}\label{exercises-5}

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  For our cluster RCT, use the simulation results to assess how much better (or worse) the different methods are to each other in terms of confidence interval coverage. What scenarios tend to result in the worst coverage?
\end{enumerate}

\chapter{Case study: Comparing different estimators}\label{case-study-comparing-different-estimators}

\emph{Features of this case study}
- Calculating performance metrics by estimator using tidyverse.
- Visualization of simulation results.
- Construction of the classic Bias + SE + RMSE performance plot.

In this case study we examine a simulation where we wish to compare different forms of
estimator for estimating the same thing. We still generate data, evaluate it,
and see how well our evaluation works. The difference is we now evaluate it
multiple ways, storing how the different ways work.

For our simple working example we are going to compare estimation of the
center of a symmetric distribution via mean, trimmed mean, and median (so the
mean and median are the same). These are the three estimation strategies
that we might be comparing in a paper (pretend we have ``invented'' the trimmed
mean and want to demonstrate its utility).

We are, as usual, going to break building this simulation evaluation down into lots of
functions to show the general framework. This framework can readily be
extended to more complicated simulation studies.
This case study illustrates how methodologists might compare different strategies for estimation, and is
closest to what we might see in the ``simulation'' section of a stats paper.

\section{The data generating process}\label{the-data-generating-process-1}

For our data-generation function we will use the scaled \(t\)-distribution so
the standard deviation will always be 1 but we will have different fatness of
tails (high chance of outliers):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gen.data }\OtherTok{=} \ControlFlowTok{function}\NormalTok{( n, df0 ) \{}
    \FunctionTok{rt}\NormalTok{( n, }\AttributeTok{df=}\NormalTok{df0 ) }\SpecialCharTok{/} \FunctionTok{sqrt}\NormalTok{( df0 }\SpecialCharTok{/}\NormalTok{ (df0}\DecValTok{{-}2}\NormalTok{) )}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

The variance of a \(t\) is \(df/(df-2)\), so if we divide our observations by the
square root of this, we will standardize them so they have unit variance.
See, the standard deviation is 1 (up to random error, and as long as df0 \textgreater{}
2)!:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sd}\NormalTok{( }\FunctionTok{gen.data}\NormalTok{( }\DecValTok{100000}\NormalTok{, }\AttributeTok{df0 =} \DecValTok{3}\NormalTok{ ) )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1.01542
\end{verbatim}

(Normally our data generation code would be a bit more fancy.)

We next define the parameter we want (in our case this is the mean, is what
we are trying to estimate):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mu }\OtherTok{=} \DecValTok{0}
\end{Highlighting}
\end{Shaded}

\section{The data analysis methods}\label{the-data-analysis-methods}

We then write a function that takes data and uses all our different
estimators on it. We return a data frame of the three estimates, with each
row being one of our estimators. This is useful if our estimators return an
estimate and a standard error, for example.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{analyze.data }\OtherTok{=} \ControlFlowTok{function}\NormalTok{( data ) \{}
\NormalTok{    mn }\OtherTok{=} \FunctionTok{mean}\NormalTok{( data )}
\NormalTok{    md }\OtherTok{=} \FunctionTok{median}\NormalTok{( data )}
\NormalTok{    mn.tr }\OtherTok{=} \FunctionTok{mean}\NormalTok{( data, }\AttributeTok{trim=}\FloatTok{0.1}\NormalTok{ )}
    \FunctionTok{data.frame}\NormalTok{( }\AttributeTok{estimator =} \FunctionTok{c}\NormalTok{( }\StringTok{"mean"}\NormalTok{, }\StringTok{"trim.mean"}\NormalTok{, }\StringTok{"median"}\NormalTok{ ),}
                \AttributeTok{estimate =} \FunctionTok{c}\NormalTok{( mn, mn.tr, md ) )}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Let's test:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dt }\OtherTok{=} \FunctionTok{gen.data}\NormalTok{( }\DecValTok{100}\NormalTok{, }\DecValTok{3}\NormalTok{ )}
\FunctionTok{analyze.data}\NormalTok{( dt )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   estimator     estimate
## 1      mean -0.044891002
## 2 trim.mean  0.005257327
## 3    median  0.004890847
\end{verbatim}

Note that we have bundled our multiple methods into a single function.
With complex methods we generally advocate a separate function for each method, but sometimes for a target simulation having a host of methods wrapped in a single function is clean and tidy code.

Also note the three lines of output for our returned value.
This long-form output will make processing the simulation results easier.
That being said, returning in wide format is also completely legitimate.

\section{The simulation itself}\label{the-simulation-itself}

To evaluate, do a bunch of times, and assess results. Let's start by looking
at a specific case. We generate 1000 datasets of size 10, and estimate the
center using our three different estimators.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{raw.exps }\OtherTok{\textless{}{-}} \FunctionTok{replicate}\NormalTok{( }\DecValTok{1000}\NormalTok{, \{}
\NormalTok{    dt }\OtherTok{=} \FunctionTok{gen.data}\NormalTok{( }\AttributeTok{n=}\DecValTok{10}\NormalTok{, }\AttributeTok{df0=}\DecValTok{5}\NormalTok{ )}
    \FunctionTok{analyze.data}\NormalTok{( dt )}
\NormalTok{\}, }\AttributeTok{simplify =} \ConstantTok{FALSE}\NormalTok{ )}
\NormalTok{raw.exps }\OtherTok{=} \FunctionTok{bind\_rows}\NormalTok{( raw.exps, }\AttributeTok{.id =} \StringTok{"runID"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

Note how our \texttt{.id} argument gives each simulation run an ID. This can be
useful to see how the estimators covary.

We now have 1000 estimates for each of our estimators:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{( raw.exps )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   runID estimator    estimate
## 1     1      mean -0.09919345
## 2     1 trim.mean -0.20887036
## 3     1    median -0.12237738
## 4     2      mean -0.19312165
## 5     2 trim.mean -0.22091715
## 6     2    median -0.18534152
\end{verbatim}

\section{Calculating performance measures for all our estimators}\label{calculating-performance-measures-for-all-our-estimators}

We then want to assess estimator performance for each estimator.
We first write a function to calculate what we want from 1000 estimates:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{estimator.quality }\OtherTok{=} \ControlFlowTok{function}\NormalTok{( estimates, mu ) \{}
\NormalTok{    RMSE }\OtherTok{=} \FunctionTok{sqrt}\NormalTok{( }\FunctionTok{mean}\NormalTok{( (estimates }\SpecialCharTok{{-}}\NormalTok{ mu)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{ ) )}
\NormalTok{    bias }\OtherTok{=} \FunctionTok{mean}\NormalTok{( estimates }\SpecialCharTok{{-}}\NormalTok{ mu )}
\NormalTok{    SE }\OtherTok{=} \FunctionTok{sd}\NormalTok{( estimates )}
    \FunctionTok{data.frame}\NormalTok{( }\AttributeTok{RMSE=}\NormalTok{RMSE, }\AttributeTok{bias=}\NormalTok{bias, }\AttributeTok{SE=}\NormalTok{SE )}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

The key is our function is estimation-method agnostic: we will use it for each of our three estimators.
Here we evaluate our `mean' estimator:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{filter}\NormalTok{( raw.exps, estimator }\SpecialCharTok{==} \StringTok{"mean"}\NormalTok{ ) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{pull}\NormalTok{( estimate ) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{estimator.quality}\NormalTok{( }\AttributeTok{mu =}\NormalTok{ mu )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##        RMSE        bias        SE
## 1 0.3318663 -0.01079814 0.3318566
\end{verbatim}

Aside: Perhaps, code-wise, the above is piping having gone too far? If you don't like this style, you can do
this:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{estimator.quality}\NormalTok{( raw.exps}\SpecialCharTok{$}\NormalTok{estimate[ raw.exps}\SpecialCharTok{$}\NormalTok{estimator}\SpecialCharTok{==}\StringTok{"mean"}\NormalTok{], mu )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##        RMSE        bias        SE
## 1 0.3318663 -0.01079814 0.3318566
\end{verbatim}

To do all our three estimators, we group by estimator and evaluate for each
estimator. In tidyverse 1.0 \texttt{summarise} can handle multiple responses, but they
will look a bit weird in our output, hence the `unpack()' argument which
makes each column its own column (if we do not unpack, we have a ``data frame
column'' which is an odd thing).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{raw.exps }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{group\_by}\NormalTok{( estimator ) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{summarise}\NormalTok{( }\AttributeTok{qual =} \FunctionTok{estimator.quality}\NormalTok{( estimate, }\AttributeTok{mu =} \DecValTok{0}\NormalTok{ ) ) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    tidyr}\SpecialCharTok{::}\FunctionTok{unpack}\NormalTok{( }\AttributeTok{cols=}\FunctionTok{c}\NormalTok{(qual) )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 3 x 4
##   estimator  RMSE     bias    SE
##   <chr>     <dbl>    <dbl> <dbl>
## 1 mean      0.332 -0.0108  0.332
## 2 median    0.331 -0.00855 0.331
## 3 trim.mean 0.311 -0.0105  0.311
\end{verbatim}

We then pack up the above into a function, as usual.
Our function takes our two parameters of sample size and degrees of freedom, and returns a data frame of results.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{run.simulation }\OtherTok{=} \ControlFlowTok{function}\NormalTok{( n, df0 ) \{}
\NormalTok{    raw.exps }\OtherTok{\textless{}{-}} \FunctionTok{replicate}\NormalTok{( }\DecValTok{1000}\NormalTok{, \{}
\NormalTok{        dt }\OtherTok{=} \FunctionTok{gen.data}\NormalTok{( }\AttributeTok{n=}\NormalTok{n, }\AttributeTok{df0=}\NormalTok{df0 )}
        \FunctionTok{analyze.data}\NormalTok{( dt )}
\NormalTok{    \}, }\AttributeTok{simplify =} \ConstantTok{FALSE}\NormalTok{ )}
\NormalTok{    raw.exps }\OtherTok{=} \FunctionTok{bind\_rows}\NormalTok{( raw.exps, }\AttributeTok{.id =} \StringTok{"runID"}\NormalTok{ )}

\NormalTok{    rs }\OtherTok{\textless{}{-}}\NormalTok{ raw.exps }\SpecialCharTok{\%\textgreater{}\%}
        \FunctionTok{group\_by}\NormalTok{( estimator ) }\SpecialCharTok{\%\textgreater{}\%}
        \FunctionTok{summarise}\NormalTok{( }\AttributeTok{qual =} \FunctionTok{estimator.quality}\NormalTok{( estimate, }\AttributeTok{mu =} \DecValTok{0}\NormalTok{ ) ) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{        tidyr}\SpecialCharTok{::}\FunctionTok{unpack}\NormalTok{( }\AttributeTok{cols=}\FunctionTok{c}\NormalTok{( qual ) )}

\NormalTok{    rs}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Our function will take our two parameters, run a simulation, and give us the
results. We see here that none of our estimators are particularly biased and
the trimmed mean has, possibly, the smallest RMSE, although it is a close
call.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{run.simulation}\NormalTok{( }\DecValTok{10}\NormalTok{, }\DecValTok{5}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 3 x 4
##   estimator  RMSE    bias    SE
##   <chr>     <dbl>   <dbl> <dbl>
## 1 mean      0.318 0.0136  0.318
## 2 median    0.315 0.00888 0.315
## 3 trim.mean 0.294 0.0111  0.294
\end{verbatim}

Ok, now we want to see how sample size impacts our different estimators. If
we also vary degrees of freedom we have a \emph{three}-factor experiment, where
one of the factors is our estimator itself. We are going to use a new clever
trick. As before, we use \texttt{pmap()}, but now we store the entire dataframe of
results we get back from our function in a new column of our original
dataframe. See R for DS, Chapter 25.3. This trick works best if we have
everything as a \texttt{tibble} which is basically a dataframe that prints a lot
nicer and doesn't try to second-guess what you are up to all the time.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ns }\OtherTok{=} \FunctionTok{c}\NormalTok{( }\DecValTok{10}\NormalTok{, }\DecValTok{50}\NormalTok{, }\DecValTok{250}\NormalTok{, }\DecValTok{1250}\NormalTok{ )}
\NormalTok{dfs }\OtherTok{=} \FunctionTok{c}\NormalTok{( }\DecValTok{3}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{15}\NormalTok{, }\DecValTok{30}\NormalTok{ )}
\NormalTok{lvls }\OtherTok{=} \FunctionTok{expand\_grid}\NormalTok{( }\AttributeTok{n=}\NormalTok{ns, }\AttributeTok{df=}\NormalTok{dfs )}

\CommentTok{\# So it stores our dataframe results in our lvls data properly.}
\NormalTok{lvls }\OtherTok{=} \FunctionTok{as\_tibble}\NormalTok{(lvls)}

\NormalTok{results }\OtherTok{\textless{}{-}}\NormalTok{ lvls }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{mutate}\NormalTok{( }\AttributeTok{results =} \FunctionTok{pmap}\NormalTok{( lvls, run.simulation ) )}
\end{Highlighting}
\end{Shaded}

We have stored our results (a bunch of dataframes) in our main matrix of
simulation runs.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{print}\NormalTok{( results, }\AttributeTok{n=}\DecValTok{4}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 16 x 3
##       n    df results         
##   <dbl> <dbl> <list>          
## 1    10     3 <tibble [3 x 4]>
## 2    10     5 <tibble [3 x 4]>
## 3    10    15 <tibble [3 x 4]>
## 4    10    30 <tibble [3 x 4]>
## # i 12 more rows
\end{verbatim}

The \texttt{unnest()} function will stack up our dataframes, replicating the other columns in the main dataframe so it makes a nice rectangular dataset, all nice like. See (hard to read) R for DS Chapter 25.4.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{results }\OtherTok{\textless{}{-}} \FunctionTok{unnest}\NormalTok{( results, }\AttributeTok{cols=}\StringTok{"results"}\NormalTok{ )}
\NormalTok{results}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 48 x 6
##        n    df estimator  RMSE     bias    SE
##    <dbl> <dbl> <chr>     <dbl>    <dbl> <dbl>
##  1    10     3 mean      0.309 -0.0119  0.309
##  2    10     3 median    0.243 -0.0102  0.243
##  3    10     3 trim.mean 0.249 -0.0120  0.249
##  4    10     5 mean      0.320  0.00627 0.320
##  5    10     5 median    0.319  0.0121  0.319
##  6    10     5 trim.mean 0.298  0.00692 0.298
##  7    10    15 mean      0.321  0.00444 0.321
##  8    10    15 median    0.361  0.00925 0.361
##  9    10    15 trim.mean 0.323  0.00801 0.323
## 10    10    30 mean      0.309 -0.00229 0.309
## # i 38 more rows
\end{verbatim}

And plot:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{( results, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{n, }\AttributeTok{y=}\NormalTok{RMSE, }\AttributeTok{col=}\NormalTok{estimator) ) }\SpecialCharTok{+}
            \FunctionTok{facet\_wrap}\NormalTok{( }\SpecialCharTok{\textasciitilde{}}\NormalTok{ df, }\AttributeTok{nrow=}\DecValTok{1}\NormalTok{ ) }\SpecialCharTok{+}
            \FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
            \FunctionTok{scale\_x\_log10}\NormalTok{( }\AttributeTok{breaks=}\NormalTok{ns )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{Designing-Simulations-in-R_files/figure-latex/unnamed-chunk-162-1} \end{center}

\section{Improving the visualization of the results}\label{improving-the-visualization-of-the-results}

The above doesn't show differences clearly because all the RMSE goes to zero.
It helps to log our outcome, or otherwise rescale. The logging version shows
differences are relatively constant given changing sample size.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{( results, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{n, }\AttributeTok{y=}\NormalTok{RMSE, }\AttributeTok{col=}\NormalTok{estimator) ) }\SpecialCharTok{+}
    \FunctionTok{facet\_wrap}\NormalTok{( }\SpecialCharTok{\textasciitilde{}}\NormalTok{ df, }\AttributeTok{nrow=}\DecValTok{1}\NormalTok{ ) }\SpecialCharTok{+}
    \FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
    \FunctionTok{scale\_x\_log10}\NormalTok{( }\AttributeTok{breaks=}\NormalTok{ns ) }\SpecialCharTok{+}
    \FunctionTok{scale\_y\_log10}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{Designing-Simulations-in-R_files/figure-latex/unnamed-chunk-163-1} \end{center}

Better is to rescale using our knowledge of standard errors. If we scale by
the square root of sample size, we should get horizontal lines. We now
clearly see the trends.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{results }\OtherTok{\textless{}{-}} \FunctionTok{mutate}\NormalTok{( results, }\AttributeTok{scaleRMSE =}\NormalTok{ RMSE }\SpecialCharTok{*} \FunctionTok{sqrt}\NormalTok{(n) )}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{( results, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{n, }\AttributeTok{y=}\NormalTok{scaleRMSE, }\AttributeTok{col=}\NormalTok{estimator) ) }\SpecialCharTok{+}
    \FunctionTok{facet\_wrap}\NormalTok{( }\SpecialCharTok{\textasciitilde{}}\NormalTok{ df, }\AttributeTok{nrow=}\DecValTok{1}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
    \FunctionTok{scale\_x\_log10}\NormalTok{( }\AttributeTok{breaks=}\NormalTok{ns )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{Designing-Simulations-in-R_files/figure-latex/unnamed-chunk-165-1} \end{center}

Overall, we see the scaled error of the mean it is stable across the
different distributions. The trimmed mean is a real advantage when the
degrees of freedom are small. We are cropping outliers that destabilize our
estimate which leads to great wins. As the distribution grows more normal,
this is no longer an advantage and we get closer to the mean in terms of
performance. Here we are penalized slightly bye having dropped 10\% of our
data, so the standard errors will be slightly larger.

The median is not able to take advantage of the nuances of a data set because
it is entirely determined by the middle value. When outliers cause real
concern, this cost is minimal. When outliers are not a concern, the median is
just worse.

Overall, the trimmed mean seems an excellent choice: in the presence of
outliers it is far more stable than the mean, and when there are no outliers
the cost of using it is small.

In terms of thinking about designing simulation studies, we see clear
visual displays of simulation results can tell very clear stories. Eschew
complicated tables with lots of numbers.

\section{Extension: The Bias-variance tradeoff}\label{extension-the-bias-variance-tradeoff}

We can use the above simulation to examine these same estimators when we the
median is not the same as the mean. Say we want the mean of a distribution,
but have systematic outliers. If we just use the median, or trimmed mean, we
might have bias if the outliers tend to be on one side or another. For
example, consider the exponential distribution:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nums }\OtherTok{=} \FunctionTok{rexp}\NormalTok{( }\DecValTok{100000}\NormalTok{ )}
\FunctionTok{mean}\NormalTok{( nums )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.9990221
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mean}\NormalTok{( nums, }\AttributeTok{trim=}\FloatTok{0.1}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.83185
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{median}\NormalTok{( nums )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.6940772
\end{verbatim}

Our trimming, etc., is \emph{biased} if we think of our goal as estimating the
mean. But if the trimmed estimators are much more stable, we might still
wish to use them. Let's find out.

Let's generate a mixture distribution, just for fun. It will have a nice
normal base with some extreme outliers. We will make sure the overall mean,
including the outliers, is always 1, however. (So our target, \(\mu\) is now 1,
not 0.)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gen.data.outliers }\OtherTok{=} \ControlFlowTok{function}\NormalTok{( n, }\AttributeTok{prob.outlier =} \FloatTok{0.05}\NormalTok{ ) \{}
\NormalTok{    nN }\OtherTok{=} \FunctionTok{rbinom}\NormalTok{( }\DecValTok{1}\NormalTok{, n, prob.outlier )}
\NormalTok{    nrm }\OtherTok{=} \FunctionTok{rnorm}\NormalTok{( n }\SpecialCharTok{{-}}\NormalTok{ nN, }\AttributeTok{mean=}\FloatTok{0.5}\NormalTok{, }\AttributeTok{sd=}\DecValTok{1}\NormalTok{ )}
\NormalTok{    outmean }\OtherTok{=}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ (}\DecValTok{1}\SpecialCharTok{{-}}\NormalTok{prob.outlier)}\SpecialCharTok{/}\DecValTok{2}\NormalTok{) }\SpecialCharTok{/}\NormalTok{ prob.outlier}
\NormalTok{    outs }\OtherTok{=} \FunctionTok{rnorm}\NormalTok{( nN, }\AttributeTok{mean=}\NormalTok{outmean, }\AttributeTok{sd=}\DecValTok{10}\NormalTok{ )}
    \FunctionTok{c}\NormalTok{( nrm, outs )}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Let's look at our distribution

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Y }\OtherTok{=} \FunctionTok{gen.data.outliers}\NormalTok{( }\DecValTok{10000000}\NormalTok{, }\AttributeTok{prob.outlier =} \FloatTok{0.05}\NormalTok{ )}
\FunctionTok{mean}\NormalTok{( Y )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1.000219
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sd}\NormalTok{( Y )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 3.271463
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{hist}\NormalTok{( Y, }\AttributeTok{breaks=}\DecValTok{30}\NormalTok{, }\AttributeTok{col=}\StringTok{"grey"}\NormalTok{, }\AttributeTok{prob=}\ConstantTok{TRUE}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{Designing-Simulations-in-R_files/figure-latex/unnamed-chunk-168-1} \end{center}

We steal the code from above, modifying it slightly for our new function and
changing our target parameter from 0 to 1:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{run.simulation.exp }\OtherTok{=} \ControlFlowTok{function}\NormalTok{( n ) \{}
\NormalTok{    raw.exps }\OtherTok{\textless{}{-}} \FunctionTok{replicate}\NormalTok{( }\DecValTok{1000}\NormalTok{, \{}
\NormalTok{        dt }\OtherTok{=} \FunctionTok{gen.data.outliers}\NormalTok{( }\AttributeTok{n=}\NormalTok{n )}
        \FunctionTok{analyze.data}\NormalTok{( dt )}
\NormalTok{    \}, }\AttributeTok{simplify =} \ConstantTok{FALSE}\NormalTok{ )}
\NormalTok{    raw.exps }\OtherTok{=} \FunctionTok{bind\_rows}\NormalTok{( raw.exps, }\AttributeTok{.id =} \StringTok{"runID"}\NormalTok{ )}

\NormalTok{    rs }\OtherTok{\textless{}{-}}\NormalTok{ raw.exps }\SpecialCharTok{\%\textgreater{}\%}
        \FunctionTok{group\_by}\NormalTok{( estimator ) }\SpecialCharTok{\%\textgreater{}\%}
        \FunctionTok{summarise}\NormalTok{( }\AttributeTok{qual =} \FunctionTok{estimator.quality}\NormalTok{( estimate, }\AttributeTok{mu =} \DecValTok{1}\NormalTok{ ) ) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{        tidyr}\SpecialCharTok{::}\FunctionTok{unpack}\NormalTok{( }\AttributeTok{cols =} \FunctionTok{c}\NormalTok{( qual ) )}

\NormalTok{    rs}
\NormalTok{\}}

\NormalTok{res }\OtherTok{=} \FunctionTok{run.simulation.exp}\NormalTok{( }\DecValTok{100}\NormalTok{ )}
\NormalTok{res}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 3 x 4
##   estimator  RMSE     bias    SE
##   <chr>     <dbl>    <dbl> <dbl>
## 1 mean      0.315  0.00185 0.315
## 2 median    0.467 -0.450   0.127
## 3 trim.mean 0.450 -0.436   0.110
\end{verbatim}

And for our experiment we vary the sample size

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ns }\OtherTok{=} \FunctionTok{c}\NormalTok{( }\DecValTok{10}\NormalTok{, }\DecValTok{20}\NormalTok{, }\DecValTok{40}\NormalTok{, }\DecValTok{80}\NormalTok{, }\DecValTok{160}\NormalTok{, }\DecValTok{320}\NormalTok{ )}
\NormalTok{lvls }\OtherTok{=} \FunctionTok{tibble}\NormalTok{( }\AttributeTok{n=}\NormalTok{ns )}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{results }\OtherTok{\textless{}{-}}\NormalTok{ lvls }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{( }\AttributeTok{results =} \FunctionTok{pmap}\NormalTok{( lvls, run.simulation.exp ) ) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{unnest}\NormalTok{( }\AttributeTok{cols =} \FunctionTok{c}\NormalTok{(results) )}
\FunctionTok{head}\NormalTok{( results )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 5
##       n estimator  RMSE    bias    SE
##   <dbl> <chr>     <dbl>   <dbl> <dbl>
## 1    10 mean      1.00  -0.0667 1.00 
## 2    10 median    0.603 -0.452  0.399
## 3    10 trim.mean 0.641 -0.384  0.513
## 4    20 mean      0.722  0.0192 0.722
## 5    20 median    0.537 -0.453  0.289
## 6    20 trim.mean 0.505 -0.417  0.285
\end{verbatim}

Here we are going to plot multiple outcomes. Often with the simulation study
we are interested in different measures of performance. For us, we want to
know the standard error, bias, and overall error (RMSE). To plot this we
first gather our outcomes to make a long form dataframe of results:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{res2 }\OtherTok{=} \FunctionTok{gather}\NormalTok{( results, RMSE, bias, SE, }\AttributeTok{key=}\StringTok{"Measure"}\NormalTok{,}\AttributeTok{value=}\StringTok{"value"}\NormalTok{ )}
\NormalTok{res2 }\OtherTok{=} \FunctionTok{mutate}\NormalTok{( res2, }\AttributeTok{Measure =} \FunctionTok{factor}\NormalTok{( Measure, }\AttributeTok{levels=}\FunctionTok{c}\NormalTok{(}\StringTok{"SE"}\NormalTok{,}\StringTok{"bias"}\NormalTok{,}\StringTok{"RMSE"}\NormalTok{ )))}
\end{Highlighting}
\end{Shaded}

And then we plot, making a facet for each outcome of interest:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{( res2, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{n, }\AttributeTok{y=}\NormalTok{value, }\AttributeTok{col=}\NormalTok{estimator) ) }\SpecialCharTok{+}
    \FunctionTok{facet\_grid}\NormalTok{( . }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Measure ) }\SpecialCharTok{+}
    \FunctionTok{geom\_hline}\NormalTok{( }\AttributeTok{yintercept=}\DecValTok{0}\NormalTok{, }\AttributeTok{col=}\StringTok{"darkgrey"}\NormalTok{ ) }\SpecialCharTok{+}
    \FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
    \FunctionTok{scale\_x\_log10}\NormalTok{( }\AttributeTok{breaks=}\NormalTok{ns ) }\SpecialCharTok{+}
    \FunctionTok{labs}\NormalTok{( }\AttributeTok{y=}\StringTok{""}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{Designing-Simulations-in-R_files/figure-latex/unnamed-chunk-172-1} \end{center}

We see how different estimators have different biases and different
uncertainties. The bias is negative for our trimmed estimators because we
are losing the big outliers above and so getting answers that are too low.

The RMSE captures the trade-off in terms of what estimator gives the lowest
overall \emph{error}. For this distribution, the mean wins as the sample size
increases because the bias basically stays the same and the SE drops. But
for smaller samples the trimming is superior. The median (essentially
trimming 50\% above and below) is overkill and has too much negative bias.

From a simulation study point of view, notice how we are looking at three
different qualities of our estimators. Some people really care about bias,
some care about RMSE. By presenting all results we are transparent about how
the different estimators operate.

Next steps would be to also examine the associated estimated standard errors
for the estimators, seeing if these estimates of estimator uncertainty are
good or poor. This leads to investigation of coverage rates and similar.

\chapter{Presentation of simulation results}\label{presentation-of-simulation-results}

Last chapter, we started to investigate how to present a multifactor experiment.
In this chapter, we talk about some principles behind the choices one might make in generating final reports of a simulation.
There are three primary approaches to the analysis and presentation of simulation results:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Tabulation
\item
  Visualization
\item
  Modeling
\end{enumerate}

There are generally two primary goals for your results:

\begin{itemize}
\tightlist
\item
  Understand the effects of all of the factors manipulated in the simulation.
\item
  Develop evidence that addresses your research questions.
\end{itemize}

For your final write-up, you will not want to present everything.
A wall of numbers and observations will serve to pummel the reader, rather than inform them; readers rarely enjoy being pummeled, and the solution is quite often to skim such material while feeling hurt and betayed.
Instead, you should present selected results that clearly illustrate the main findings from the study and anything unusual/anomolous.
This will typically be with a few well-chosen figues.
Then, in the text of your write-up, you might include examples that make specific numerical comparisons.
Do not include too many of theses, and be sure to say why the numerical comparisons you include are important.
Finally, have supplementary materials that contain further detail such as additional figures and analysis, and complete simulation results.

If you want to be a moral person worthy of the awards of Heaven, you should also provide reproducible code so others could, if so desired, rerun the simulation and conduct the analysis themselves.
This last part provides a great legitimacy bump to your work: even if no one touches your code, knowing that they could builds confidence.
People naturally think, ``if that researcher is so willing to let me see what they actually did, then they must be fairly confident it does not contain too many horrendous mistakes and it is probably right.''

We briefly walk through the three modes of engaging with one's simulation results, with a few examples taken from the literature.

\section{Tabulation}\label{tabulation}

Traditionally, simulation study results are presented in big tables.
We think this doesn't really make the take-aways of a simulation readily apparent.
Perhaps tables are fine if\ldots{}
- they involve only a few numbers, and a few targeted comparisons
- it is important to report \emph{exact} values for some quantities

Unfortunately, simulations usually produce lots of numbers, and involve making lots of comparisons.
You are going to want to show, for example, the relative performance of alternative estimators, or the performance of your estimators under different conditions for the data-generating model.
This means a lot of rows, and a lot of dimensions.
Tables can do two dimensions; when you try to cram more than that into a table, no one is particularly well served.

Furthermore, in simulation, exact values for your bias/RMSE/type-I error, or whatever, are not usually of interest. And in fact, we rarely have them due to Monte Carlo simulation error.
The tables provide a false sense of security, unless you include uncertainty, which clutters your table even further.

Tables and simulations do not particularly well mix.
In particular, if you are ever tempted into putting your table in landscape mode to get it to fit on the page, think again.
It is often more useful and insightful to present results in graphs (Gelman, Pasarica, \& Dodhia, 2002).

So, onwards.

\section{Visualization}\label{visualization}

Visualization should nearly always be the first step in analyzing simulation results.

This often requires creating a \emph{BUNCH} of graphs to look at different aspects of the data.

Helpful tools/concepts:

\begin{itemize}
\tightlist
\item
  Boxplots are often useful for depicting range and central tendency across many combinations of parameter values.
\item
  Use color, shape, and line type to encode different factors
\item
  Small multiples (faceting) can then encode further factors (e.g., varying sample size)
\end{itemize}

We next present a series of visualizations taken from our published work, illustrating some different themes behind visualization that we believe are important.

\subsection{Example 1: Biserial correlation estimation}\label{example-1-biserial-correlation-estimation}

Our first example shows the bias of a biserial correlation estimate from an extreme groups design.
This simulation was a \(96 \times 2 \times 5 \times 5\) factorial design (true correlation for a range of values, cut-off type, cut-off percentile, and sample size).
The correlation, with 96 levels, forms the \(x\)-axis, giving us nice performance curves.
We use line type for the sample size, allowing us to easily see how bias collapses as sample size increases.
Finally, the facet grid gives our final factors of cut-off type and cut-off percentile.
All our factors, and nearly 5000 explored simulation scenarios, are visible in a single plot.

\begin{verbatim}
## `geom_smooth()` using formula = 'y ~ x'
\end{verbatim}

\begin{center}\includegraphics[width=0.75\linewidth]{Designing-Simulations-in-R_files/figure-latex/unnamed-chunk-174-1} \end{center}

Source: Pustejovsky, J. E. (2014). Converting from d to r to z when the design uses extreme groups, dichotomization, or experimental control. Psychological Methods, 19(1), 92-112.

Note that in our figure, we have smoothed the lines with respect to \texttt{rho} using \texttt{geom\_smooth()}.
This is a nice tool for taking some of the simulation jitter out of an analysis to show overall trends more directly.

\subsection{Example 2: Variance estimation and Meta-regression}\label{example-2-variance-estimation-and-meta-regression}

\begin{itemize}
\tightlist
\item
  Type-I error rates of small-sample corrected F-tests based on cluster-robust variance estimation in meta-regression
\item
  Comparison of 5 different small-sample corrections
\item
  Complex experimental design, varying

  \begin{itemize}
  \tightlist
  \item
    sample size (\(m\))
  \item
    dimension of hypothesis (\(q\))
  \item
    covariates tested
  \item
    degree of model mis-specification
  \end{itemize}
\end{itemize}

\begin{center}\includegraphics[width=0.75\linewidth]{Designing-Simulations-in-R_files/figure-latex/unnamed-chunk-175-1} \end{center}

Source: Tipton, E., \& Pustejovsky, J. E. (2015). Small-sample adjustments for tests of moderators and model fit using robust variance estimation in meta-regression. \emph{Journal of Educational and Behavioral Statistics, 40}(6), 604-634.

\subsection{Example: Heat maps of coverage}\label{example-heat-maps-of-coverage}

The visualization below shows the coverage of parametric bootstrap confidence intervals for momentary time sampling data
In this simulation study the authors were comparing maximum likelihood estimators to posterior mode (penalized likelihood) estimators of prevalence.
We have a 2-dimensional parameter space of prevalence (19 levels) by incidence (10 levels).
We also have 15 levels of sample size.

One option here is to use a heat map, showing the combinations of prevelance and incidence as a grid for each sample size level.
We break coverage into ranges of interest, with green being ``good'' (near 95\%) and yellow being ``close'' (92.5\% or above).
For this to work, we need our MCSE to be small enough that our coverage is estimated precisely enough to show structure.

\begin{center}\includegraphics[width=0.75\linewidth]{Designing-Simulations-in-R_files/figure-latex/swan_example_setup-1} \end{center}

\begin{center}\includegraphics[width=0.75\linewidth]{Designing-Simulations-in-R_files/figure-latex/unnamed-chunk-176-1} \end{center}

To see this plot IRL, see Pustejovsky, J. E., \& Swan, D. M. (2015). Four methods for analyzing partial interval recording data, with application to single-case research. \emph{Multivariate Behavioral Research, 50}(3), 365-380.

\section{Modeling}\label{modeling}

Simulations are designed experiments, often with a full factorial structure.
We can therefore leverage classic means for analyzing such full factorial experiment.
In particular, we in effect model how a performance measure varies as a function of the different experimental factors.
We can use regression or other modeling to do this.

First, in the language of a full factor experiment, we might be interested in the ``main effects'' or ``interaction effects.''
A main effect is whether, averaging across the other factors in our experiment, a factor of interest systematically impacts our peformance measure.
When we look at a main effect, the other factors help ensure our main effect is generalizable: if we see a trend when we average over the other varying aspects, then we can state that for a host of simulation contexts, grouped by levels of our main effect, we see a trend.

For example, consider the Bias of biserial correlation estimate from an extreme groups design example from above.
Visually, we see that most factors appear to matter for bias, but we might want to get a sense of how much.
In particular, does the the population vs sample cutoff option matter, on average, for bias?

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{options}\NormalTok{(}\AttributeTok{scipen =} \DecValTok{5}\NormalTok{)}
\NormalTok{mod }\OtherTok{=} \FunctionTok{lm}\NormalTok{( bias }\SpecialCharTok{\textasciitilde{}}\NormalTok{ fixed }\SpecialCharTok{+}\NormalTok{ rho }\SpecialCharTok{+} \FunctionTok{I}\NormalTok{(rho}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) }\SpecialCharTok{+}\NormalTok{ p1 }\SpecialCharTok{+}\NormalTok{ n, }\AttributeTok{data =}\NormalTok{ r\_F)}
\FunctionTok{summary}\NormalTok{(mod, }\AttributeTok{digits=}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = bias ~ fixed + rho + I(rho^2) + p1 + n, data = r_F)
## 
## Residuals:
##        Min         1Q     Median         3Q 
## -0.0215935 -0.0013608  0.0003823  0.0015677 
##        Max 
##  0.0081802 
## 
## Coefficients:
##                       Estimate  Std. Error
## (Intercept)         0.00218473  0.00015107
## fixedSample cutoff -0.00363520  0.00009733
## rho                -0.00942338  0.00069578
## I(rho^2)            0.00720857  0.00070868
## p1.L                0.00461700  0.00010882
## p1.Q               -0.00160546  0.00010882
## p1.C                0.00081464  0.00010882
## p1^4               -0.00011190  0.00010882
## n.L                 0.00362949  0.00010882
## n.Q                -0.00103981  0.00010882
## n.C                 0.00027941  0.00010882
## n^4                 0.00001976  0.00010882
##                    t value Pr(>|t|)    
## (Intercept)         14.462  < 2e-16 ***
## fixedSample cutoff -37.347  < 2e-16 ***
## rho                -13.544  < 2e-16 ***
## I(rho^2)            10.172  < 2e-16 ***
## p1.L                42.426  < 2e-16 ***
## p1.Q               -14.753  < 2e-16 ***
## p1.C                 7.486 8.41e-14 ***
## p1^4                -1.028   0.3039    
## n.L                 33.352  < 2e-16 ***
## n.Q                 -9.555  < 2e-16 ***
## n.C                  2.568   0.0103 *  
## n^4                  0.182   0.8559    
## ---
## Signif. codes:  
## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.003372 on 4788 degrees of freedom
## Multiple R-squared:  0.5107, Adjusted R-squared:  0.5096 
## F-statistic: 454.4 on 11 and 4788 DF,  p-value: < 2.2e-16
\end{verbatim}

The above printout gives main effects for each factor, averaged across other factors.
It is automatically generating linear, quadradic, cubic and fourth order contrasts for the ordered factors of p1 and n.
We see that, across other contexts, the sample cutoff is around 0.004 lower than population.

We next discuss two additional tools:

\begin{quote}
\begin{itemize}
\tightlist
\item
  ANOVA can be useful for understanding major sources of variation in simulation results (e.g., identifying which factors have negligible/minor influence on the bias of an estimator).
\item
  Smoothing (e.g., local linear regression) over continuous factors
\end{itemize}
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{anova\_table }\OtherTok{\textless{}{-}} \FunctionTok{aov}\NormalTok{(bias }\SpecialCharTok{\textasciitilde{}}\NormalTok{ rho }\SpecialCharTok{*}\NormalTok{ p1 }\SpecialCharTok{*}\NormalTok{ fixed }\SpecialCharTok{*}\NormalTok{ n, }\AttributeTok{data =}\NormalTok{ r\_F)}
\FunctionTok{summary}\NormalTok{(anova\_table)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                  Df   Sum Sq  Mean Sq  F value
## rho               1 0.002444 0.002444  1673.25
## p1                4 0.023588 0.005897  4036.41
## fixed             1 0.015858 0.015858 10854.52
## n                 4 0.013760 0.003440  2354.60
## rho:p1            4 0.001722 0.000431   294.71
## rho:fixed         1 0.003440 0.003440  2354.69
## p1:fixed          4 0.001683 0.000421   287.98
## rho:n             4 0.002000 0.000500   342.31
## p1:n             16 0.019810 0.001238   847.51
## fixed:n           4 0.013359 0.003340  2285.97
## rho:p1:fixed      4 0.000473 0.000118    80.87
## rho:p1:n         16 0.001470 0.000092    62.91
## rho:fixed:n       4 0.002929 0.000732   501.23
## p1:fixed:n       16 0.001429 0.000089    61.12
## rho:p1:fixed:n   16 0.000429 0.000027    18.36
## Residuals      4700 0.006866 0.000001         
##                Pr(>F)    
## rho            <2e-16 ***
## p1             <2e-16 ***
## fixed          <2e-16 ***
## n              <2e-16 ***
## rho:p1         <2e-16 ***
## rho:fixed      <2e-16 ***
## p1:fixed       <2e-16 ***
## rho:n          <2e-16 ***
## p1:n           <2e-16 ***
## fixed:n        <2e-16 ***
## rho:p1:fixed   <2e-16 ***
## rho:p1:n       <2e-16 ***
## rho:fixed:n    <2e-16 ***
## p1:fixed:n     <2e-16 ***
## rho:p1:fixed:n <2e-16 ***
## Residuals                
## ---
## Signif. codes:  
## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(lsr)}
\FunctionTok{etaSquared}\NormalTok{(anova\_table)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                     eta.sq eta.sq.part
## rho            0.021971037  0.26254289
## p1             0.212004203  0.77453319
## fixed          0.142527898  0.69783705
## n              0.123670355  0.66710072
## rho:p1         0.015479114  0.20052330
## rho:fixed      0.030918819  0.33377652
## p1:fixed       0.015125570  0.19684488
## rho:n          0.017979185  0.22560369
## p1:n           0.178055588  0.74260975
## fixed:n        0.120065971  0.66049991
## rho:p1:fixed   0.004247472  0.06439275
## rho:p1:n       0.013216569  0.17638308
## rho:fixed:n    0.026326074  0.29902214
## p1:fixed:n     0.012839790  0.17222072
## rho:p1:fixed:n 0.003857877  0.05883389
\end{verbatim}

\cmntM{Perhaps we need an example where some things don't matter?  We need to discuss what one learns from this table.}

\chapter{Ensuring reproducibility}\label{sec_reproducability}

In the prior section we built a simulation driver.
Because this function involves generating random numbers, re-running it with the exact same input parameters will still produce different results:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{run\_alpha\_sim}\NormalTok{(}\AttributeTok{iterations =} \DecValTok{10}\NormalTok{, }\AttributeTok{n =} \DecValTok{50}\NormalTok{, }\AttributeTok{p =} \DecValTok{6}\NormalTok{, }\AttributeTok{alpha =} \FloatTok{0.73}\NormalTok{, }\AttributeTok{df =} \DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##         criterion         est        MCSE
## 1      alpha bias -0.02181369 0.025173513
## 2      alpha RMSE  0.07860782 0.005927365
## 3 V relative bias  0.68820771 0.142023637
## 4        coverage  0.80000000 0.068920244
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{run\_alpha\_sim}\NormalTok{(}\AttributeTok{iterations =} \DecValTok{10}\NormalTok{, }\AttributeTok{n =} \DecValTok{50}\NormalTok{, }\AttributeTok{p =} \DecValTok{6}\NormalTok{, }\AttributeTok{alpha =} \FloatTok{0.73}\NormalTok{, }\AttributeTok{df =} \DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##         criterion         est       MCSE
## 1      alpha bias -0.07737343 0.03992368
## 2      alpha RMSE  0.14258945 0.01028178
## 3 V relative bias  0.40659026 0.12055579
## 4        coverage  0.50000000 0.06892024
\end{verbatim}

Of course, using a larger number of iterations will give us more precise estimates of the performance criteria. If we want to get the \emph{exact} same results, however, we have to control the random process.

This is more possible than it sounds: Monte Carlo simulations are random, but computers are not.
When we generate ``random numbers'' they actually come from a chain of mathematical equations that, given a number, will generate the next number in a deterministic sequence.
Given that number, it will generate the next, and so on.
The numbers we get back are a part of this chain of (very large) numbers that, ideally, cycles through an extremely long list of numbers in a haphazard and random looking fashion.

This is what the \texttt{seed} argument that we have glossed over before is all about.
If we set the same seed, we get the same results:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{run\_alpha\_sim}\NormalTok{(}\AttributeTok{iterations =} \DecValTok{10}\NormalTok{, }\AttributeTok{n =} \DecValTok{50}\NormalTok{, }\AttributeTok{p =} \DecValTok{6}\NormalTok{, }\AttributeTok{alpha =} \FloatTok{0.73}\NormalTok{, }\AttributeTok{df =} \DecValTok{5}\NormalTok{,}
              \AttributeTok{seed =} \DecValTok{6}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##         criterion         est       MCSE
## 1      alpha bias -0.02053560 0.02585963
## 2      alpha RMSE  0.08025083 0.01344827
## 3 V relative bias  0.64909209 1.43035647
## 4        coverage  0.90000000 0.06892024
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{run\_alpha\_sim}\NormalTok{(}\AttributeTok{iterations =} \DecValTok{10}\NormalTok{, }\AttributeTok{n =} \DecValTok{50}\NormalTok{, }\AttributeTok{p =} \DecValTok{6}\NormalTok{, }\AttributeTok{alpha =} \FloatTok{0.73}\NormalTok{, }\AttributeTok{df =} \DecValTok{5}\NormalTok{, }
              \AttributeTok{seed =} \DecValTok{6}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##         criterion         est       MCSE
## 1      alpha bias -0.02053560 0.02585963
## 2      alpha RMSE  0.08025083 0.01344827
## 3 V relative bias  0.64909209 1.43035647
## 4        coverage  0.90000000 0.06892024
\end{verbatim}

This is useful because it ensure the full reproducibility of the results. In practice, it is a good idea to always set seed values for your simulations, so that you (or someone else!) can exactly reproduce the results.
Let's look more at how this works.

\section{Seeds and pseudo-random number generators}\label{seeds-and-pseudo-random-number-generators}

In R, we can start a sequence of \textbf{deterministic} but \textbf{random-seeming} numbers by setting a ``seed''.
This means all the random numbers that come after it will be the same.

Compare this:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{rchisq}\NormalTok{(}\DecValTok{3}\NormalTok{, }\AttributeTok{df =} \DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1.193539 5.338936 6.294274
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{rchisq}\NormalTok{(}\DecValTok{3}\NormalTok{, }\AttributeTok{df =} \DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.7189536 6.3697718 8.2913988
\end{verbatim}

To this:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{20210527}\NormalTok{)}
\FunctionTok{rchisq}\NormalTok{(}\DecValTok{3}\NormalTok{, }\AttributeTok{df =} \DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1.643698 6.229613 3.249164
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{20210527}\NormalTok{)}
\FunctionTok{rchisq}\NormalTok{(}\DecValTok{3}\NormalTok{, }\AttributeTok{df =} \DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1.643698 6.229613 3.249164
\end{verbatim}

By setting the seed the second time we reset our sequence of random numbers.
Similarly, to ensure reproducibility in our simulation, we will add an option to set the seed value of the random number generator.

This seed is low-level, meaning if we are generating numbers via \texttt{rnorm} or \texttt{rexp} or \texttt{rchisq}, it doesn't matter.
Each time we ask for a random number from the low-level pseudo-random number generator, it gives us the next number back.
These other functions, like \texttt{rnorm()}, etc., all call this low-level generator and than transform the number to be of the correct distribution.

\section{Including seed in our simulation driver}\label{including-seed-in-our-simulation-driver}

The easy way to ensure reproducability is to pass a seed as a parameter.
If we leave it NULL, we ignore it and just continue generating random numbers from wherever we are in the system.
If we specify a seed, however, we set it at the beginning of the scenario, and then all the numbers that follow will be in sequence.
Our code will typically look like this:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{run\_alpha\_sim }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(iterations, n, p, alpha, df, }\AttributeTok{coverage =} \FloatTok{0.95}\NormalTok{, }\AttributeTok{seed =} \ConstantTok{NULL}\NormalTok{) \{}
  
  \ControlFlowTok{if}\NormalTok{ (}\SpecialCharTok{!}\FunctionTok{is.null}\NormalTok{(seed)) }\FunctionTok{set.seed}\NormalTok{(seed)}

\NormalTok{  results }\OtherTok{\textless{}{-}} 
    \FunctionTok{replicate}\NormalTok{(}\AttributeTok{n =}\NormalTok{ iterations, \{}
\NormalTok{      dat }\OtherTok{\textless{}{-}} \FunctionTok{r\_mvt\_items}\NormalTok{(}\AttributeTok{n =}\NormalTok{ n, }\AttributeTok{p =}\NormalTok{ p, }\AttributeTok{alpha =}\NormalTok{ alpha, }\AttributeTok{df =}\NormalTok{ df)}
      \FunctionTok{estimate\_alpha}\NormalTok{(dat, }\AttributeTok{coverage =}\NormalTok{ coverage)}
\NormalTok{    \}, }\AttributeTok{simplify =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{bind\_rows}\NormalTok{()}
  
  \FunctionTok{alpha\_performance}\NormalTok{(results, }\AttributeTok{alpha =}\NormalTok{ alpha, }\AttributeTok{coverage =}\NormalTok{ coverage)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Using our seed, we get identical results, as we saw in the intro, above.

\section{Reasons for setting the seed}\label{reasons-for-setting-the-seed}

Reproducibility allows us to easily check if we are running the same code that generate the results in some report.
It also helps with debugging.
For example, say we had an error that showed up one in a thousand, causing our simulation to crash sometimes.

If we set a seed, and see that it crashes, we can then go try and catch the error and repair our code, and then rerun the simulation.
If it runs clean, we know we got the error.
If we had not set the seed, we would not know if we were just getting (un) lucky, and avoiding the error by chance.

\chapter{Optimizing code (and why you often shouldn't)}\label{optimize_code}

Optimizing code is when you spend a bit more human effort to write code that will run faster on your computer.
In some cases, this can be a critical boost to running a simulation, where you inherently will be doing things a lot of times.
Cutting runtime down will always be tempting, as it allows you to run more replicates and get more precisely estimated performance measures for your simulation.

That being said, beyond a few obvious coding tricks we will discuss, one should optimize code only after you discover you need to.
Optimizing as you go usually means you will spend a lot of time wrestling with code far more complicated than it needs to be.
For example, often it is the estimation method that will take a lot of computational time, so having very fast data generation code won't help overall simulation runtimes much, as you are tweaking something that is only a small part of the overall pie, in terms of time.
Keep things simple; in general your time is more important than the computer's time.

In the next sections we will look at a few optimization efforts applied to the ANOVA example in the prior chapters.

\section{Hand-building functions}\label{hand-building-functions}

In the Welch example above, we used the system-implemented ANOVA.
An alternative approach would be to ``hand roll'' the ANOVA F statistic and test directly.
Doing so by hand can set you up to implement modified versions of these tests later on.
Also, although hand-building a method does take more work to program, it can result in a faster piece of code (this actually is the case here) which in turn can make the overall simulation faster.

Following the formulas on p.~129 of Brown and Forsythe (1974) we have (using data as generated in Chapter \citet{chap_DGP}):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ANOVA\_F }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(sim\_data) \{}

\NormalTok{  x\_bar }\OtherTok{\textless{}{-}} \FunctionTok{with}\NormalTok{(sim\_data, }\FunctionTok{tapply}\NormalTok{(x, group, mean))}
\NormalTok{  s\_sq }\OtherTok{\textless{}{-}} \FunctionTok{with}\NormalTok{(sim\_data, }\FunctionTok{tapply}\NormalTok{(x, group, var))}
\NormalTok{  n }\OtherTok{\textless{}{-}} \FunctionTok{table}\NormalTok{(sim\_data}\SpecialCharTok{$}\NormalTok{group)}
\NormalTok{  g }\OtherTok{\textless{}{-}} \FunctionTok{length}\NormalTok{(x\_bar)}

\NormalTok{  df1 }\OtherTok{\textless{}{-}}\NormalTok{ g }\SpecialCharTok{{-}} \DecValTok{1}
\NormalTok{  df2 }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(n) }\SpecialCharTok{{-}}\NormalTok{ g}

\NormalTok{  msbtw }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(n }\SpecialCharTok{*}\NormalTok{ (x\_bar }\SpecialCharTok{{-}} \FunctionTok{mean}\NormalTok{(sim\_data}\SpecialCharTok{$}\NormalTok{x))}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) }\SpecialCharTok{/}\NormalTok{ df1}
\NormalTok{  mswn }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{((n }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{) }\SpecialCharTok{*}\NormalTok{ s\_sq) }\SpecialCharTok{/}\NormalTok{ df2}
\NormalTok{  fstat }\OtherTok{\textless{}{-}}\NormalTok{ msbtw }\SpecialCharTok{/}\NormalTok{ mswn}
\NormalTok{  pval }\OtherTok{\textless{}{-}} \FunctionTok{pf}\NormalTok{(fstat, df1, df2, }\AttributeTok{lower.tail =} \ConstantTok{FALSE}\NormalTok{)}

  \FunctionTok{return}\NormalTok{(pval)}
\NormalTok{\}}

\FunctionTok{ANOVA\_F}\NormalTok{(sim\_data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.009208908
\end{verbatim}

To see the difference between our version and R's version, we can use an R package called \texttt{microbenchmark} to test how long the computations take for each version of the function.
The \texttt{microbenchmark} function runs each expression 100 times (by default) and tracks how long the computations take. It then summarizes the distribution of timings:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(microbenchmark)}
\NormalTok{timings }\OtherTok{\textless{}{-}} \FunctionTok{microbenchmark}\NormalTok{(}\AttributeTok{Rfunction =} \FunctionTok{ANOVA\_F\_aov}\NormalTok{(sim\_data),}
                          \AttributeTok{direct    =} \FunctionTok{ANOVA\_F}\NormalTok{(sim\_data))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning in microbenchmark(Rfunction =
## ANOVA_F_aov(sim_data), direct =
## ANOVA_F(sim_data)): less accurate nanosecond
## times to avoid potential integer overflows
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{timings}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Unit: microseconds
##       expr     min       lq     mean   median
##  Rfunction 202.704 214.7375 245.8885 226.6685
##     direct  89.339  97.3750 106.0371 103.1560
##       uq      max neval cld
##  241.080 1303.308   100  a 
##  109.716  208.854   100   b
\end{verbatim}

The direct function is 2.3 times faster than the built-in R function.

This result is not unusual.
Built-in R functions usually include lots of checks and error-handling, which take time to compute. These checks are crucial for messy, real-world data analysis but unnecessary with our pristine, simulated data.
Here we can skip them by doing the calculations directly.
In general, however, this is a trade-off: writing something yourself gives you a lot of chance to do something wrong, throwing off all your simulations. It might be faster, but you may pay dearly for it in terms of extra hours coding and debugging.
Optimize only if you need to!

\section{Computational efficiency versus simplicity}\label{sec_comp_efficiency}

An alternative approach to having a function that, for each call, generates a single set of data, would be to write a function that generates \emph{multiple} sets of simulated data all at once.

For example, for our ANOVA example we could specify that we want \texttt{R} replications of the study and have the function spit out a matrix with \texttt{R} columns, one for each simulated dataset:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{generate\_data\_matrix }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(mu, sigma\_sq, sample\_size, R) \{}

\NormalTok{  N }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(sample\_size) }
\NormalTok{  g }\OtherTok{\textless{}{-}} \FunctionTok{length}\NormalTok{(sample\_size) }
  
\NormalTok{  group }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{g, }\AttributeTok{times =}\NormalTok{ sample\_size) }
\NormalTok{  mu\_long }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(mu, }\AttributeTok{times =}\NormalTok{ sample\_size)}
\NormalTok{  sigma\_long }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(}\FunctionTok{sqrt}\NormalTok{(sigma\_sq), }\AttributeTok{times =}\NormalTok{ sample\_size) }

\NormalTok{  x\_mat }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{rnorm}\NormalTok{(N }\SpecialCharTok{*}\NormalTok{ R, }\AttributeTok{mean =}\NormalTok{ mu\_long, }\AttributeTok{sd =}\NormalTok{ sigma\_long),}
                  \AttributeTok{nrow =}\NormalTok{ N, }\AttributeTok{ncol =}\NormalTok{ R)}
\NormalTok{  sim\_data }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\AttributeTok{group =}\NormalTok{ group, }\AttributeTok{x\_mat =}\NormalTok{ x\_mat)}
    
  \FunctionTok{return}\NormalTok{(sim\_data)}
\NormalTok{\}}

\FunctionTok{generate\_data\_matrix}\NormalTok{(}\AttributeTok{mu =}\NormalTok{ mu, }\AttributeTok{sigma\_sq =}\NormalTok{ sigma\_sq,}
                     \AttributeTok{sample\_size =}\NormalTok{ sample\_size, }\AttributeTok{R =} \DecValTok{4}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $group
##  [1] 1 1 1 2 2 2 2 2 2 3 3 4 4 4 4
## 
## $x_mat
##             [,1]      [,2]       [,3]       [,4]
##  [1,]  2.1639323  1.646791 -0.9445957  1.2050393
##  [2,] -2.8225439  3.289125  0.8863700  3.7816431
##  [3,]  1.3419979 -3.020363  1.6196826  0.6425271
##  [4,] -0.5939495  1.184961  1.0200117  2.8301117
##  [5,]  0.1054279  3.499446  1.0267897  2.3166570
##  [6,]  4.0618127  4.340879  4.1291935 -1.0532910
##  [7,]  2.0611736  2.544518  1.3772973 -0.3861088
##  [8,]  2.2921786  2.299136  0.1706634  2.0329848
##  [9,]  2.5000308  3.686171  1.8712605  2.8557713
## [10,]  4.0966732  5.824634  6.6448117  2.6354699
## [11,]  5.9111997  3.480771  2.9453236  4.1680932
## [12,]  6.2201594  6.116809  5.8446326  6.2140816
## [13,]  5.9843999  6.488239  7.1954303  6.6280499
## [14,]  6.3928820  4.699057  5.9033074  8.8796297
## [15,]  7.9611464  7.282270  5.9095360  5.9131754
\end{verbatim}

This approach is a bit more computationally efficient because the setup calculations (getting \texttt{N}, \texttt{g}, \texttt{group}, \texttt{mu\_full}, and \texttt{sigma\_full}) only have to be done once instead of once per replication. It also makes clever use of vector recycling in the call to \texttt{rnorm()}. However, the structure of the resulting data is more complicated, which will make it more difficult to do the later estimation steps.
Furthermore, if the number of replicates \texttt{R} is large and each replication produces a large dataset, this ``all-at-once'' approach will entail generating and holding very large amounts of data in memory, which can create other performance issues.
On balance, we recommend the simpler approach of writing a function that generates a single simulated dataset per call (unless and until you have a principled reason to do otherwise).

\section{Reusing code to speed up computation}\label{reusing-code-to-speed-up-computation}

Computational and programming efficiency should usually be a secondary consideration when you are starting to design a simulation study.
It is better to produce accurate code, even if it is a bit slow, than to write code that is speedy but hard to follow (or even worse, that produces incorrect results).
All that said, there is some glaring redundancy in the two functions used for the ANOVA simulation.
Both \texttt{ANOVA\_F} and \texttt{Welch\_F} start by taking the simulated data and calculating summary statistics for each group, using the following code:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x\_bar }\OtherTok{\textless{}{-}} \FunctionTok{with}\NormalTok{(sim\_data, }\FunctionTok{tapply}\NormalTok{(x, group, mean))}
\NormalTok{s\_sq }\OtherTok{\textless{}{-}} \FunctionTok{with}\NormalTok{(sim\_data, }\FunctionTok{tapply}\NormalTok{(x, group, var))}
\NormalTok{n }\OtherTok{\textless{}{-}} \FunctionTok{table}\NormalTok{(sim\_data}\SpecialCharTok{$}\NormalTok{group)}
\NormalTok{g }\OtherTok{\textless{}{-}} \FunctionTok{length}\NormalTok{(x\_bar)}
\end{Highlighting}
\end{Shaded}

In the interest of not repeating ourselves, it would better to pull this code out as a separate function and then re-write the \texttt{ANOVA\_F} and \texttt{Welch\_F} functions to take the summary statistics as input. Here is a function that takes simulated data and returns a list of summary statistics:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{summarize\_data }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(sim\_data) \{}
  
\NormalTok{  res }\OtherTok{\textless{}{-}}\NormalTok{ sim\_data }\SpecialCharTok{\%\textgreater{}\%} 
    \FunctionTok{group\_by}\NormalTok{( group ) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{summarise}\NormalTok{( }\AttributeTok{x\_bar =} \FunctionTok{mean}\NormalTok{( x ),}
               \AttributeTok{s\_sq =} \FunctionTok{var}\NormalTok{( x ),}
               \AttributeTok{n =} \FunctionTok{n}\NormalTok{() )}
\NormalTok{  res}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

We just packaged the code from above, and puts our results in a nice table (and thus pivoted to using tidyverse to calculate these things):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sim\_data }\OtherTok{=} \FunctionTok{generate\_data}\NormalTok{(}\AttributeTok{mu=}\NormalTok{mu, }\AttributeTok{sigma\_sq=}\NormalTok{sigma\_sq, }\AttributeTok{sample\_size=}\NormalTok{sample\_size)}
\FunctionTok{summarize\_data}\NormalTok{(sim\_data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 4 x 4
##   group x_bar  s_sq     n
##   <int> <dbl> <dbl> <int>
## 1     1  1.90 1.42      3
## 2     2  1.82 2.10      6
## 3     3  4.40 0.626     2
## 4     4  6.58 0.485     4
\end{verbatim}

Now we can re-write both \(F\)-test functions to use the output of this function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ANOVA\_F\_agg }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x\_bar, s\_sq, n) \{}
\NormalTok{  g }\OtherTok{=} \FunctionTok{length}\NormalTok{(x\_bar)}
\NormalTok{  df1 }\OtherTok{\textless{}{-}}\NormalTok{ g }\SpecialCharTok{{-}} \DecValTok{1}
\NormalTok{  df2 }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(n) }\SpecialCharTok{{-}}\NormalTok{ g}
  
\NormalTok{  msbtw }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(n }\SpecialCharTok{*}\NormalTok{ (x\_bar }\SpecialCharTok{{-}} \FunctionTok{weighted.mean}\NormalTok{(x\_bar, }\AttributeTok{w =}\NormalTok{ n))}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) }\SpecialCharTok{/}\NormalTok{ df1}
\NormalTok{  mswn }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{((n }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{) }\SpecialCharTok{*}\NormalTok{ s\_sq) }\SpecialCharTok{/}\NormalTok{ df2}
\NormalTok{  fstat }\OtherTok{\textless{}{-}}\NormalTok{ msbtw }\SpecialCharTok{/}\NormalTok{ mswn}
\NormalTok{  pval }\OtherTok{\textless{}{-}} \FunctionTok{pf}\NormalTok{(fstat, df1, df2, }\AttributeTok{lower.tail =} \ConstantTok{FALSE}\NormalTok{)}
 
  \FunctionTok{return}\NormalTok{(pval)}
\NormalTok{\}}

\NormalTok{summary\_stats }\OtherTok{\textless{}{-}} \FunctionTok{summarize\_data}\NormalTok{(sim\_data)}
\FunctionTok{with}\NormalTok{(summary\_stats, }\FunctionTok{ANOVA\_F\_agg}\NormalTok{(}\AttributeTok{x\_bar =}\NormalTok{ x\_bar, }\AttributeTok{s\_sq =}\NormalTok{ s\_sq, }\AttributeTok{n =}\NormalTok{ n))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.0003129849
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Welch\_F\_agg }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x\_bar, s\_sq, n) \{}
\NormalTok{  g }\OtherTok{=} \FunctionTok{length}\NormalTok{(x\_bar)}
\NormalTok{  w }\OtherTok{\textless{}{-}}\NormalTok{ n }\SpecialCharTok{/}\NormalTok{ s\_sq}
\NormalTok{  u }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(w)}
\NormalTok{  x\_tilde }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(w }\SpecialCharTok{*}\NormalTok{ x\_bar) }\SpecialCharTok{/}\NormalTok{ u}
\NormalTok{  msbtw }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(w }\SpecialCharTok{*}\NormalTok{ (x\_bar }\SpecialCharTok{{-}}\NormalTok{ x\_tilde)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) }\SpecialCharTok{/}\NormalTok{ (g }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{)}

\NormalTok{  G }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{((}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ w }\SpecialCharTok{/}\NormalTok{ u)}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{/}\NormalTok{ (n }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{))}
\NormalTok{  denom }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{+}\NormalTok{  G }\SpecialCharTok{*} \DecValTok{2} \SpecialCharTok{*}\NormalTok{ (g }\SpecialCharTok{{-}} \DecValTok{2}\NormalTok{) }\SpecialCharTok{/}\NormalTok{ (g}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{{-}} \DecValTok{1}\NormalTok{)}
\NormalTok{  W }\OtherTok{\textless{}{-}}\NormalTok{ msbtw }\SpecialCharTok{/}\NormalTok{ denom}
\NormalTok{  f }\OtherTok{\textless{}{-}}\NormalTok{ (g}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{{-}} \DecValTok{1}\NormalTok{) }\SpecialCharTok{/}\NormalTok{ (}\DecValTok{3} \SpecialCharTok{*}\NormalTok{ G)}

\NormalTok{  pval }\OtherTok{\textless{}{-}} \FunctionTok{pf}\NormalTok{(W, }\AttributeTok{df1 =}\NormalTok{ g }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{, }\AttributeTok{df2 =}\NormalTok{ f, }\AttributeTok{lower.tail =} \ConstantTok{FALSE}\NormalTok{)}

  \FunctionTok{return}\NormalTok{(pval)}
\NormalTok{\}}

\FunctionTok{with}\NormalTok{(summary\_stats, }\FunctionTok{ANOVA\_F\_agg}\NormalTok{(}\AttributeTok{x\_bar =}\NormalTok{ x\_bar, }\AttributeTok{s\_sq =}\NormalTok{ s\_sq, }\AttributeTok{n =}\NormalTok{ n))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.0003129849
\end{verbatim}

The results are the same as before.

We should always test any optimized code against something we know is stable, since optimization is an easy way to get bad bugs.
Here we check against R's implementation:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{summary\_stats }\OtherTok{\textless{}{-}} \FunctionTok{summarize\_data}\NormalTok{(sim\_data)}
\NormalTok{F\_results }\OtherTok{\textless{}{-}} \FunctionTok{with}\NormalTok{(summary\_stats,}
                  \FunctionTok{ANOVA\_F\_agg}\NormalTok{(}\AttributeTok{x\_bar =}\NormalTok{ x\_bar, }\AttributeTok{s\_sq =}\NormalTok{ s\_sq, }\AttributeTok{n =}\NormalTok{ n))}
\NormalTok{aov\_results }\OtherTok{\textless{}{-}} \FunctionTok{oneway.test}\NormalTok{(x }\SpecialCharTok{\textasciitilde{}} \FunctionTok{factor}\NormalTok{(group), }\AttributeTok{data =}\NormalTok{ sim\_data, }
                           \AttributeTok{var.equal =} \ConstantTok{TRUE}\NormalTok{)}
\FunctionTok{all.equal}\NormalTok{(aov\_results}\SpecialCharTok{$}\NormalTok{p.value, F\_results)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] TRUE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{W\_results }\OtherTok{\textless{}{-}} \FunctionTok{with}\NormalTok{(summary\_stats,}
                  \FunctionTok{Welch\_F\_agg}\NormalTok{( }\AttributeTok{x\_bar =}\NormalTok{ x\_bar,}
                               \AttributeTok{s\_sq =}\NormalTok{ s\_sq, }\AttributeTok{n =}\NormalTok{ n))}
\NormalTok{aov\_results }\OtherTok{\textless{}{-}} \FunctionTok{oneway.test}\NormalTok{(x }\SpecialCharTok{\textasciitilde{}} \FunctionTok{factor}\NormalTok{(group),}
                           \AttributeTok{data =}\NormalTok{ sim\_data, }
                           \AttributeTok{var.equal =} \ConstantTok{FALSE}\NormalTok{)}
\FunctionTok{all.equal}\NormalTok{(aov\_results}\SpecialCharTok{$}\NormalTok{p.value, W\_results)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] TRUE
\end{verbatim}

Here we are able to check against a known baseline.
Checking estimation functions can be a bit more difficult for procedures that are not already implemented in R. For example, the two other procedures examined by Brown and Forsythe, the James' test and Brown and Forsythe's \(F*\) test, are not available in base R.
They are, however, available in the user-contributed package \texttt{onewaytests}, found by searching for ``Brown-Forsythe'' at \url{http://rseek.org/}. We could benchmark our calculations against this package, but of course there is some risk that the package might not be correct. Another route is to verify your results on numerical examples reported in authoritative papers, on the assumption that there's less risk of an error there. In the original paper that proposed the test, Welch (1951) provides a worked numerical example of the procedure. He reports the following summary statistics:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{g }\OtherTok{\textless{}{-}} \DecValTok{3}
\NormalTok{x\_bar }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FloatTok{27.8}\NormalTok{, }\FloatTok{24.1}\NormalTok{, }\FloatTok{22.2}\NormalTok{)}
\NormalTok{s\_sq }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FloatTok{60.1}\NormalTok{, }\FloatTok{6.3}\NormalTok{, }\FloatTok{15.4}\NormalTok{)}
\NormalTok{n }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{20}\NormalTok{, }\DecValTok{20}\NormalTok{, }\DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

He also reports \(W = 3.35\) and \(f = 22.6\). Replicating the calculations with our \texttt{Welch\_F\_agg} function:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{Welch\_F\_agg}\NormalTok{(}\AttributeTok{x\_bar =}\NormalTok{ x\_bar, }\AttributeTok{s\_sq =}\NormalTok{ s\_sq, }\AttributeTok{n =}\NormalTok{ n)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.05479049
\end{verbatim}

We get slightly different results! But we know that our function is correct---or at least consistent with \texttt{oneway.test}---so what's going on? It turns out that there was an error in some of Welch's intermediate calculations, which can only be spotted because he reported all of his work in the paper.

We then put all these pieces in our revised \texttt{one\_run()} method as so:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{one\_run\_fast }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{( mu, sigma\_sq, sample\_size ) \{}
\NormalTok{  sim\_data }\OtherTok{\textless{}{-}} \FunctionTok{generate\_data}\NormalTok{(}\AttributeTok{mu =}\NormalTok{ mu, }\AttributeTok{sigma\_sq =}\NormalTok{ sigma\_sq,}
                            \AttributeTok{sample\_size =}\NormalTok{ sample\_size)}
\NormalTok{  summary\_stats }\OtherTok{\textless{}{-}} \FunctionTok{summarize\_data}\NormalTok{(sim\_data)}
\NormalTok{  anova\_p }\OtherTok{\textless{}{-}} \FunctionTok{with}\NormalTok{(summary\_stats, }
                  \FunctionTok{ANOVA\_F\_agg}\NormalTok{(}\AttributeTok{x\_bar =}\NormalTok{ x\_bar,}\AttributeTok{s\_sq =}\NormalTok{ s\_sq, }\AttributeTok{n =}\NormalTok{ n))}
\NormalTok{  Welch\_p }\OtherTok{\textless{}{-}} \FunctionTok{with}\NormalTok{(summary\_stats, }
                  \FunctionTok{Welch\_F\_agg}\NormalTok{(}\AttributeTok{x\_bar =}\NormalTok{ x\_bar, }\AttributeTok{s\_sq =}\NormalTok{ s\_sq, }\AttributeTok{n =}\NormalTok{ n))}
  \FunctionTok{tibble}\NormalTok{(}\AttributeTok{ANOVA =}\NormalTok{ anova\_p, }\AttributeTok{Welch =}\NormalTok{ Welch\_p)}
\NormalTok{\}}

\FunctionTok{one\_run\_fast}\NormalTok{( }\AttributeTok{mu =}\NormalTok{ mu, }\AttributeTok{sigma\_sq =}\NormalTok{ sigma\_sq,}
              \AttributeTok{sample\_size =}\NormalTok{ sample\_size )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1 x 2
##      ANOVA  Welch
##      <dbl>  <dbl>
## 1 0.000982 0.0209
\end{verbatim}

The reason this is important is we are now doing our group aggregation only once, rather than once per method. We can use our microbenchmark to see our speedup:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(microbenchmark)}
\NormalTok{timings }\OtherTok{\textless{}{-}} \FunctionTok{microbenchmark}\NormalTok{(}\AttributeTok{noagg =} \FunctionTok{one\_run}\NormalTok{(}\AttributeTok{mu =}\NormalTok{ mu, }\AttributeTok{sigma\_sq =}\NormalTok{ sigma\_sq, }
                                          \AttributeTok{sample\_size =}\NormalTok{ sample\_size),}
                          \AttributeTok{agg =} \FunctionTok{one\_run\_fast}\NormalTok{(}\AttributeTok{mu =}\NormalTok{ mu, }\AttributeTok{sigma\_sq =}\NormalTok{ sigma\_sq, }
                                             \AttributeTok{sample\_size =}\NormalTok{ sample\_size) )}
\NormalTok{timings}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Unit: microseconds
##   expr      min        lq      mean   median
##  noagg  569.408  586.4845  757.3856  613.360
##    agg 1357.100 1381.0440 1450.7891 1395.476
##        uq       max neval cld
##   641.404 10931.338   100  a 
##  1482.088  4113.694   100   b
\end{verbatim}

And our relative speedup is:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{with}\NormalTok{(}\FunctionTok{summary}\NormalTok{(timings), }\FunctionTok{round}\NormalTok{(mean[}\DecValTok{1}\NormalTok{] }\SpecialCharTok{/}\NormalTok{ mean[}\DecValTok{2}\NormalTok{], }\DecValTok{1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.5
\end{verbatim}

To recap, there are two advantages of this kind of coding:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Code reuse is generally good because when you have the same code in multiple places it can make it harder to read and understand your code. If you see two blocks of code you might worry they are only mostly similar, not exactly similar, and waste time trying to differentiate. If you have a single, well-named function, you immediately know what a block of code is doing.
\item
  Saving the results of calculations can speed up your computation since you are saving your partial work. This can be useful to reduce calculations that are particularly time intensive.
\end{enumerate}

\chapter{Error trapping and other headaches}\label{error-trapping-and-other-headaches}

If you have an advanced estimator, or are relying on some package, it is quite possible that every so often your estimate will trigger an error, or give you a NA result, or something similarly bad.
More innocuous, you might have estimators that can generate warnings; if you run 10,000 trials, that can add up to a lot of warnings, which can be overwhelming to sort through.
In some cases, these warnings might be coupled with the estimator returning a very off result; it is unclear, in this case, whether we should include that result in our overall performance measures for that estimator.
After all, it tried to warn us!

In this section, we talk about some ways to make your simulations safe and robust, and also discuss some ways to track warnings and include them in performance measures.

\section{Safe code}\label{safe_code}

Sometimes if you write a function that does a lot of complex things with
uncertain objects -- i.e.~consider a complex estimator using an unstable package not of your own creation -- you can run into trouble where you have a intermittent error.

This can really be annoying; consider the case where your simulation crashes on 1 out of 500 chance: if you run 1000 simulation trials, your program will likely not make it to the end, thus wasting all of your time.
To protect yourself, you can write code that can, instead of stopping when it reaches an error, trap the error and move on with the next simulation trial.

To illustrate, consider the following broken function that sometimes gives us what we want, sometimes gives us a NaN due to taking the square root of a negative number, and sometimes crashes completely due to \texttt{broken\_code()} not existing:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_complex\_function }\OtherTok{=} \ControlFlowTok{function}\NormalTok{( param ) \{}
    
\NormalTok{    vals }\OtherTok{=} \FunctionTok{rnorm}\NormalTok{( param, }\AttributeTok{mean =} \FloatTok{0.5}\NormalTok{ )}
    \ControlFlowTok{if}\NormalTok{ ( }\FunctionTok{sum}\NormalTok{( vals ) }\SpecialCharTok{\textgreater{}} \DecValTok{5}\NormalTok{ ) \{}
        \FunctionTok{broken\_code}\NormalTok{( }\DecValTok{4}\NormalTok{ )}
\NormalTok{    \} }\ControlFlowTok{else}\NormalTok{ \{}
        \FunctionTok{sqrt}\NormalTok{( }\FunctionTok{sum}\NormalTok{( vals ) }\SpecialCharTok{*} \FunctionTok{sign}\NormalTok{( vals )[[}\DecValTok{1}\NormalTok{]] )}
\NormalTok{    \}}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

We run it like so:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{my\_complex\_function}\NormalTok{( }\DecValTok{2}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.9437257
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{my\_complex\_function}\NormalTok{( }\DecValTok{2}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning in sqrt(sum(vals) * sign(vals)[[1]]):
## NaNs produced
\end{verbatim}

\begin{verbatim}
## [1] NaN
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{my\_complex\_function}\NormalTok{( }\DecValTok{7}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1.419031
\end{verbatim}

So far so good, other than a NaN warning. Now let's run it a bunch of times:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{resu }\OtherTok{=} \FunctionTok{map}\NormalTok{( }\DecValTok{1}\SpecialCharTok{:}\DecValTok{20}\NormalTok{, }\SpecialCharTok{\textasciitilde{}} \FunctionTok{my\_complex\_function}\NormalTok{( }\DecValTok{7}\NormalTok{ ) )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning in sqrt(sum(vals) * sign(vals)[[1]]):
## NaNs produced
\end{verbatim}

\begin{verbatim}
## Error in `map()`:
## i In index: 2.
## Caused by error in `broken_code()`:
## ! could not find function "broken_code"
\end{verbatim}

Oh no! Our function crashes sometimes.
To trap the errors we use the purrr package to make a ``safe'' function as so:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_safe\_function }\OtherTok{=} \FunctionTok{safely}\NormalTok{( my\_complex\_function,}
                           \AttributeTok{otherwise =} \ConstantTok{NA}\NormalTok{ )}
\FunctionTok{my\_safe\_function}\NormalTok{( }\DecValTok{7}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $result
## [1] NA
## 
## $error
## <simpleError in broken_code(4): could not find function "broken_code">
\end{verbatim}

Our safe version of the function gives us back a list of things: the result (or NULL if there was an error), and the error message (or NULL if there was no error).
\texttt{safely()} is an example of a \emph{function wrapper}, which takes a function and gives you back a new function that does something slightly different.

We include \texttt{otherwise\ =\ NA} so we always get a result, even if it is a NA result. Otherwise we would get a NULL when there is an error, which might be harder to track.

For example, we can use the above repeatedly, and then do a nice trick to get a list of error message separate from our list of results:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{resu }\OtherTok{=} \FunctionTok{map}\NormalTok{( }\DecValTok{1}\SpecialCharTok{:}\DecValTok{20}\NormalTok{, }\SpecialCharTok{\textasciitilde{}} \FunctionTok{my\_safe\_function}\NormalTok{( }\DecValTok{7}\NormalTok{ ) )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning in sqrt(sum(vals) * sign(vals)[[1]]):
## NaNs produced

## Warning in sqrt(sum(vals) * sign(vals)[[1]]):
## NaNs produced

## Warning in sqrt(sum(vals) * sign(vals)[[1]]):
## NaNs produced

## Warning in sqrt(sum(vals) * sign(vals)[[1]]):
## NaNs produced
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{resu }\OtherTok{\textless{}{-}} \FunctionTok{transpose}\NormalTok{( resu )}
\FunctionTok{unlist}\NormalTok{( resu}\SpecialCharTok{$}\NormalTok{result )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1]       NaN        NA       NaN 0.4354079
##  [5] 1.0734109        NA 1.5196734        NA
##  [9]       NaN 0.6382972 1.0836506        NA
## [13]        NA 1.6244187 1.1915743       NaN
## [17] 2.1165066 2.0638450 1.6465782        NA
\end{verbatim}

The \texttt{transpose()} method takes a list of lists, and reorganizes them to give you a list of all the first elements, a list of all the second elements, etc.
This is very powerful for wrangling data, because then we can make a tibble with list columns as so:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tb }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{( }\AttributeTok{result =} \FunctionTok{unlist}\NormalTok{( resu}\SpecialCharTok{$}\NormalTok{result ),}
        \AttributeTok{error =}\NormalTok{ resu}\SpecialCharTok{$}\NormalTok{error )}
\FunctionTok{print}\NormalTok{( tb, }\AttributeTok{n =} \DecValTok{4}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 20 x 2
##    result error     
##     <dbl> <list>    
## 1 NaN     <NULL>    
## 2  NA     <smplErrr>
## 3 NaN     <NULL>    
## 4   0.435 <NULL>    
## # i 16 more rows
\end{verbatim}

There are other function wrappers in this ``safe computing'' family, such as ``possibly,'' which will try to run something and give you a default value if it fails:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_possible\_function }\OtherTok{=} \FunctionTok{possibly}\NormalTok{( my\_complex\_function, }
                                 \AttributeTok{otherwise =} \ConstantTok{NA}\NormalTok{ )}
\FunctionTok{my\_possible\_function}\NormalTok{( }\DecValTok{7}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1.960578
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rs }\OtherTok{\textless{}{-}} \FunctionTok{map\_dbl}\NormalTok{( }\DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{, }\SpecialCharTok{\textasciitilde{}} \FunctionTok{my\_possible\_function}\NormalTok{(}\DecValTok{7}\NormalTok{) )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning in sqrt(sum(vals) * sign(vals)[[1]]):
## NaNs produced

## Warning in sqrt(sum(vals) * sign(vals)[[1]]):
## NaNs produced
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rs}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] 1.197171       NA 1.533298      NaN       NA
##  [6]       NA       NA       NA      NaN 2.149232
\end{verbatim}

There is also ``quietly,'' which makes warnings and messages get bundled, rather than printed to the console:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_quiet\_function }\OtherTok{=} \FunctionTok{quietly}\NormalTok{( my\_complex\_function )}

\FunctionTok{my\_quiet\_function}\NormalTok{( }\DecValTok{1}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $result
## [1] 0.3686244
## 
## $output
## [1] ""
## 
## $warnings
## character(0)
## 
## $messages
## character(0)
\end{verbatim}

This can be especially valuable to control massive amounts of printout in a simulation. If you have lots of extraneous printout, it can slow down the execution of your code far more than you might think.

Note that \texttt{quietly()} does not trap errors, just warnings:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rs }\OtherTok{\textless{}{-}} \FunctionTok{map}\NormalTok{( }\DecValTok{1}\SpecialCharTok{:}\DecValTok{20}\NormalTok{, }\SpecialCharTok{\textasciitilde{}} \FunctionTok{my\_quiet\_function}\NormalTok{(}\DecValTok{7}\NormalTok{) )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Error in `map()`:
## i In index: 3.
## Caused by error in `broken_code()`:
## ! could not find function "broken_code"
\end{verbatim}

You can ``double wrap'' your function, if you want, but what you get back is a bit of a mess:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_safe\_quiet\_function }\OtherTok{=} \FunctionTok{quietly}\NormalTok{( }\FunctionTok{safely}\NormalTok{( my\_complex\_function, }\AttributeTok{otherwise =} \ConstantTok{NA}\NormalTok{ ) )}
\FunctionTok{my\_safe\_quiet\_function}\NormalTok{(}\DecValTok{7}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $result
## $result$result
## [1] NA
## 
## $result$error
## <simpleError in broken_code(4): could not find function "broken_code">
## 
## 
## $output
## [1] ""
## 
## $warnings
## character(0)
## 
## $messages
## character(0)
\end{verbatim}

\subsection{Making a safe function call}\label{making-a-safe-function-call}

You can do some magical massaging to make things work better upfront so you can capture both errors and warnings and get the results in a nice tidy tibble.
For example, say our \texttt{my\_complex\_function()} is a method designed to analyze our data.
We might then write a wrapper that cleans up our safe and quiet version

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{unpack\_mess }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{( mess ) \{}
\NormalTok{  rs }\OtherTok{=} \FunctionTok{tibble}\NormalTok{( }\AttributeTok{result =} \ConstantTok{NA}\NormalTok{, }\AttributeTok{error =} \ConstantTok{NA}\NormalTok{,}
               \AttributeTok{warnings =} \StringTok{""}\NormalTok{ )}
\NormalTok{  rs}\SpecialCharTok{$}\NormalTok{result }\OtherTok{=}\NormalTok{ mess}\SpecialCharTok{$}\NormalTok{result}\SpecialCharTok{$}\NormalTok{result}
\NormalTok{  rs}\SpecialCharTok{$}\NormalTok{error }\OtherTok{=} \FunctionTok{list}\NormalTok{( mess}\SpecialCharTok{$}\NormalTok{result}\SpecialCharTok{$}\NormalTok{error )}
  \ControlFlowTok{if}\NormalTok{ ( }\FunctionTok{length}\NormalTok{( mess}\SpecialCharTok{$}\NormalTok{warnings ) }\SpecialCharTok{\textgreater{}} \DecValTok{0}\NormalTok{ ) \{}
\NormalTok{    rs}\SpecialCharTok{$}\NormalTok{warnings }\OtherTok{=} \FunctionTok{paste}\NormalTok{( mess}\SpecialCharTok{$}\NormalTok{warnings, }\AttributeTok{collapse=}\StringTok{"; "}\NormalTok{ )}
\NormalTok{  \}}
\NormalTok{  rs}
\NormalTok{\}}
\FunctionTok{unpack\_mess}\NormalTok{( }\FunctionTok{my\_safe\_quiet\_function}\NormalTok{( }\DecValTok{7}\NormalTok{ ) )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1 x 3
##   result error  warnings
##    <dbl> <list> <chr>   
## 1   1.56 <NULL> ""
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_safe\_quiet\_function }\OtherTok{\textless{}{-}} \FunctionTok{compose}\NormalTok{( unpack\_mess, my\_safe\_quiet\_function )}
\FunctionTok{my\_safe\_quiet\_function}\NormalTok{(}\DecValTok{7}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1 x 3
##   result error  warnings
##    <dbl> <list> <chr>   
## 1   2.05 <NULL> ""
\end{verbatim}

The \texttt{compose()} method makes a new function that will call \texttt{unpack\_mess()} automatically.
We have now wrapped our original function, in effect, three times.
The result is a new function that takes the same parameters as our original \texttt{my\_complex\_function()} method, and returns some results, including information about errors and warnings, in a nicely organized way.

We could use such a modified function in a loop, and stack our results:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rs }\OtherTok{\textless{}{-}} \FunctionTok{map\_df}\NormalTok{( }\DecValTok{1}\SpecialCharTok{:}\DecValTok{20}\NormalTok{, }\SpecialCharTok{\textasciitilde{}} \FunctionTok{my\_safe\_quiet\_function}\NormalTok{(}\DecValTok{6}\NormalTok{) )}
\NormalTok{rs}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 20 x 3
##    result error      warnings       
##     <dbl> <list>     <chr>          
##  1   1.76 <NULL>     ""             
##  2   2.11 <NULL>     ""             
##  3   1.68 <NULL>     ""             
##  4 NaN    <NULL>     "NaNs produced"
##  5  NA    <smplErrr> ""             
##  6   1.66 <NULL>     ""             
##  7  NA    <smplErrr> ""             
##  8   1.34 <NULL>     ""             
##  9   1.35 <NULL>     ""             
## 10   1.94 <NULL>     ""             
## 11   2.15 <NULL>     ""             
## 12   1.79 <NULL>     ""             
## 13  NA    <smplErrr> ""             
## 14 NaN    <NULL>     "NaNs produced"
## 15   1.91 <NULL>     ""             
## 16   1.67 <NULL>     ""             
## 17 NaN    <NULL>     "NaNs produced"
## 18   2.13 <NULL>     ""             
## 19   2.10 <NULL>     ""             
## 20 NaN    <NULL>     "NaNs produced"
\end{verbatim}

These tools allow us to take existing analytic code and make it safe and quiet, so we can keep things organized in our simulation.

\subsection{What to do with warnings in simulations}\label{what-to-do-with-warnings-in-simulations}

Sometimes our analytic strategy might give some sort of warning (or fail altogether).
For example, from the cluster randomized experiment case study we have:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{101012}\NormalTok{)  }\CommentTok{\# (I picked this to show a warning.)}
\NormalTok{dat }\OtherTok{=} \FunctionTok{gen\_dat\_model}\NormalTok{( }\AttributeTok{J =} \DecValTok{50}\NormalTok{, }\AttributeTok{n\_bar =} \DecValTok{100}\NormalTok{, }\AttributeTok{sigma2\_u =} \DecValTok{0}\NormalTok{ )}
\NormalTok{mod }\OtherTok{\textless{}{-}} \FunctionTok{lmer}\NormalTok{( Yobs }\SpecialCharTok{\textasciitilde{}} \DecValTok{1} \SpecialCharTok{+}\NormalTok{ Z }\SpecialCharTok{+}\NormalTok{ (}\DecValTok{1}\SpecialCharTok{|}\NormalTok{sid), }\AttributeTok{data=}\NormalTok{dat )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## boundary (singular) fit: see help('isSingular')
\end{verbatim}

We have to make a deliberate decision as to what to do about this:

\begin{itemize}
\tightlist
\item
  Keep these ``weird'' trials?
\item
  Drop them?
\end{itemize}

If you decide to drop them, you should drop the entire simulation iteration including the other estimators, even if they worked fine!
If there is something particularly unusual about the dataset, then dropping for one estimator, and keeping for the others that maybe didn't give a warning, but did struggle to estimate the estimand, would be unfair: in the final performance measures the estimators that did not give a warning could be being held to a higher standard, making the comparisons between estimators biased.

If your estimators generate warnings, you should calculate the rate of errors or warning messages as a performance measure.
Especially if you drop some trials, it is important to see how often things are acting pecularly.

The main tool for doing this is the \texttt{quietly()} function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{quiet\_lmer }\OtherTok{=} \FunctionTok{quietly}\NormalTok{( lmer )}
\NormalTok{qmod }\OtherTok{\textless{}{-}} \FunctionTok{quiet\_lmer}\NormalTok{( Yobs }\SpecialCharTok{\textasciitilde{}} \DecValTok{1} \SpecialCharTok{+}\NormalTok{ Z }\SpecialCharTok{+}\NormalTok{ (}\DecValTok{1}\SpecialCharTok{|}\NormalTok{sid), }\AttributeTok{data=}\NormalTok{dat )}
\NormalTok{qmod}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $result
## Linear mixed model fit by REML ['lmerModLmerTest']
## Formula: ..1
##    Data: ..2
## REML criterion at convergence: 6485.293
## Random effects:
##  Groups   Name        Std.Dev.
##  sid      (Intercept) 0.0000  
##  Residual             0.9879  
## Number of obs: 2302, groups:  sid, 50
## Fixed Effects:
## (Intercept)            Z  
##    -0.03143      0.03433  
## optimizer (nloptwrap) convergence code: 0 (OK) ; 0 optimizer warnings; 1 lme4 warnings 
## 
## $output
## [1] ""
## 
## $warnings
## character(0)
## 
## $messages
## [1] "boundary (singular) fit: see help('isSingular')\n"
\end{verbatim}

You then might have, in your analyzing code:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{analyze\_data }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{( dat ) \{}
    
\NormalTok{    M1 }\OtherTok{\textless{}{-}} \FunctionTok{quiet\_lmer}\NormalTok{( Yobs }\SpecialCharTok{\textasciitilde{}} \DecValTok{1} \SpecialCharTok{+}\NormalTok{ Z }\SpecialCharTok{+}\NormalTok{ (}\DecValTok{1}\SpecialCharTok{|}\NormalTok{sid), }\AttributeTok{data=}\NormalTok{dat )}
\NormalTok{    message1 }\OtherTok{=} \FunctionTok{ifelse}\NormalTok{( }\FunctionTok{length}\NormalTok{( M1}\SpecialCharTok{$}\NormalTok{message ) }\SpecialCharTok{\textgreater{}} \DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{ )}
\NormalTok{    warning1 }\OtherTok{=} \FunctionTok{ifelse}\NormalTok{( }\FunctionTok{length}\NormalTok{( M1}\SpecialCharTok{$}\NormalTok{warning ) }\SpecialCharTok{\textgreater{}} \DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{ )}

    \CommentTok{\# Compile our results}
    \FunctionTok{tibble}\NormalTok{( }\AttributeTok{ATE\_hat =} \FunctionTok{coef}\NormalTok{(M1)[}\StringTok{"Z"}\NormalTok{],}
            \AttributeTok{SE\_hat =} \FunctionTok{se.coef}\NormalTok{(M1)[}\StringTok{"Z"}\NormalTok{],}
            \AttributeTok{message =}\NormalTok{ message1,}
            \AttributeTok{warning =}\NormalTok{ warning1 )}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Now you have your primary estimates, and also flags for whether there was a convergence issue.
In the analysis section you can then evaluate what proportion of the time there was a warning or message, and then do subset analyses to those simulation trials where there was no such warning.

\section{Protecting your functions with ``stop''}\label{about_stopifnot}

When writing functions, especially those that take a lot of parameters, it is often wise to include \texttt{stopifnot()} statements at the top to verify the function is getting what it expects.
These are sometimes called ``assert statements'' and are a tool for making errors show up as early as possible.
For example, look at this (fake) example of generating data with different means and variances

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{make\_groups }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{( means, sds ) \{}
\NormalTok{  Y }\OtherTok{=} \FunctionTok{rnorm}\NormalTok{( }\FunctionTok{length}\NormalTok{(means), }\AttributeTok{mean=}\NormalTok{means, }\AttributeTok{sd =}\NormalTok{ sds )}
  \FunctionTok{round}\NormalTok{( Y )}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

If we call it, but provide different lengths for our means and variances, nothing happens, because R simply recycles the standard deviation parameter:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{make\_groups}\NormalTok{( }\FunctionTok{c}\NormalTok{(}\DecValTok{100}\NormalTok{,}\DecValTok{200}\NormalTok{,}\DecValTok{300}\NormalTok{,}\DecValTok{400}\NormalTok{), }\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{100}\NormalTok{,}\DecValTok{10000}\NormalTok{) )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  100  133 1675  402
\end{verbatim}

If this function was used in our data generating code, we would see the warnings but might not know exactly what they were.
We can instead protect our function by putting an \emph{assert statements} in our function like this:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{make\_groups }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{( means, sds ) \{}
  \FunctionTok{stopifnot}\NormalTok{( }\FunctionTok{length}\NormalTok{(means) }\SpecialCharTok{==} \FunctionTok{length}\NormalTok{(sds) )}
\NormalTok{  Y }\OtherTok{=} \FunctionTok{rnorm}\NormalTok{( }\FunctionTok{length}\NormalTok{(means), }\AttributeTok{mean=}\NormalTok{means, }\AttributeTok{sd =}\NormalTok{ sds )}
  \FunctionTok{round}\NormalTok{( Y )}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

This ensures your code is getting called as you intended.
What is nasty about this possible error is nothing is telling you something is wrong!
You could build an entire simulation on this, not realizing that your fourth group has the variance of your first, and get results that make no sense to you.
You could even publish something based on a finding that depends on this error, which would eventually be quite embarrasing.

These statements can also serve as a sort of documentation as to what you expect.
Consider, for example:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{make\_xy }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{( N, mu\_x, mu\_y, rho ) \{}
  \FunctionTok{stopifnot}\NormalTok{( }\SpecialCharTok{{-}}\DecValTok{1} \SpecialCharTok{\textless{}=}\NormalTok{ rho }\SpecialCharTok{\&\&}\NormalTok{ rho }\SpecialCharTok{\textless{}=} \DecValTok{1}\NormalTok{ )}
\NormalTok{  X }\OtherTok{=}\NormalTok{ mu\_x }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{( N )}
\NormalTok{  Y }\OtherTok{=}\NormalTok{ mu\_y }\SpecialCharTok{+}\NormalTok{ rho }\SpecialCharTok{*}\NormalTok{ X }\SpecialCharTok{+} \FunctionTok{sqrt}\NormalTok{(}\DecValTok{1}\SpecialCharTok{{-}}\NormalTok{rho}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}\SpecialCharTok{*}\FunctionTok{rnorm}\NormalTok{(N)}
  \FunctionTok{tibble}\NormalTok{(}\AttributeTok{X =}\NormalTok{ X, }\AttributeTok{Y=}\NormalTok{Y)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Here we see that rho should be between -1 and 1 quite clearly.
A good reminder of what the parameter is for.

This also protects you from inadvetently misremembering the order of your parameters when you call the function (although it is good practice to name your parameters as you pass).
Consider:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{a }\OtherTok{\textless{}{-}} \FunctionTok{make\_xy}\NormalTok{( }\DecValTok{10}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\FloatTok{0.75}\NormalTok{ )}
\NormalTok{b }\OtherTok{\textless{}{-}} \FunctionTok{make\_xy}\NormalTok{( }\DecValTok{10}\NormalTok{, }\FloatTok{0.75}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Error in make_xy(10, 0.75, 2, 3): -1 <= rho && rho <= 1 is not TRUE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{c }\OtherTok{\textless{}{-}} \FunctionTok{make\_xy}\NormalTok{( }\DecValTok{10}\NormalTok{, }\AttributeTok{rho =} \FloatTok{0.75}\NormalTok{, }\AttributeTok{mu\_x =} \DecValTok{2}\NormalTok{, }\AttributeTok{mu\_y =} \DecValTok{3}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\chapter{Saving files and results}\label{saving_files}

Always save your simulation results to a file.
Simulations are painful and time consuming to run, and you will invariably want to analyze the results of them in a variety of different ways, once you have looked at your preliminary analysis.
We advocate saving your simulation as soon as it is complete.
But there are some ways to do better than that, such as saving as you go.
This can protect you if your simulation occasionally crashes, or if you want to rerun only parts of your simulation for some reason.

\section{Saving simulations in general}\label{saving-simulations-in-general}

Once your simulation has completed, you can save it like so:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{dir.create}\NormalTok{(}\StringTok{"results"}\NormalTok{, }\AttributeTok{showWarnings =} \ConstantTok{FALSE}\NormalTok{ )}
\FunctionTok{write\_csv}\NormalTok{( res, }\StringTok{"results/simulation\_CRT.csv"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\texttt{write\_csv()} is a tidyverse file-writing command; see ``R for Data Science''
textbook, 11.5.

You can then load it, just before analysis, as so:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{res }\OtherTok{=} \FunctionTok{read\_csv}\NormalTok{( }\StringTok{"results/simulation\_CRT.csv"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

There are two general tools for saving. The \texttt{read/write\_csv} methods save your file in a way where you can open it with a spreadsheet program and look at it.
But your results should be in a vanilla format (non-fancy data frame without list columns).

Alternatively, you can use the \texttt{saveRDS()} and \texttt{readRDS()} methods; these save objects to a file such that when you load them, they are as you left them.
(The simpler format of a csv file means your factors, if you have them, may not preseve as factors, and so forth.)

\section{Saving simulations as you go}\label{saving-simulations-as-you-go}

If you are not sure you have time to run your entire simulation, or you think your computer might crash half way through, or something similar, you can save each chunk you run as you go, in its own file. You then stack those files at the end to get your final results.
With clever design, you can even then selectively delete files to rerun only parts of your larger simulation---but be sure to rerun everything from scratch before you run off and publish your results, to avoid embarrassing errors.

Here, for example, is a script from a research project examining how one might use post-stratification to improve the precision of an IV estimate.
This is the script that runs the simulation.
Note the sourcing of other scripts that have all the relevant functions; these are not important here.
Due to modular programming, we can see what this script does, even without those detail.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{source}\NormalTok{( }\StringTok{"pack\_simulation\_functions.R"}\NormalTok{ )}

\ControlFlowTok{if}\NormalTok{ ( }\SpecialCharTok{!}\FunctionTok{file.exists}\NormalTok{(}\StringTok{"results/frags"}\NormalTok{ ) ) \{}
    \FunctionTok{dir.create}\NormalTok{(}\StringTok{"results/frags"}\NormalTok{)}
\NormalTok{\}}

\CommentTok{\# Number of simulation replicates per scenario}
\NormalTok{R }\OtherTok{=} \DecValTok{1000}

\CommentTok{\# Do simulation breaking up R into this many chunks}
\NormalTok{M\_CHUNK }\OtherTok{=} \DecValTok{10}

\DocumentationTok{\#\#\#\#\#\# Set up the multifactor simulation \#\#\#\#\#\#\#}

\CommentTok{\# chunkNo is a hack to make a bunch of smaller chunks for doing parallel more}
\CommentTok{\# efficiently.}
\NormalTok{factors }\OtherTok{=} \FunctionTok{expand\_grid}\NormalTok{( }\AttributeTok{chunkNo =} \DecValTok{1}\SpecialCharTok{:}\NormalTok{M\_CHUNK,}
                       \AttributeTok{N =} \FunctionTok{c}\NormalTok{( }\DecValTok{500}\NormalTok{, }\DecValTok{1000}\NormalTok{, }\DecValTok{2000}\NormalTok{ ),}
                       \AttributeTok{pi\_c =} \FunctionTok{c}\NormalTok{( }\FloatTok{0.05}\NormalTok{, }\FloatTok{0.075}\NormalTok{, }\FloatTok{0.10}\NormalTok{ ),}
                       \AttributeTok{nt\_shift =} \FunctionTok{c}\NormalTok{( }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{ ),}
                       \AttributeTok{pred\_comp =} \FunctionTok{c}\NormalTok{( }\StringTok{"yes"}\NormalTok{, }\StringTok{"no"}\NormalTok{ ),}
                       \AttributeTok{pred\_Y =} \FunctionTok{c}\NormalTok{( }\StringTok{"yes"}\NormalTok{, }\StringTok{"no"}\NormalTok{ ),}
                       \AttributeTok{het\_tx =} \FunctionTok{c}\NormalTok{( }\StringTok{"yes"}\NormalTok{, }\StringTok{"no"}\NormalTok{ ),}
                       \AttributeTok{sd0 =} \DecValTok{1}
\NormalTok{                       )}
\NormalTok{factors }\OtherTok{\textless{}{-}}\NormalTok{ factors }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{mutate}\NormalTok{(}
    \AttributeTok{reps =}\NormalTok{ R }\SpecialCharTok{/}\NormalTok{ M\_CHUNK,}
    \AttributeTok{seed =} \DecValTok{16200320} \SpecialCharTok{+} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{n}\NormalTok{()}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

This generates a data frame of all our factor combinations.
This is our list of ``tasks'' (each row of factors).
These tasks have repeats: the ``chunks'' means we do a portion of each scenario, as specified by our simulation factors, as a process.
This would allow for greater parallelization (e.g., if we had more cores), and also lets us save our work without finishing an entire scenario of, in this case, 1000 iterations.

To set up our simulation we make a little helper method to do one row.
With each row, once we have run it, we save it to disk.
This means if we kill our simulation half-way through, most of the work would be saved.
Our function is then going to either do the simulation (and save the result to disk immediately), or, if it can find the file with the results from a previous run, load those results from disk:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{safe\_run\_sim }\OtherTok{=} \FunctionTok{safely}\NormalTok{( run\_sim )}
\NormalTok{file\_saving\_sim }\OtherTok{=} \ControlFlowTok{function}\NormalTok{( chunkNo, seed, ... ) \{}
\NormalTok{    fname }\OtherTok{=} \FunctionTok{paste0}\NormalTok{( }\StringTok{"results/frags/fragment\_"}\NormalTok{, chunkNo, }\StringTok{"\_"}\NormalTok{, seed, }\StringTok{".rds"}\NormalTok{ )}
\NormalTok{    res }\OtherTok{=} \ConstantTok{NA}
    \ControlFlowTok{if}\NormalTok{ ( }\SpecialCharTok{!}\FunctionTok{file.exists}\NormalTok{(fname) ) \{}
\NormalTok{        res }\OtherTok{=} \FunctionTok{safe\_run\_sim}\NormalTok{( }\AttributeTok{chunkNo=}\NormalTok{chunkNo, }\AttributeTok{seed=}\NormalTok{seed, ... )}
        \FunctionTok{saveRDS}\NormalTok{(res, }\AttributeTok{file =}\NormalTok{ fname )}
\NormalTok{    \} }\ControlFlowTok{else}\NormalTok{ \{}
\NormalTok{        res }\OtherTok{=} \FunctionTok{readRDS}\NormalTok{( }\AttributeTok{file=}\NormalTok{fname )}
\NormalTok{    \}}
    \FunctionTok{return}\NormalTok{( res )}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Note how we wrap our core \texttt{run\_sim} method in \texttt{safely}; it was crashing very occasionally, and so to make the code more robust, we wrapped it so we could see any error messages.

We next run the simulation.
We shuffle the rows of our task list so that which process gets what task is randomized.
If some tasks are much longer (e.g., due to larger sample size) then this will get balanced out across our processes.

We have an \texttt{if-then} structure to easily switch between parallel and nonparallel code.
This makes debugging easier: when running in parallel, stuff printed to the console does not show until the simulation is over.
Plus it would be all mixed up since multiple processes are working simultaneously.

This overall structure allows the researcher to delete one of the ``fragment'' files from the disk, run the simulation code, and have it just do one tiny piece of the simulation.
This means the researcher can insert a \texttt{browser()} command somewhere inside the code, and debug the code, in the natural context of how the simulation is being run.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Shuffle the rows so we run in random order to load balance.}
\NormalTok{factors }\OtherTok{=} \FunctionTok{sample\_n}\NormalTok{(factors, }\FunctionTok{nrow}\NormalTok{(factors) )}

\ControlFlowTok{if}\NormalTok{ ( }\ConstantTok{TRUE}\NormalTok{ ) \{}
    \CommentTok{\# Run in parallel}
\NormalTok{    parallel}\SpecialCharTok{::}\FunctionTok{detectCores}\NormalTok{()}
    
    \FunctionTok{library}\NormalTok{(future)}
    \FunctionTok{library}\NormalTok{(furrr)}
    
    \CommentTok{\#plan(multiprocess) \# choose an appropriate plan from future package}
    \CommentTok{\#plan(multicore)}
    \FunctionTok{plan}\NormalTok{(multisession, }\AttributeTok{workers =}\NormalTok{ parallel}\SpecialCharTok{::}\FunctionTok{detectCores}\NormalTok{() }\SpecialCharTok{{-}} \DecValTok{2}\NormalTok{ )}
    
\NormalTok{    factors}\SpecialCharTok{$}\NormalTok{res }\OtherTok{\textless{}{-}} \FunctionTok{future\_pmap}\NormalTok{(factors, }\AttributeTok{.f =}\NormalTok{ file\_saving\_sim,}
                          \AttributeTok{.options =} \FunctionTok{furrr\_options}\NormalTok{(}\AttributeTok{seed =} \ConstantTok{NULL}\NormalTok{),}
                          \AttributeTok{.progress =} \ConstantTok{TRUE}\NormalTok{ )}
    
\NormalTok{\} }\ControlFlowTok{else}\NormalTok{ \{}
  \CommentTok{\# Run not in parallel, used for debugging}
\NormalTok{  factors}\SpecialCharTok{$}\NormalTok{res }\OtherTok{\textless{}{-}} \FunctionTok{pmap}\NormalTok{(factors, }\AttributeTok{.f =}\NormalTok{ file\_saving\_sim )}
\NormalTok{\}}

\NormalTok{tictoc}\SpecialCharTok{::}\FunctionTok{toc}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

Our method cleverly loads files in, or generates them, for each chunk.
The seed setting ensures reproducibility.
Once we are done, we need to clean up our results:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sim\_results }\OtherTok{\textless{}{-}} 
\NormalTok{    factors }\SpecialCharTok{\%\textgreater{}\%} 
    \FunctionTok{unnest}\NormalTok{(}\AttributeTok{cols =}\NormalTok{ res)}

\CommentTok{\# Cut apart the results and error messages}
\NormalTok{sim\_results}\SpecialCharTok{$}\NormalTok{sr }\OtherTok{=} \FunctionTok{rep}\NormalTok{( }\FunctionTok{c}\NormalTok{(}\StringTok{"res"}\NormalTok{,}\StringTok{"err"}\NormalTok{), }\FunctionTok{nrow}\NormalTok{(sim\_results)}\SpecialCharTok{/}\DecValTok{2}\NormalTok{)}
\NormalTok{sim\_results }\OtherTok{=} \FunctionTok{pivot\_wider}\NormalTok{( sim\_results, }\AttributeTok{names\_from =}\NormalTok{ sr, }\AttributeTok{values\_from =}\NormalTok{ res )}

\FunctionTok{saveRDS}\NormalTok{( sim\_results, }\AttributeTok{file=}\StringTok{"results/simulation\_results.rds"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\section{Dynamically making directories}\label{dynamically-making-directories}

If you are generating a lot of files, then you should put them somewhere.
But where?
It is nice to dynamically generate a directory for your files on fly.
One way to do this is to write a function that will make any needed directory, if it doesn't exist, and then put your file in that spot.
For example, you might have your own version of \texttt{write\_csv} as:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_write\_csv }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{( data, path, file ) \{}
  
  \ControlFlowTok{if}\NormalTok{ ( }\SpecialCharTok{!}\FunctionTok{dir.exists}\NormalTok{( here}\SpecialCharTok{::}\FunctionTok{here}\NormalTok{( path ) ) ) \{}
    \FunctionTok{dir.create}\NormalTok{( here}\SpecialCharTok{::}\FunctionTok{here}\NormalTok{( path ), }\AttributeTok{recursive=}\ConstantTok{TRUE}\NormalTok{ ) }
\NormalTok{  \}}
  \FunctionTok{write\_csv}\NormalTok{( data, }\FunctionTok{paste0}\NormalTok{( path, file ) )}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

This will look for a path (starting from your R Project, by taking advantage of the \texttt{here} package), and put your data file in that spot.
If the spot doesn't exist, it will make it for you.

\section{Loading and combining files of simulation results}\label{loading-and-combining-files-of-simulation-results}

Once your simulation files are all generated, the following code will stack them all into a giant set of results, assuming all the files are themselves data frames stored in RDS objects.
This function will try and stack all files found in a given directory; for it to work, you should ensure there are no other files stored there.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{load.all.sims }\OtherTok{=} \ControlFlowTok{function}\NormalTok{( }\AttributeTok{filehead=}\StringTok{"results/"}\NormalTok{ ) \{}
  
\NormalTok{  files }\OtherTok{=} \FunctionTok{list.files}\NormalTok{( filehead, }\AttributeTok{full.names=}\ConstantTok{TRUE}\NormalTok{)}
  
\NormalTok{  res }\OtherTok{=} \FunctionTok{map\_df}\NormalTok{( files, }\ControlFlowTok{function}\NormalTok{( fname ) \{}
    \FunctionTok{cat}\NormalTok{( }\StringTok{"Reading results from "}\NormalTok{, fname, }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{ )}
\NormalTok{    rs }\OtherTok{=} \FunctionTok{readRDS}\NormalTok{( }\AttributeTok{file =}\NormalTok{ fname )}
\NormalTok{    rs}\SpecialCharTok{$}\NormalTok{filename }\OtherTok{=}\NormalTok{ fname}
\NormalTok{    rs}
\NormalTok{  \})}
\NormalTok{  res}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

You would use as so:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{results }\OtherTok{=} \FunctionTok{load.all.sims}\NormalTok{( }\AttributeTok{filehead=}\StringTok{"raw\_results/"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\chapter{Parallel Processing}\label{parallel-processing}

Especially if you take our advice of ``when in doubt, go more general'' and if you calculate monte carlo standard errors, you will quickly come up against the limits of your computer.
Simultions can be incredibly computationally intensive, and there are a few means for dealing with that.
The first, touched on at times throughout the book, is to optimize ones code by looking for ways to remove extraneous calculation (e.g., by writing ones own methods rather than using the safety-checking and thus sometimes slower methods in R, or by saving calculations that are shared across different estimation approaches).
The second is to use more computing power.
This latter approach is the topic of this chapter.

There are two general ways to do parallel calculation.
The first is to take advantage of the fact that most modern computers have multiple cores (i.e., computers) built in.
With this approach, we tell R to use more of the processing power of your desktop or laptop.
If your computer has eight cores, you can easily get a near eight-fold increase in the speed of your simulation.

The second is to use cloud computing, or compute on a cluster.
A computing cluster is a network of hundreds or thousands of computers, coupled with commands where you break apart a simulation into pieces and send the pieces to your army of computers.
Conceptually, this is the same as when you do baby parallel on your desktop: more cores equals more simulations per minute and thus faster simulation overall.
But the interface to a cluster can be a bit tricky, and very cluster-dependent.

But once you get it up and running, it can be a very powerful tool.
First, it takes the computing off your computer entirely, making it easier to set up a job to run for days or weeks without making your day to day life any more difficult.
Second, it gives you hundreds of cores, potentially, which means a speed-up of hundreds rather than four or eight.

Simulations are a very natural choice for parallel computation.
With a multifactor experiment it is very easy to break apart the overall into pieces.
For example, you might send each factor combination to a single machine.
Even without multi factor experiments, due to the cycle of ``generate data, then analyze,'' it is easy to have a bunch of computers doing the same thing, with a final collection step where all the individual iterations are combined into one at the end.

\section{Parallel on your computer}\label{parallel-on-your-computer}

Most modern computers have multiple cores, so you can run a parallel simulation right in the privacy of your own home!

To assess how many cores you have on your computer, you can use the \texttt{detectCores()} method in the \texttt{parallel} package:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{parallel}\SpecialCharTok{::}\FunctionTok{detectCores}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 8
\end{verbatim}

Normally, unless you tell it to do otherwise, \textbf{\emph{R only uses one core}}.
This is obviously a bit lazy on R's part.
But it is easy to take advantage of multiple cores using the \texttt{future} and \texttt{furrr} packages.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(future)}
\FunctionTok{library}\NormalTok{(furrr)}
\end{Highlighting}
\end{Shaded}

In particular, the \texttt{furrr} package replicates our \texttt{map} functions, but in parallel.
We first tell our R session what kind of parallel processing we want using the \texttt{future} package.
In general, using \texttt{plan(multisession)} is the cleanest: it will start one entire R session per core, and have each session do work for you.
The alternative, \texttt{multicore} does not seem to work well with Windows machines, nor with RStudio in general.

The call is simple:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plan}\NormalTok{(multisession, }\AttributeTok{workers =}\NormalTok{ parallel}\SpecialCharTok{::}\FunctionTok{detectCores}\NormalTok{() }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

The \texttt{workers} parameter specifies how many of your cores you want to use.
Using all but one will let your computer still operate mostly normally for checking email and so forth.
You are carving out a bit of space for your own adventures.

Once you set up your plan, you use \texttt{future\_pmap()}; it works just like \texttt{pmap()} but evaluates across all available workers specified in the plan call.
Here we are running a parallel version of the multifactor experiment discussed in Chapter @ref(exp\_design) (see chapter @ref(case\_Cronback) for the simulation itself).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tictoc}\SpecialCharTok{::}\FunctionTok{tic}\NormalTok{()}
\NormalTok{params}\SpecialCharTok{$}\NormalTok{res }\OtherTok{=} \FunctionTok{future\_pmap}\NormalTok{(params,}
                         \AttributeTok{.f =}\NormalTok{ run\_alpha\_sim,}
                         \AttributeTok{.options =} \FunctionTok{furrr\_options}\NormalTok{(}\AttributeTok{seed =} \ConstantTok{NULL}\NormalTok{))}
\NormalTok{tictoc}\SpecialCharTok{::}\FunctionTok{tic}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

Note the \texttt{.options\ =\ furrr\_options(seed\ =\ NULL)} part of the argument.
This is to silence some warnings.
Given how tasks are handed out, R will get upset if you don't do some handholding regarding how it should set seeds for pseudoranom number generation.
In particular, if you don't set the seed, the multiple sessions could end up having the same starting seed and thus run the exact same simulations (in principle).
We have seen before how to set specific seed for each simulation scenario, but \texttt{furrr} doesn't know we have done this.
This is why the extra argument about seeds: it is being explicit that we are handling seed setting on our own.

We can compare the running time to running in serial (i.e.~using only one worker):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tictoc}\SpecialCharTok{::}\FunctionTok{tic}\NormalTok{()}
\NormalTok{params}\SpecialCharTok{$}\NormalTok{res2 }\OtherTok{=}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(params, n}\SpecialCharTok{:}\NormalTok{seed) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{pmap}\NormalTok{(}\AttributeTok{.f =}\NormalTok{ run\_alpha\_sim)}
\NormalTok{tictoc}\SpecialCharTok{::}\FunctionTok{tic}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

(The \texttt{select} command is to drop the \texttt{res} column from the parallel run; it would otherwise be passed as as parameter to \texttt{run\_alpha\_sim} which would in turn cause an error due to the unrecognized parameter.)

\section{Parallel off your computer}\label{parallel-off-your-computer}

In general, a ``cluster'' is a system of computers that are connected up to form a large distributed network that many different people can use to do large computational tasks (e.g., simulations!).
These clusters will have some overlaying coordinating programs that you, the user, will interact with to set up a ``job,'' or set of jobs, which is a set of tasks you want some number of the computers on the cluster to do for you in tandum.

These coordinating programs will differ, depending on what cluster you are using, but have some similarities that bear mention.
For running simulations, you only need the smallest amount of knowledge about how to engage with these systems because you don't need all the individual computers working on your project communicating with each other (which is the hard part of distributed computing, in general).

\subsection{What is a command-line interface?}\label{what-is-a-command-line-interface}

In the good ol' days, when things were simpler, yet more difficult, you would interact with your computer via a ``command-line interface.''
The easiest way to think about this is as an R console, but in a different language that the entire computer speaks.
A command line interface is designed to do things like find files with a specific name, or copy entire directories, or, importantly, start different programs.
Another place you may have used a command line inteface is when working with Git: anything fancy with Git is often done via command-line.
People will talk about a ``shell'' (a generic term for this computer interface) or ``bash'' or ``csh.''
You can get access to a shell from within RStudio by clicking on the ``Terminal'' tab.
Try it, if you've never done anything like this before, and type

\begin{verbatim}
ls
\end{verbatim}

It should list some file names.
Note this command does \emph{not} have the parenthesis after the command, like in R or most other programming languages.
The syntax of a shell is usually mystifying and brutal: it is best to just steal scripts from the internet and try not to think about it too much, unless you want to think about it a lot.

Importantly for us, from the command line interface you can start an R program, telling it to start up and run a script for you.
This way of running R is noninteractive: you say ``go do this thing,'' and R starts up, goes and does it, and then quits.
Any output R generates on the way will be saved in a file, and any files you save along the way will also be at your disposal once R has completed.

To see this in action make the following script in a file called ``dumb\_job.R'':

\begin{verbatim}
library( tidyverse )
cat( "Making numbers\n" )
Sys.sleep(30)
cat( "Now I'm ready\n" )
dat = tibble( A = rnorm( 1000 ), B = runif( 1000 ) * A )
write_csv( dat, file="sim_results.csv" )
Sys.sleep(30)
cat( "Finished\n" )
\end{verbatim}

Then open the terminal and type (the ``\textgreater{}'' is not part of what you type):

\begin{verbatim}
> ls
\end{verbatim}

Do you see your \texttt{dumb\_job.R} file? If not, your terminal session is in the wrong directory.
In your computer system, files are stored in a directory structure, and when you open a terminal, you are somewhere in that structure.

To find out where, you can type

\begin{verbatim}
> pwd
\end{verbatim}

for ``Print Working Directory''.
Save your dumb job file to wherever the above says.
You can also change directories using \texttt{cd}, e.g., \texttt{cd\ \textasciitilde{}/Desktop/temp} means ``change directory to the temp folder inside Desktop inside my home directory'' (the \texttt{\textasciitilde{}} is shorthand for home directory).
One more useful commands is \texttt{cd\ ..} (go up to the parent directory).

Once you are in the directory with your file, type:

\begin{verbatim}
> R CMD BATCH dumb_job.R R_output.txt --no-save
\end{verbatim}

The above command says ``Run R'' (the first part) in batch mode (the ``CMD BATCH'' part), meaning source the \texttt{dumb\_job.R} script as soon as R starts, saving all console output in the file \texttt{R\_output.txt} (it will be saved in the current directory where you run the program), and where you don't save the workspace when finished.

This command should take about a minute to complete, because our script sleeps a lot (the sleep represents your script doing a lot of work, like a real simulation would do).
Once the command completes (you will see your ``\textgreater{}'' prompt come back), verify that you have the \texttt{R\_output.txt} and the data file \texttt{sim\_results.csv} by typing \texttt{ls}.
If you open up your Finder or Microsoft equivilent, you can actually see the \texttt{R\_output.txt} file appear half-way through, while your job is running.
If you open it, you will see the usual header of R telling you what it loading, the ``Making numbers'' comment, and so forth.
R is saving everything as it works through your script.

Running R in this fashion is the key element to a basic way of setting up a massive job on the cluster: you will have a bunch of R programs all ``going and doing something'' on different computers in the cluster.
They will all save their results to files (they will have files of different names, or you will not be happy with the end result) and then you will gather these files together to get your final set of results.

\emph{Small Exercise:} Try putting an error in your \texttt{dumb\_job.R} script. What happens when you run it in batch mode?

\subsection{Running a job on a cluster}\label{running-a-job-on-a-cluster}

In the above, you can run a command on the command-line, and the command line interfact will pause while it runs.
As you saw, when you hit return with the above R command, the program just sat there for a minute before you got your command-line prompt back, due to the sleep.

When you properlly run a big job (program) on a cluster, it doesn't quite work that way.
You will instead set a program to run, but tell the cluster to run it somewhere else (people might say ``run in the background'').
This is good because you get your command-line prompt back, and can do other things, while the program runs in the background.

There are various methods for doing this, but they usually boil down to a request from you to some sort of managerial process that takes requests and assigns some computer, somewhere, to do them.
(Imagine a dispatcher at a taxi company. You call up, ask for a ride, and it sends you a taxi to do it. The dispatcher is just fielding requests, assinging them to taxis.)

For example, one dispatcher is the slurm (which may or may not be on the cluster you are attempting to use; this is where a lot of this information gets very cluster-specific).

You first set up a script that describes the job to be run.
It is like a work request.
This would be a plain text file, such as this example (\texttt{sbatch\_runScript.txt}):

\begin{verbatim}
#!/bin/bash
#SBATCH -n 32                                                   # Number of cores requested
#SBATCH -N 1                                                      # Ensure that all cores are on one machine
#SBATCH -t 480                                                  # Runtime in minutes
#SBATCH -p stats                                                # Partition to submit to
#SBATCH --mem-per-cpu=1000                    # Memory per cpu in MB
#SBATCH --open-mode=append                      # Append to output file, don't truncate
#SBATCH -o /output/directory/out/%j.out # Standard out goes to this file
#SBATCH -e /output/directory/out/%j.err # Standard err goes to this file
#SBATCH --mail-type=ALL                         # Type of email notification- BEGIN,END,FAIL,ALL
#SBATCH --mail-user=email@gmail.com         # Email address

# You might have some special loading of modules in the computing environment
source new-modules.sh
module load gcc/7.1.0-fasrc01
module load R
export R_LIBS_USER=$HOME/apps/R:$R_LIBS_USER

#R file to run, and txt files to produce for output and errors
R CMD BATCH estimator_performance_simulation.R logs/R_output_${INDEX_VAR}.txt --no-save --no-restore
\end{verbatim}

This file starts with a bunch of variables that describe how sbatch should handle the request.
It then has a series of commands that get the computer environment ready.
Finally, it has the \texttt{R\ CMD\ BATCH} command that does the work you want.

These scripts can be quite confusing to understand.
There are so many options!
What do these things even do?
The answer is, for researchers early on their journey to do this kind of work, ``Who knows?''
The general rule is to find an example file for the system you are working on that works, and then modify it for your own purposes.

Once you have such a file, you could run it on the command line, like this:

\begin{verbatim}
sbatch -o stdout.txt \
        --job-name=my_script \
        sbatch_runScript.txt
\end{verbatim}

You do this, and it will \emph{not} sit there and wait for the job to be done.
The \texttt{sbatch} command will instead send the job off to some computer which will do the work in parallel.

Interestingly, your R script could, at this point, do the ``one computer'' parallel type code listed above.
Note the script above has 32 cores; your single job could then have 32 cores all working away on their individual pieces of the simulation, as before (e.g., with \texttt{future\_pmap}).
You would have a 32-fold speedup, in this case.

This is the core element to having your simulation run on a cluster.
The next step is to do this \emph{a lot}, sending off a bunch of these jobs to different computers.

Some final tips

\begin{itemize}
\item
  Remember to save a workspace or RDS!! Once you tell Odyssey to run an R file, it, well, runs the R file. But, you probably want information after it's done - like an R object or even an R workspace. For any R file you want to run on Odyssey, remember at the end of the R file to put a command to save something after everything else is done. If you want to save a bunch of R objects, an R workspace might be a good way to go, but those files can be huge. A lot of times I find myself wanting only one or two R objects, and RDS files are a lot smaller.
\item
  Moving files from a cluster to your computer. You will need to first upload your files and code to the cluster, and then, once you've saved your workspace/RDS, you need those back on your computer. Using a scp client such as FileZilla is an easy way to do this file-transfer stuff. You can also use a Git repo for the code, but checking in the simulation results is not generally advisable: they are big, and not really in the spirit of a verson control system. Download your simulation results outside of Git, and keep your code in Git, is a good rule of thumb.
\end{itemize}

\subsection{Checking on a job}\label{checking-on-a-job}

Once your job is working on the cluster, it will keep at it until it finishes (or crashes, or is terminated for taking up too much memory or time).
As it chugs away, there will be different ways to check on it.
For example, you can, from the console, list the jobs you have running to see what is happening:

\begin{verbatim}
sacct -u lmiratrix
\end{verbatim}

except, of course, ``\texttt{lmiratrix}'' would be changed to whatever your username is.
This will list if your file is running, pending, timed out, etc. If it's pending, that usually means that someone else is hogging up space on the cluster and your job request is in a queue waiting to be assigned.

The \texttt{sacct} command is customizable, e.g.,

\begin{verbatim}
sacct -u lmiratrix --format=JobID,JobName%30,State
\end{verbatim}

will not truncate your job names, so you can find them more easily.

You can check on a specific job, if you know the ID:

\begin{verbatim}
squeue -j JOBID
\end{verbatim}

Something that's fun is you can check who's running files on the stats server by typing:

\begin{verbatim}
showq-slurm -p stats -o
\end{verbatim}

You can also look at the log files

\begin{verbatim}
tail my_log_file.log
\end{verbatim}

to see if it is logging information as it is working.

The email arguments, above, cause the system to email you before and after the job is complete.
The email notifications you can choose are \texttt{BEGIN}, \texttt{END}, \texttt{FAIL}, and \texttt{ALL}; \texttt{ALL} is generally good. What is a few more emails?

\subsection{Running lots of jobs on a cluster}\label{running-lots-of-jobs-on-a-cluster}

We have seen how to fire off a job (possibly a big job) that can run over days or weeks to give you your results.
There is one more piece that can allow you to use even more computing resources to do things even faster, which is to do a whole bunch of job requests like the above, all at once.
This multiple dispatching of sbatch commands is the final component for large simulations on a cluster: you are setting in motion a bunch of processes, each set to a specific task.

Asking for multiple, smaller, jobs is also nicer for the cluster than having one giant job that goes on for a long time.
By dividing a job into smaller pieces, and asking the scheduler to schedule those pieces, you can let the scheduler share and allocate resources between you and others more fairly.
It can make a list of your jobs, and farm them out as it has space.
This might go faster for you; with a really big job, the scheduler can't even allocate it until the needed number of workers is available.
With smaller jobs, you can take a lot of little spaces to get your work done.
Especially since simulation is so independent (just doing the same thing over and over) there is rarely any need for one giant process that has to do everything.

To make multiple, related, requests, we create a for-loop in the Terminal to make a whole series sbatch requests.
Then, each sbatch request will do one part of the overall simulation.
We can write this program in the shell, just like you can write R scripts in R.
A shell scripts does a bunch of shell commands for you, and can even have variables and loops and all of that fun stuff.

For example, the following \texttt{run\_full\_simulation.sh} is a script that fires off a bunch of jobs for a simulation.
Note that it makes a variable \texttt{INDEX\_VAR}, and sets up a loop so it can run 500 tasks indexed 1 through 500.

The first \texttt{export} line adds a collection of R libraries to the path stored in \texttt{R\_LIBS\_USER} (a ``path'' is a list of places where R will look for libraries).
The next line sets up a for loop: it will run the indented code once for each number from 1 to 500.
The script also specifies where to put log files and names each job with the index so you can know who is generating what file.

\begin{verbatim}
export R_LIBS_USER=$HOME/apps/R:$R_LIBS_USER

for INDEX_VAR in $(seq 1 500); do

  #print out indexes
  echo "${INDEX_VAR}"

  #give indexes to R so it can find them.
  export INDEX_VAR 

  #Run R script, and produce output files
  sbatch -o logs/sbout_p${INDEX_VAR}.stdout.txt \
        --job-name=runScr_p${INDEX_VAR} \
        sbatch_runScript.txt
  
  sleep 1 # pause to be kind to the scheduler

done
\end{verbatim}

One question is then how do the different processes know what part of the simulation they should be working on?
E.g., each worker needs to have its own seed so it don't do exactly the same simulation as a different worker!
The workers also need their own filenames so they save things in their own files.
The key is the \texttt{export\ INDEX\_VAR} line: this puts a variable in the environment that will be set to a specific number.
Inside your R script, you can get that index like so:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{index }\OtherTok{\textless{}{-}} \FunctionTok{as.numeric}\NormalTok{(}\FunctionTok{as.character}\NormalTok{(}\FunctionTok{Sys.getenv}\NormalTok{(}\StringTok{"INDEX\_VAR"}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

You can then use the index to make unique filenames when you save your results, so each process has its own filename:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{filename }\OtherTok{=} \FunctionTok{paste0}\NormalTok{( }\StringTok{"raw\_results/simulation\_results\_"}\NormalTok{, index, \_}\StringTok{".rds"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

You can also modify your seed such as with:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{factors }\OtherTok{=} \FunctionTok{mutate}\NormalTok{( factors,}
                  \AttributeTok{seed =} \FunctionTok{set.seed}\NormalTok{( }\DecValTok{1000} \SpecialCharTok{*}\NormalTok{ seed }\SpecialCharTok{+}\NormalTok{ index ) )}
\end{Highlighting}
\end{Shaded}

Now even if you have a series of seeds within the simulation script (as we have seen before), each script will have unique seeds not shared by any other script (assuming you have fewer than 1000 separate job requests).

This still doesn't exactly answer how to have each worker know what to work on.
Conider the case of our multifactor experiment, where we have a large combination of simulation trials we want to run.

There are two approaches one might use here.
One simple approach is the following: we first generate all the factors with \texttt{expand\_grid()} as usual, and then we take the row of this grid that corresponds to our index.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sim\_factors }\OtherTok{=} \FunctionTok{expand\_grid}\NormalTok{( ... )}
\NormalTok{index }\OtherTok{\textless{}{-}} \FunctionTok{as.numeric}\NormalTok{(}\FunctionTok{as.character}\NormalTok{(}\FunctionTok{Sys.getenv}\NormalTok{(}\StringTok{"INDEX\_VAR"}\NormalTok{)))}
\NormalTok{filename }\OtherTok{=} \FunctionTok{paste0}\NormalTok{( }\StringTok{"raw\_results/simulation\_results\_"}\NormalTok{, index, \_}\StringTok{".rds"}\NormalTok{ )}

\FunctionTok{stopifnot}\NormalTok{( index }\SpecialCharTok{\textgreater{}=} \DecValTok{1} \SpecialCharTok{\&\&}\NormalTok{ index }\SpecialCharTok{\textless{}=} \FunctionTok{nrow}\NormalTok{(sim\_factors ) )}
\FunctionTok{do.call}\NormalTok{( my\_sim\_function, sim\_factors[ index, ] )}
\end{Highlighting}
\end{Shaded}

The \texttt{do.call()} command runs the simulation function, passing all the arguments listed in the targeted row.
You then need to make sure you have your shell call the right number of workers to run your entire simulation.

One problem with this approach is some simulations might be a lot more work than others: consider your simulation with a huge sample size vs.~one with a small sample size.
Instead, you can have each worker run a small number of simulations of each scenario, and then stack your results later.
E.g.,

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sim\_factors }\OtherTok{=} \FunctionTok{expand\_grid}\NormalTok{( ... )}
\NormalTok{index }\OtherTok{\textless{}{-}} \FunctionTok{as.numeric}\NormalTok{(}\FunctionTok{as.character}\NormalTok{(}\FunctionTok{Sys.getenv}\NormalTok{(}\StringTok{"INDEX\_VAR"}\NormalTok{)))}
\NormalTok{sim\_factors}\SpecialCharTok{$}\NormalTok{seed }\OtherTok{=} \DecValTok{1000000} \SpecialCharTok{*}\NormalTok{ index }\SpecialCharTok{+} \DecValTok{17} \SpecialCharTok{*} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{nrow}\NormalTok{(sim\_factors)}
\end{Highlighting}
\end{Shaded}

and then do your usual \texttt{pmap} call with \texttt{R\ =\ 10} (or some other small number of replicates.)

For saving files and then loading and combining them for analysis, see Section \citet{saving_files}.

\subsection{Resources for Harvard's Odyssey}\label{resources-for-harvards-odyssey}

The above guidiance is tailored for Harvard' computing environment, primarily.
For that environment in particular, there are many additional resources such as:

\begin{itemize}
\tightlist
\item
  Odyssey Guide: \url{https://rc.fas.harvard.edu/resources/odyssey-quickstart-guide/}
\item
  R on Odyssey: \url{https://rc.fas.harvard.edu/resources/documentation/software/r/}
\end{itemize}

For installing R packages so they are seen by the scripts run by sbatch, see (\url{https://www.rc.fas.harvard.edu/resources/documentation/software-on-odyssey/r/})

Other clusters should have similar documents giving needed guidance for their specific contexts.

\subsection{Acknowledgements}\label{acknowledgements-1}

Some of the above material is based on tutorials built by Kristen Hunter and Zach Branson, past doctoral students of Harvard's statistics department.

\chapter{Using simulation as a power calculator}\label{using-simulation-as-a-power-calculator}

We can use simulation as a power calculator.
In particular, to estimate power, we generate data according to our best guess as to what we might find in a planned evaluation, and then analyze these synthetic data and see if we detect the effect we built into our DGP.
We then do this repeatedly, and see how often we detect our effect.
This is power.

Now, if we are generally right about our guesses about our DGP and the associated parameters we plugged into it, in terms of some planned study, then our power will be right on.
This is all a power analysis is, using simulation or otherwise.

Simulation has benefits over using power calculators because we can take into account odd aspects of our modeling, and also do non-standard approaches to evaluation that we might not find in a normal power calculator.

We illustrate this idea with a case study.
In this example, we are planning a school-level intervention to reduce rates of discipline via a socio-emotional targeting intervention on both teachers and students, where we have strongly predictive historic data and a time-series component.
This is a planned RCT, where we will treat entire schools (so a cluster-randomized study).
We are struggling because treating each school is very expensive (we have to run a large training and coaching of the staff), so each unit is a major decision.
We want something like 4, 5, or maybe 6 treated schools.
Our diving question is: Can we get away with this?

\section{Getting design parameters from pilot data}\label{getting-design-parameters-from-pilot-data}

We had pilot data from school administrative records (in particular discipline rates for each school and year for a series of five years), and we use those to estimate parameters to plug into our simulation.
We assume our experimental sample will be on schools that have chronic issues
with discipline, so we filtered our historic data to get schools we imagined to likely be in our study.

We ended up with the following data, with log-transformed discipline rates for each year (we did this to put things on a multiplicative scale, and to make our data more normal given heavy skew in the original). Each row is a potential school in the district.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{datW }\OtherTok{=} \FunctionTok{read\_csv}\NormalTok{( }\StringTok{"data/discipline\_data.csv"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Rows: 27 Columns: 6
## -- Column specification ------------------------------------------------------------------------------------------------------
## Delimiter: ","
## chr (1): Code
## dbl (5): 2015, 2016, 2017, 2018, 2019
## 
## i Use `spec()` to retrieve the full column specification for this data.
## i Specify the column types or set `show_col_types = FALSE` to quiet this message.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{datW}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 27 x 6
##    Code  `2015` `2016` `2017` `2018` `2019`
##    <chr>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>
##  1 S1     -2.87  -2.81  -2.93  -3.52  -4.90
##  2 S2     -3.60  -2.83  -2.56  -2.76  -3.32
##  3 S3     -3.00  -2.88  -2.81  -3.39  -4.91
##  4 S4     -3.90  -3.20  -2.53  -3.67  -4.34
##  5 S5     -2.46  -2.00  -3.34  -3.66  -4.71
##  6 S6     -2.86  -2.74  -2.51  -3.21  -3.80
##  7 S7     -2.47  -2.59  -2.69  -2.15  -2.43
##  8 S8     -2.13  -1.93  -1.82  -2.21  -2.95
##  9 S9     -3.36  -3.16  -3.06  -3.26  -3.10
## 10 S10    -2.89  -2.54  -2.26  -2.89  -3.25
## # i 17 more rows
\end{verbatim}

We use these to calculate a mean and covariance structure for generating data:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lpd\_mns }\OtherTok{=} \FunctionTok{apply}\NormalTok{( datW[,}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{], }\DecValTok{2}\NormalTok{, mean )}
\NormalTok{lpd\_mns}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      2015      2016      2017      2018      2019 
## -3.076298 -2.868337 -2.931562 -3.337221 -4.011440
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lpd\_cov }\OtherTok{=} \FunctionTok{cov}\NormalTok{( datW[,}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{] )}
\NormalTok{lpd\_cov}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           2015      2016      2017      2018
## 2015 0.4191843 0.2996073 0.2282627 0.3691894
## 2016 0.2996073 0.3656335 0.2014201 0.2511376
## 2017 0.2282627 0.2014201 0.3084799 0.2927782
## 2018 0.3691894 0.2511376 0.2927782 0.5767486
## 2019 0.1921622 0.1623542 0.2541191 0.2927812
##           2019
## 2015 0.1921622
## 2016 0.1623542
## 2017 0.2541191
## 2018 0.2927812
## 2019 0.5425783
\end{verbatim}

\section{The data generating process}\label{the-data-generating-process-2}

We then write a data generator that, given a desired number of control and treatment schools, and a treatment effect, makes a dataset by sampling vectors of discipline rates, and then imposes a ``treatment effect'' of scaling the discipline rate by the treatment coefficient for the last two years.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{make\_dat\_param }\OtherTok{=} \ControlFlowTok{function}\NormalTok{( n\_c, n\_t, }\AttributeTok{tx=}\DecValTok{1}\NormalTok{ ) \{}
\NormalTok{    n }\OtherTok{=}\NormalTok{ n\_c }\SpecialCharTok{+}\NormalTok{ n\_t}
\NormalTok{    lpdisc }\OtherTok{=}\NormalTok{ MASS}\SpecialCharTok{::}\FunctionTok{mvrnorm}\NormalTok{( n, }\AttributeTok{mu =}\NormalTok{ lpd\_mns, }\AttributeTok{Sigma =}\NormalTok{ lpd\_cov )}
\NormalTok{    lpdisc }\OtherTok{=} \FunctionTok{exp}\NormalTok{( lpdisc )}
    \FunctionTok{colnames}\NormalTok{( lpdisc ) }\OtherTok{=} \FunctionTok{paste0}\NormalTok{( }\StringTok{"pdisc\_"}\NormalTok{, }\FunctionTok{colnames}\NormalTok{( lpdisc ) )}
\NormalTok{    lpdisc }\OtherTok{=} \FunctionTok{as.data.frame}\NormalTok{( lpdisc ) }\SpecialCharTok{\%\textgreater{}\%}
        \FunctionTok{mutate}\NormalTok{( }\AttributeTok{ID =} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{n}\NormalTok{(),}
                \AttributeTok{Z =} \DecValTok{0} \SpecialCharTok{+}\NormalTok{ ( }\FunctionTok{sample}\NormalTok{( n ) }\SpecialCharTok{\textless{}=}\NormalTok{ n\_t ) )}
    
    \CommentTok{\# Add in treatment effect}
\NormalTok{    lpdisc }\OtherTok{=} \FunctionTok{mutate}\NormalTok{( lpdisc, }
                     \AttributeTok{pdisc\_2018 =}\NormalTok{ pdisc\_2018 }\SpecialCharTok{*} \FunctionTok{ifelse}\NormalTok{( Z }\SpecialCharTok{==} \DecValTok{1}\NormalTok{, tx, }\DecValTok{1}\NormalTok{ ),}
                     \AttributeTok{pdisc\_2019 =}\NormalTok{ pdisc\_2019 }\SpecialCharTok{*} \FunctionTok{ifelse}\NormalTok{( Z }\SpecialCharTok{==} \DecValTok{1}\NormalTok{, tx, }\DecValTok{1}\NormalTok{ ) )}
 
\NormalTok{    lpdisc }\SpecialCharTok{\%\textgreater{}\%} 
      \FunctionTok{relocate}\NormalTok{( ID, Z )}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Our function generates schools with discipline given by the provided mean and covariance structure; we have calibrated our data generating process to give us data that looks very similar to the data we would see in the field.

For our impact model, the treatment kicks in for the final two years, multiplying discipline rate by \texttt{tx} (so \texttt{tx\ =\ 1} means no treatment effect).

Testing our function gives this:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{( }\DecValTok{59585}\NormalTok{ )}
\NormalTok{a }\OtherTok{=} \FunctionTok{make\_dat\_param}\NormalTok{( }\DecValTok{100}\NormalTok{, }\DecValTok{100}\NormalTok{, }\FloatTok{0.5}\NormalTok{ )  }
\FunctionTok{head}\NormalTok{( a, }\AttributeTok{n =} \DecValTok{4}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   ID Z pdisc_2015 pdisc_2016 pdisc_2017
## 1  1 1 0.03118036 0.05594299 0.03283595
## 2  2 1 0.04209213 0.01716175 0.02388633
## 3  3 1 0.18736890 0.26266248 0.14379326
## 4  4 0 0.04389430 0.04571297 0.03378810
##   pdisc_2018  pdisc_2019
## 1 0.01051475 0.005732411
## 2 0.01155785 0.009120705
## 3 0.04832345 0.027621311
## 4 0.01101846 0.007154335
\end{verbatim}

We can group each treatment arm and look at discipline over the years:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{  aL }\OtherTok{=}\NormalTok{ a }\SpecialCharTok{\%\textgreater{}\%} 
        \FunctionTok{pivot\_longer}\NormalTok{( pdisc\_2015}\SpecialCharTok{:}\NormalTok{pdisc\_2019, }
                      \AttributeTok{names\_to =} \FunctionTok{c}\NormalTok{( }\StringTok{".value"}\NormalTok{, }\StringTok{"year"}\NormalTok{ ),}
                      \AttributeTok{names\_pattern =} \StringTok{"(.*)\_(.*)"}\NormalTok{ ) }\SpecialCharTok{\%\textgreater{}\%}
        \FunctionTok{mutate}\NormalTok{( }\AttributeTok{year =} \FunctionTok{as.numeric}\NormalTok{( year ) )}
    
\NormalTok{    aLg }\OtherTok{=}\NormalTok{ aL }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{group\_by}\NormalTok{( year, Z ) }\SpecialCharTok{\%\textgreater{}\%}
        \FunctionTok{summarise}\NormalTok{( }\AttributeTok{pdisc =} \FunctionTok{mean}\NormalTok{( pdisc ) )}
    \FunctionTok{ggplot}\NormalTok{( aLg, }\FunctionTok{aes}\NormalTok{( year, pdisc, }\AttributeTok{col=}\FunctionTok{as.factor}\NormalTok{(Z) ) ) }\SpecialCharTok{+}
        \FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+}
      \FunctionTok{labs}\NormalTok{( }\AttributeTok{color =} \StringTok{"Tx?"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{Designing-Simulations-in-R_files/figure-latex/unnamed-chunk-235-1} \end{center}

Our treatment group drops faster than the control. We see the nonlinear structure actually observed in our original data in terms of discipline over time has been replicated.

We next write some functions to analyze our data.
This should feel very familiar: we are just doing our simulation framework, as usual.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{eval\_dat }\OtherTok{=} \ControlFlowTok{function}\NormalTok{( sdat ) \{}
    
    \CommentTok{\# No covariate adjustment, average change model (on log outcome)}
\NormalTok{    M\_raw }\OtherTok{=} \FunctionTok{lm}\NormalTok{( }\FunctionTok{log}\NormalTok{( pdisc\_2018 ) }\SpecialCharTok{\textasciitilde{}} \DecValTok{1} \SpecialCharTok{+}\NormalTok{ Z, }\AttributeTok{data=}\NormalTok{sdat )}

    \CommentTok{\# Simple average change model using 2018 as outcome.}
\NormalTok{    M\_simple }\OtherTok{=} \FunctionTok{lm}\NormalTok{( pdisc\_2018 }\SpecialCharTok{\textasciitilde{}} \DecValTok{1} \SpecialCharTok{+}\NormalTok{ Z }\SpecialCharTok{+}\NormalTok{ pdisc\_2017 }\SpecialCharTok{+}\NormalTok{ pdisc\_2016 }\SpecialCharTok{+}\NormalTok{ pdisc\_2015,}
                   \AttributeTok{data=}\NormalTok{sdat )}

    \CommentTok{\# Simple model on logged outcome}
\NormalTok{    M\_log }\OtherTok{=} \FunctionTok{lm}\NormalTok{( }\FunctionTok{log}\NormalTok{( pdisc\_2018 ) }\SpecialCharTok{\textasciitilde{}} \DecValTok{1} \SpecialCharTok{+}\NormalTok{ Z }\SpecialCharTok{+} \FunctionTok{log}\NormalTok{( pdisc\_2017) }\SpecialCharTok{+} \FunctionTok{log}\NormalTok{( pdisc\_2016) }\SpecialCharTok{+} \FunctionTok{log}\NormalTok{( pdisc\_2015 ),}
                \AttributeTok{data=}\NormalTok{sdat )}
    
    \CommentTok{\# Ratio of average disc to average prior disc as outcome}
\NormalTok{    sdat }\OtherTok{=} \FunctionTok{mutate}\NormalTok{( sdat,}
                   \AttributeTok{avg\_disc =}\NormalTok{ (pdisc\_2018 }\SpecialCharTok{+}\NormalTok{ pdisc\_2019)}\SpecialCharTok{/}\DecValTok{2}\NormalTok{,}
                   \AttributeTok{prior\_disc =}\NormalTok{ (pdisc\_2017 }\SpecialCharTok{+}\NormalTok{ pdisc\_2016 }\SpecialCharTok{+}\NormalTok{ pdisc\_2015 )}\SpecialCharTok{/}\DecValTok{3}\NormalTok{,}
                   \AttributeTok{disc =}\NormalTok{ pdisc\_2018 }\SpecialCharTok{/}\NormalTok{ prior\_disc,}
                   \AttributeTok{disc\_two =}\NormalTok{ avg\_disc }\SpecialCharTok{/}\NormalTok{ prior\_disc )}
\NormalTok{    M\_ratio }\OtherTok{=} \FunctionTok{lm}\NormalTok{( disc }\SpecialCharTok{\textasciitilde{}} \DecValTok{1} \SpecialCharTok{+}\NormalTok{ Z, }\AttributeTok{data =}\NormalTok{ sdat )}
\NormalTok{    M\_ratio\_twopost }\OtherTok{=} \FunctionTok{lm}\NormalTok{( disc\_two }\SpecialCharTok{\textasciitilde{}} \DecValTok{1} \SpecialCharTok{+}\NormalTok{ Z, }\AttributeTok{data =}\NormalTok{ sdat )}
    
    \CommentTok{\# Use average of two post{-}tx time periods, averaged to reduce noise}
\NormalTok{    M\_twopost }\OtherTok{=} \FunctionTok{lm}\NormalTok{( }\FunctionTok{log}\NormalTok{( avg\_disc ) }\SpecialCharTok{\textasciitilde{}} \DecValTok{1} \SpecialCharTok{+}\NormalTok{ Z }\SpecialCharTok{+} \FunctionTok{log}\NormalTok{( pdisc\_2017 ) }\SpecialCharTok{+} \FunctionTok{log}\NormalTok{( pdisc\_2016 ) }\SpecialCharTok{+} \FunctionTok{log}\NormalTok{( pdisc\_2015 ), }\AttributeTok{data=}\NormalTok{sdat )}

    \CommentTok{\# Time and unit fixed effects}
\NormalTok{    sdatL }\OtherTok{=} \FunctionTok{pivot\_longer}\NormalTok{( sdat, }\AttributeTok{cols =}\NormalTok{ pdisc\_2015}\SpecialCharTok{:}\NormalTok{pdisc\_2019, }
                          \AttributeTok{names\_to =} \StringTok{"year"}\NormalTok{,}
                          \AttributeTok{values\_to =} \StringTok{"pdisc"}\NormalTok{ ) }\SpecialCharTok{\%\textgreater{}\%}
        \FunctionTok{mutate}\NormalTok{( }\AttributeTok{Z =}\NormalTok{ Z }\SpecialCharTok{*}\NormalTok{ (year }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{( }\StringTok{"pdisc\_2018"}\NormalTok{, }\StringTok{"pdisc\_2019"}\NormalTok{ ) ),}
                \AttributeTok{ID =} \FunctionTok{paste0}\NormalTok{( }\StringTok{"S"}\NormalTok{, ID ) )}
    
\NormalTok{    M\_2wfe }\OtherTok{=} \FunctionTok{lm}\NormalTok{( }\FunctionTok{log}\NormalTok{( pdisc ) }\SpecialCharTok{\textasciitilde{}} \DecValTok{0} \SpecialCharTok{+}\NormalTok{ ID }\SpecialCharTok{+}\NormalTok{ year }\SpecialCharTok{+}\NormalTok{ Z,}
                 \AttributeTok{data=}\NormalTok{sdatL )}

    \CommentTok{\# Bundle all our models by getting the estimated treatment impact}
    \CommentTok{\# from each.}
\NormalTok{    models }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{( }\AttributeTok{raw=}\NormalTok{M\_raw, }\AttributeTok{simple=}\NormalTok{M\_simple,}
                       \AttributeTok{log=}\NormalTok{M\_log, }\AttributeTok{ratio =}\NormalTok{ M\_ratio, }
                       \AttributeTok{ratio\_twopost =}\NormalTok{ M\_ratio\_twopost,}
                       \AttributeTok{log\_twopost =}\NormalTok{ M\_twopost, }
                       \AttributeTok{FE =}\NormalTok{ M\_2wfe )}
\NormalTok{    rs }\OtherTok{\textless{}{-}} \FunctionTok{map\_df}\NormalTok{( models, broom}\SpecialCharTok{::}\NormalTok{tidy, }\AttributeTok{.id=}\StringTok{"model"}\NormalTok{ ) }\SpecialCharTok{\%\textgreater{}\%}
        \FunctionTok{filter}\NormalTok{( term}\SpecialCharTok{==}\StringTok{"Z"}\NormalTok{ ) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{        dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{( }\SpecialCharTok{{-}}\NormalTok{term ) }\SpecialCharTok{\%\textgreater{}\%}
      \FunctionTok{arrange}\NormalTok{( model )}
    
\NormalTok{    rs}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Our method marches through a host of models; we weren't sure what the gains would be from one model to another, so we decided to conduct power analyses on all of them.
Again, we look at what our evaluation function does:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dat }\OtherTok{=} \FunctionTok{make\_dat\_param}\NormalTok{( }\AttributeTok{n\_c =} \DecValTok{4}\NormalTok{, }\AttributeTok{n\_t =} \DecValTok{4}\NormalTok{, }\AttributeTok{tx =} \FloatTok{0.5}\NormalTok{ )}
\FunctionTok{eval\_dat}\NormalTok{( dat )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 7 x 5
##   model       estimate std.error statistic p.value
##   <chr>          <dbl>     <dbl>     <dbl>   <dbl>
## 1 FE          -0.644      0.325     -1.98   0.0576
## 2 log         -0.841      0.478     -1.76   0.176 
## 3 log_twopost -1.22       0.437     -2.80   0.0680
## 4 ratio       -0.126      0.136     -0.923  0.392 
## 5 ratio_twop~ -0.269      0.139     -1.93   0.102 
## 6 raw         -0.650      0.300     -2.17   0.0733
## 7 simple      -0.00983    0.0133    -0.742  0.512
\end{verbatim}

We have a nice set of estimates, one for each model.

\section{Running the simulation}\label{running-the-simulation-2}

Now we put it all together in our classic simulator:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sim\_run }\OtherTok{=} \ControlFlowTok{function}\NormalTok{( n\_c, n\_t, tx, R, }\AttributeTok{seed =} \ConstantTok{NULL}\NormalTok{ ) \{}
    \ControlFlowTok{if}\NormalTok{ ( }\SpecialCharTok{!}\FunctionTok{is.null}\NormalTok{( seed ) ) \{}
        \FunctionTok{set.seed}\NormalTok{(seed)}
\NormalTok{    \}}
    \FunctionTok{cat}\NormalTok{( }\StringTok{"Running n\_c, n\_t ="}\NormalTok{, n\_c, n\_t, }\StringTok{"tx ="}\NormalTok{, tx, }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{ )}
\NormalTok{    rps }\OtherTok{=} \FunctionTok{rerun}\NormalTok{( R, \{}
\NormalTok{        sdat }\OtherTok{=} \FunctionTok{make\_dat\_param}\NormalTok{(}\AttributeTok{n\_c =}\NormalTok{ n\_c, }\AttributeTok{n\_t =}\NormalTok{ n\_t, }\AttributeTok{tx =}\NormalTok{ tx)}
        \FunctionTok{eval\_dat}\NormalTok{( sdat )}
\NormalTok{    \})}
    \FunctionTok{bind\_rows}\NormalTok{( rps )}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

We then do the usual to run across a set of scenarios, running \texttt{sim\_run} on each row of the following:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{res }\OtherTok{=} \FunctionTok{expand\_grid}\NormalTok{( }\AttributeTok{tx =} \FunctionTok{c}\NormalTok{( }\DecValTok{1}\NormalTok{, }\FloatTok{0.75}\NormalTok{, }\FloatTok{0.5}\NormalTok{ ),}
                   \AttributeTok{n\_c =} \FunctionTok{c}\NormalTok{( }\DecValTok{4}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{12}\NormalTok{, }\DecValTok{20}\NormalTok{ ),}
                   \AttributeTok{n\_t =} \FunctionTok{c}\NormalTok{( }\DecValTok{4}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{6}\NormalTok{ ) )}
\NormalTok{res}\SpecialCharTok{$}\NormalTok{R }\OtherTok{=} \DecValTok{1000}
\NormalTok{res}\SpecialCharTok{$}\NormalTok{seed }\OtherTok{=} \DecValTok{1010203} \SpecialCharTok{+} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{nrow}\NormalTok{(res)}
\end{Highlighting}
\end{Shaded}

For evaluation, we load our saved results and calculate rejection rates (we use an alpha of 0.10 since we are doing one-sided testing):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{res }\OtherTok{=} \FunctionTok{readRDS}\NormalTok{( }\AttributeTok{file=}\StringTok{"data/discipline\_simulation.rds"}\NormalTok{ )}

\NormalTok{sres }\OtherTok{\textless{}{-}}\NormalTok{ res }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{group\_by}\NormalTok{( n\_c, n\_t, tx, model ) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{summarise}\NormalTok{( }\AttributeTok{E\_est =} \FunctionTok{mean}\NormalTok{( estimate ),}
               \AttributeTok{SE =} \FunctionTok{sd}\NormalTok{( estimate ),}
               \AttributeTok{E\_SE\_hat =} \FunctionTok{mean}\NormalTok{( std.error ),}
               \AttributeTok{pow =} \FunctionTok{mean}\NormalTok{( p.value }\SpecialCharTok{\textless{}=} \FloatTok{0.10}\NormalTok{ ) ) }\CommentTok{\# one{-}sided testing}
\NormalTok{sres}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 378 x 8
## # Groups:   n_c, n_t, tx [54]
##      n_c   n_t    tx model   E_est     SE E_SE_hat
##    <dbl> <dbl> <dbl> <chr>   <dbl>  <dbl>    <dbl>
##  1     4     4  0.5  FE    -0.693  0.313    0.277 
##  2     4     4  0.5  log   -0.694  0.476    0.430 
##  3     4     4  0.5  log_~ -0.692  0.431    0.383 
##  4     4     4  0.5  ratio -0.374  0.219    0.203 
##  5     4     4  0.5  rati~ -0.291  0.151    0.139 
##  6     4     4  0.5  raw   -0.719  0.535    0.515 
##  7     4     4  0.5  simp~ -0.0195 0.0194   0.0157
##  8     4     4  0.75 FE    -0.292  0.310    0.274 
##  9     4     4  0.75 log   -0.295  0.488    0.435 
## 10     4     4  0.75 log_~ -0.305  0.424    0.373 
## # i 368 more rows
## # i 1 more variable: pow <dbl>
\end{verbatim}

\section{Evaluating power}\label{evaluating-power}

Once our simulation is run, we can explore power as a function of the design characteristics.
In particular, we eventually want to calculate the chance of noticing effects of different sizes, given various sample sizes we might employ.
Our driving question is how few schools on the treated side can we get away with?
Also, we want to know how much having more schools on the control side allows us to get away with fewer schools on the treated side.

\subsection{Checking validity of our models}\label{checking-validity-of-our-models}

Before we look at power, we need to check on whether our different models are valid.
This is especiallt important as we are in a small \(n\) context, so we know asymptotics may not hold as they should.
To check our models for validity we subset our trials to where \texttt{tx\ =\ 1}, and look at the rejection rates.

We first run a regression to see if rejection is a function of sample size (are smaller samples more invalid) and treatment-control imbalance.
We center both variables so our intercepts are overall average rejection rates for each model considered:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sres }\OtherTok{=} \FunctionTok{mutate}\NormalTok{( sres,}
               \AttributeTok{n =}\NormalTok{ n\_c }\SpecialCharTok{+}\NormalTok{ n\_t,}
               \AttributeTok{imbalance =} \FunctionTok{pmax}\NormalTok{( n\_t }\SpecialCharTok{/}\NormalTok{ n\_c, n\_c }\SpecialCharTok{/}\NormalTok{ n\_t ) }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{ )}
\NormalTok{sres}\SpecialCharTok{$}\NormalTok{n }\OtherTok{=}\NormalTok{ (sres}\SpecialCharTok{$}\NormalTok{n }\SpecialCharTok{{-}} \FunctionTok{mean}\NormalTok{(sres}\SpecialCharTok{$}\NormalTok{n)) }\SpecialCharTok{/} \FunctionTok{sd}\NormalTok{(sres}\SpecialCharTok{$}\NormalTok{n)}
\NormalTok{mod }\OtherTok{=} \FunctionTok{lm}\NormalTok{( pow }\SpecialCharTok{\textasciitilde{}} \DecValTok{0} \SpecialCharTok{+}\NormalTok{ (n }\SpecialCharTok{+}\NormalTok{ imbalance) }\SpecialCharTok{*}\NormalTok{ model }\SpecialCharTok{{-}}\NormalTok{ n }\SpecialCharTok{{-}}\NormalTok{ imbalance,}
          \AttributeTok{data =} \FunctionTok{filter}\NormalTok{( sres, tx }\SpecialCharTok{==} \DecValTok{1}\NormalTok{ ) )}
\NormalTok{broom}\SpecialCharTok{::}\FunctionTok{tidy}\NormalTok{(mod) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{( }\AttributeTok{digits =} \DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{l|r|r|r|r}
\hline
term & estimate & std.error & statistic & p.value\\
\hline
modelFE & 0.143 & 0.006 & 24.593 & 0.000\\
\hline
modellog & 0.099 & 0.006 & 16.999 & 0.000\\
\hline
modellog\_twopost & 0.093 & 0.006 & 15.939 & 0.000\\
\hline
modelratio & 0.090 & 0.006 & 15.437 & 0.000\\
\hline
modelratio\_twopost & 0.093 & 0.006 & 15.967 & 0.000\\
\hline
modelraw & 0.092 & 0.006 & 15.751 & 0.000\\
\hline
modelsimple & 0.091 & 0.006 & 15.571 & 0.000\\
\hline
n:modelFE & -0.003 & 0.006 & -0.459 & 0.647\\
\hline
n:modellog & 0.001 & 0.006 & 0.141 & 0.888\\
\hline
n:modellog\_twopost & -0.005 & 0.006 & -0.919 & 0.360\\
\hline
n:modelratio & 0.000 & 0.006 & 0.071 & 0.944\\
\hline
n:modelratio\_twopost & 0.002 & 0.006 & 0.414 & 0.680\\
\hline
n:modelraw & -0.006 & 0.006 & -1.008 & 0.316\\
\hline
n:modelsimple & 0.001 & 0.006 & 0.191 & 0.849\\
\hline
imbalance:modelFE & 0.005 & 0.005 & 0.857 & 0.394\\
\hline
imbalance:modellog & -0.003 & 0.005 & -0.587 & 0.558\\
\hline
imbalance:modellog\_twopost & 0.004 & 0.005 & 0.708 & 0.480\\
\hline
imbalance:modelratio & 0.000 & 0.005 & -0.001 & 0.999\\
\hline
imbalance:modelratio\_twopost & 0.001 & 0.005 & 0.249 & 0.804\\
\hline
imbalance:modelraw & 0.006 & 0.005 & 1.154 & 0.251\\
\hline
imbalance:modelsimple & 0.001 & 0.005 & 0.180 & 0.858\\
\hline
\end{tabular}

We can also plot the nominal rejection rates under the null:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sres }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{filter}\NormalTok{( tx }\SpecialCharTok{==} \DecValTok{1}\NormalTok{ ) }\SpecialCharTok{\%\textgreater{}\%}
\FunctionTok{ggplot}\NormalTok{( }\FunctionTok{aes}\NormalTok{( n\_c, pow, }\AttributeTok{col=}\NormalTok{model ) ) }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{( }\SpecialCharTok{\textasciitilde{}}\NormalTok{ n\_t, }\AttributeTok{nrow=}\DecValTok{1}\NormalTok{ ) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_hline}\NormalTok{( }\AttributeTok{yintercept =} \FloatTok{0.10}\NormalTok{ ) }\SpecialCharTok{+}
  \FunctionTok{scale\_x\_log10}\NormalTok{(}\AttributeTok{breaks=}\FunctionTok{unique}\NormalTok{(sres}\SpecialCharTok{$}\NormalTok{n\_c) )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{Designing-Simulations-in-R_files/figure-latex/unnamed-chunk-241-1} \end{center}

We see the fixed effect models have elevated rates of rejection.
Interestingly, these rates do not seem particularly dependent on sample size or treatment-control imbalance (note lack of significant coefficeints on our regression model).
The other models all appear valid.

We can also check for bias of our methods:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sres }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{group\_by}\NormalTok{( model, tx ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{( }\AttributeTok{E\_est =} \FunctionTok{mean}\NormalTok{( E\_est ) ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{pivot\_wider}\NormalTok{( }\AttributeTok{names\_from=}\StringTok{"tx"}\NormalTok{, }\AttributeTok{values\_from=}\StringTok{"E\_est"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 7 x 4
## # Groups:   model [7]
##   model           `0.5`  `0.75`        `1`
##   <chr>           <dbl>   <dbl>      <dbl>
## 1 FE            -0.692  -0.290  -0.000703 
## 2 log           -0.692  -0.288   0.00120  
## 3 log_twopost   -0.692  -0.291   0.00241  
## 4 ratio         -0.372  -0.187  -0.000937 
## 5 ratio_twopost -0.289  -0.145  -0.00108  
## 6 raw           -0.694  -0.290   0.00327  
## 7 simple        -0.0206 -0.0104  0.0000998
\end{verbatim}

We see our models are estimating different things, none of which are the treatment effect as we parameterized it.
In particular, ``FE,'' ``log,'' ``raw,'' and ``log\_twopost'' are all estimating the impact on the log scale.
Note that \(log( 0.5 ) \approx -0.69\) and \(log( 0.75 ) \approx -0.29\).
Our ``simple'' estimator is estimating the impact on the absolute scale; reducing discipline rates by 50\% corresponds to about a 2\% reduction in actual cases.
Finally, ``ratio'' and ``ratio\_twopost'' are estimating the change in the average ratio of post-policy discipline to pre; they are akin to a gain score as compared to the log regressions.

\subsection{Assessing Precision (SE)}\label{assessing-precision-se}

Now, which methods are the most precise?
We look at the true standard errors across our methods (we drop ``simple'' and the ``ratio'' estimators since they are not on the ratio scale):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sres }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{( model, n\_c, n\_t ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{( }\AttributeTok{SE =} \FunctionTok{mean}\NormalTok{(SE ) ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{( }\SpecialCharTok{!}\NormalTok{(model }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{( }\StringTok{"simple"}\NormalTok{, }\StringTok{"ratio"}\NormalTok{, }\StringTok{"ratio\_twopost"}\NormalTok{ ) ) ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{( }\FunctionTok{aes}\NormalTok{( n\_c, SE, }\AttributeTok{col=}\NormalTok{model )) }\SpecialCharTok{+}
    \FunctionTok{facet\_grid}\NormalTok{( . }\SpecialCharTok{\textasciitilde{}}\NormalTok{ n\_t ) }\SpecialCharTok{+}
    \FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
    \FunctionTok{geom\_hline}\NormalTok{( }\AttributeTok{yintercept =} \DecValTok{0}\NormalTok{ ) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{( }\AttributeTok{colour =} \StringTok{"Model"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{Designing-Simulations-in-R_files/figure-latex/disc_precision-1} \end{center}

It looks like averaging two years for the outcome is helpful, and bumps up precision.
The two way fixed effects model seems to react to the number of control units differently than the other estimators; it is way more precise when the number of controls is few, but the other estimators catch up.
The ``raw'' estimator gives a baseline of no covariate adjustment; everything is substantially more precise than it.
The covariates matter a lot.

\subsection{Assessing power}\label{assessing-power}

We next look at power over our explored contexts, for the models that we find to be valid (i.e., not FE).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sres }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{( model }\SpecialCharTok{!=} \StringTok{"FE"}\NormalTok{,tx }\SpecialCharTok{!=} \DecValTok{1}\NormalTok{ ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{( }\FunctionTok{aes}\NormalTok{( n\_c, pow, }\AttributeTok{col=}\NormalTok{model )) }\SpecialCharTok{+}
    \FunctionTok{facet\_grid}\NormalTok{(  . }\SpecialCharTok{\textasciitilde{}}\NormalTok{ tx }\SpecialCharTok{+}\NormalTok{ n\_t, }\AttributeTok{labeller =}\NormalTok{ label\_both ) }\SpecialCharTok{+}
    \FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
    \FunctionTok{geom\_hline}\NormalTok{( }\AttributeTok{yintercept =} \DecValTok{0}\NormalTok{, }\AttributeTok{col=}\StringTok{"grey"}\NormalTok{ ) }\SpecialCharTok{+}
    \FunctionTok{geom\_hline}\NormalTok{( }\AttributeTok{yintercept =} \FunctionTok{c}\NormalTok{( }\FloatTok{0.10}\NormalTok{, }\FloatTok{0.80}\NormalTok{ ), }\AttributeTok{lty=}\DecValTok{2}\NormalTok{ ) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{()}\SpecialCharTok{+} \FunctionTok{theme}\NormalTok{( }\AttributeTok{legend.position=}\StringTok{"bottom"}\NormalTok{,}
                          \AttributeTok{legend.direction=}\StringTok{"horizontal"}\NormalTok{,}
                          \AttributeTok{legend.key.width=}\FunctionTok{unit}\NormalTok{(}\DecValTok{1}\NormalTok{,}\StringTok{"cm"}\NormalTok{),}
                          \AttributeTok{panel.border =} \FunctionTok{element\_blank}\NormalTok{() ) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{( }\AttributeTok{title=}\StringTok{"Power for various methods vs number of controls."}\NormalTok{,}
      \AttributeTok{y =} \StringTok{"Power"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{Designing-Simulations-in-R_files/figure-latex/disc_power-1} \end{center}

We mark 80\% power with a dashed line. For a 25\% reduction in discipline, nothing reaches desired levels of power.
For 50\% reduction, some designs do, but we need substantial numbers of control schools.
Averaging two years of outcomes post-treatment seems important: the ``twopost'' methods have a distinct power bump.
For a single year of outcome data, the log model seems our best bet.

\subsection{Assessing Minimum Detectable Effects}\label{assessing-minimum-detectable-effects}

Sometimes we want to know, given a design, what size effect we might be able to detect.
The usual measure for this is the Minimum Detectable Effect (MDE), which is usually the size of the smallest effect we could detect with power 80\%.

To calculate Minimal Detectable Effects (MDEs) for the log-scale estimators,
we first average our SEs over our different designs, grouped by sample size, and then convert the SEs to MDEs by multiplying by 2.8.
We then have to convert to our treatment scale by flipping the sign and exponentiating, to get out of the log scale.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sres2 }\OtherTok{=}\NormalTok{ sres }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{( model, n\_c, n\_t ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{( }\AttributeTok{SE =} \FunctionTok{mean}\NormalTok{( SE ),}
             \AttributeTok{E\_SE\_hat =} \FunctionTok{mean}\NormalTok{( E\_SE\_hat ) ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{( }\AttributeTok{MDE =} \FunctionTok{exp}\NormalTok{( }\SpecialCharTok{{-}}\NormalTok{ (}\FloatTok{1.64} \SpecialCharTok{+} \FloatTok{0.8}\NormalTok{) }\SpecialCharTok{*}\NormalTok{ SE ) )}

\NormalTok{sres2 }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{( }\SpecialCharTok{!}\NormalTok{(model }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{( }\StringTok{"simple"}\NormalTok{, }\StringTok{"ratio"}\NormalTok{, }\StringTok{"ratio\_twopost"}\NormalTok{ ) ) ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{( }\FunctionTok{aes}\NormalTok{( n\_c, MDE, }\AttributeTok{col=}\NormalTok{model ) ) }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{( }\SpecialCharTok{\textasciitilde{}}\NormalTok{ n\_t, }\AttributeTok{labeller =}\NormalTok{ label\_both ) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{geom\_line}\NormalTok{()  }\SpecialCharTok{+}
  \FunctionTok{geom\_hline}\NormalTok{( }\AttributeTok{yintercept =} \FloatTok{0.5}\NormalTok{ ) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{scale\_x\_log10}\NormalTok{( }\AttributeTok{breaks =} \FunctionTok{unique}\NormalTok{( sres}\SpecialCharTok{$}\NormalTok{n\_c ) ) }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{( }\AttributeTok{legend.position=}\StringTok{"bottom"}\NormalTok{,}
         \AttributeTok{legend.direction=}\StringTok{"horizontal"}\NormalTok{, }\AttributeTok{legend.key.width=}\FunctionTok{unit}\NormalTok{(}\DecValTok{1}\NormalTok{,}\StringTok{"cm"}\NormalTok{),}
         \AttributeTok{panel.border =} \FunctionTok{element\_blank}\NormalTok{() ) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{( }\AttributeTok{x =} \StringTok{"Number of control units"}\NormalTok{, }\AttributeTok{y =} \StringTok{"MDE (proportion reduction of rate)"}\NormalTok{,}
        \AttributeTok{caption =} \StringTok{"A MDE of 0.6 means a 60\% reduction (more than half) in discipline rates"}\NormalTok{,}
        \AttributeTok{title =} \StringTok{"MDE vs number of control units for various methods"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{Designing-Simulations-in-R_files/figure-latex/disc_mde-1} \end{center}

Corresponding with our findings regarding precision, above, the twopost estimator is the most sensitive, finding the smallest effects.

\chapter{Simulation under the Potential Outcomes Framework}\label{chap_potential_outcomes}

If we are in the business of evaluating how various methods such as matching or propensity score weighting work in practice, we would probably turn to the potential outcomes framework for our simulations.
The potential outcomes framework is a framework typically used in the causal inference literature to make very explicit statements regarding the mechanics of causality and the associated estimands one might target when estimating causal effects.
While we recommend reading, for a more thourough overview, either {[}CITE Raudenbush or Field Experiments textbook{]}, we briefly outline this framework here to set out our notation.

Take a sample of experimental units, indexed by \(i\).
For each unit, we can treat it or not.
Denote treatment as \(Z_i = 1\) for treated or \(Z_i = 0\) for not treated.
Now we imagine each unit has two potential outcomes being the outcome we would see if we treated it (\(Y_i(1)\)) or if we did not (\(Y_i(0)\)).
Finally, our observed outcome is then
\[ Y_i^{obs} = Z_i Y_i(1) + (1-Z_i)Y_i(0) .\]
For a unit, the treatment effect is \(\tau_i = Y_i(1) - Y_i(0)\); it is how much our outcome changes if we treat vs.~not treat.
Frustratingly, for each unit we can only see one of its two potential outcomes, so we can never get an estimate of these individual \(\tau_i\).
Under this view, causality is a missing data problem: if we only were able to impute the missing potential outcomes, we could have a dataset where we could calculate any estimands we wanted. E.g., the true average treatment effect \emph{for the sample} \(\mathcal{S}\) would be:

\[ ATE_{\mathcal{S}} = \frac{1}{N} \sum_{i} Y_i(1) - Y_i( 0 ) . \]
The average proportion increase, by contrast, would be

\[ API_{\mathcal{S}} = \frac{1}{N} \sum_{i} \frac{Y_i(1)}{Y_i(0)} \]

\section{Finite vs.~Superpopulation inference}\label{finite-vs.-superpopulation-inference}

Consider a sample of \(n\) units, \(\mathcal{S}\), along with their set of potential outcomes.
We can talk about the true ATE of the sample, or, if we thought of the sample as being drawn from some larger population, we could talk about the true ATE of that larger population.

This is a tension that often arises in potential outcomes based simulations: if we are focused on \(ATE_{\mathcal{S}}\) then for each sample we generate, our estimand could be (maybe only slightly) different, depending on whether our sample has more or fewer units with high \(\tau_i\).
If, on the other hand, we are focused on where the units came from (which is our data generating model), our estimand is a property of the DGP, and would be the same for each sample generated.

The catch is when we calculate our performance metrics, we now have two possible targets to pick from.
Furthermore, if we are targeting the superpopulation ATE, then our error in estimation may be due in part to the representativeness of the sample, \emph{not} the estimation or uncertainty due to the random assignment.

We will follow this theme throughout this chapter.

\section{Data generation processes for potential outcomes}\label{data-generation-processes-for-potential-outcomes}

If we want to write a simulation using the potential outcomes framework, it is clear and transparent to first generate a complete set of potential outcomes, then generate a random assignment based on some assignment mechanism, and finally generate the observed outcomes as a function of assignment and original potential outcomes.

For example, we might say that our data generation process is as follows: First generate each unit \(i = 1, \ldots, n\), as
\[
\begin{aligned}
X_i &\sim exp( 1 ) - 1 \\
Y_i(0) &= \beta_0 + \beta_1 X_i + \epsilon_i \mbox{ with } \epsilon_i \sim N( 0, \sigma^2 ) \\
\tau_i &= \tau_0 + \tau_1 X_i + \alpha u_i \mbox{ with } u_i \sim t_{df} \\
Y_i(1) &= Y_i(0) + \tau_i 
\end{aligned}
\]
with \(exp(1)\) being the standard exponential distribution and \(t_{df}\) being a \(t\) distribution with \(df\) degrees of freedom.
We subtract 1 from \(X_i\) to zero-center it (it is often convenient to have zero-centered covariates so we can then, e.g., interpret \(\tau_0\) as the true superpopulation ATE of our experiment).

The above model is saying that we first, for each unit, generate a covariate.
We then generate our two potential outcomes.
I.e., we are generating what the outcome would be for each unit if it were treated and if it were not treated.
We are driving both the level and the treatment effect with \(X_i\), assuming \(\beta_1\) and \(\tau_1\) are non-zero.

One advantage of generating all the potential outcomes is we can then calculate the finite-sample estimands such as the true average treatment effect for the generated sample: we just take the average of \(Y_i(1) - Y_i(0)\) for our sample.

Here is some code to illustrate the first part of the data generating process (we leave treatment assignment to later):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gen\_data }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{( }\AttributeTok{n =} \DecValTok{100}\NormalTok{,}
                      \AttributeTok{R2 =} \FloatTok{0.5}\NormalTok{,}
                      \AttributeTok{beta\_0 =} \DecValTok{0}\NormalTok{, }\AttributeTok{beta\_1 =} \DecValTok{1}\NormalTok{,}
                      \AttributeTok{tau\_0 =} \DecValTok{1}\NormalTok{, }\AttributeTok{tau\_1 =} \DecValTok{1}\NormalTok{, }
                      \AttributeTok{alpha =} \DecValTok{1}\NormalTok{, }\AttributeTok{df =} \DecValTok{3}\NormalTok{ ) \{}
  \FunctionTok{stopifnot}\NormalTok{( R2 }\SpecialCharTok{\textgreater{}=} \DecValTok{0} \SpecialCharTok{\&\&}\NormalTok{ R2 }\SpecialCharTok{\textless{}} \DecValTok{1}\NormalTok{ )}
\NormalTok{  X\_i }\OtherTok{=} \FunctionTok{rexp}\NormalTok{( n, }\AttributeTok{rate =} \DecValTok{1}\NormalTok{ ) }\SpecialCharTok{{-}} \DecValTok{1}
\NormalTok{  beta\_1 }\OtherTok{=} \FunctionTok{sqrt}\NormalTok{( }\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ R2 )}
\NormalTok{  sigma\_e }\OtherTok{=} \FunctionTok{sqrt}\NormalTok{( R2 )}
\NormalTok{  Y0\_i }\OtherTok{=}\NormalTok{ beta\_0 }\SpecialCharTok{+}\NormalTok{ beta\_1 }\SpecialCharTok{*}\NormalTok{ X\_i }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{( n, }\AttributeTok{sd=}\NormalTok{sigma\_e )}
\NormalTok{  tau\_i }\OtherTok{=}\NormalTok{ tau\_0 }\SpecialCharTok{+}\NormalTok{ tau\_1 }\SpecialCharTok{*}\NormalTok{ X\_i }\SpecialCharTok{+}\NormalTok{ alpha }\SpecialCharTok{*} \FunctionTok{rt}\NormalTok{( n, }\AttributeTok{df =}\NormalTok{ df )}
\NormalTok{  Y1\_i }\OtherTok{=}\NormalTok{ Y0\_i }\SpecialCharTok{+}\NormalTok{ tau\_i}
  
  \FunctionTok{tibble}\NormalTok{( }\AttributeTok{X =}\NormalTok{ X\_i, }\AttributeTok{Y0 =}\NormalTok{ Y0\_i, }\AttributeTok{Y1 =}\NormalTok{ Y1\_i )}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

And now we see our estimand can change:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{( }\DecValTok{40454}\NormalTok{ )}
\NormalTok{d1 }\OtherTok{\textless{}{-}} \FunctionTok{gen\_data}\NormalTok{( }\DecValTok{50}\NormalTok{ )}
\FunctionTok{mean}\NormalTok{( d1}\SpecialCharTok{$}\NormalTok{Y1 }\SpecialCharTok{{-}}\NormalTok{ d1}\SpecialCharTok{$}\NormalTok{Y0 )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.6374925
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{d2 }\OtherTok{\textless{}{-}} \FunctionTok{gen\_data}\NormalTok{( }\DecValTok{50}\NormalTok{ )}
\FunctionTok{mean}\NormalTok{( d2}\SpecialCharTok{$}\NormalTok{Y1 }\SpecialCharTok{{-}}\NormalTok{ d2}\SpecialCharTok{$}\NormalTok{Y0 )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.5479788
\end{verbatim}

In reviewing our code, we know our superpopulation ATE should be \texttt{tau}, or 1 exactly.
If our estimate for \texttt{d1} is 0.6 do we say that is close or far from the target?
From a finite sample performance approach, we nailed it.
From superpopulation, less so.

Also in looking at the above, there are a few details to call out:

\begin{itemize}
\tightlist
\item
  We can store the latent, intermediate quantities (both potential outcomes, in particular) so we can calculate the estimands of interest or learn about our data generating process. When we hand the data to an estimator, we would not provide this ``secret'' information.
\item
  We are using a trick to index our DGP by an R2 value rather than coefficients on X so we can have a standardized control-side outcome (the expected variation of \(Y_i(0)\) will be 1). The treatment outcomes will have more variation due to the heterogeniety of the treatment impacts.
\item
  If we were generating data with a constant treatment impact, then \(ATE_{\mathcal{S}} = ATE\) always; this is typical for many similations in the literature. That being said, treatment variation is what causes a lot of methods to fail and so having simulations that have this variation is usually important.
\end{itemize}

Once we have our \emph{schedule of potential outcomes}, we would then generate the \emph{observed outcomes} by assigning our (synthetic, randomly generated) \(n\) units to treatment or control.
For example, say we wanted to simulate an observational context where treatment was a function of our covariate.
We could model each unit as flipping a weighted coin with some probability that was a function of \(X_i\) as so:

\[
\begin{aligned}
p_i &= logit^{-1}( \xi_0 + \xi_1 X_i ) \\
Z_i &= Bern( p_i ) \\
Y_i &= Z_i Y_i(1) + (1-Z_i) Y_i(0) 
\end{aligned}
\]

Here is code for assigning our data to treatment and control:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{assign\_data }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{( dat,}
                         \AttributeTok{xi\_0 =} \SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\AttributeTok{xi\_1 =} \DecValTok{1}\NormalTok{ ) \{}
\NormalTok{  n }\OtherTok{=} \FunctionTok{nrow}\NormalTok{(dat)}
\NormalTok{  dat }\OtherTok{=} \FunctionTok{mutate}\NormalTok{( dat,}
                \AttributeTok{p =}\NormalTok{ arm}\SpecialCharTok{::}\FunctionTok{invlogit}\NormalTok{( xi\_0 }\SpecialCharTok{+}\NormalTok{ xi\_1 }\SpecialCharTok{*}\NormalTok{ X ),}
                \AttributeTok{Z =} \FunctionTok{rbinom}\NormalTok{( n, }\DecValTok{1}\NormalTok{, }\AttributeTok{prob=}\NormalTok{p ),}
                \AttributeTok{Yobs =} \FunctionTok{ifelse}\NormalTok{( Z }\SpecialCharTok{==} \DecValTok{1}\NormalTok{, Y1, Y0 ) )}
\NormalTok{  dat}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

We can then add our assignment variable to our given data as so:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{assign\_data}\NormalTok{( d2 )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 50 x 6
##           X     Y0      Y1     p     Z    Yobs
##       <dbl>  <dbl>   <dbl> <dbl> <int>   <dbl>
##  1  0.670    0.667   2.58  0.418     1   2.58 
##  2  0.371    0.314   4.57  0.348     1   4.57 
##  3  1.94     1.29    3.03  0.719     0   1.29 
##  4 -0.244    0.119 -10.0   0.224     1 -10.0  
##  5  0.00850  1.44    2.88  0.271     0   1.44 
##  6  1.41     1.14    5.02  0.600     1   5.02 
##  7 -0.864    0.461   0.802 0.134     1   0.802
##  8 -0.00533 -0.914  -1.17  0.268     0  -0.914
##  9 -0.907   -0.202   0.555 0.129     1   0.555
## 10 -0.363   -0.141   1.16  0.204     1   1.16 
## # i 40 more rows
\end{verbatim}

Note how \texttt{Yobs} is, depending on \texttt{Z}, either \texttt{Y0} or \texttt{Y1}.
Separating our our DGP and our random assignment underscores the potential outcomes framework adage of the data are what they are, and we the experimenters (or nature) is randomly assigning these whole units to various conditions and observing the consequences.

In general, we might instead put the \texttt{p\_i} part of the model in our code generating the outcomes, if we wanted to view the chance of treatment assignment as inherent to the unit (which is what we usually expect in an observational context).

\section{Finite sample performance measures}\label{finite-sample-performance-measures}

Let's generate a single dataset with our DGP from above, and run a small experiment where we actually randomize units to treatment and control:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n }\OtherTok{=} \DecValTok{100}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{442423}\NormalTok{)}
\NormalTok{dat }\OtherTok{=} \FunctionTok{gen\_data}\NormalTok{(n, }\AttributeTok{tau\_1 =} \SpecialCharTok{{-}}\DecValTok{1}\NormalTok{)}
\NormalTok{dat }\OtherTok{=} \FunctionTok{mutate}\NormalTok{( dat,}
              \AttributeTok{Z =} \DecValTok{0} \SpecialCharTok{+}\NormalTok{ (}\FunctionTok{sample}\NormalTok{( n ) }\SpecialCharTok{\textless{}=}\NormalTok{ n}\SpecialCharTok{/}\DecValTok{2}\NormalTok{),}
              \AttributeTok{Yobs =} \FunctionTok{ifelse}\NormalTok{( Z }\SpecialCharTok{==} \DecValTok{1}\NormalTok{, Y1, Y0 ) )}
\NormalTok{mod }\OtherTok{=} \FunctionTok{lm}\NormalTok{( Yobs }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Z, }\AttributeTok{data=}\NormalTok{dat )}
\FunctionTok{coef}\NormalTok{(mod)[[}\StringTok{"Z"}\NormalTok{]]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.8914992
\end{verbatim}

We can compare this to the true finite-sample ATE:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mean}\NormalTok{( dat}\SpecialCharTok{$}\NormalTok{Y1 }\SpecialCharTok{{-}}\NormalTok{ dat}\SpecialCharTok{$}\NormalTok{Y0 )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1.154018
\end{verbatim}

Our finite-population simulation would be:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rps }\OtherTok{\textless{}{-}} \FunctionTok{rerun}\NormalTok{( }\DecValTok{1000}\NormalTok{, \{}
\NormalTok{  dat }\OtherTok{=} \FunctionTok{mutate}\NormalTok{( dat,}
              \AttributeTok{Z =} \DecValTok{0} \SpecialCharTok{+}\NormalTok{ (}\FunctionTok{sample}\NormalTok{( n ) }\SpecialCharTok{\textless{}=}\NormalTok{ n}\SpecialCharTok{/}\DecValTok{2}\NormalTok{),}
              \AttributeTok{Yobs =} \FunctionTok{ifelse}\NormalTok{( Z }\SpecialCharTok{==} \DecValTok{1}\NormalTok{, Y1, Y0 ) )}
\NormalTok{  mod }\OtherTok{=} \FunctionTok{lm}\NormalTok{( Yobs }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Z, }\AttributeTok{data=}\NormalTok{dat )}
  \FunctionTok{tibble}\NormalTok{( }\AttributeTok{ATE\_hat =} \FunctionTok{coef}\NormalTok{(mod)[[}\StringTok{"Z"}\NormalTok{]],}
          \AttributeTok{SE\_hat =}\NormalTok{ arm}\SpecialCharTok{::}\FunctionTok{se.coef}\NormalTok{(mod)[[}\StringTok{"Z"}\NormalTok{]] )}
\NormalTok{  \}) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{bind\_rows}\NormalTok{()}

\NormalTok{rps }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{summarise}\NormalTok{( }\AttributeTok{EATE\_hat =} \FunctionTok{mean}\NormalTok{( ATE\_hat ),}
                   \AttributeTok{SE =} \FunctionTok{sd}\NormalTok{( ATE\_hat ),}
                   \AttributeTok{ESE\_hat =} \FunctionTok{mean}\NormalTok{( SE\_hat ) )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1 x 3
##   EATE_hat    SE ESE_hat
##      <dbl> <dbl>   <dbl>
## 1     1.16 0.248   0.307
\end{verbatim}

We are simulating on a single dataset.
In particular, our set of potential outcomes is entirely fixed; the only source of randomness (and thus the randomness behind our SE) is the random assignment.
Now this opens up some room for critique: what if our single dataset is non-standard?

Our super-population simulation would be, by contrast:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rps\_sup }\OtherTok{\textless{}{-}} \FunctionTok{rerun}\NormalTok{( }\DecValTok{1000}\NormalTok{, \{}
\NormalTok{  dat }\OtherTok{=} \FunctionTok{gen\_data}\NormalTok{(n)}
\NormalTok{  dat }\OtherTok{=} \FunctionTok{mutate}\NormalTok{( dat,}
              \AttributeTok{Z =} \DecValTok{0} \SpecialCharTok{+}\NormalTok{ (}\FunctionTok{sample}\NormalTok{( n ) }\SpecialCharTok{\textless{}=}\NormalTok{ n}\SpecialCharTok{/}\DecValTok{2}\NormalTok{),}
              \AttributeTok{Yobs =} \FunctionTok{ifelse}\NormalTok{( Z }\SpecialCharTok{==} \DecValTok{1}\NormalTok{, Y1, Y0 ) )}
\NormalTok{  mod }\OtherTok{=} \FunctionTok{lm}\NormalTok{( Yobs }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Z, }\AttributeTok{data=}\NormalTok{dat )}
  \FunctionTok{tibble}\NormalTok{( }\AttributeTok{ATE\_hat =} \FunctionTok{coef}\NormalTok{(mod)[[}\StringTok{"Z"}\NormalTok{]],}
          \AttributeTok{SE\_hat =}\NormalTok{ arm}\SpecialCharTok{::}\FunctionTok{se.coef}\NormalTok{(mod)[[}\StringTok{"Z"}\NormalTok{]] )}
\NormalTok{  \}) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{bind\_rows}\NormalTok{()}

\NormalTok{rps\_sup }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{summarise}\NormalTok{( }\AttributeTok{EATE\_hat =} \FunctionTok{mean}\NormalTok{( ATE\_hat ),}
                   \AttributeTok{SE =} \FunctionTok{sd}\NormalTok{( ATE\_hat ),}
                   \AttributeTok{ESE\_hat =} \FunctionTok{mean}\NormalTok{( SE\_hat ))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1 x 3
##   EATE_hat    SE ESE_hat
##      <dbl> <dbl>   <dbl>
## 1     1.00 0.381   0.378
\end{verbatim}

First, note our superpopulation simulation is not biased for the superpopulation ATE.
Also note the true SE is larger than our finite-sample simulation; this is because part of the uncertainty in our estimator is the uncertainty of whether our sample is representative of the superpopulation.

Finally, this clarifies that our linear regression estimator is estimating standard errors assuming a superpopulation model.
The true finite sample standard error is less than the expected estimated error: from a finite sample perspective, our estimator is giving overly conservative uncertainty estimates.
(This discrepancy is often called the correlation of potential outcomes problem.)

\section{Nested finite simulation procedure}\label{nested-finite-simulation-procedure}

We just saw a difference between a specific, single, finite-sample dataset and a superpopulation.
What if we wanted to know if this phenomenon was more general across a set of datasets?
This question can be levied more broadly: if we run a simulation on a single dataset, this is even more narrow than running on a single scenario: if we compare methods and find one is superior to another for our single dataset, how do we know this is not an artifact of some specific characteristic of \emph{that data} and not a general phenomonen at all?

One way forward is to run a nested simulation, where we generate a series of finite sample datasets, and then for each dataset run a small simulation.
We then calculate the expected finite sample performance across the datasets.
One could almost think of the datasets themselves as a ``factor'' in our multifactor experiment.
This is what we did in {[}CITE estimands paper{]}

Borrowing from the simulation appendix of {[}CITE estimands paper{]}, repeat \(R\) times:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Generate a dataset using a particular DGP. This data generation is the ``sampling step'' for a superpopulation (SP) framework. The DGP represents an inﬁnite superpopulation. Each dataset includes, for each observation, the potential outcome under treatment or control.
\item
  Record the true finite-sample ATE, both person and site weighted.
\item
  Then, three times, do a finite simulation as follows:
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  Randomize units to treatment and control.
\item
  Calculate the corresponding observed outcomes.
\item
  Analyze the results using the methods of interest, recording both the point estimate and estimated standard error for each.
\end{enumerate}

Having only three trials will give a poor estimate of within-dataset variability for each dataset, but the average across the
\(R\) datasets in a given scenario gives a reasonable estimate of expected variability across datasets of the type we would see given the scenario parameters.

To demonstrate we first make a mini-finite sample driver:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{one\_finite\_run }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{( }\AttributeTok{R0 =} \DecValTok{3}\NormalTok{, }\AttributeTok{n =} \DecValTok{100}\NormalTok{, ... ) \{}
\NormalTok{  dat }\OtherTok{=} \FunctionTok{gen\_data}\NormalTok{( }\AttributeTok{n =}\NormalTok{ n, ... )}
\NormalTok{  rps }\OtherTok{\textless{}{-}} \FunctionTok{rerun}\NormalTok{( R0, \{}
\NormalTok{         dat }\OtherTok{=} \FunctionTok{mutate}\NormalTok{( dat,}
                    \AttributeTok{Z =} \DecValTok{0} \SpecialCharTok{+}\NormalTok{ (}\FunctionTok{sample}\NormalTok{( n ) }\SpecialCharTok{\textless{}=}\NormalTok{ n}\SpecialCharTok{/}\DecValTok{2}\NormalTok{),}
                    \AttributeTok{Yobs =} \FunctionTok{ifelse}\NormalTok{( Z }\SpecialCharTok{==} \DecValTok{1}\NormalTok{, Y1, Y0 ) )}
\NormalTok{        mod }\OtherTok{=} \FunctionTok{lm}\NormalTok{( Yobs }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Z, }\AttributeTok{data=}\NormalTok{dat )}
        \FunctionTok{tibble}\NormalTok{( }\AttributeTok{ATE\_hat =} \FunctionTok{coef}\NormalTok{(mod)[[}\StringTok{"Z"}\NormalTok{]],}
                \AttributeTok{SE\_hat =}\NormalTok{ arm}\SpecialCharTok{::}\FunctionTok{se.coef}\NormalTok{(mod)[[}\StringTok{"Z"}\NormalTok{]] )}
\NormalTok{    \}) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{bind\_rows}\NormalTok{()}
\NormalTok{  rps}\SpecialCharTok{$}\NormalTok{ATE }\OtherTok{=} \FunctionTok{mean}\NormalTok{( dat}\SpecialCharTok{$}\NormalTok{Y1 }\SpecialCharTok{{-}}\NormalTok{ dat}\SpecialCharTok{$}\NormalTok{Y0 )}
\NormalTok{  rps}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

This driver also stores the finite sample ATE for future reference:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{one\_finite\_run}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: `rerun()` was deprecated in purrr 1.0.0.
## i Please use `map()` instead.
##   # Previously
## rerun(3, {
## dat = mutate(dat, Z = 0 + (sample(n) <= n / 2),
## Yobs = ifelse(Z == 1, Y1, Y0))
## mod = lm(Yobs ~ Z, data = dat)
## tibble(ATE_hat = coef(mod)[["Z"]], SE_hat = arm::se.coef(
## mod)[["Z"]])
## })
## 
##   # Now
## map(1:3, ~ {
## dat = mutate(dat, Z = 0 + (sample(n) <= n / 2),
## Yobs = ifelse(Z == 1, Y1, Y0))
## mod = lm(Yobs ~ Z, data = dat)
## tibble(ATE_hat = coef(mod)[["Z"]], SE_hat = arm::se.coef(
## mod)[["Z"]])
## })
## This warning is displayed once every 8 hours.
## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated.
\end{verbatim}

\begin{verbatim}
## # A tibble: 3 x 3
##   ATE_hat SE_hat   ATE
##     <dbl>  <dbl> <dbl>
## 1   0.348  0.421 0.768
## 2   1.32   0.472 0.768
## 3   1.17   0.549 0.768
\end{verbatim}

We then run a bunch of finite runs.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{runs }\OtherTok{\textless{}{-}} \FunctionTok{rerun}\NormalTok{( }\DecValTok{500}\NormalTok{, }\FunctionTok{one\_finite\_run}\NormalTok{() ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{bind\_rows}\NormalTok{( }\AttributeTok{.id =} \StringTok{"runID"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: `rerun()` was deprecated in purrr 1.0.0.
## i Please use `map()` instead.
##   # Previously
##   rerun(500, one_finite_run())
## 
##   # Now
##   map(1:500, ~ one_finite_run())
## This warning is displayed once every 8 hours.
## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated.
\end{verbatim}

We use \texttt{.id} because we will need to separate out each finite run and analyze separately, and then aggregate.

Each finite run is a very noisy simulation for a fixed dataset.
This means when we calculate performance measures we have to be careful to avoid bias in the calculations; in particular, we need to focus on estimating \(SE^2\) across the finite runs, not \(SE\), to avoid the bias caused by having a few observations with every estimate.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fruns }\OtherTok{\textless{}{-}}\NormalTok{ runs }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{group\_by}\NormalTok{( runID ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{( }\AttributeTok{EATE\_hat =} \FunctionTok{mean}\NormalTok{( ATE\_hat ),}
             \AttributeTok{SE2 =} \FunctionTok{var}\NormalTok{( ATE\_hat ),}
             \AttributeTok{ESE\_hat =} \FunctionTok{mean}\NormalTok{( SE\_hat ),}
             \AttributeTok{.groups =} \StringTok{"drop"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

And then we aggregate our finite sample runs:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{res }\OtherTok{\textless{}{-}}\NormalTok{ fruns }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{( }\AttributeTok{EEATE\_hat =} \FunctionTok{mean}\NormalTok{( EATE\_hat ),}
             \AttributeTok{EESE\_hat =} \FunctionTok{sqrt}\NormalTok{( }\FunctionTok{mean}\NormalTok{( ESE\_hat}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{ ) ),}
             \AttributeTok{ESE =} \FunctionTok{sqrt}\NormalTok{( }\FunctionTok{mean}\NormalTok{( SE2 ) ) ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{( }\AttributeTok{calib =} \DecValTok{100} \SpecialCharTok{*}\NormalTok{ EESE\_hat }\SpecialCharTok{/}\NormalTok{ ESE )}

\NormalTok{res}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1 x 4
##   EEATE_hat EESE_hat   ESE calib
##       <dbl>    <dbl> <dbl> <dbl>
## 1     0.996    0.380 0.331  115.
\end{verbatim}

We see our expected standard error estimate is, across the collection of finite sample scenarios all sharing a similar parent superpopulation DGP, 15\% too large for the true expected finite-sample standard error.

We need to keep the squaring. If we look at the SEs themselves, we have further apparent bias due to our \emph{estimated} \texttt{ESE\_hat} being so unstable due to too few observations:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mean}\NormalTok{( }\FunctionTok{sqrt}\NormalTok{( fruns}\SpecialCharTok{$}\NormalTok{SE2 ) )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.2944556
\end{verbatim}

We can use our collection of mini-finite-sample runs to estimate superpopulation quantities as well.
Given that the simulation datasets are i.i.d. draws, we can simply take expectations across all our simulations.
The only concern is our estimates of MCSE will be off due to the clustering in our simulation runs.

Here we calculate superpopulation performance measures (both with the squared SE and without; we prefer the squared version):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{runs }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{( }\AttributeTok{EATE\_hat =} \FunctionTok{mean}\NormalTok{( ATE\_hat ),}
             \AttributeTok{SE\_true =} \FunctionTok{sd}\NormalTok{( ATE\_hat ),}
             \AttributeTok{SE\_hat =} \FunctionTok{mean}\NormalTok{( SE\_hat ),}
             \AttributeTok{SE2\_true =} \FunctionTok{var}\NormalTok{( ATE\_hat ),}
             \AttributeTok{SE2\_hat =} \FunctionTok{mean}\NormalTok{( SE\_hat}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{ ) ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{pivot\_longer}\NormalTok{( }\AttributeTok{cols =} \FunctionTok{c}\NormalTok{(SE\_true}\SpecialCharTok{:}\NormalTok{SE2\_hat ),}
                \AttributeTok{names\_to =} \FunctionTok{c}\NormalTok{( }\StringTok{"estimand"}\NormalTok{, }\StringTok{".value"}\NormalTok{ ),}
                \AttributeTok{names\_sep =}\StringTok{"\_"}\NormalTok{ ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{( }\AttributeTok{inflate =} \DecValTok{100} \SpecialCharTok{*}\NormalTok{ hat }\SpecialCharTok{/}\NormalTok{ true )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 2 x 5
##   EATE_hat estimand  true   hat inflate
##      <dbl> <chr>    <dbl> <dbl>   <dbl>
## 1    0.996 SE       0.389 0.377    96.9
## 2    0.996 SE2      0.151 0.142    93.9
\end{verbatim}

\chapter{Simulations as evidence}\label{simulations-as-evidence}

We began this book with an acknowledgement that simulation is fraught with the potential for misuse: \emph{simulations are doomed to succeed}.
We close by reiterating this point, and also disussing several ways researchers might argue for their simulations being more useful than typical.

In particular a researcher might do any of the following

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Use extensive multi-factor simulations
\item
  Beat them at their own game. Generate simulations based on prior literature.
\item
  Build calibrated simulations
\end{enumerate}

\section{Use extensive multi-factor simulations}\label{use-extensive-multi-factor-simulations}

``If a single simulation is not convincing, use more of them,'' is one principle a reseacher might take.
By conducting extensive multifactor simulations, once can explore a large space of possible data generating scenarios.
If, across the full range of scenarios, a general story bears out, then perhaps that will be more convincing than a narrower range.

Of course, the critic will claim that some aspect that is not varying is the real culprit.
If this is unrealistic, then the findings, across the board, may be less relevant.
Thus, pick the factors one varies with care.

\section{Beat them at their own game}\label{beat-them-at-their-own-game}

If a prior paper uses a simulation to make a case, one approach is to replicate that simulation, adding in the new estimator one wants to evaluate.
This makes it (more) clear that you are not fishing: you are using something established in the literature as a published benchmark.
By constraining oneself to published simulations, one has less wiggle room to cherry pick a data generating process that works the way you want.

\section{Calibrated simulations}\label{calibrated-simulations}

A practice in increasing vogue is to generate \emph{calibrated simulations}.
These are simulations tailored to a specific applied contexts, with the results of the simulation studies designed to more narrowly inform what assumptions and structures are necessary in order to make progress in that context.

Often these simulations are built out of existing data.
For example, one might sample, with replacement, from the covariate distribution of an actual dataset so that the distribution of covariates is authentic in how the covariates are distributed and, more importantly, how they co-relate.

The problem with this approach is one still needs to generate a ground truth to assess how well the estimators work in practice.
It is very easy to accidently put a very simple model in place for this component, thus making a calibrated simulation quite naive in the very way that counts.

The potential outcomes framework provides a natural way of generating calibrated simulations \citep{Kern_calibrated}.
Calbirated simulations are simulations tailored to specific real-world scenarios, to maximize their face validity as being representative of something we would see in practice.
One way to generate a calibrated simulation is to use an existing datasets to generate plausible scenarios.

One way to do this with potential outcomes is to take an existing randomized experiment or observational study and impute all the missing potential outcomes under some specific scheme.
This fully defines the sample of interest and thus any target parameters, such as a measure of heterogeneity, are then known.
We will then synthetically, and repeatedly, randomize and ``observe'\,' outcomes to be analyzed with the methods we are testing.
We could also resample from our dataset to generate datasets of different size, or to have a superpopulation target as our estimand.

The key feature is the imputation step.
One baseline method one can use is to generate a matched-pairs dataset by, for each unit, finding a close match given all the demographic and other covariate information of the sample. We then use the matched unit as the imputed potential outcome.\\
By doing this (with replacement) for all units we can generate a fully imputed dataset which we then use as our population.
This can preserve complex relationships in the data that are not model dependent.
In particular, if outcomes tend to be coarsely defined (e.g., on an integer scale) or have specific clumps (such as zero-inflation or rounding), this structure will be preserved.

One concern with this approach is the noise in the matching will in general dilute the structure of the treatment effect.\\
This is akin to measurement error diluting found relationships in linear models.
We can then sharpen these relationships towards a given model by first imputing missing outcomes using a specified model, and then matching on all units including the imputed potential outcome.
This is not a data analysis strategy, but instead a method of generating synthetic data that both has a given structure of interest and also remains faithful to the idiosyncrasies of an actual dataset.

A second approach that allows for varying the level of a systematic effect is to specify the treatment effect model and impute treatment outcomes for all control units.\\
Then the complex structure between covariates and \(Y(0)\) would be perfectly preserved.
Unfortunately, this would give 0 idiosyncratic treatment variation (unit-to-unit variation in the \(\tau_i\) that is not explained by a model).
To add in idiosyncratic variation we would then need to generate a distribution of perturbations and add these to the imputed outcomes just as an error term in a regression model.

Regardless, once a fully observed sample has been obtained we can investigate several aspects of our estimators as above.

\chapter{The Parametric bootstrap}\label{the-parametric-bootstrap}

An inference procedure very much connected to simulation studies is the parametric bootstrap.
The parametric bootstrap is a bootstrap technique designed to obtain standard error estimates for an estimated parametric model.
It can do better than the case-wise bootstrap in some circumstances, usually when there is need to avoid the discrete, chunky nature of a casewise bootstrap (which will only give values that exist in the original dataset).

For a parametric bootstrap, the core idea is to fit a given model to actual data, and then take the parameters we estimate from that model as the DGP parameters in a simulation study.
The parametric bootstrap is a simulation study for a specific scenario, and our goal is to assess how variable (and, possibly, biased) our estimator is for this specific scenario.
If the behavior of our estimator in our simulated scenario is similar to what it would be under repeated trials in the real world, then our bootstrap answers will be informative as to how well our original estimator performs in practice.
This is the bootstrap principle, or analogy with an additional assumption that the real-world is effectively well specified as the parameteric model we are fitting.

In particular we do the following:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  generate data from a model with coefficients as estimated on the original data.
\item
  repeatedly estimate our target quantity on a series of synthetic data sets, all generated from this model.
\item
  examine this collection of estimates to assess the character of the estimates themselves, i.e.~how much they vary, whether we are systematically estimating too high or too low, and so forth.
\item
  The variance and bias of our estimates in our simulation is probably like the actual variance and bias of our original estimate (this is precisely the bootstrap analogy).
\end{enumerate}

A key feature of the parametric bootstrap is it is not, generally, a multifactor simulation experiment.
We fit our model to the data, and use our best estimate of the world, as given by the fit model, to generate our data.
This means we generally want to simulate in contexts that are (mostly) \emph{pivotal}, meaning the distribution of our test statistic or point estimate is relatively stable across different scenarios.
In other words, we want the uncertainty of our estimator to not heavily depend on the exact parameter values we use in our simulation, so that if we are simulating with incorrect parameters our bootstrap analogy will still hold.

Often, to achieve a reasonable claim of being pivotal, we will focus on standardized statistics, such as the \(t\)-statistic of

\[ t = \frac{est}{\widehat{SE}} \]
It is more common for the distribution of a standardized test statistic to have a canonical distribution across scenarios than an absolute estimate.

\section{Air conditioners: a stolen case study}\label{air-conditioners-a-stolen-case-study}

Following the case study presented in {[}CITE bootstrap book{]}, consider some failure times of air conditioning units:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dat }\OtherTok{=} \FunctionTok{c}\NormalTok{( }\DecValTok{3}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{7}\NormalTok{, }\DecValTok{18}\NormalTok{, }\DecValTok{43}\NormalTok{, }\DecValTok{85}\NormalTok{, }\DecValTok{91}\NormalTok{, }\DecValTok{98}\NormalTok{, }\DecValTok{100}\NormalTok{, }\DecValTok{130}\NormalTok{, }\DecValTok{230}\NormalTok{, }\DecValTok{487}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

We are interested in the log of the average failure time:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n }\OtherTok{=} \FunctionTok{length}\NormalTok{(dat)}
\NormalTok{y.bar }\OtherTok{=} \FunctionTok{mean}\NormalTok{(dat)}
\NormalTok{theta.hat }\OtherTok{=} \FunctionTok{log}\NormalTok{( y.bar )}

\FunctionTok{c}\NormalTok{( }\AttributeTok{n =}\NormalTok{ n, }\AttributeTok{y.bar =}\NormalTok{ y.bar, }\AttributeTok{theta.hat =}\NormalTok{ theta.hat )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##          n      y.bar  theta.hat 
##  12.000000 108.083333   4.682903
\end{verbatim}

We are interested in this because we are modeling the failure time of the air conditioners with an exponential distribution.
This means we will generate new failure times with an exponential distribution:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{reps }\OtherTok{=} \FunctionTok{replicate}\NormalTok{( }\DecValTok{10000}\NormalTok{, \{}
\NormalTok{    smp }\OtherTok{=} \FunctionTok{rexp}\NormalTok{(n, }\DecValTok{1}\SpecialCharTok{/}\NormalTok{y.bar)}
    \FunctionTok{log}\NormalTok{( }\FunctionTok{mean}\NormalTok{( smp ) )}
\NormalTok{\})}

\NormalTok{res\_par }\OtherTok{=} \FunctionTok{tibble}\NormalTok{( }
  \AttributeTok{bias.hat =} \FunctionTok{mean}\NormalTok{( reps ) }\SpecialCharTok{{-}}\NormalTok{ theta.hat,}
  \AttributeTok{var.hat =} \FunctionTok{var}\NormalTok{( reps ),}
  \AttributeTok{CIlog\_low =}\NormalTok{ theta.hat }\SpecialCharTok{+}\NormalTok{ bias.hat }\SpecialCharTok{{-}} \FunctionTok{sqrt}\NormalTok{(var.hat) }\SpecialCharTok{*} \FunctionTok{qnorm}\NormalTok{(}\FloatTok{0.975}\NormalTok{),}
  \AttributeTok{CIlog\_high =}\NormalTok{ theta.hat }\SpecialCharTok{+}\NormalTok{ bias.hat }\SpecialCharTok{{-}} \FunctionTok{sqrt}\NormalTok{(var.hat) }\SpecialCharTok{*} \FunctionTok{qnorm}\NormalTok{(}\FloatTok{0.025}\NormalTok{),}
  \AttributeTok{CI\_low =} \FunctionTok{exp}\NormalTok{( CIlog\_low ),}
  \AttributeTok{CI\_high =} \FunctionTok{exp}\NormalTok{( CIlog\_high ) )}
\NormalTok{res\_par}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1 x 6
##   bias.hat var.hat CIlog_low CIlog_high CI_low
##      <dbl>   <dbl>     <dbl>      <dbl>  <dbl>
## 1  -0.0420  0.0856      4.07       5.21   58.4
## # i 1 more variable: CI_high <dbl>
\end{verbatim}

Note how we are, as usual, in our standard simulation framework of repeatidly (1) generating data and (2) analyzing the simulated data.
Nothing is changed.

The nonparametric, or case-wise, bootstrap (this is what people normally mean when they say bootstrap) would look like this:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{reps }\OtherTok{=} \FunctionTok{replicate}\NormalTok{( }\DecValTok{10000}\NormalTok{, \{}
\NormalTok{    smp }\OtherTok{=} \FunctionTok{sample}\NormalTok{( dat, }\AttributeTok{replace=}\ConstantTok{TRUE}\NormalTok{ )}
    \FunctionTok{log}\NormalTok{( }\FunctionTok{mean}\NormalTok{( smp ) )}
\NormalTok{\})}

\NormalTok{res\_np }\OtherTok{=} \FunctionTok{tibble}\NormalTok{( }
  \AttributeTok{bias.hat =} \FunctionTok{mean}\NormalTok{( reps ) }\SpecialCharTok{{-}}\NormalTok{ theta.hat,}
  \AttributeTok{var.hat =} \FunctionTok{var}\NormalTok{( reps ),}
  \AttributeTok{CIlog\_low =}\NormalTok{ theta.hat }\SpecialCharTok{+}\NormalTok{ bias.hat }\SpecialCharTok{{-}} \FunctionTok{sqrt}\NormalTok{(var.hat) }\SpecialCharTok{*} \FunctionTok{qnorm}\NormalTok{(}\FloatTok{0.975}\NormalTok{),}
  \AttributeTok{CIlog\_high =}\NormalTok{ theta.hat }\SpecialCharTok{+}\NormalTok{ bias.hat }\SpecialCharTok{{-}} \FunctionTok{sqrt}\NormalTok{(var.hat) }\SpecialCharTok{*} \FunctionTok{qnorm}\NormalTok{(}\FloatTok{0.025}\NormalTok{),}
  \AttributeTok{CI\_low =} \FunctionTok{exp}\NormalTok{( CIlog\_low ),}
  \AttributeTok{CI\_high =} \FunctionTok{exp}\NormalTok{( CIlog\_high ) )}


\FunctionTok{bind\_rows}\NormalTok{( }\AttributeTok{parametric =}\NormalTok{ res\_par, }
           \AttributeTok{casewise =}\NormalTok{ res\_np, }\AttributeTok{.id =} \StringTok{"method"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{( }\AttributeTok{length =}\NormalTok{ CI\_high }\SpecialCharTok{{-}}\NormalTok{ CI\_low )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 2 x 8
##   method     bias.hat var.hat CIlog_low CIlog_high
##   <chr>         <dbl>   <dbl>     <dbl>      <dbl>
## 1 parametric  -0.0420  0.0856      4.07       5.21
## 2 casewise    -0.0651  0.132       3.90       5.33
## # i 3 more variables: CI_low <dbl>,
## #   CI_high <dbl>, length <dbl>
\end{verbatim}

This is \emph{also} a simulation: our data generating process is a bit more vague, however, as we are just resampling the data.
This means our estimands are not as clearly specified.
For example, in our parameteric approach, our target parameter is known to be true.
In the case-wise, the connection between our DGP and the parameter \texttt{theta.hat} is less explicit.

Overall, in this case, our parametric bootstrap can model the tail behavior of an exponential better than case-wise.
Especially considering the small number of observations, it is going to be a more faithful representation of what we are doing--provided our model is well specified for the real world distribution.

\chapter{Appendix: Coding tidbits}\label{on_functions}

This chapter is not about simulation, but does have a few tips and tricks regarding coding that are worth attending to.

\section{Other ways of repeating yourself}\label{repeating_oneself}

There are several ways to call a bit of code (e.g., \texttt{one\_run()} over and over).
We have seen \texttt{map()} in the main part of the textbook.

In the past, there was a tidyverse method called \texttt{rerun()}, but it is currently out of favor.
Originally, \texttt{rerun()} did exactly that: you gave it a number and a block of code, and it would rerun the block of code that many times, giving you the results as a list.

Another, more classic, way to repeat oneself is to use an R function called \texttt{replicate}; \texttt{rerun()} and \texttt{replicate()} are near equivalents.
\texttt{replicate()} does what its name suggests---it replicates the result of an expression a specified number of times. The first argument is the number of times to replicate and the next argument is an expression (a short piece of code to run). A further argument, \texttt{simplify} allows you to control how the results are structured. Setting \texttt{simplify\ =\ FALSE} returns the output as a list (just like \texttt{rerun()}.

\section{Default arguments for functions}\label{default_arguments}

To generate code both easy to use and configure, use default arguments.
For example,

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_function }\OtherTok{=} \ControlFlowTok{function}\NormalTok{( }\AttributeTok{a =} \DecValTok{10}\NormalTok{, }\AttributeTok{b =} \DecValTok{20}\NormalTok{ ) \{}
     \DecValTok{100} \SpecialCharTok{*}\NormalTok{ a }\SpecialCharTok{+}\NormalTok{ b}
\NormalTok{\}}

\FunctionTok{my\_function}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1020
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{my\_function}\NormalTok{( }\DecValTok{5}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 520
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{my\_function}\NormalTok{( }\AttributeTok{b =} \DecValTok{5}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1005
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{my\_function}\NormalTok{( }\AttributeTok{b =} \DecValTok{5}\NormalTok{, }\AttributeTok{a =} \DecValTok{1}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 105
\end{verbatim}

We can still call \texttt{my\_function()} when we don't know what the arguments are, but then when we know more about the function, we can specify things of interest.
Lots of R commands work exactly this way, and for good reason.

Especially for code to generate random datasets, default arguments can be a lifesaver as you can then call the method before you know exactly what everything means.

For example, consider the \texttt{blkvar} package that has some code to generate blocked randomized datasets.
We might locate a promising method, and type it in:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{( blkvar )}
\FunctionTok{generate\_blocked\_data}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Error in generate_blocked_data(): argument "n_k" is missing, with no default
\end{verbatim}

That didn't work, but let's provide some block sizes and see what happens:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{generate\_blocked\_data}\NormalTok{( }\AttributeTok{n\_k =} \FunctionTok{c}\NormalTok{( }\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{ ) )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    B         Y0       Y1
## 1 B1  1.7317207 5.110982
## 2 B1 -0.1608224 5.174334
## 3 B1  1.7023413 5.233891
## 4 B2  0.2529889 5.939471
## 5 B2 -2.0312750 3.780810
\end{verbatim}

Nice! We see that we have a block ID and the control and treatment potential outcomes. We also don't see a random assignment variable, so that tells us we probably need some other methods as well.
But we can play with this as it stands right away.

Next we can see that there are many things we might tune:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{args}\NormalTok{( generate\_blocked\_data )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## function (n_k, sigma_alpha = 1, sigma_beta = 0, beta = 5, sigma_0 = 1, 
##     sigma_1 = 1, corr = 0.5, exact = FALSE) 
## NULL
\end{verbatim}

The documentation will tell us more, but if we just need some sample data, we can quickly assess our method before having to do much reading and understanding.
Only once we have identified what we need do we have to turn to the documentation itself.

\section{Testing and debugging code in your scripts}\label{testing-and-debugging-code-in-your-scripts}

If you have an extended script with a list of functions, you might have a lot of code that runs each function in turn, so you can easily remind yourself of what it does, or what the output looks like.
One way to keep this code around, but not have it run all the time when you run your script, is to put the code inside a ``FALSE block,'' that might look like so:

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{if}\NormalTok{ ( }\ConstantTok{FALSE}\NormalTok{ ) \{}
\NormalTok{  res }\OtherTok{\textless{}{-}} \FunctionTok{my\_function}\NormalTok{( }\DecValTok{10}\NormalTok{, }\DecValTok{20}\NormalTok{, }\DecValTok{30}\NormalTok{ )}
\NormalTok{  res}
  \CommentTok{\# Some notes as to what I want to see.}
  
  \FunctionTok{sd}\NormalTok{( res )}
  \CommentTok{\# This should be around 20}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

You can then, when looking at the script, paste the code inside the block into the console when you want to run it.
If you source the script, however, it will not run at all, and thus your code will source faster and not print out any extraneous output.

\section{Keep multiple files of code}\label{keep-multiple-files-of-code}

Simulations have two general phases: generate your results and analyze your results.
The ending of the first phase should be to save the generated results.
The beginning of the second phase should then be to load the results from a file and analyze them.
These phases can be in a seperate `.R' files
This allows for easily changing how one analyzes an experiment
without re-running the entire thing.

\section{The source command and keeping things organized}\label{the-source-command-and-keeping-things-organized}

Once you have your multifactor simulation, if it is a particularly complex one, you will likely have three general collections of code:

\begin{itemize}
\tightlist
\item
  Code for generating data
\item
  Code for analyzing data
\item
  Code for running a single simulation scenario
\end{itemize}

If each of these pieces is large and complex, you might consider putting them in three different \texttt{.R} files.
Then, in your primary simulation, you would source these files.
E.g.,

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{source}\NormalTok{( }\StringTok{"pack\_data\_generators.R"}\NormalTok{ )}
\FunctionTok{source}\NormalTok{( }\StringTok{"pack\_estimators.R"}\NormalTok{ )}
\FunctionTok{source}\NormalTok{( }\StringTok{"pack\_simulation\_support.R"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

You might also have \texttt{pack\_simulation\_support.R} source the other two files, and then source the single simulation support file in your primay file.

One reason for putting code in individual files is you can then have testing code in each of your files (in False blocks, like described above), testing each of your components.
Then, when you are not focused on that component, you don't have to look at that testing code.

Another good reason for this type of modular organizing is you can then have a variety of data generators, forming a library of options.
You can then easily create different simulations that use different pieces, in a larger project.

For example, in one recent simulation project on estimators for an Instrumental Variable analysis, we had several different data generators for generating different types of compliance patterns (IVs are often used to handle noncompliance in randomized experiments).
Our data generation code file then had several methods:

\begin{verbatim}
> ls()
[1] "describe_sim_data"  "make_dat"           "make.dat.1side"     
[4] "make.dat.1side.old" "make.dat.orig"      "make.dat.simple"
[7] "make.dat.tuned"     "rand.exp"           "summarize_sim_data"
\end{verbatim}

The describe and summarize methods printed various statistics about a sample dataset; these are used to debug and understand how the generated data looks.
We also had a variety of different DGP methods because we had different versions that came up as we were trying to chase down errors in our estimators and understand strange behavior.

Putting the estimators in a different file also had a nice additional purpose: we also had an applied data example in our work, and we could simply source that file and use those estimators on our actual data.
This ensured our simulation and applied analysis were perfectly aligned in terms of the estimators we were using.
Also, as we debugged our estimators and tweaked them, we immediately could re-run our applied analysis to update those results with minimal effort.

Modular programming is key.

\section{Debugging with browser}\label{debugging-with-browser}

Consider the code taken from a simulation:

\begin{Shaded}
\begin{Highlighting}[]
    \ControlFlowTok{if}\NormalTok{ ( }\FunctionTok{any}\NormalTok{( }\FunctionTok{is.na}\NormalTok{( rs}\SpecialCharTok{$}\NormalTok{estimate ) ) ) \{}
        \FunctionTok{browser}\NormalTok{()}
\NormalTok{    \}}
\end{Highlighting}
\end{Shaded}

The \texttt{browser()} command stops your code and puts you in an interactive console where you can look at different objects and see what is happening.
Having it triggered when something bad happens (in this case when a set of estimates has an unexpected NA) can help untangle what is driving a rare event.

\chapter{Further readings and resources}\label{further-readings-and-resources}

We close with a list of things of interest we have discovered while writing this text.

\begin{itemize}
\item
  \href{https://doi.org/10.1002/sim.8086}{Morris, White, \& Crowther (2019)}. Using simulation studies to evaluate statistical methods.
\item
  High-level simulation design considerations.
\item
  Details about performance criteria calculations.
\item
  Stata-centric.
\item
  \href{https://github.com/philchalmers/SimDesign/wiki}{SimDesign} R package (Chalmers, 2019)
\item
  Tools for building generic simulation workflows.
\item
  \href{http://www.tqmp.org/RegularArticles/vol16-4/p248/}{Chalmers \& Adkin (2019)}. Writing effective and reliable Monte Carlo simulations with the SimDesign package.
\item
  \href{https://declaredesign.org/}{DeclareDesign} (Blair, Cooper, Coppock, \& Humphreys)
\item
  Specialized suite of R packages for simulating research designs.
\item
  Design philosophy is very similar to ``tidy'' simulation approach.
\item
  \href{https://meghapsimatrix.github.io/simhelpers/index.html}{SimHelpers} R package (Joshi \& Pustejovsky, 2020)
\item
  Helper functions for calculating performance criteria.
\item
  Includes Monte Carlo standard errors.
\end{itemize}

  \bibliography{book.bib,packages.bib}

\end{document}
