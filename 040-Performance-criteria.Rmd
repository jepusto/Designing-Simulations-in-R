---
output:
  pdf_document: default
  html_document: default
---

```{r include = FALSE}
library(tidyverse)
options(list(dplyr.summarise.inform = FALSE))
source("case_study_code/gen_cluster_RCT_rev.R")

cluster_RCT_res <- 
  readRDS("results/cluster_RCT_simulation.rds") %>%
  mutate(
    method = recode(method, Agg = "Aggregation", LR = "Linear regression", MLM = "Multilevel model")
  )

```

<!-- JEP TO DO:  -->
<!-- Be sure to define E() and V() operators.  -->
<!-- Use numbered equations for important stuff. -->

# Performance criteria {#performance-criteria}

$$
\E(X) = \E[\E(X | Y)], \qquad \Var(X) = \Var[\E(X | Y)] + \E[\Var(X | Y)]
$$

Once we run a simulation, we end up with a pile of results to sort through. 
For example, Figure \@ref(fig:CRT-ATE-hist) depicts the distribution of average treatment effect estimates from the cluster-randomized experiment simulation, which we generated in Chapter \@ref(running-the-simulation-process). 
There are three different estimators 1000 replications of each.
Each histogram is an approximation of the _sampling distribution_ of the estimator, meaning its distribution across repetitions of a data-generating process.

```{r CRT-ATE-hist}
#| fig.width: 8
#| fig.height: 3
#| echo: false
#| message: false
#| fig.cap: "Distribution of average treatment effect estimates from a cluster-randomized trial with a true average treatment effect of 0.3."

cluster_RCT_mean <- 
  cluster_RCT_res %>%
  summarize(ATE_hat = mean(ATE_hat), .by = method)

ggplot(cluster_RCT_res) + 
  aes(ATE_hat, fill = method) +
  geom_vline(xintercept = 0.3, linetype = "dashed") + 
  geom_vline(data = cluster_RCT_mean, aes(xintercept = ATE_hat, color = method)) + 
  geom_point(data = cluster_RCT_mean, aes(x = ATE_hat, y = -1, color = method)) + 
  geom_histogram(alpha = 0.5) + 
  facet_wrap(~ method, nrow = 1) + 
  theme_minimal() + 
  labs(x = "Average treatment effect estimate", y = "") + 
  scale_y_continuous(labels = NULL) + 
  theme(legend.position = "none")
```

With results such as these, the question before is now: How do we evaluate how well these procedures worked? And, if we are comparing several different estimators, how do we determine which ones work better or worse than others? In this chapter, we look at a variety of __performance criteria__ that can answer these questions. 
Performance criteria are methods of summarizing a sampling distribution---formulas for boiling down the thousands of values from the simulation in order to describe how an estimator or data analysis procedure behaves on average (or in the long run) if we could repeat the data-generating process an infinite number of times.
For example, the bias of an estimator is the difference between the average value of the estimator and the corresponding target parameter. Bias measures the central tendency of the sampling distribution, capturing how far off, on average, the estimator would be from the true parameter value if you repeated the data-generating process an infinite number of times.
In Figure \@ref(fig:CRT-ATE-hist), black dashed lines mark the true average treatment effect of 0.3 and the colored vertical lines with circles at the end mark the means of the estimators.
Bias is therefore the distance between the colored lines and the black dashed lines. 
This distance is nearly zero for the aggregation estimator and the multilevel model estimator, but larger for the linear regression estimator.

Different types of data-analysis results produce different types of information, and so require using different types of performance criteria to summarize.
For procedures that produce point estimates or point predictions, conventional performance criteria include bias, variance, and root mean squared error. 
If the point estimates come with corresponding standard errors, then we may also want to evaluate how accurately the standard errors represent the true uncertainty of the point estimators; conventional performance criteria include the relative bias and relative root mean squared error of the variance estimator.
For procedures that produce confidence intervals or other types of interval estimates, conventional performance criteria include the coverage rate and average interval width.
Finally, for inferential procedures that involve hypothesis tests (or more generally, classification tasks), conventional performance criteria include Type I error rates and power. 
We describe each of these criteria in Sections \@ref(assessing-point-estimators) through \@ref(assessing-inferential-procedures).

In practice, many data-analysis procedures produce multiple pieces of information---not just point estimates, but also standard errors and confidence intervals and p-values from null hypothesis tests---and those pieces are inter-related. 
For instance, a confidence interval is usually computed from a point estimate and its standard error, and so the performance of that confidence interval will be strongly affected if the point estimator is biased or the standard error understates the true uncertainty.
Likewise, the performance of a hypothesis testing procedure will often strongly depend on the properties of the point estimator and standard error used to compute the test.  
In many simulations, the main aim is to compare the performance of several different estimators or determining which of several data analysis procedures is preferable. 
Thus, we will need to use the performance measures to understand whether a set of procedures work differently, when and how one is superior to the other, and what factors influence differences in performance.
To fully understand the advantages and trade-offs among a set of estimators, we will generally need to compare them using several performance criteria.

Performance criteria are defined with respect to sampling distributions, or the results of applying a data analysis procedure to data generated according to a particular process across an infinite number of replications.
For some simple combinations of data-generating processes and data analysis procedures, it may be possible to derive exact mathematical formulas for calculating some performance criteria (such as exact mathematical expressions for the bias and variance of the linear regression estimator).  
But for many problems, the math is difficult or intractable---that's why we do simulations in the first place.
However, simulations do not produce the _exact_ sampling distribution or give us _exact_ values of performance criteria.
Instead, simulations yield _samples_---usually large samples---from the the sampling distribution, and we can use these to compute _estimates_ of the performance criteria of interest.
In Figure \@ref(fig:CRT-ATE-hist), we calculated the bias of each estimator by taking the mean of 1000 observations from its sampling distribution; if we were to repeat the whole set of calculations (with a different seed), then our bias results would shift slightly.

In working with simulation results, it is important to keep track of the degree of uncertainty in performance criteria estimates. 
We call such uncertainty _Monte Carlo error_ because it is the error arising from using a finite number of replications of the Monte Carlo simulation process.
One way to quantify it is with the _Monte Carlo standard error (MCSE)_, or the standard error of a performance estimate based on a finite number of replications. 
Just as when we analyze real, empirical data, we can apply statistical techniques to estimate the MCSE and even use them to generate confidence intervals for performance criteria. 

The size of an MCSE is driven by how many replications we use: if we only use a few, we will have noisy estimates of performance with large MCSEs; if we use millions of replications, the MCSE will be tiny.
It's important to keep in mind that the MCSE is not measuring anything about how a data analysis procedure performs in general; it only describes how precisely we have approximated a performance criterion, an artifact of how we conducted the simulation.
Moreover, MCSEs are under our control.
Given a desired MCSE, we can determine how many replications we would need to ensure our performance estimates have the specified level of precision.
Section \@ref(MCSE) provides details about how to compute MCSEs for conventional performance measures, along with some discussion of general techniques for computing MCSE for less conventional measures.

## Assessing a Point Estimator {#assessing-point-estimators}

The most common performance measures used to assess a point estimator are bias, variance, mean squared error, and root mean squared error).
As we noted previously, bias compares the mean of the sampling distribution to the target parameter.
Positive bias implies that the estimator tends to systematically over-state the quantity of interest, while negative bias implies that it systematically under-shoots the quantity of interest.
If bias is zero (or nearly zero), we say that the estimator is unbiased (or approximately unbiased). 
Variance (or its square root, the true standard error) describes the spread of the sampling distribution, or the extent to which it varies around its central tendency. 
All else equal, we would like estimators to have low variance (or to be more precise).
Root mean squared error (RMSE) is a conventional measure the overall accuracy of an estimator, or its average degree of error with respect to the target parameter.

To define these quantities more precisely, let's consider a generic estimator $T$ that is targeting a parameter $\theta$.
We call the target parameter the _estimand_.
In most cases, in running our simulation we set the estimand $\theta$ and then generate a (typically large) series of $R$ datasets, in each of which $\theta$ is the true target parameter.
We then analyze each dataset, obtaining a sample of estimates $T_1,...,T_R$.
Formally, the bias, variance, and RMSE of $T$ are defined as
$$
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathbb{V}}
\newcommand{\Bias}{\text{Bias}}
\newcommand{\RMSE}{\text{RMSE}}
\Bias(T) = \E(T) - \theta, \qquad \Var(T) = \E\left[\left(T - \E (T)\right)^2 \right], \qquad \RMSE(T) = \sqrt{\E\left[\left(T - \theta\right)^2 \right]}.
$$
These three measures are inter-connected. 
In particular, RMSE is the combination of (squared) bias and variance, as in 
$$ 
\left[\RMSE(T)\right]^2 = \left[\Bias(T)\right]^2 + \Var(T).
$$
It is important to clarify an important point: the _true standard error_ of an estimator $\hat{\gamma_1}$ is the standard deviation of $\hat{\gamma_1}$ across multiple datasets.

When conducting a simulation, we do not compute these performance measures directly but rather must estimate them using the replicates $T_1,...,T_R$ generated from the sampling distribution.
There's nothing very suprising about how we construct estimates of the performance measures.
IT is just a matter of substituting sample quantities in place of the expectations and variances.
Specifically, we estimate bias by taking
$$\widehat{\Bias} = \bar{T} - \theta,$$
where $\bar{T}$ is the arithmetic mean of the replicates,
$$ \bar{T} = \frac{1}{R}\sum_{r=1}^R T_r.$$
We estimate variance by taking the sample variance of the replicates, as
$$
S_T^2 = \frac{1}{R - 1}\sum_{r=1}^R \left(T_r - \bar{T}\right)^2.
$$
The square root of $S^2_T$, $S_T$ is an estimate of the true standard error of $T$.
We usually prefer to work with the true SE $S_T$ because it has the same units as the target parameter.
Finally, the RMSE estimate can be calculated as
$$ 
\RMSE = \sqrt{\frac{1}{R} \sum_{r = 1}^R \left( T_r - \theta\right)^2 }. 
$$
Often people talk about the MSE (Mean Squared Error)--this is just the RMSE squared.
Just like the true SE is versus the variance, the RMSE is more interpretable than the MSE.


In practice, we never know this value, but in a simulation we can obtain it as the standard deviation of our simulation trial estimates.
Generally, when people say "Standard Error" they actually mean _estimated_ Standard Error, ($\widehat{SE}$), obtained when we use empirical data to estimate the $SE$.
In general research, we often neglect to monitor that the standard errors themselves are estimates of a true parameter, and thus have their own uncertainty.
For assessing actual properties in a simulation, by contrast, we can actually have the true standard error (up to Monte Carlo simulation error), along with our set of estimates.

For absolute assessments of performance, an estimator with low bias, low variance, and thus low RMSE is desired.
For comparisons of relative performance, an estimator with lower RMSE is usually preferable to an estimator with higher RMSE; if two estimators have comparable RMSE, then the estimator with lower bias would usually be preferable. 

It is important to recognize that the above performance measures depend on the scale of the parameter.
For example, if our estimators are measuring a treatment impact in dollars, then our bias, SE, and RMSE are all in dollars.
The variance and MSE would be in dollars squared, which is why we take their square roots to put them back on an intepretable dollars scale.

Often the scale of the outcome is irrelevant in our simulations.
This is especially the case if we are comparing one estimator to the other.
To ease interpretation, we might want to assess estimators relative to the baseline variation.
To achieve this, we can generate data so the outcome has unit variance (i.e., we generate _standardized data_).
Then the bias, median bias, and root mean-squared error would all be in standard deviation units. 

By contrast, a nonlinear change of scale of a parameter can lead to nonlinear changes in performance.
For instance, suppose that $\theta$ is a measure of the proportion of time that a behavior occurs.
A natural way to transform this parameter would be to put it on the log-odds (logit) scale.
However, because of the nonlinear aspect of the logit, 
$$\text{Bias}\left[\text{logit}(T)\right] \neq \text{logit}\left(\text{Bias}[T]\right), \qquad \text{RMSE}\left[\text{logit}(T)\right] \neq \text{logit}\left(\text{RMSE}[T]\right),$$
and so on.
This is fine, but one should be aware that this can happen and do it on purpose.





### Comparing the Performances of the Cluster RCT Estimation Procedures

```{r, include=FALSE}
ATE = 0.30
  runs = readRDS( file = "results/cluster_RCT_simulation.rds" ) 
```

Given our simulation results generated in the last chapter, we next assess the bias, standard error, and RMSE of our three different estimators of the ATE.
These performance criteria address these primary questions:

 - Is the estimator systematically off? (bias)
 - Is it precise? (standard error)
 - Does it predict well? (RMSE)
 
Let us see how the three estimators compare on these criteria.


**Are the estimators biased?**
Bias is with respect to a target estimand.
Here we assess whether our estimates are systematically different from the $\gamma_1$ parameter we used to generate the data (this is the ATE parameter, which we had set to 0.30).

```{r cluster_bias}
runs %>% 
  group_by( method ) %>%
  summarise( 
    mean_ATE_hat = mean( ATE_hat ),
    bias = mean( ATE_hat - ATE )  )
```

Linear regression, with a bias of about 0.09 effect size units, appears about ten times as biased as the other estimators.
There is no evidence of major bias for Agg or MLM.
This is because the linear regression is targeting the person-average average treatment effect.
Our data generating process makes larger sites have larger effects, so the person average is going to be higher since those larger sites will count more.
Our estimand, by contrast, is the site average treatment effect, i.e., the simple average of each site's true impact, which our DGP has set to 0.30.
The Agg and MLM methods, by contrast, estimate this site-average effect, putting them in line with our DGP.

If we had instead decided our target estimand was the person average effect, then we would see linear regression as unbiased, and Agg and MLM as biased; it is important to think carefully about what the estimators are targeting, and report bias with respect to a clearly articulated goal.



**Which method has the smallest standard error?**
The true Standard Error is simply how variable a point estimator is, and is calculated as the standard deviation of the point estimates for a given estimator.
The Standard Error reflects how stable our estimates are across datasets that all came from the same data generating process.
We calculate the standard error, and also the relative standard error using linear regression as a baseline:

```{r}
true_SE <- runs %>% 
  group_by( method ) %>%
  summarise( 
    SE = sd( ATE_hat )
  )
true_SE %>%
  mutate( per_SE = SE / SE[method=="LR"] )
```

These standard errors are all what we would be trying to estimate with a standard error estimator in a normal data analysis.
The other methods appear to have SEs about 8% smaller than Linear Regression.


**Which method has the smallest Root Mean Squared Error?**

So far linear regression is not doing well: it has more bias and a larger standard error than the other two.
We can assess overall performance by combining these two quantities with the RMSE:

```{r}
runs %>% 
  group_by( method ) %>%
  summarise( 
    bias = mean( ATE_hat - ATE ),
    SE = sd( ATE_hat ),
    RMSE = sqrt( mean( (ATE_hat - ATE)^2 ) )
  ) %>%
  mutate( per_RMSE = RMSE / RMSE[method=="LR"] )

```
We also include SE and bias as reference.

RMSE is a way of taking both bias and variance into account, all at once. 
For Agg and MLM, the RMSE is basically the standard error; this makes sense as they are not biased.
For LR, the combination of bias plus increased variability gives a higher RMSE.
That said, clearly the standard error dominates the bias term (note how RMSE and SE are more similar than RMSE and bias).
This is especially the case as RMSE is the square root of the bias and standard errors _squared_; this makes difference between them even more extreme.
Overall, Agg and MLM have RMSEs around 16% smaller than LR--this seems notable.

### Estimands Not Represented By a Parameter

In our Cluster RCT example, we focused on the estimand of the ATE as captured by our model parameter $\gamma_1$.
But say we were interested in the person-average effect.
This is not represented by any number in our data generating process, so we would have to calculate it and then compare all of our estimates to it.

We offer two ways of doing this.
The first is to simply generate a massive dataset, and then average across it to get a good estimate of the true person-average effect.
If our dataset is big enough, then the uncertainty in this estimate will be negligible compared to the uncertainty in our simulation.

Here we try this:
```{r, cache=TRUE}
dat = gen_cluster_RCT( n_bar = 30, J=100000, 
                     gamma_1 = 0.3, gamma_2 = 0.5,
                     sigma2_u = 0.20, sigma2_e = 0.80,
                     alpha = 0.75  )
ATE_person = mean( dat$Yobs[dat$Z==1] ) - mean( dat$Yobs[dat$Z==0] )
ATE_person
```
Note our estimate of the person-average effect of `r round( ATE_person, 0.2)` is about what we would expect given the bias we saw earlier for the linear model!

With respect to the `ATE_person` estimand, the bias and RMSE of our estimators will shift, but SE will stay the same:

```{r}
runs %>% 
  group_by( method ) %>%
  summarise( 
    bias = mean( ATE_hat - ATE_person ),
    SE = sd( ATE_hat ),
    RMSE = sqrt( mean( (ATE_hat - ATE_person)^2 ) )
  ) %>%
  mutate( per_RMSE = RMSE / RMSE[method=="LR"] )
```

For the person-weighted estimand, Agg and MLM are biased, and LR is unbiased.
RMSE is now a tension between bias and reduced variance.
Overall, Agg and MLM are 4% worse than LR in terms of RMSE, because they have lower SEs but more bias.

The second method of calculating `ATE_person` would be to record the true person average effect of the dataset with each simulation iteration, and then average those at the end. This is in effect generating our massive dataset in pieces; the overall average of our dataset-specific `ATE_person`s gives our population person-weighted ATE.
To do this we would need to modify our `gen_cluster_RCT()` DGP code to track this additional information.
We might have, for example

```{r, eval=FALSE}
tx_effect = gamma_1 + gamma_2 * (nj-n_bar)/n_bar
beta_0j = gamma_0 + Zj * tx_effect + u0j
```
and then we would return `tx_effect` as well as `Yobs` and `Z` as a column in our dataset.
This is similar to directly calculating _potential outcomes_, as discussed in Chapter \@ref(potential-outcomes).

Once we modified our DGP code, we _also_ need to modify our analysis functions to record this information.
We might have, for example:
```{r, eval=FALSE}
analyze_data = function( dat ) {
  MLM = analysis_MLM( dat )
  LR = analysis_OLS( dat )
  Agg = analysis_agg( dat )
  res <- bind_rows( MLM = MLM, LR = LR, Agg = Agg,
             .id = "method" )
  res$ATE_person = mean( dat$tx_effect )
  return( res )
}
```

Now when we run our simulation, we would have a column which is the true person average treatment effect for each dataset.
We could then take the average of those across our datasets to estimate the true person average treatment effect in the population, and then compare our point estimators to that value.

Clearly, an estimand that is not represented by a parameter is more difficult to work with, but it is not impossible.
The key is to be clear as to what you are trying to estimate: the performance of an estimator depends on the estimand the estimator is compared to.


## Assessing a Standard Error Estimator

Statistics is perhaps more about assessing how good an estimate is than making an estimate in the first place.
This translates to simulation studies: in our simulation we can know an estimator's actual properties, but if we were to use this estimator in practice we would have to also estimate its associated standard error, and generate confidence intervals and so forth using this standard error estimate.
To understand if this would work in practice, we would need to evaluate not only the behavior of the estimator itself, but the behavior of these associated things.
In other words, we generally not only want to know whether our point estimator is doing a good job, but we usually want to know whether we are able to get a good standard error for that point estimator as well.

To do this we first compare the expected value of $\widehat{SE}$ (estimated with the average $\widehat{SE}$ across our simulation trials) to the actual $SE$.
This tells us whether our uncertainty estimates are _biased_.
We could also examine the standard deviation of $\widehat{SE}$ across trials, which tells us whether our estimates of uncertainty are relatively stable.
We finally could examine whether there is correlation between $\widehat{SE}$ and the actual error (e.g., $\left|T - \theta \right|$).
Good estimates of uncertainty should predict error in a given context (especially if calculating conditional estimates); see @sundberg2003conditional.

For the first assessment, we usually assess the quality of a standard error estimator with a relative performance criteria, rather than an absolute one, meaning we compare the estimated standard error to the true standard error as a ratio.

For an example, suppose that in our simulation we are examining the performance of a point-estimator $T$ for a parameter $\theta$ along with an estimator $\widehat{SE}$ for the standard error of $T$.
In this case, we likely do not know the true standard error of $T$, for our simulation context, prior to the simulation.
However, we can use the variance of $T$ across the replications ($S_T^2$) to directly estimate the true sampling variance $\text{Var}(T) = SE^2(T)$.
The _relative bias_ of $\widehat{SE}^2$ would then be estimated by $RB = \bar{V} / S_T^2$, where $\bar{V}$ is the average of $\widehat{SE}^2$ across simulation runs.
Note that a value of 1 for relative bias corresponds to exact unbiasedness of the variance estimator.
The relative bias measure is a measure of _proportionate_ under- or over-estimation.
For example, a relative bias of 1.12 would mean the standard error was, on average, 12% too large.
We discuss relative performance measures further in Section \@ref(sec-relative-performance).



### Why Not Assess the Estimated SE directly?

We typically see assessment of $\widehat{SE}^2$, not $\widehat{SE}$.
In other words, we typically work with assessing whether the variance estimator is unbiased, etc., rather than the standard error estimator.
This comes out of a few reasons.
First, in practice, so-called unbiased standard errors usually are not in fact actually unbiased (see the delightfully titled section 11.5, "The Joke Is on Us: The Standard Deviation Estimator is Biased after All," in @westfall2013understanding for further discussion).
For linear regression, for example, the classic standard error estimator is an unbiased _variance_ estimator, meaning that we have a small amount of bias due to the square-rooting because:

$$ E[ \sqrt{ V } ] \neq \sqrt{ E[ V ] } . $$

Variance is also the component that gives us the classic bias-variance breakdown of $MSE = Variance + Bias^2$, so if we are trying to assign whether an overall MSE is due to instability or systematic bias, operating in this squared space may be preferable.

That being said, to put things in terms of performance criteria humans understand, it is usually nicer to put final evaluation metrics back into standard error units.
For example, saying there is a 10% reduction in the standard error is more meaningful (even if less impressive sounding) than saying there is a 19% reduction in the variance.


### Assessing SEs for Our Cluster RCT Simulation

To assess whether our estimated SEs are about right, we can look at the average _estimated_ (squared) standard error and compare it to the true standard error.
Our standard errors are _inflated_ if they are systematically larger than they should be, across the simulation runs.
We can also look at how stable our standard error estimates are, by taking the standard deviation of our standard error estimates.
We interpret this quantity relative to the actual standard error to get how far off, as a percent of the actual standard error, we tend to be.

```{r calcuate_cluster_RCT_performances}
runs %>%  group_by( method ) %>%
  summarise( 
    SE = sd( ATE_hat ),
    mean_SEhat = sqrt( mean( SE_hat^2 ) ),
    infl = 100 * mean_SEhat / SE,
    sd_SEhat = sd( SE_hat ),
    stability = 100 * sd_SEhat / SE )
```

The SEs for Agg and MLM appear to be a bit conservative on average.  (3 or 4 percentage points too big).

The last column (`stability`) shows how variable the standard error estimates are relative to the true standard error.
50% would mean the standard error estimates can easily be off by 50% of the truth, which would not be particularly good.
Here we see the linear regression is more unstable than the other methods (cluster-robust standard errors are generally known to be a bit unstable, so this is not too surprising).
It is a bad day for linear regression.



## Assessing Confidence Intervals

Some estimation procedures result in confidence intervals (or sets) which are ranges of values that should contain the true answer with some specified degree of confidence.
For example, a normal-based confidence interval is a combination of an estimator and its estimated uncertainty.

We typically score a confidence interval along two dimensions, __coverage rate__ and __average length__.
To calculate coverage rate, we score whether each interval "captured" the true parameter.
A success is if the true parameter is inside the interval.
To calculate average length, we record each confidence interval's length, and then average across simulation runs.
We say an estimator has good properties if it has good coverage, i.e. it is capturing the true value at least $1-\alpha$ of the time, and if it is generally short (i.e.,  the average length of the interval is less than the average length for other methods).

Confidence interval coverage is simultaneously evaluating the estimators in terms of how well they estimate (precision) and their inferential properties. 
We have combined inference and estimation.

Suppose that the confidence intervals are for the target parameter $\theta$ and have coverage level $\beta$.
Let $A_r$ and $B_r$ denote the lower and upper end-points of the confidence interval from simulation replication $r$, and let $W_r = B_r - A_r$, all for $r = 1,...,R$.
The coverage rate $\omega_\beta$ and average length $\text{E}(W)$ criteria are then as defined in the table below.

| Criterion       | Definition                                         | Estimate             |
|-----------------|-----------------------------|----------------------|
| Coverage        | $\omega_\beta = \text{Pr}(A \leq \theta \leq B)$   | $\frac{1}{R}\sum_{r=1}^R I(A_r \leq \theta \leq B_r)$  |
| Expected length | $\text{E}(W) = \text{E}(B - A)$                    | $\bar{W} = \bar{B} - \bar{A}$ |



Just as with hypothesis testing, a strict statistical interpretation would deem a hypothesis testing procedure acceptable if it has actual coverage rate greater than or equal to $\beta$.
If multiple tests satisfy this criterion, then the test with the lowest expected length would be preferable. Some analysts prefer to look at lower and upper coverage separately, where lower coverage is $\text{Pr}(A \leq \theta)$ and upper coverage is $\text{Pr}(\theta \leq B)$. 


### Confidence Intervals in our Cluster RCT Example

For our CRT simulation, we first have to calculate confidence intervals, and then assess coverage.
We could have used methods such as `confint()` in the estimation approaches; this would be preferred if we wanted more accurately calculated confidence intervals that used $t$-distributions and so forth to account for the moderate number of clusters.

But if we want to use normal assumption confidence intervals we can calculate them post-hoc:
```{r}
runs %>% mutate( CI_l = ATE_hat - 1.96*SE_hat,
                 CI_h = ATE_hat + 1.96*SE_hat,
                 covered = CI_l <= ATE & ATE <= CI_h,
                 width = CI_h - CI_l ) %>%
  group_by( method ) %>%
  summarise( coverage = mean( covered ),
             width = mean( width ))
```
Our coverage is about right for Agg and MLM, and around 5 percentage points too low for LR.
Linear regression is taking a hit from the bias term.
The CIs of LR are a bit wider than the other methods due to the estimated SEs being slightly larger.


## Assessing an Inferential Procedure (Hypothesis Testing) {#assessing-inferential-procedures}

When hypothesis tests are used in practice, the researcher specifies a null (e.g., no treatment effect), collects data, and generates a $p$-value, which is a measure of how extreme the observed data are from what we would expect to naturally occur, if the null were true.
When we assess a method for hypothesis testing, we are therefore typically concerned with two aspects: *validity* and *power*.

### Validity

Validity revolves around whether we erroneously reject a true null more than we should.
Put another way, we say an inference method is valid if it has no more than an $\alpha$ chance of rejecting the null, when it is true, when we are testing at the $\alpha$ level.
This means if we used this method 1000 times, where the null was true for all of those 1000 times, we should not see more than about $1000 \alpha$ rejections (so, 50, if we were using the classic $\alpha = 0.05$ rule).

To assess validity we would therefore specify a data generating process where the null is in fact true.
We then, for a series of such data sets with a true null, conduct our inferential processes on the data, record the $p$-value, and score whether we reject the null hypothesis or not.  

We might then test our methods by exploring more extreme data generation processes, where the null is true but other aspects of the data (such as outliers or heavy skew) make estimation difficult.
This allows us to understand if our methods are robust to strange data patterns in finite sample contexts.

The key concept for validity is that the date we generate, no matter how we do it, must be data with a true null.
The check is always then to see if we reject the null more than we should.


### Power

Power is, loosely speaking, how often we notice an effect when one is there.
Power is a much more nebulous concept than validity, because some effects (e.g. large effects) are clearly easier to notice than others.
If we are comparing estimators to each other, the overall chance of noticing is less of a concern, because we are typically interested in relative performance.
That being said, in order to generate data for a power evaluation, we have to generate data where there is something to detect.
In other words, we need to commit to what the alternative is, and this can be a tricky business.

Typically, we think of power as a function of sample size or effect size. Therefore, we will typically examine a sequence of scenarios with steadily increasing sample size or effect size, estimating the power for each scenario in the sequence. 

We then, for each sample in our series, estimate the power by the same process as for validity, above. 
When assessing validity, we want rejection rates to be low, below $\alpha$, and when assessing power we want them to be as high as possible. But the simulation process itself, other than the data generating process, is exactly the same.


### The Rejection Rate
To put some technical terms to this framing, for both validity and power assessment the main performance criterion is the __rejection rate__ of the hypothesis test. When the data are simulated from a model in which the null hypothesis being tested is true, then the rejection rate is equivalent to the __Type-I error rate__ of the test. When the data are simulated from a model in which the null hypothesis is false, then the rejection rate is equivalent to the __power__ of the test (for the given alternate hypothesis represented by the DGP).
Ideally, a testing procedure should have actual Type-I error equal to the nominal level $\alpha$ (this is the definition of validity), but such exact tests are rare.

There are some different perspectives on how close the actual Type-I error rate should be in order to qualify as suitable for use in practice. Following a strict statistical definition, a hypothesis testing procedure is said to be __level-$\alpha$__  if its actual Type-I error rate is _always_ less than or equal to $\alpha$.
Among a set of level-$\alpha$ tests, the test with highest power would be preferred.
If looking only at null rejection rates, then the test with Type-I error closest to $\alpha$ would usually be preferred.
A less stringent criteria is sometimes used instead, where type I error would be considered acceptable if it is within 50\% of the desired $\alpha$.

Often, it is of interest to evaluate the performance of the test at several different $\alpha$ levels.
A convenient way to calculate a set of different rejection rates is to record the simulated $p$-values and then calculate from those.
To illustrate, suppose that $P_r$ is the $p$-value from simulation replication $k$, for $k = 1,...,R$.
Then the rejection rate for a level-$\alpha$ test is defined as $\rho_\alpha = \text{Pr}\left(P_r < \alpha\right)$ and estimated as, using the recorded $p$-values,
$$r_\alpha = \frac{1}{R} \sum_{r=1}^R I(P_r < \alpha).$$

For a null DGP, one can also plot the emperical cumulative density function of the $p$-values; a valid test should give a $45^\circ$ line as the $p$-values should be standard uniform in distribution.


### Inference in our Cluster RCT Simulation

For our scenario, we generated data with an actual treatment effect.
Without further simulation, we therefore could only assess power, not validity.
This is easily solved!
We simply rerun our simulation code that we made last chapter with `simhelpers`, but with setting `ATE = 0`.


```{r secret_run_cluster_rct, include=FALSE}
if ( !file.exists("results/cluster_RCT_simulation_validity.rds" )) {
tictoc::tic()  # Start the clock!
set.seed( 404044 )
runs_val <- 
  purrr::rerun( R, one_run( 0 ) ) %>%
  bind_rows( .id="runID" )
tictoc::toc()

saveRDS( runs_val, file = "results/cluster_RCT_simulation_validity.rds" )
} else {
  runs_val = readRDS( "results/cluster_RCT_simulation_validity.rds" )
}
```



```{r run_cluster_rct, eval=FALSE}
set.seed( 404044 )
runs_val <- sim_function( R, n_bar = 30, J = 20, gamma_1 = 0 )
saveRDS( runs_val, file = "results/cluster_RCT_simulation_validity.rds" )
```


```{r, include=FALSE}
runs <- readRDS("results/cluster_RCT_simulation.rds" )
runs_val <- readRDS("results/cluster_RCT_simulation_validity.rds" )
```


Assessing power and validity is exactly the same calculation: we see how often we have a $p$-value less than 0.05.
For power we have:

```{r demo_calc_power}
runs %>% group_by( method ) %>%
  summarise( power = mean( p_value <= 0.05 ) )
```
For validity:
```{r demo_calc_validity}
runs_val %>% group_by( method ) %>%
  summarise( power = mean( p_value <= 0.05 ) )
```

The power when there is an effect (for this specific scenario) is not particularly high, and the validity is around 0.05, as desired.

Linear regression has notabily higher power... but this may be in part due to the invalidity of the test (note the rejection rate is around 6%, rather than the target of 5%).
The elevated power is also likely due to the upward bias in estimation.
As discussed above, LR is targeting the person-average impact which, in this case, is not 0 even under our null because we have kept our impact heterogeniety parameter to its default of $\gamma_2=0.2$, meaning we have treatment variation around 0.
We could run our simulation with truly null effects to see if the false rejection rate goes down.



## Assessing Confidence Intervals {#assessing-confidence-intervals}

Some estimation procedures result in confidence intervals (or sets) which are ranges of values that should contain the true answer with some specified degree of confidence.
For example, a normal-based confidence interval is a combination of an estimator and its estimated uncertainty.

We typically score a confidence interval along two dimensions, __coverage rate__ and __average length__.
To calculate coverage rate, we score whether each interval "captured" the true parameter.
A success is if the true parameter is inside the interval.
To calculate average length, we record each confidence interval's length, and then average across simulation runs.
We say an estimator has good properties if it has good coverage, i.e. it is capturing the true value at least $1-\alpha$ of the time, and if it is generally short (i.e.,  the average length of the interval is less than the average length for other methods).

Confidence interval coverage is simultaneously evaluating the estimators in terms of how well they estimate (precision) and their inferential properties. 
We have combined inference and estimation.

Suppose that the confidence intervals are for the target parameter $\theta$ and have coverage level $\beta$.
Let $A_r$ and $B_r$ denote the lower and upper end-points of the confidence interval from simulation replication $r$, and let $W_r = B_r - A_r$, all for $r = 1,...,R$.
The coverage rate $\omega_\beta$ and average length $\text{E}(W)$ criteria are then as defined in the table below.

| Criterion       | Definition                                         | Estimate             |
|-----------------|-----------------------------|----------------------|
| Coverage        | $\omega_\beta = \text{Pr}(A \leq \theta \leq B)$   | $\frac{1}{R}\sum_{r=1}^R I(A_r \leq \theta \leq B_r)$  |
| Expected length | $\text{E}(W) = \text{E}(B - A)$                    | $\bar{W} = \bar{B} - \bar{A}$ |



Just as with hypothesis testing, a strict statistical interpretation would deem a hypothesis testing procedure acceptable if it has actual coverage rate greater than or equal to $\beta$.
If multiple tests satisfy this criterion, then the test with the lowest expected length would be preferable. Some analysts prefer to look at lower and upper coverage separately, where lower coverage is $\text{Pr}(A \leq \theta)$ and upper coverage is $\text{Pr}(\theta \leq B)$. 


### Confidence Intervals in our Cluster RCT Example

For our CRT simulation, we first have to calculate confidence intervals, and then assess coverage.
We could have used methods such as `confint()` in the estimation approaches; this would be preferred if we wanted more accurately calculated confidence intervals that used $t$-distributions and so forth to account for the moderate number of clusters.

But if we want to use normal assumption confidence intervals we can calculate them post-hoc:
```{r}
runs %>% mutate( CI_l = ATE_hat - 1.96*SE_hat,
                 CI_h = ATE_hat + 1.96*SE_hat,
                 covered = CI_l <= ATE & ATE <= CI_h,
                 width = CI_h - CI_l ) %>%
  group_by( method ) %>%
  summarise( coverage = mean( covered ),
             width = mean( width ))
```
Our coverage is about right for Agg and MLM, and around 5 percentage points too low for LR.
Linear regression is taking a hit from the bias term.
The CIs of LR are a bit wider than the other methods due to the estimated SEs being slightly larger.


## Additional Thoughts on Measuring Performance

In this section we provide some additional thoughts on performance measures.
We first discuss relative vs. absolute criteria some more, then touch on robust measures of performance.
We finally summarize the measures we discuss in this chapter.

### Selecting Relative vs. Absolute Criteria {#sec-relative-performance}

We have primarily examined performance estimators for point estimators using absolute criteria, focusing on measures like bias directly on the scale of the outcome.
In contrast, for evaluation things such as estimated standard errors, which are always positive and scale-dependent, it often makes sense to use relative criteria, i.e., criteria calculated as proportions of the target parameter ($T/\theta$) rather than as differences ($T - \theta$).
We typically apply absolute criteria to point estimators and relative criteria to standard error estimators (we are setting aside, for the moment, the relative criteria of a measure from one estimation procedure to another, as we saw earlier when we compared the SEs to a baseline SE of linear regression for the cluster randomized trial simulation.
So how do we select when to use what?

As a first piece of guidance, establish whether we expect the performance (e.g., bias, standard error, or RMSE) of a point estimate to depend on the magnitude of the estimand.
For example, if we are estimating some mean $\theta$, and we generate data where $\theta = 100$ vs where $\theta = 1000$ (or any other arbitrary number), we would not generally expect the value of $\theta$ to change the magnitude of bias, variance, or MSE.
On the other hand, these different $\theta$s would have a large impact on the _relative_ bias and _relative_ MSE.
(Want smaller relative bias? Just add a million to the parameter!)
For these sorts of "location parameters" we generally use absolute measures of performance.

That being said, a more principled approach for determining whether to use absolute or relative performance criteria depends on assessing performance for _multiple_ values of the parameter.
In many simulation studies, replications are generated and performance criteria are calculated for several different values of a parameter, say $\theta = \theta_1,...,\theta_p$.
Let's focus on bias for now, and say that we've estimated (from a large number of replications) the bias at each parameter value.
We present two hypothetical scenarios, A and B, in the figures below.

```{r, echo = FALSE, fig.width = 5, fig.height = 2.5}
library(ggplot2)
theta <- seq(0, 5, 0.5)
bias1 <- rnorm(length(theta), mean = 0.06, sd = 0.004)
bias2 <- rnorm(length(theta), mean = theta * 0.12 / 5, sd = 0.004)
type <- rep(c("Scenario A","Scenario B"), each = length(theta))
dat <- data.frame(type, theta, bias = c(bias1, bias2))
dat$theta = dat$theta / 12
dat$bias = dat$bias * 2
ggplot(dat, aes(theta, bias)) + 
  geom_point() + geom_line() + 
  facet_wrap(~ type) + 
  theme_minimal()

```


If the absolute bias is roughly the same for all values of $\theta$ (as in Scenario A), then it makes sense to report absolute bias as the summary performance criterion.
On the other hand, if the bias grows roughly in proportion to $\theta$ (as in Scenario B), then relative bias might be a better summary criterion.


**Performance relative to a baseline estimator.**

Another relative measure, as we saw earlier, is to calculate performance relative to some baseline.
For example, if one of the estimators is the "generic method," we could calculate ratios of the RMSE of our estimators to the baseline RMSE.
This can provide a way of standardizing across simulation scenarios where the overall scale of the RMSE changes radically.
This could be critical to, for example, examining trends across simulations that have different sample sizes, where we would expect all estimators' performance measures to improve as sample size grows.
This kind of relative standardization allows us to make statements such as "Aggregation has standard errors around 8% smaller than linear regression"--which is very interpretable, more interpretable than saying "Aggregation has standard errors around 0.01 smaller than linear regression."
In the latter case, we do not know if that is big or small.

While a powerful tool, standardization is not without risks: if you scale relative to something, then higher or lower ratios can either be due to the primary method of interest (the numerator) or due to the behavior of the reference method in the denominator.
These relative ratios can end up being confusing to interpret due to this tension.

They can also break when everything is on a constrained scale, like power.
If we have a power of 0.05, and we improve it to 0.10, we have doubled our power, but if it is 0.10 and we increase to 0.15, we have only increased by 50%.
Ratios when near zero can be very deceiving.


### Robust Measures of Performance

Depending on the model and estimation procedures being examined, a range of different criteria might be used to assess estimator performance.
For point estimation, we have seen bias, variance and MSE as the three core measures of performance.
Other criteria exist, such as the median bias and the median absolute deviation of $T$, where we use the median $\tilde{T}$ of our estimates rather than the mean $\bar{T}$.

The usual bias, variance and MSE measures can be sensitive to outliers.
If an estimator generally does well, except for an occasional large mistake, these classic measures can return very poor overall performance.
Instead, we might turn to quantities such as the median bias (sort all the estimation errors across the simulation scenarios, and take the middle), or the Median Absolute Distance (MAD, where you take the median of the absolute values of the errors, which is an alternative to RMSE) as a measure of performance.

Other robust measures are also possible, such as simply truncating all errors to a maximum size (this is called Windsorizing).
This is a way of saying "I don't care if you are off by 1000, I am only going to count it as 10."


### Summary of Peformance Measures

We list most of the performance criteria we saw in this chapter in the table below, for reference:


| Criterion     | Definition            | Estimate             |
|---------------|-----------------------|----------------------|
| Bias          | $\text{E}(T) - \theta$                                  | $\bar{T} - \theta$   |
| Median bias   | $\text{M}(T) - \theta$                                  | $\tilde{T} - \theta$ |
| Variance      | $\text{E}\left[\left(T - \text{E}(T)\right)^2\right]$   | $S_T^2$              |
| MSE           | $\text{E}\left[\left(T - \theta\right)^2\right]$ | $\left(\bar{T} - \theta\right)^2 + S_T^2$ |
| MAD           | $\text{M}\left[\left|T - \theta\right|\right]$          | $\left[\left|T - \theta\right|\right]_{R/2}$   |
| Relative bias          | $\text{E}(T) / \theta$ | $\bar{T} / \theta$   | $\sqrt{S_T^2 / \left(R\theta^2\right)}$        |
| Relative median bias   | $\text{M}(T) / \theta$ | $\tilde{T} / \theta$ |
| Relative MSE           | $\text{E}\left[\left(T - \theta\right)^2\right] / \theta^2$ | $\frac{\left(\bar{T} - \theta\right)^2 + S_T^2}{\theta^2}$ |


* Bias and median bias are measures of whether the estimator is systematically higher or lower than the target parameter. 
* Variance is a measure of the __precision__ of the estimator---that is, how far it deviates _from its average_.  We might look at the square root of this, to assess the precision in the units of the original measure. This is the true SE of the estimator.
* Mean-squared error is a measure of __overall accuracy__, i.e. is a measure how far we typically are from the truth.  We more frequently use the root mean-squared error, or RMSE, which is just the square root of the MSE.
* The median absolute deviation (MAD) is another measure of overall accuracy that is less sensitive to outlier estimates. The RMSE can be driven up by a single bad egg.  The MAD is less sensitive to this.








## Uncertainty in Performance Estimates (the Monte Carlo Standard Error) {#MCSE}

Our performance criteria are defined as average performance across an infinite number of trials.
Of course, in our simulations we only run a finite number of trials, and estimate the performance criteria with the sample of trials we generate.
For example, if we are assessing coverage across 100 trials, we can calculate what fraction rejected the null for that 100.
This is an _estimate_ of the true coverage rate.
Due to random chance, we might see a higher, or lower, proportion rejected than what we would see if we ran the simulation forever.

To account for estimation uncertainty we want associated uncertainty estimates to go with our point estimates of performance.
We want to, in other words, treat our simulation results as a dataset in its own right.
(And yes, this is quite meta!)

Once we frame the problem in these terms, it is relatively straightforward to calculate standard errors for most of the performance critera because we have an independent and identically distributed set of measurements.
We call these standard errors Monte Carlo Simulation Errors, or MCSEs.
For some of the performance criteria we have to be a bit more clever, as we will discuss below.

We list MCSE expressions for many of our straightforward performance measures on the following table.
In reading the table, recall that, for an estimator $T$, we have $S_T$ being the standard deviation of $T$ across our simulation runs (i.e., our estimated true Standard Error).
We also have

 - Sample skewness (standardized): $\displaystyle{g_T = \frac{1}{R S_T^3}\sum_{r=1}^R \left(T_r - \bar{T}\right)^3}$
 - Sample kurtosis (standardized): $\displaystyle{k_T = \frac{1}{R S_T^4} \sum_{r=1}^R \left(T_r - \bar{T}\right)^4}$


| Criterion for T      | MCSE   |
|----------------|--------|
| Bias ($T-\theta$)           | $\sqrt{S_T^2/ R}$        |
| Variance ($S_T^2$)     | $\displaystyle{S_T^2 \sqrt{\frac{k_T - 1}{R}}}$ |
| MSE            | see below |
| MAD            | -   |
| Power & Validity ($r_\alpha$)     | $\sqrt{ r_\alpha \left(1 - r_\alpha\right) / R}$ |
| Coverage  ($\omega_\beta$)             | $\sqrt{\omega_\beta \left(1 - \omega_\beta\right) / R}$ |
| Average length ($\text{E}(W)$)         |  $\sqrt{S_W^2 / R}$     |

The MCSE for the MSE is a bit more complicated, and does not quite fit on our table:
$$ \widehat{MCSE}( \widehat{MSE} ) = \displaystyle{\sqrt{\frac{1}{R}\left[S_T^4 (k_T - 1) + 4 S_T^3 g_T\left(\bar{T} - \theta\right) + 4 S_T^2 \left(\bar{T} - \theta\right)^2\right]}} .$$

For relative quantities with respect to an estimand, simply divide the criterion by the target estimand.
E.g., for relative bias $T / \theta$, the standard error would be
$$ SE\left( \frac{T}{\theta} \right) = \frac{1}{\theta} SE(T) = \sqrt{\frac{S_T^2}{R\theta^2}} .$$

For square rooted quantities, such as the SE for the true SE (square root of the Variance) or the RMSE (square root of MSE) we can use the Delta method.
The Delta method says (with some conditions), that if we assume $X \sim N( \phi, V )$, then we can approximate the distribution of $g(X)$ for some continuous function $g(\cdot)$ as
$$ g(X) \sim N\left( g(\phi), \;\; g'(\phi)^2\cdot V \right) , $$
where $g'(\phi)$ is the derivative of $g(\cdot)$ evaluated at $\phi$.
In other words, 
$$ SE( g(\hat{X}) ) \approx g'(\theta)  \times SE(\hat{X}) .$$
For estimation, we plug in $\hat{\theta}$ and our estimate of $SE(\hat{X})$ into the above.
Back to the square root, we have $g(x) = \sqrt(x)$ and $g'(x) = 1/2\sqrt(x)$.
This gives, for example, the estimated MCSE of the SE as
$$ \widehat{SE}( \widehat{SE} ) = \widehat{SE}( S^2_T ) = \frac{1}{2S^2_T} \widehat{SE}( S^2_T ) = \frac{1}{2S^2_T} S_T^2 \sqrt{\frac{k_T - 1}{R}} = \frac{1}{2} \sqrt{\frac{k_T - 1}{R}} .$$


### MCSE for Relative Variance Estimators

Estimating the MCSE of the relative bias or relative MSE of a (squared) standard error estimator, i.e., of $E( \widehat{SE^2} - SE^2 ) / SE^2 )$ or $\widehat{MSE} / MSE$, is complicated by the appearance of an estimated quantity, $SE^2$ or $MSE$, in the denominator of the ratio.
This renders the simple division approach from above unusable, technically speaking.
The problem is we cannot use our clean expressions for MCSEs of relative performance measures since we are not taking the uncertainty of our denominator into account.

To properly assess the overall MCSE, we need to do something else.
One approach is to use the _jackknife_ technique.
Let $\bar{V}_{(j)}$ and $S_{T(j)}^2$ be the average squared standard error estimate and the true variance estimate calculated from the set of replicates __*that excludes replicate $j$*__, for $j = 1,...,R$.
The relative bias estimate, excluding replicate $j$ would then be $\bar{V}_{(j)} / S_{T(j)}^2$.
Calculating all $R$ versions of this relative bias estimate and taking the variance of these $R$ versions yields the jackknife variance estimator:

$$
MCSE\left( \frac{ \widehat{SE}^2 }{SE^2} \right) = \frac{1}{R} \sum_{j=1}^R \left(\frac{\bar{V}_{(j)}}{S_{T(j)}^2} - \frac{\bar{V}}{S_T^2}\right)^2.
$$

This would be quite time-consuming to compute if we did it by brute force. However, a few algebra tricks provide a much quicker way. The tricks come from observing that

$$
\begin{aligned}
\bar{V}_{(j)} &= \frac{1}{R - 1}\left(R \bar{V} - V_j\right) \\
S_{T(j)}^2 &= \frac{1}{R - 2} \left[(R - 1) S_T^2 - \frac{R}{R - 1}\left(T_j - \bar{T}\right)^2\right]
\end{aligned}
$$
These formulas can be used to avoid re-computing the mean and sample variance from every subsample.
Instead, you calculate the overall mean and overall variance, and then do a small adjustment with each jackknife iteration.
You can even implement this with vector processing in R!


### Calculating MCSEs With the `simhelpers` Package

The `simhelper` package is designed to calculate MCSEs (and the performance metrics themselves) for you.
It is easy to use: take this set of simulation runs on the Welch dataset:

```{r}
library( simhelpers )
data( welch_res )
welch <- welch_res %>%
  filter( method == "t-test" ) %>%
  dplyr::select( -method, -seed, -iterations )

welch
```

We can calculate performance metrics across all the range of scenarios.
Here is the rejection rate:

```{r}
welch_sub = filter( welch, n1 == 50, n2 == 50, mean_diff==0 )
calc_rejection(welch_sub, p_val)
```

And coverage:
```{r}
calc_coverage(welch_sub, lower_bound, upper_bound, mean_diff)
```

Using `tidyverse` it is easy to process across scenarios (more on experimental design and multiple scenarios later):
```{r}
welch %>% group_by(n1,n2,mean_diff) %>%
  summarise( calc_rejection( p_values = p_val ) )
```





### MCSE Calculation in our Cluster RCT Example

We can check our MCSEs for our performance measures to see if we have enough simulation trials to give us precise enough estimates to believe the differences we reported earlier.
In particular, we have:

```{r cluster_MCSE_calculation}
library( simhelpers )
runs$ATE = ATE
runs %>% 
  summarise( calc_absolute( estimates = ATE_hat,
                            true_param = ATE,
                            criteria = c("bias","stddev", "rmse")) ) %>%
  dplyr::select( -K_absolute ) %>%
  knitr::kable(digits=3)
```

We see the MCSEs are quite small relative to the linear regression bias term and all the SEs (`stddev`) and RMSEs: we have simulated enough runs to see the gross trends identified.
We have _not_ simulated enough to for sure know if MLM and Agg are not slightly biased.  Given our MCSEs, they could have true bias of around 0.01 (two MCSEs).








## Exercises

1. Continuing the exercises from the prior chapters, estimate rejection rates of the BFF\* test for the parameter values in the fifth line of Table 1 of Brown and Forsythe (1974).


2. Implement the jackknife as described above in code.  Check your answers against the `simhelpers` package for the built-in `t_res` dataset:
```{r}
library( simhelpers )
calc_relative(data = t_res, estimates = est, true_param = true_param)
```

3. As foreground to the following chapters, can you explore multiple scenarios for the cluster RCT example to see if the trends are common?  First write a function that takes a parameter, runs the entire simulation, and returns the results as a small table. You pick which parameter, e.g., average treatment effect, `alpha`, or whatever you like), that you wish to vary.  Here is a skeleton for the function:

```{r, eval=FALSE}
my_simulation <- function( my_param ) {
  # call the sim_function() simulation function from the end of last
  # chapter, setting the parameter you want to vary to my_param
  
  # Analyze the results, generating a table of performance metrics,
  # e.g., bias or coverage. Make sure your analysis is a data frame,
  # like we saw earlier this chapter.
  
  # Return results
}
```
Then use code like the following to generate a set of results measured as a function of a varying parameter:

```{r, eval=FALSE}
vals = seq( start, stop, length.out = 5 )
res = map_df( vals, my_simulation ) 
```

The above code will give you a data frame of results, one column for each performance measure.
Finally, you can use this table and plot the performance measure as a function of the varying parameter.


