# Data-generating models

In the abstract, a function that implements a data-generating model should have the following form:
```{r}
generate_data <- function(parameters) {

  # simulations and calculations
  
  return(sim_data)
}
```
The function takes a set of parameter values as input, simulates random numbers and does calculations, and produces as output a set of simulated data. In practice, the parameters will typically consist of _multiple_ values, including not only the model parameters, but also sample sizes and other study design parameters.

```{r}
generate_data <- function(mu, sigma_sq, sample_size) {

  N <- sum(sample_size) 
  g <- length(sample_size) 
  
  group <- rep(1:g, times = sample_size) 
  mu_long <- rep(mu, times = sample_size)
  sigma_long <- rep(sqrt(sigma_sq), times = sample_size) 
  
  x <- rnorm(N, mean = mu_long, sd = sigma_long)
  sim_data <- data.frame(group = group, x = x)
    
  return(sim_data)
}

generate_data(mu = mu, sigma_sq = sigma_sq, sample_size = sample_size)
```

## Efficiency versus simplicity

An alternative approach to the above would be to write a function that generates _multiple_ sets of simulated data all at once. For example, we could specify that we want `R` replications of the study and have the function spit out a matrix with `R` columns, one for each simulated dataset:

```{r}

generate_data_matrix <- function(mu, sigma_sq, sample_size, R) {

  N <- sum(sample_size) 
  g <- length(sample_size) 
  
  group <- rep(1:g, times = sample_size) 
  mu_long <- rep(mu, times = sample_size)
  sigma_long <- rep(sqrt(sigma_sq), times = sample_size) 

  x_mat <- matrix(rnorm(N * R, mean = mu_long, sd = sigma_long), nrow = N, ncol = R)
  sim_data <- list(group = group, x_mat = x_mat)
    
  return(sim_data)
}

generate_data_matrix(mu = mu, sigma_sq = sigma_sq, sample_size = sample_size, R = 4)
```

This approach is a bit more computationally efficient because the setup calculations (getting `N`, `g`, `group`, `mu_full`, and `sigma_full`) only have to be done once instead of once per replication. It also makes clever use of vector recycling in the call to `rnorm()`. However, the structure of the resulting data is more complicated, which will make it more difficult to do the later estimation steps. Furthermore, if `R` is large and each replication produces a large dataset, this "all-at-once" approach will entail generating and holding very large amounts of data in memory, which can create other performance issues. On balance, I recommend following the simpler approach of writing a function that generates a single simulated dataset per call (unless and until you have a principled reason to do otherwise). 

## Checking the data-generating function

An important part of learning to program in R---particularly learning to write functions---is finding ways to test and check the correctness of your code. Thus, after writing a data-generating function, we need to consider how to test whether the output it produces is correct. How best to do this will depend on the data-generating model being implemented. 

For the heteroskedastic ANOVA problem, one basic thing we could do is check that the simulated data from each group follows a normal distribution. By generating very large samples from each group, we can effectively check characteristics of the population distribution. In the following code, I simulate very large samples from each of the four groups, check that the means and variances agree with the input parameters, and check normality using QQ plots:

```{r, fig.width = 4, fig.height = 4, fig.show = "hold"}
check_data <- generate_data(mu = mu, sigma_sq = sigma_sq, sample_size = rep(10000, 4))

table(check_data$group) # check sample sizes
with(check_data, tapply(x, group, mean)) # calculate means by group
mu # compare to mean parameters
with(check_data, tapply(x, group, var)) # calculate variances by group
sigma_sq # compare to variance parameters

# check normality
with(check_data, qqnorm(x[group==1]))
with(check_data, qqnorm(x[group==2]))
with(check_data, qqnorm(x[group==3]))
with(check_data, qqnorm(x[group==4]))
```

## Exercises

### Shifted-and-scaled t distribution

The shifted-and-scaled t distribution has parameters $\mu$ (mean), $\sigma$ (scale), and $\nu$ (degrees of freedom). If $T$ follows a student's t distribution with $\nu$ degrees of freedom, then $S = \mu + \sigma T$ follows a shifted-and-scaled t distribution. The following function will generate random draws from this distribution:
```{r}
r_tss <- function(n, mean, sd, df) {
  mean + sd * rt(n = n, df = df)
}

r_tss(n = 8, mean = 3, sd = 2, df = 5)
```

Modify that `simulate_data` function to generate data from shifted-and-scaled t distributions rather than from normal distributions. Include the degrees of freedom as an input argument. Re-run the Type-I error rate calculations from the previous question. Do the results change substantially?
