% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{book}
\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{fancyvrb}
\usepackage{xcolor}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{booktabs}
\usepackage{amsthm}
\makeatletter
\def\thm@space@setup{%
  \thm@preskip=8pt plus 2pt minus 4pt
  \thm@postskip=\thm@preskip
}
\makeatother

% Added by Miratrix
\usepackage{color}
\newcommand\cmnt[2]{\qquad{{\color{red} \em #1---#2} \qquad}}
\newcommand\cmntM[1]{\cmnt{#1}{Miratrix}}
\newcommand\awk{{{\color{red} {$\leftarrow$ Awkward phrasing}}\qquad}}
\newcommand\cmntMp[1]{{\color{red} $\leftarrow$ {\em #1 -Miratrix} \qquad}}

\usepackage{subfig}
\newcommand{\Prob}{\text{Pr}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\M}{\mathbb{M}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\Var}{\text{Var}}
\newcommand{\Bias}{\text{Bias}}
\newcommand{\RMSE}{\text{RMSE}}
\newcommand{\Cov}{\text{Cov}}
\newcommand{\cor}{\text{cor}}
\newcommand{\diag}{\text{diag}}
\newcommand{\mat}[1]{\mathbf{#1}}
\newcommand{\bs}{\boldsymbol}
\newcommand{\trace}{\text{tr}}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[]{natbib}
\bibliographystyle{apalike}
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\VerbatimFootnotes % allow verbatim text in footnotes
\hypersetup{
  pdftitle={Designing Monte Carlo Simulations in R},
  pdfauthor={Luke W. Miratrix and James E. Pustejovsky (Equal authors)},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{Designing Monte Carlo Simulations in R}
\author{Luke W. Miratrix and James E. Pustejovsky
(Equal authors)}
\date{2025-11-11}

\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\chapter*{Welcome}\label{welcome}
\addcontentsline{toc}{chapter}{Welcome}

Monte Carlo simulations are a computational technique for investigating how well something works, or for investigating what might happen in a given circumstance.
When we write a simulation, we are able to control how data are generated, which means we can know what the ``right answer'' is.
Then, by repeatedly generating data and then applying some statistical method that data, we can assess how well a statistical method works in practice.

Monte Carlo simulations are an essential tool of inquiry for quantitative methodologists and students of statistics, useful both for small-scale or informal investigations and for formal methodological research.
Despite the ubiquity of simulation work, most quantitative researchers get little formal training in the design and implementation of Monte Carlo simulations. As a result, the simulation studies presented in academic journal articles are highly variable in terms of their high-level logic, scope, programming, and presentation.
Although there has long been discussion of simulation design and implementation among statisticians and methodologists, the available guidance is scattered across many different disciplines, and much of it is focused on mechanics and computing tools, rather than on principles.

In this monograph, we aim to provide an introduction to the logic and mechanics of designing simulation studies, using the R programming language.
Our focus is on simulation studies for formal research purposes (i.e., as might appear in a journal article or dissertation) and for informing the design of empirical studies (e.g., power analysis).
That being said, the ideas of simulation are used in many different contexts and for many different problems, and we believe the overall concepts illustrated by these ``conventional'' simulations readily carry over into all sorts of other types of use, even statistical inference!
Our focus is on the best practices of simulation design and how to use simulation to be a more informed and effective quantitative analyst.
In particular, we try to provide a guide to designing simulation studies to answer questions about statistical methodology.

Mainly, this book gives practical tools (i.e., lots of code to simply take and repurpose) along with some thoughts and guidance for writing simulations.
We hope you find it to be a useful handbook to help you with your own projects, whatever they happen to be!

\section*{License}\label{license}
\addcontentsline{toc}{section}{License}

This book is licensed to you under \href{http://creativecommons.org/licenses/by-nc-nd/4.0/}{Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License}.

The code samples in this book are licensed under \href{https://creativecommons.org/publicdomain/zero/1.0/}{Creative Commons CC0 1.0 Universal (CC0 1.0)}, i.e.~public domain.

\section*{About the authors}\label{about-the-authors}
\addcontentsline{toc}{section}{About the authors}

We wrote this book in full collaboration, because we thought it would be fun to have some reason to talk about how to write simulations, and we wanted more people to be writing high-quality simulations.
Our author order is alphabetical, but perhaps imagine it as a circle, or something with no start or end:

\begin{center}\includegraphics[width=0.75\linewidth]{Designing-Simulations-in-R_files/figure-latex/unnamed-chunk-2-1} \end{center}

But anymore, more about us.

\textbf{James E. Pustejovsky} is an associate professor at the University of Wisconsin - Madison, where he teaches in the Quantitative Methods Program within the Department of Educational Psychology. He completed a Ph.D.~in Statistics at Northwestern University.

\textbf{Luke Miratrix}: I am currently an associate professor at Harvard University's Graduate School of Education. I completed a Ph.D.~in Statistics at UC Berkeley after having traveled through three different graduate programs (computer science at MIT, education at UC Berkeley, and then finally statistics at UC Berkeley).
I then ended up as an assistant professor in Harvard's statistics department, and moved (back) to Education a few years later.

Over the years, simulation has become a way for me to think.
This might be because I am fundamentally lazy, and the idea of sitting down and trying to do a bunch of math to figure something out seems less fun than writing up some code ``real quick'' so I can see how things operate. Of course, ``real quick'' rarely is that quick -- and before I know it I got sucked into trying to learn some esoteric aspect of how to best program something, and then a few rabbit holes later I may have discovered something interesting! I find simulation quite absorbing, and I also find them reassuring (usually with regards to whether I have correctly implemented some statistical method). This book has been a real pleasure to write, because it's given me actual license to sit down and think about why I do the various things I do, and also which way I actually prefer to approach a problem. And getting to write this book with my co-author has been a particular pleasure, for talking about the business of writing simulations is rarely done in practice. This has been a real gift, and I have learned so much.

\section*{Acknowledgements}\label{acknowledgements}
\addcontentsline{toc}{section}{Acknowledgements}

The material in this book was initially developed through courses that we offered at the University of Texas at Austin (James) and Harvard University (Luke) as well as from a series of workshops that we offered through the Society for Research on Educational Effectiveness in June of 2021. We are grateful for feedback, questions, and corrections we have received from many students who participated in these courses. Some parts of this book are based on memos or other writings generated for various purposes, some of which were written by others.
This has been attributed throughout.

\part{An Introductory Look}\label{part-an-introductory-look}

\chapter{Introduction}\label{introduction}

Monte Carlo simulations are a tool for studying the behavior of random processes, such as the behavior of a statistical estimation procedure when applied to a sample of data.
Within quantitatively oriented fields, researchers developing new statistical methods or evaluating the use of existing methods nearly always use Monte Carlo simulations as part of their research process.
In the context of methodological development, researchers use simulations in a way analogous to how a chef would use their test kitchen to develop new recipes before putting them on the menu, how a manufacturer would use a materials testing laboratory to evaluate the safety and durability of a new consumer good before bringing it to market, or how an astronaut would prepare for a spacewalk by practicing the process in an underwater mock-up.
Simulation studies provide a clean and controlled environment for testing out data analysis approaches before putting them to use with real empirical data.

More broadly, Monte Carlo studies are an essential tool in many different fields of science---climate science, engineering, and education research are three examples---and are used for a variety of different purposes.
Simulations are used to model complex stochastic processes such as weather patterns \citep{jones2012IntroductionScientificProgramming, robert2010IntroducingMonteCarlo};
to generate parameter estimates from complex statistical models, as in Markov Chain Monte Carlo sampling \citep{gelman2013BayesianDataAnalysis};
and even to estimate uncertainty in statistical summaries, as in bootstrapping \citep{davison1997BootstrapMethodsTheir}.
In this book, we shall focus on using simulation for the development and validation of methods for data analysis, which are everyday concerns within the fields of statistics and quantitative methodology.
However, we also believe that many of the general principles of simulation design and execution that we will discuss are broadly applicable to these other purposes, and we note connections as they occur.

At a very high level, Monte Carlo simulation provides a method for understanding the performance of a statistical model or data analysis method under conditions where the truth is known and can be controlled.
The basic approach for doing so is as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Create artificial data using random number generators based on a specific statistical model, or more generally, a \emph{Data-Generating Process} (DGP).
\item
  Apply one or more data-analysis procedures to the artificial data. (These procedures might be something as simple as calculating a difference in sample means or fitting a regression model, or it might be an involved, multi-step procedure involving cleansing the data of apparent outliers, imputing missing values, applying a machine-learning algorithm, and carrying out further calculations on the predictions of the algorithm.)
\item
  Repeat Steps 1 and 2 many times.
\item
  Summarize the results across these repetitions in order to understand the general trends or patterns in how the method works.
\end{enumerate}

Simulation is useful because one can control the data-generating process and therefore fully know the truth---something that is almost always uncertain when analyzing real, empirical data.
Having full control of the data-generating process makes it possible to assess how well a procedure works by comparing the estimates produced by the data analysis procedure against this known truth.
For instance, we can see if estimates from a statistical procedure are consistently too high or too low (i.e., whether an estimator is systematically biased).
We can also compare multiple data analysis procedures by assessing the degree of error in each set of results to determine which procedure is generally more accurate when applied to the same collection of artificial datasets.

This basic process of simulation can be used to investigate an array of different questions that arise in statistics and quantitative methodology.
To seed the field, we now give a high-level overview of some of the major use cases. We then discuss some of the major limitations and common pitfalls of simulation studies, which are important to keep in mind as we proceed.

\section{Some of simulation's many uses}\label{some-of-simulations-many-uses}

Monte Carlo simulations allow for rapid exploration of different data analysis procedures and, even more broadly, different approaches to designing studies and collecting measurements. Simulations are especially useful because they provide a means to answer questions that are difficult or impossible to answer by other means.
Many statistical models and estimation methods \emph{can} be analyzed mathematically, but only by using asymptotic approximations that describe how the methods work as sample size increases towards infinity.
In contrast, simulation methods provide answers for specific, finite sample sizes.
Thus, they allow researchers to study models and estimation methods where relevant mathematical formulas are not available, not easily applied, or not sufficiently accurate.

Circumstances where simulations are helpful---or even essential---occur in a range of different situations within quantitative research.
To set the stage for our subsequent presentation, consider the following areas where one might find need of simulation.

\subsection{Comparing statistical approaches}\label{comparing-statistical-approaches}

One of the more common uses of Monte Carlo simulation is to compare alternative statistical approaches to analyzing the same type of data.
In the academic literature on statistical methodology, authors frequently report simulation studies comparing a newly proposed method against more traditional approaches, to make a case for the utility of their method.
As Dr.~Little puts it in his article \emph{Ten simple powerful ideas for the statistical scientist} (with ``Embrace Well-Designed Simulation Experiments'' being one of the ten!), ``thoughtfully designed frequentist simulation experiments can cast useful light on the properties of a method'' \citep{little2013praise}.

A classic example of simulation for such evaluation is \citet{brown1974SmallSampleBehavior}, who compared four different procedures for conducting a hypothesis test for equality of means in several populations (i.e., one-way ANOVA) when the population variances are not equal.
A subsequent study by \citet{mehrotra1997ImprovingBrownforsytheSolution} built on Brown and Forsythe's work, proposing a more refined method and using simulations to demonstrate that it is superior to the existing methods.
We explore the \citet{brown1974SmallSampleBehavior} study in the case study of Chapter \ref{case-ANOVA}.

Comparative simulation can also have a practical application: In many situations, more than one data analysis approach is possible for addressing a given research question (or estimating a specified target parameter).
Simulations comparing multiple approaches can be quite informative and can help to guide the design of an analytic plan (such as plans included in a pre-registered study protocol).
For instance, researchers designing a multi-site randomized experiment might wonder whether they should use an analytic model that allows for variation in the site-specific impact estimates \citep{miratrix2021applied} or a simpler model that treats the impact as homogeneous across sites.
What are the practical benefits and costs of using the more complex model?
In the ideal case, simulations can identify best practices for how to approach analysis of a certain type of data and can surface trade-offs between competing approaches that occur in practice.

\subsection{Assessing performance of complex pipelines}\label{assessing-performance-of-complex-pipelines}

In practice, statistical methods are often used as part of a multi-step workflow.
For instance, in a regression model, one might first use a statistical test for heteroskedasticity (e.g., the White test or the Breusch-Pagan test) and then determine whether to use conventional or heteroskedasticity-robust standard errors depending on the result of the test.
This combination of an initial diagnostic test followed by contingent use of different statistical procedures is quite difficult to analyze mathematically, but it is straight-forward to simulate \citep[see, for example,][]{longUsingHeteroscedasticityConsistent2000}.
In particular, simulations are a straight-foward way to assess whether a proposed workflow is \emph{valid}---that is, whether the conclusions from a pipeline are correct at a given level of certainty.

Beyond just evaluating the performance characteristics of a workflow, simulating a multi-step workflow can actually be used as a technique for \emph{conducting} statistical inference with real data.
Data analysis approaches such as randomization inference and bootstrapping involve repeatedly simulating data and putting it through an analytic pipeline, in order to assess the uncertainty of the original estimate based on real data.
In bootstrapping, the variation in a point estimate across replications of the simulation is used as the standard error for the context being simulated;
an argument by analogy (the bootstrap analogy) is what connects this to inference on the original data and point estimate.
See the first few chapters of \citet{davison1997BootstrapMethodsTheir} or \citet{efron2000BootstrapModernStatistics} for further discussion of bootstrapping, and see \citet{good2013permutation} or \citet{lehmann1975statistical} for more on permutation inference.

\subsection{Assessing performance under misspecification}\label{assessing-performance-under-misspecification}

Many statistical estimation procedures are known to perform well when the assumptions they entail are correct.
However, data analysts must also be concerned with the \emph{robustness} of estimation procedures---that is, their performance when one or more of the assumptions is violated to some degree.
For example, in a multilevel model, how important is the assumption that the random effects are normally distributed? What about normality of the individual-level error terms? What about homoskedasticity of the individual-level error terms?
Quantitative researchers routinely contend with such questions when analyzing empirical data, and simulation can provide some answers.

Similar concerns arise for researchers considering the trade-offs between methods that make relatively stringent assumptions versus methods that are more flexible or adaptive. When the true data-generating process meets stringent assumptions (e.g., a treatment effect that is constant across the population of participants), what are the potential gain of exploiting such structure in the estimation process?
Conversely, what are the costs (in terms of computation time or precision) of using more flexible methods that do not impose strong assumptions?
A researcher designing an analytic plan would want to be well-informed of such trade-offs and, ideally, would want to situate their understanding in the context of the empirical phenomena that they study.
Simulation allows for such investigation and comparison.

\subsection{Assessing the finite-sample performance of a statistical approach}\label{assessing-the-finite-sample-performance-of-a-statistical-approach}

Many statistical estimation procedures can be shown (through mathematical analysis) to work well \emph{asymptotically}---that is, given an infinite amount of data---but their performance for data of a given, finite size is more difficult to quantify.
Although mathematical theory can inform us about ``asymptopia,'' empirical researchers live in a world of finite sample sizes, where it can be difficult to gauge if one's real data is large enough that the asymptotic approximations apply.
For example, this is of particular concern with hierarchical data structures that include only 20 to 40 clusters---a common circumstance in many randomized field trials in education research.
Simulation is a tractable approach for assessing the small-sample performance of such estimation methods or for determining minimum required sample sizes for adequate performance.

One example of a simulation investigating questions of finite-sample behavior comes from \citet{longUsingHeteroscedasticityConsistent2000}, whose evaluated the performance of heteroskedasticity-robust standard errors (HRSE) in linear regression models.
Asymptotic analysis indicates that HRSEs work well (in the sense of providing correct assessments of uncertainty) in sufficiently large samples \citep{White1980heteroskedasticity}, but what about in realistic contexts where small samples occur?
\citet{longUsingHeteroscedasticityConsistent2000} use extensive simulations to investigate the properties of different versions of HRSEs for linear regression across a range of sample sizes, demonstrating that the most commonly used form of these estimators often does \emph{not} work well with sample sizes found in typical social science applications.
Via simulation, they provided compelling evidence about a problem without having to wade into a technical (and potentially inaccessible) mathematical analysis of the problem.

\subsection{Conducting Power Analyses}\label{conducting-power-analyses}

During the process of proposing, seeking funding for, and planning an empirical research study, researchers need to justify the design of the study, including the size of the sample that they aim to collect.
Part of such justifications may involve a \emph{power analysis}, or an approximation of the probability that the study will show a statistically significant effect, given assumptions about the magnitude of true effects and other aspects of the data-generating process.
Researchers may also wish to compare the power of different possible designs in order to inform decisions about how to carry out the proposed study given a set of monetary and temporal constraints.

Many guidelines and tools are available for conducting power analysis for various research designs, including software such as \href{https://www.causalevaluation.org/power-analysis.html}{PowerUp!} \citep{dong2013PowerUpToolCalculating}, \href{https://www.thegeneralizer.org/}{the Generalizer} \citep{tipton2014stratified}, \href{https://www.psychologie.hhu.de/arbeitsgruppen/allgemeine-psychologie-und-arbeitspsychologie/gpower}{G*Power} \citep{faul2009StatisticalPowerAnalyses}, and \href{https://cran.r-project.org/web//packages/PUMP/index.html}{PUMP} \citep{hunter2023PowerMultiplicityProject}.
These tools use analytic formulas for power, which are often derived using approximations and simplifying assumptions about a planned design. Simulation provides a very general-purpose alternative for power calculations, which can avoid such approximations and simplifications.
By repeatedly simulating data based on a hypothetical process and then analyzing data following a specific protocol, one can \emph{computationally} approximate the power to detect an effect of a specified size.

Using simulation instead of analytic formulas allows for power analyses that are more nuanced and more tailored to the researcher's circumstance than what can be obtained from available software.
For example, simulation can be useful for the following:

\begin{itemize}
\item
  When estimating power in multi-site, block- or cluster-randomized trials, the formulas implemented in available software assume that sites are of equal size and that outcome distributions are unrelated to the size of each site.
  Small deviations from these assumptions are unlikely to change the results, but in practice, researchers may face situations where sites vary quite widely in size or where site-level outcomes are related to site size.
  Simulation can estimate power in this case.
\item
  Available software such as \href{https://www.causalevaluation.org/power-analysis.html}{PowerUp!} allows investigators to build in assumptions about anticipated rates of attrition in cluster-randomized trials, under the assumption that attrition is completely at random and unrelated to anything.
  However, researchers might anticipate that, in practice, attrition will be related to baseline characteristics.
  Simulation can be used to assess how this might affect the power of a planned study.
\item
  There are some closed-form expressions for power to test mediational relations (i.e., indirect and direct effects) in a variety of different experimental designs, and these formulas are now available in \href{https://www.causalevaluation.org/power-analysis.html}{PowerUp!}.
  However, the formulas involve a large number of parameters (including some where it may be difficult to develop credible assumptions) and they apply only to a specific analytic model for the mediating relationships.
  Researchers planning a study to investigate mediation might therefore find it useful to generate realistic data structures and conduct power analysis via simulation.
\end{itemize}

\subsection{Simulating processess}\label{simulating-processess}

Yet another common use for Monte Carlo simulation is as a way to emulate a complex process as a means to better understand it or to evaluate the consequences of modifying it.
A famous area of process simulation are climate models, where researchers simulate the process of climate change.
These physical simulations mimic very complex systems to try and understand how perturbations (e.g., more carbon release) will impact downstream trends.

Another example of process simulation arises in education research.
Some large school districts such as New York City have centralized lotteries for school assignment, which entail having families rank schools by order of preference.
The central office then assigns students to schools via a lottery procedure where each student gets a lottery number that breaks ties when there are too many students desiring to go to certain schools.
Students' school assignments are therefore based in part on random chance, but the the process is quite complex: each student has some probability of assignment to each school on their list, but the probabilities depend on their choices and the choices of other students.

The school lottery process creates a natural experiment, based on which researchers can estimate the causal impact of being assigned to one school vs.~another.
A defensible analysis of the process requires knowing the probabilities of school assignment. \citet{abdulkadirouglu2017research} conducted such an evaluation using the school lottery process in New York City.
They calculated school assignment probabilities via simulation, by running the school lottery over and over, changing only students' lottery numbers, and recording students' school assignments in each repetition of the process.
Simulating the lottery process a large number of times provided precise estimates of each students' assignment probabilities, based on which \citet{abdulkadirouglu2017research} were able to estimate causal impacts of school assignment.

For another example, one that possibly illustrates the perils of simulation as taking us away from results that pass face validity, \citet{staiger2010searching} simulated the process of firing teachers depending on their estimated value-added scores.
Based on their simulations, which model firing different proportions of teachers, they suggest that firing substantial portions of the teacher workforce annually would substantially improve student test scores.
Their work offers a clear illustration of how simulations can be used to examine the potential consequences of various policy decisions, assuming the underlying assumptions hold true.
This example also brings home a core concern of simulation: we only learn about the world we are simulating, and the relevance of simulation evidence to the real world is by no means guaranteed.

\section{The perils of simulation as evidence}\label{the-perils-of-simulation-as-evidence}

Simulation has the potential to be a powerful tool for investigating quantitative methods.
However, evidence from simulation studies is also fundamentally limited in certain ways, and thus very susceptible to critique.
The core advantage of simulation studies is that they allow for evaluation of data analysis methods under \emph{specific and exact conditions}, avoiding the need for approximation.
The core limitation of simulations stems from this same property: they provide information about the performance of data analysis methods under specified conditions, but provide no guarantee that patterns of performance hold in general.
One can partially address questions of generalization by examining a wide range of conditions, looking to see whether a pattern holds consistently or changes depending on features of the data-generating process.
Even this strategy has limitations, though.
Except for very simple processes, we can seldom consider every possible set of conditions.

As we will see in later chapters, the design of a simulation study typically entails making choices over very large spaces of possibility.
This flexibility leaves lots of room for discretion and judgement, and even for personal or professional biases \citep{boulesteix2020Replication}.
Due to this flexibility, simulation findings are held in great skepticism by many.
The following motto summarizes the skeptic's concern:

\begin{quote}
Simulations are doomed to succeed.
\end{quote}

As this motto captures, simulations are alluring: once a simulation framework is set up, it is easy to tweak and adjust.
It is natural for us all to continue to do this until the simulation works ``as it should.''
If our goal is to show something that we already believe is correct (e.g., that our fancy new estimation procedure is better than existing methods), we could probably find a way to align our simulation with our intuition.\footnote{A comment from James: I recall attending seminars in the statistics department during graduate school, where guest speakers usually presented both some theory and some simulation results. A few years into my graduate studies, I realized that the simulation part of the presentation could nearly always be replaced with a single slide that said ``we did some simulations and showed that our new method works better than old methods under conditions that we have cleverly selected to be favorable for our approach.'' I hope that my own work is not as boring or predictable as my memory of these seminars.}

Critiques of simulation studies often revolve around the \emph{realism}, \emph{relevance}, or \emph{generality} of the data generating process.
Are the simulated data realistic, in the sense that they follow similar patterns to what one would see in real empirical data?
Are the explored aspects of the simulation relevant to what we would expect to find in practice?
Was the simulation systematic in exploring a wide variety of scenarios, so that general conclusions are warranted?

We see at least three principles for addressing such questions in one's own work.
Perhaps most fundamental is to be transparent in one's methods and reasoning:
explicitly state what was done, and provide code so that others can reproduce one's results or tweak them to test variations of the data-generating process or alternative analysis strategies.
Another important component of a robust argument is to systematically vary the conditions under examination.
This is facilitated by writing code in a way to make it easy to simulate across a range of different data-generating scenarios.
Once that is in place, one can systematically explore myriad scenarios and report all of the results.
An aspiration of the simulation architect should be to explore the boundary conditions that separate where preferred methods work and where they break or fail.
Finally, one can draw on relevant statistical theory to support the design of a simulation and interpretation of its results.
Mathematical analysis might indicate that some features of a data-generating process will have a strong influence on the performance of a method, while other features will not matter at all when sample sizes are sufficiently large.
Well designed simulations will examine conditions that are motivated by or complement what is known based on existing statistical theory.

In addition to these principles, methodologists have proposed broader changes in practice to counter the potential for bias in methodological simulation studies.
\citet{morris2019UsingSimulationStudies} introduced a formal framework, called ADEMP, to guide the reporting of methodological simulations.
\citet{boulesteix2013Plea} argued for greater use of neutral comparison studies, in which the performance of alternative statistical methods are compared under a range of relevant conditions by methodological researchers who do not have vested interests in any specific method \citep[see also][]{boulesteix2017evidencebased}.
Further, \citet{siepe2024SimulationStudiesMethodological} argue for more routine pre-registration of methodological simulations to bring greater transparency and reduce the possibility of bias arising from flexibility in their design.

\section{Simulating to learn}\label{simulating-to-learn}

Most of the examples of Monte Carlo simulation that we have mentioned thus far are drawn from formal methodological research, published in methodologically focused research journals.
If you do not identify as a methodologist, you may be wondering whether there is any benefit to learning how to do simulations---what's the point, if you are never going to conduct methodological research studies or use simulation to aid in planning an empirical study?
However, we believe that simulation is an incredibly useful tool---and well worth learning, even outside the context of formal methodological research---for at least two reasons.

First, in order to do any sort of quantitative data analysis, you will need to make decisions about what methods to use.
Across fields, existing guidance about data analysis practice is almost certainly informed by simulation research of some form, whether well-designed and thorough or haphazard and poorly reasoned.
Consequently, having a high-level understanding of the logic and limitations of simulation will help you to be a critical consumer of methods research, even if you do not intend to conduct methods research of your own.

Second, we believe conducting simulations deepens one's understanding of the logic of statistical modeling and statistical inference.
Learning a new statistical model (such as generalized linear mixed models) or analytic technique (such as multiple imputation by chained equations) requires taking in \emph{a lot} of detailed information, from the assumptions of the model to the interpretation of parameter estimates to the best practices for estimation and what to do if some part of the process goes off.
To thoroughly digest all these details, we have found it invaluable to \emph{simulate} data based on the model under consideration.
This usually requires translating mathematical notation into computer code, an exercise which makes the components of the model more tangible than just a jumble of Greek letters.
The simulated data is then available for inspection, summarizing, graphing, and further calculation, all of which can aid comprehension and interpretation.
Moreover, the process of simulating yields a dataset which can then be used to practice implementing the analysis procedure and interpreting the results.
We have found that building a habit of simulating is a highly effective way to learn new models and methods, worthwhile even if one has no intention of carrying out methodological research.
We might even go so far as to argue that \emph{whatever you might think, you don't really understand a statistical model until you've done a simulation of it.}

\section{Why R?}\label{why-r}

This book aims not only to introduce the conceptual principles of Monte Carlo simulation, but also to provide a practical guide to actually \emph{conducting} simulation studies (whether for personal learning purposes or for formal methodological research).
And conducting simulations requires writing computer code (sometimes, lots of code!).
The computational principles and practices that we will describe are very general, not specific to any particular programming language, but for purposes of demonstrating, presenting examples, and practicing the process of developing a simulation, it helps to be specific.
To that end, we will be using R, a popular programming language that is widely used among statisticians, quantitative methodologists, and data scientists.
Our presentation will assume that readers are comfortable with writing R scripts to carry out tasks such as cleaning variables, summarizing data, creating data-based graphics, and running regression models (or more generally, estimating statistical models).

We have chosen to focus on R (rather than some other programming language) because both of us are intimately familiar with R and use it extensively in our day-to-day work.
Simply put, it is much easier to write in your native language than in one in which you are less fluent.
But beyond our own habits and preferences, there are several more principled reasons for using R.

R is free and open source software, which can be run under many different operating systems (Windows, Mac, Linux, etc.).
This is advantageous not only because of the cost, but also because it means that anyone with a computer---anywhere in the world---can access the software and could, if they wanted, re-run our provided code for themselves.
This makes R a good choice for practicing transparent and open science processes.

There is a very large, active, and diverse community of people who use, teach, and develop R.
It is used widely for applied data analysis and statistical work in such fields as education, psychology, economics, epidemiology, public health, and political science,
and is widely taught in quantitative methods and applied statistics courses.
Integral to the appeal of R is that it includes tens of thousands of contributed packages, which extend the core functionality of the language in myriad ways.
New statistical techniques are often quickly available in R, or can be accessed through R interfaces.
Increasingly, R can also be used to interface with other languages and platforms, such as running Python code via the \href{https://rstudio.github.io/reticulate/}{\texttt{reticulate}} package, running Stan programs for Bayesian modeling via \href{https://mc-stan.org/users/interfaces/rstan}{\texttt{RStan}}, or calling the h2o machine learning library using the \href{https://cran.r-project.org/package=h2o}{\texttt{h2o} package} \citep{fryda2014H2oInterfaceH2O}.
The huge variety of statistical tools available in R makes it a fascinating place to learn and practice.

R does have a persistent reputation as being a challenging and difficult language to use.
This reputation might be partly attributable to its early roots, having been developed by highly technical statisticians who did not necessarily prioritize accessibility, legibility of code, or ease of use.
However, as the R community has grown, the availability of introductory documentation and learning materials has improved drastically, so that it is now much easier to access pedagogical materials and find help.

R's reputation also probably partly stems from being a decentralized, open source project with many, many contributors.
Contributed R packages vary hugely in quality and depth of development; there are some amazingly powerful tools available but also much that is half-baked, poorly executed, or flat out wrong.
Because there is no central oversight or quality control, the onus is on the user to critically evaluate the packages that they use.
For newer users especially, we recommend focusing on more established and widely used packages, seeking input and feedback from more knowledgeable users, and taking time to validate functionality against other packages or software when possible.

A final contributor to R's intimidating reputation might be its extreme flexibility.
As both a statistical analysis package and a fully functional programming language, R can do many things that other software packages cannot, but this also means that there are often many different ways to accomplish the same task.
In light of this situation, it is good to keep in mind that knowing a single way to do something is usually adequate---there is no need to learn six different words for hello, when one is enough to start a conversation.

On balance, we think that the many strengths of R make it worthwhile to learn and continue exploring. For simulation, in particular, R's facility to easily write functions (bundles of commands that you can easily call in different manners), to work with multiple datasets in play at the same time, and to leverage the vast array of other people's work all make it a very attractive language.

\section{Organization of the text}\label{organization-of-the-text}

We think of simulation studies as falling on a spectrum of formality.
On the least formal end, we may use simulations to learn a statistical model, investigating questions purely to satisfy our own curiosity.
On the most formal end, we may conduct carefully designed, pre-registered simulations that compare the performance of competing statistical methods across an array of data-generating conditions designed to inform practice in a particular research area.

Our central goal is to help you learn to work anywhere along this spectrum, from the most casual to the most principled.
To support that goal, the book is designed with a spiral structure, where we first present simpler and less formal examples that illustrate high-level principles, then revisit the component pieces, dissecting and exploring them in greater depth.
We defer discussion of concepts that are relevant only to more formal use-cases (such as the ADEMP framework of \citet{morris2019UsingSimulationStudies}) until later chapters.
We have also included many case studies throughout the book; these are designed to make the topics under discussion tangible, and are also designed to provide chunks of code that you emulate or even directly copy and use for your own purposes.

We divided the book into several parts.
In Part I (which you are reading now), we lay out our case for learning simulation, introduces some guidance on programming, and presents an initial, very simple simulation to set the stage for later discussion of design principles.
Part II lays out the core components (generating artificial data, applying analytic procedures, executing the simulations, and analyzing the simulation results) for simulating a single scenario. It then presents some more involved case studies that illustrate the principles of modular, tidy simulation design.
Part III moves to multifactor simulations, meaning simulations that look at more than one scenario or context.
Multifactor simulation is central to the design of more formal simulation studies because it is only by evaluating or comparing estimation procedures across multiple scenarios that we can begin to understand their general properties.

The book closes with two final parts.
Part IV covers some technical challenges that are commonly encountered when programming simulations, including reproducibility, parallel computing, and error handling.
Part V covers three extensions to specialized forms of simulation: simulating to calculate power, simulating within a potential outcomes framework for causal inference, and the parametric bootstrap. The specialized applications underscore how simulation can be used to answer a wide variety of questions across a range of contexts, thus connecting back to the broader purposes discussed above.
The book also includes appendices with some further guidance on writing R code and pointers to further resources.

\chapter{Programming Preliminaries}\label{programming-preliminaries}

In this chapter, we introduce some essential programming concepts that may be less familiar to readers, but which are central to how we approach writing code for simulation studies.
We also explain some of the rationale and reasoning behind how we present example code throughout the book.

\section{Welcome to the tidyverse}\label{welcome-to-the-tidyverse}

Layered on top of R are a collection of contributed packages that make data wrangling and management much, much easier.
This collection is called \texttt{tidyverse} and it includes popular packages such as \texttt{dplyr}, \texttt{tidyr}, and \texttt{ggplot2}.
We use methods from the ``tidyverse'' throughout the book because it facilitates writing clean, concise code.
In particular, we make heavy use of the \texttt{dplyr} package for group-wise data manipulation, the \texttt{purrr} package for functional programming, and the \texttt{ggplot2} package for statistical graphics.
The \href{https://r4ds.had.co.nz/}{1st edition} or \href{https://r4ds.hadley.nz/}{2nd edition} of the free online textbook \emph{R for Data Science} provide an excellent, thorough introduction to these packages, along with much more background on the tidyverse.
We will cite portions of this text throughout the book.

Loading the tidyverse packages is straightforward:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{( tidyverse )}
\FunctionTok{options}\NormalTok{(}\FunctionTok{list}\NormalTok{(}\AttributeTok{dplyr.summarise.inform =} \ConstantTok{FALSE}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

(The second line is to turn off some of the persistent warnings generated by the \texttt{dplyr} function \texttt{summarize()}.)
These lines of code appear in the header of nearly every script we use.

\section{Functions}\label{functions}

If you are comfortable using R for data analysis tasks, you will be familiar with many of R's functions.
R has function to do things like calculate a summary statistic from a list of numbers (e.g., \texttt{mean()}, \texttt{median()}, or \texttt{sd()}), calculate linear regression coefficient estimates from a dataset (\texttt{lm()}), or count the number of rows in a dataset (\texttt{nrow()}).
In the abstract, a function is a little machine for transforming ingredients into outputs, like a microwave (put a bag of kernels in and it will return hot, crunchy popcorn), a cheese shredder (put a block of mozzarella in and it transforms it into topping for your pizza), or a washing machine (put in dirty clothes and detergent and it will return clean but damp clothes).
A function takes in pieces of information specified by the user (the inputs), follows a set of instructions for transforming or summarizing those inputs, and then returns the result of the calculations (the outputs).

A function can do nearly anything as long as the calculation can be expressed in code---it can even produce output that is random.
For example, the \texttt{rnorm()} function takes as input a number \texttt{n} and returns that many random numbers, drawn from a standard normal distribution:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{rnorm}\NormalTok{(}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] -0.5203421 -0.6884585  0.1672914
\end{verbatim}

Each time the function is called, it returns a different set of numbers:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{rnorm}\NormalTok{(}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  2.3450553  0.9016549 -1.3682721
\end{verbatim}

The \texttt{rnorm()} function also has further input arguments that let the user specify the mean and standard deviation of the distribution from which numbers are drawn:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{rnorm}\NormalTok{(}\DecValTok{3}\NormalTok{, }\AttributeTok{mean =} \DecValTok{10}\NormalTok{, }\AttributeTok{sd =} \FloatTok{0.5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  9.949905 10.322265  9.214903
\end{verbatim}

In writing code for simulations, we will make extensive use of this particular function and other functions that produce sequences of random numbers.
We will have more to say about random number generation later.

\subsection{Rolling your own}\label{rolling-your-own}

In R, you can create your own function by specifying the pieces of input information, the steps to follow in transforming the inputs, and the result to return as output.
Learning to write your own functions to carry out calculations is an immensely useful skill that will greatly enhance your ability to accomplish a range of tasks.
Writing custom functions is also central to our approach to coding Monte Carlo simulations, and so we highlight some of the key considerations here.
\href{https://r4ds.had.co.nz/functions.html}{Chapter 19 of R for Data Science (1st edition)} provides an in-depth discussion of how to write your own functions.

Here is an example of a custom function called \texttt{one\_pval()}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{one\_pval }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{( N, mn, sd ) \{}
\NormalTok{  vals }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{( N, }\AttributeTok{mean =}\NormalTok{ mn, }\AttributeTok{sd =}\NormalTok{ sd )}
\NormalTok{  tt }\OtherTok{\textless{}{-}} \FunctionTok{t.test}\NormalTok{( vals )}
\NormalTok{  pvalue }\OtherTok{\textless{}{-}}\NormalTok{ tt}\SpecialCharTok{$}\NormalTok{p.value}
  \FunctionTok{return}\NormalTok{(pvalue)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

The first line specifies that we are creating a function that takes inputs \texttt{N}, \texttt{mn}, and \texttt{sd}. These are called the \emph{parameters}, \emph{inputs}, or \emph{arguments} of the function. The remaining lines inside the curly brackets are called the \emph{body} of the function. These lines specify the instructions to follow in transforming the inputs into an output:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Generate a random sample of \texttt{N} observations from a normal distribution with mean \texttt{mn} and store the result in \texttt{vals}.
\item
  Use the built-in function \texttt{t.test()} to compute a one-sample t-test for the null hypothesis that the population mean is zero, then store the result in \texttt{tt}.
\item
  Extract the p-value from the t-test store the result in \texttt{pvalue}.
\item
  Return \texttt{pvalue} as output.
\end{enumerate}

Having created the function, we can then use it with any inputs that we like:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{one\_pval}\NormalTok{( }\DecValTok{100}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{1}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 6.778675e-72
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{one\_pval}\NormalTok{( }\DecValTok{10}\NormalTok{, }\FloatTok{0.3}\NormalTok{, }\DecValTok{1}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.7639374
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{one\_pval}\NormalTok{( }\DecValTok{10}\NormalTok{, }\FloatTok{0.3}\NormalTok{, }\FloatTok{0.2}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.0002843784
\end{verbatim}

In each case, the output of the function is a p-value from a simulated sample of data. The function produces a different answer each time because its instructions involve generating random numbers each time it is called.
In essence, our custom function is just a short-cut for carrying out its instructions.
Writing it saves us from having to repeatedly write or copy-paste the lines of code inside its body.

\subsection{A dangerous function}\label{a-dangerous-function}

Writing custom functions will prove to be crucial for effectively implementing Monte Carlo simulations.
However, designing custom functions does take practice to master.
It also requires a degree of care above and beyond what is needed just to use R's built-in functions.

One of the common mistakes encountered in writing custom functions is to let the function depend on information that is not part of the input arguments.
For example, consider the following script, which includes a nonsensical custom function called \texttt{funky()}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{secret }\OtherTok{\textless{}{-}} \DecValTok{3}

\NormalTok{funky }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(input1, input2, input3) \{}
  
  \CommentTok{\# do funky stuff}
\NormalTok{  ratio }\OtherTok{\textless{}{-}}\NormalTok{ input1 }\SpecialCharTok{/}\NormalTok{ (input2 }\SpecialCharTok{+} \DecValTok{4}\NormalTok{)}
\NormalTok{  funky\_output }\OtherTok{\textless{}{-}}\NormalTok{ input3 }\SpecialCharTok{*}\NormalTok{ ratio }\SpecialCharTok{+}\NormalTok{ secret}
  
  \FunctionTok{return}\NormalTok{(funky\_output)  }
\NormalTok{\}}

\FunctionTok{funky}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 5.5
\end{verbatim}

\texttt{funky} takes inputs \texttt{input1}, \texttt{input2}, and \texttt{input3}, but its instructions also depend on the quantity \texttt{secret}.
What happens if we change the value of \texttt{secret}?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{secret }\OtherTok{\textless{}{-}} \DecValTok{100}
\FunctionTok{funky}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 102.5
\end{verbatim}

Even though we give it the same arguments as previously, the output of the function is different.
This sort of behavior is confusing.
Unless the function involves generating random numbers, we would generally expect it to return the exact same output if we give it the same inputs.
Even worse, we get a rather cryptic error if the value of \texttt{secret} is not compatible with what the function expects:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{secret }\OtherTok{\textless{}{-}} \StringTok{"A"}
\FunctionTok{funky}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Error in input3 * ratio + secret: non-numeric argument to binary operator
\end{verbatim}

If we are not careful, we will end up with very confusing code that can very easily lead to unintended results and errors.

To avoid this issue, it is important for functions to only use information that is explicitly provided to it through its arguments.
This is the principle of \emph{isolating the inputs}.
If the result of a function is supposed to depend on a quantity, then we should include that quantity among the input arguments.
We can fix our example function by including \texttt{secret} as an argument:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{secret }\OtherTok{\textless{}{-}} \DecValTok{3}

\NormalTok{funkier }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(input1, input2, input3, secret) \{}
  
  \CommentTok{\# do funky stuff}
\NormalTok{  ratio }\OtherTok{\textless{}{-}}\NormalTok{ input1 }\SpecialCharTok{/}\NormalTok{ (input2 }\SpecialCharTok{+} \DecValTok{4}\NormalTok{)}
\NormalTok{  funky\_output }\OtherTok{\textless{}{-}}\NormalTok{ input3 }\SpecialCharTok{*}\NormalTok{ ratio }\SpecialCharTok{+}\NormalTok{ secret}
  
  \FunctionTok{return}\NormalTok{(funky\_output)  }
\NormalTok{\}}

\FunctionTok{funkier}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 5.5
\end{verbatim}

Now the output of the function is always the same, regardless of the value of other objects in R:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{secret }\OtherTok{\textless{}{-}} \DecValTok{100}
\FunctionTok{funkier}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 5.5
\end{verbatim}

The \emph{input parameter} \texttt{secret} holds sway here, even though there is also an object with the same name.\footnote{To learn more about how R determines which values to use when executing a function, see \href{https://adv-r.hadley.nz/functions.html\#lexical-scoping}{Section 6.4 of Advance R}.} If we want to try 100, we have to do so \emph{explicitly}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{funkier}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{100}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 102.5
\end{verbatim}

When writing your own functions, it may not be obvious that your function depends on external quantities and does not isolate the inputs.
In our experience, one of the best ways to detect this issue is to clear the R environment and start from a fresh palette, run the code to create the function, and call the function (perhaps more than once) to ensure that it works as expected.
Here is an illustration of what happens when we follow this process with our problematic custom function:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# clear environment}
\FunctionTok{rm}\NormalTok{(}\AttributeTok{list=}\FunctionTok{ls}\NormalTok{()) }

\CommentTok{\# create function}
\NormalTok{funky }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(input1, input2, input3) \{}
  
  \CommentTok{\# do funky stuff}
\NormalTok{  ratio }\OtherTok{\textless{}{-}}\NormalTok{ input1 }\SpecialCharTok{/}\NormalTok{ (input2 }\SpecialCharTok{+} \DecValTok{4}\NormalTok{)}
\NormalTok{  funky\_output }\OtherTok{\textless{}{-}}\NormalTok{ input3 }\SpecialCharTok{*}\NormalTok{ ratio }\SpecialCharTok{+}\NormalTok{ secret}
  
  \FunctionTok{return}\NormalTok{(funky\_output)  }
\NormalTok{\}}

\CommentTok{\# test the function}
\FunctionTok{funky}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Error in funky(3, 2, 5): object 'secret' not found
\end{verbatim}

We get an error because the external quantity is not available.
Here is the same process using our corrected function:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# clear environment}
\FunctionTok{rm}\NormalTok{(}\AttributeTok{list=}\FunctionTok{ls}\NormalTok{()) }

\CommentTok{\# create function}
\NormalTok{funkier }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(input1, input2, input3, secret) \{}
  
  \CommentTok{\# do funky stuff}
\NormalTok{  ratio }\OtherTok{\textless{}{-}}\NormalTok{ input1 }\SpecialCharTok{/}\NormalTok{ (input2 }\SpecialCharTok{+} \DecValTok{4}\NormalTok{)}
\NormalTok{  funky\_output }\OtherTok{\textless{}{-}}\NormalTok{ input3 }\SpecialCharTok{*}\NormalTok{ ratio }\SpecialCharTok{+}\NormalTok{ secret}
  
  \FunctionTok{return}\NormalTok{(funky\_output)  }
\NormalTok{\}}


\CommentTok{\# test the function}
\FunctionTok{funkier}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{5}\NormalTok{, }\AttributeTok{secret =} \DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 5.5
\end{verbatim}

\subsection{Using Named Arguments}\label{using-named-arguments}

When calling a function (whether it is a built-in function or a custom function that you developed), you can specify which values correspond to which arguments using argument names.
Using argument names greatly enhances the readability and flexibility of function calls.
When you specify inputs by name, R matches the values to the arguments based on the names rather than the order in which they appear.
This feature is particularly useful in complex functions with many optional arguments.

For example, consider the function \texttt{one\_pval()} from \ref{rolling-your-own}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{one\_pval }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{( N, mn, sd ) \{}
\NormalTok{  vals }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{( N, }\AttributeTok{mean =}\NormalTok{ mn, }\AttributeTok{sd =}\NormalTok{ sd )}
\NormalTok{  tt }\OtherTok{\textless{}{-}} \FunctionTok{t.test}\NormalTok{( vals )}
\NormalTok{  pvalue }\OtherTok{\textless{}{-}}\NormalTok{ tt}\SpecialCharTok{$}\NormalTok{p.value}
  \FunctionTok{return}\NormalTok{(pvalue)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

You could call \texttt{one\_pval()} with \emph{named} arguments in any order:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{result }\OtherTok{\textless{}{-}} \FunctionTok{one\_pval}\NormalTok{(}\AttributeTok{sd =} \FloatTok{0.5}\NormalTok{, }\AttributeTok{mn =} \DecValTok{2}\NormalTok{, }\AttributeTok{N =} \DecValTok{500}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

In this call, R knows which value to assign to each argument, so we could list the arguments however we like.

Without naming, we would have to specify the arguments in the exact order that they appear in the function definition:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{result }\OtherTok{\textless{}{-}} \FunctionTok{one\_pval}\NormalTok{( }\DecValTok{500}\NormalTok{, }\DecValTok{2}\NormalTok{, }\FloatTok{0.5}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

Without the argument names, this line of code is harder to follow---you have to know more about the design of the function to understand how it is being used in this instance.

Getting in the habit of using named arguments will help you avoid errors.
If you pass arguments without naming, and in the wrong order, you can end up with very strange results that are hard to diagnose.
Even if you get it right, if someone later changes the function (say by adding a new argument in the middle of the list), your code will suddenly break with no explanation.

\subsection{Argument Defaults}\label{argument-defaults}

Default arguments allow you to specify typical values for parameters that a user might not need to change every time they use the function.
This can make the function easier to use and less error-prone because the defaults ensure that the function behaves sensibly even when some arguments are not explicitly provided. For example, let us revise the \texttt{one\_pval()} function from above to use default arguments:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{one\_pval }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{( }\AttributeTok{N =} \DecValTok{10}\NormalTok{, }\AttributeTok{mn =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{ ) \{}
\NormalTok{  vals }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{( N, }\AttributeTok{mean =}\NormalTok{ mn, }\AttributeTok{sd =}\NormalTok{ sd )}
\NormalTok{  tt }\OtherTok{\textless{}{-}} \FunctionTok{t.test}\NormalTok{( vals )}
\NormalTok{  pvalue }\OtherTok{\textless{}{-}}\NormalTok{ tt}\SpecialCharTok{$}\NormalTok{p.value}
  \FunctionTok{return}\NormalTok{(pvalue)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Now our function has a default for \texttt{N} of 10, for \texttt{mn} of 0, and for \texttt{s} of 1.
This means a user can run the function simply by calling \texttt{one\_pval()} without any inputs. Doing so will generate a p-value from a sample of 10 observations with a mean of zero and a standard deviation of 1.

Once we have our function with defaults, we can call the function while specifying only the inputs that differ from the default values:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bigger\_sample }\OtherTok{\textless{}{-}} \FunctionTok{one\_pval}\NormalTok{( }\AttributeTok{N =} \DecValTok{50}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

The function will use its default values for \texttt{mn} and \texttt{s}.
Using defaults lets the user call the function more succinctly.

Later chapters will have much more to say about the process of writing custom functions, as well as many further illustrations and examples.

\subsection{Function skeletons}\label{function-skeletons}

In discussing how to write functions for simulations, we will often present \emph{function skeletons}. By a skeleton, we mean code that creates a function with a specific set of input arguments, but where the body is left partially or fully unspecified.
Here is a cursory example of a function skeleton:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{run\_simulation }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{( N, J, mu, sigma, tau ) \{}
  \CommentTok{\# simulate data}
  \CommentTok{\# apply estimation procedure}
  \CommentTok{\# repeat}
  \CommentTok{\# summarize results}
  \FunctionTok{return}\NormalTok{(results)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

In subsequent chapters, we will use function skeletons to outline the organization of code for simulation studies.
The skeleton headers make clear what the inputs to the function need to be.
Sometimes, we will leave comments in the body of the skeleton to sketch out the general flow of calculations that need to happen.
Depending on the details of the simulation, the specifics of these steps might be quite different, but the general structure will often be quite consistent.
Finally, the last line of the skeleton indicates the value that should be returned as output of the function.
Thus, skeletons are kind of like \href{https://www.madlibs.com/}{Mad Libs}, but with R code instead of parts of speech.

\section{\texorpdfstring{\texttt{\textbackslash{}\textgreater{}} (Pipe) dreams}{\textbackslash\textgreater{} (Pipe) dreams}}\label{pipe-dreams}

Many of the functions from \texttt{tidyverse} packages are designed to make it easy to use them in sequence via the \texttt{\textbar{}\textgreater{}} symbol, or \emph{pipe}.\footnote{The pipe is a relatively recent addition to R's basic syntax. Prior to its inclusion in base R, the \texttt{magrittr} package provided---and still provides---a pipe symbol \texttt{\%\textgreater{}\%} that works similarly but has some additional syntactic nuances. We use base R's \texttt{\textbar{}\textgreater{}} because it is always available, even without loading any additional packages. To learn more about the nuanced \texttt{\%\textgreater{}\%} pipe and similar operators, see \href{https://magrittr.tidyverse.org/articles/magrittr.html}{the magrittr package}.}
The pipe allows us to \emph{compose} several functions, meaning to write a chain of several functions as a sequence, where the result of each function becomes the first input to the next function.
In code written with the pipe, the order of function calls follows like a story book or cake recipe, making it easier to see what is happening at each step in the sequence.

Consider the hypothetical functions \texttt{f()}, \texttt{g()}, and \texttt{h()}, and suppose we want to do a calculation that involves composing all three functions.
One way to write this calculation is

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{res1 }\OtherTok{\textless{}{-}} \FunctionTok{f}\NormalTok{(my\_data, }\AttributeTok{a =} \DecValTok{4}\NormalTok{)}
\NormalTok{res2 }\OtherTok{\textless{}{-}} \FunctionTok{g}\NormalTok{(res1, }\AttributeTok{b =} \ConstantTok{FALSE}\NormalTok{)}
\NormalTok{result }\OtherTok{\textless{}{-}} \FunctionTok{h}\NormalTok{(res2, }\AttributeTok{c =} \StringTok{"hot sauce"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We have to store the result of each intermediate step in an object, and it takes a careful read of the code to see that we are using \texttt{res1} as input to \texttt{g()} and \texttt{res2} as input to \texttt{h()}.

Alternately, we could try to write all the calculations as one line:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{result }\OtherTok{\textless{}{-}} \FunctionTok{h}\NormalTok{( }\FunctionTok{g}\NormalTok{( }\FunctionTok{f}\NormalTok{( my\_data, }\AttributeTok{a =} \DecValTok{4}\NormalTok{ ), }\AttributeTok{b =} \ConstantTok{FALSE}\NormalTok{ ), }\AttributeTok{c =} \StringTok{"hot sauce"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

This is a mess. It takes very careful parsing to see that the \texttt{b} argument is called as part of \texttt{g()} and the \texttt{c} argument is part of h()`, and the order in which the functions appear is not the same as the order in which they are calculated.

With the pipe we can write the same calculation as

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{result }\OtherTok{\textless{}{-}} 
\NormalTok{  my\_data }\SpecialCharTok{|\textgreater{}}         \CommentTok{\# initial dataset}
  \FunctionTok{f}\NormalTok{(}\AttributeTok{a =} \DecValTok{4}\NormalTok{) }\SpecialCharTok{|\textgreater{}}        \CommentTok{\# do f() to it}
  \FunctionTok{g}\NormalTok{(}\AttributeTok{b =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{|\textgreater{}}    \CommentTok{\# then do g()}
  \FunctionTok{h}\NormalTok{(}\AttributeTok{c =} \StringTok{"hot sauce"}\NormalTok{) }\CommentTok{\# then do h()}
\end{Highlighting}
\end{Shaded}

This addresses the all the issues with our previous attempts:
the order in which the functions appear is the same as the order in which they are executed;
the additional arguments are clearly associated with the relevant functions;
and there is only a single object holding the results of the calculations.
Pipes are a very nice technique for writing clear code that is easy for others to follow.\footnote{\href{https://r4ds.hadley.nz/data-transform.html\#sec-the-pipe}{Chapter 3.4 of R for Data Science (2nd edition)} provides more discussion and examples of how to use \texttt{\textbar{}\textgreater{}}. \href{https://r4ds.had.co.nz/pipes.html}{Chapter 18 of R for Data Science (1st edition)} provides more discussion and examples of how to use \texttt{magrittr}'s \texttt{\%\textgreater{}\%}.}

\section{Recipes versus Patterns}\label{recipes-versus-patterns}

As we will elaborate in subsequent chapters, we follow a modular approach to writing simulations, in which each component of the simulation is represented by its own custom function or its own object in R.
This modular approach leads to code that always has the same broad structure and where the process of implementing the simulation follows a set sequence of steps.
We start by coding a data-generating process, then write one or more data-analysis methods, then determine how to evaluate the performance of the methods, and finally implement an experimental design to examine the performance of the methods across multiple scenarios.
Over the next several chapters, we will walk through this process several times.

Although we always follow the same broad process, the case studies that we will present are \emph{not} intended as a cookbook that must be rigidly followed.
In our experience, the specific features of a data-generating model, estimator, or research question sometimes require tweaking the template or switching up how we implement some aspect of the simulation.
And sometimes, it might just be a question of style or preference.
Because of this, we have purposely constructed the examples presented throughout the book to use different variations of our central theme rather than always following the exact same style and structure.
We hope that presenting these variants and adaptations will both expand your sense of what is possible and also help you to recognize the core design principles---in other words, to distinguish the forest from the trees.
Of course, we would welcome and encourage you to take any of the code verbatim, tweak and adapt it for your own purposes, and use it however you see fit.
Adapting a good example is usually much easier than starting from a blank screen.

\section{Exercises}\label{exercises}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Revise the \texttt{one\_pval()} function to use \texttt{\textbar{}\textgreater{}} instead of storing the simulated sample in \texttt{vals}.
\item
  Modify the \texttt{one\_pval()} function to return a \texttt{tibble()} that includes separate columns for the test statistic (called \texttt{tt\$statistic}), the p-value, and the lower and upper end-points of the confidence interval (called \texttt{tt\$conf.int}).
\item
  Modify the \texttt{one\_pval()} function so that the sample of data is generated from a non-central t distribution by substituting R's \texttt{rt()} function in place of \texttt{rnorm()}. Make sure to modify the arguments (and argument names) of \texttt{one\_pval()} to allow the user to specify the non-centrality and degrees of freedom parameters of the non-central t distribution.
\item
  The non-central t distribution is usually parameterized in terms of non-centrality parameter \(\delta\) and degrees of freedom \(\nu\), and these parameters determine the mean and spread of the distribution. Specifically, the mean of the non-central t distribution is
  \[\text{E}(T) = \delta \times \sqrt{\frac{\nu}{2}} \times \frac{\Gamma((\nu - 1) / 2)}{\Gamma(\nu / 2)},\]
  where \(\Gamma()\) is the gamma function (called \texttt{gamma()} in R). Create a version of the \texttt{one\_pval()} function that generates data based on a non-central t distribution, but where the input arguments are \texttt{mn} for the mean and \texttt{df} for the degrees of freedom. Here is a function skeleton to get started:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{one\_pval }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{( }\AttributeTok{N =} \DecValTok{10}\NormalTok{, }\AttributeTok{mn =} \DecValTok{5}\NormalTok{, }\AttributeTok{df =} \DecValTok{4}\NormalTok{) \{}

  \CommentTok{\# generate data from non{-}central t distribution}
  \CommentTok{\# vals \textless{}{-} }

  \CommentTok{\# calculate one{-}sample t{-}test}
\NormalTok{  tt }\OtherTok{\textless{}{-}} \FunctionTok{t.test}\NormalTok{( vals )}

  \CommentTok{\# compile results into a tibble and return}
  \CommentTok{\# res \textless{}{-} }

  \FunctionTok{return}\NormalTok{(res)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}
\item
  Modify \texttt{one\_pval()} to allow the user to specify a hypothesized value for the population mean, to use when calculating the one-sample t-test results.
\end{enumerate}

\chapter{An initial simulation}\label{t-test-simulation}

To begin learning about simulation, a good starting place is to examine a small, concrete example.
This example illustrates how simulation involves replicating the data-generating and data-analysis processes, followed by aggregating the results across replications.
Our little example encapsulates the bulk of our approach to Monte Carlo simulation, touching on all the main components involved.
In subsequent chapters we will look at each of these components in greater detail.
But first, let us look at a simulation of a very simple statistical problem.

The one-sample \(t\)-test is one of the most basic methods in the statistics literature.
It tests a null hypothesis that a population mean of some variable is equal to a specific value by comparing the mean of a sample of data to the hypothesized value.
If the sample average is discrepant (very different) from the null value, relative to how uncertain we are about our estimate, then the hypothesis is rejected.
The test can also be used to generate a confidence interval for the population mean.
If the sample consists of independent observations and the variable is normally distributed in the population, then the confidence interval will have exact coverage, in the sense that 95\% intervals will include the population mean in 95 out of 100 tries.
But what if the population variable is not normally distributed?

To find out, let us look at the coverage of the \(t\)-test's 95\% confidence interval for the population mean when the method's normality assumption is violated.
\emph{Coverage} is the chance of a confidence interval capturing the true parameter value.
To examine coverage, we will simulate many samples from a non-normal population with a specified mean, calculate a confidence interval based on each sample, and see how many of the confidence intervals cover the known true population mean.

Before getting to the simulation, let's look at the data-analysis procedure we will be investigating.
Here is the result of conducting a \(t\)-test on some fake data, generated from a normal distribution:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# make fake data}
\NormalTok{dat }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{( }\AttributeTok{n =} \DecValTok{10}\NormalTok{, }\AttributeTok{mean =} \DecValTok{4}\NormalTok{, }\AttributeTok{sd =} \DecValTok{2}\NormalTok{ )}

\CommentTok{\# conduct the test}
\NormalTok{tt }\OtherTok{\textless{}{-}} \FunctionTok{t.test}\NormalTok{( dat )}
\NormalTok{tt}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  One Sample t-test
## 
## data:  dat
## t = 6.0878, df = 9, p-value = 0.0001819
## alternative hypothesis: true mean is not equal to 0
## 95 percent confidence interval:
##  2.164025 4.723248
## sample estimates:
## mean of x 
##  3.443636
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# examine the confidence interval}
\NormalTok{tt}\SpecialCharTok{$}\NormalTok{conf.int}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 2.164025 4.723248
## attr(,"conf.level")
## [1] 0.95
\end{verbatim}

We generated data with a true (population) mean of 4. Did we capture it? To check, we can use the \texttt{findInterval()} function, which checks to see where the first number lies relative to the range given in the second argument.
Here is an illustration of the syntax:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{findInterval}\NormalTok{( }\DecValTok{1}\NormalTok{, }\FunctionTok{c}\NormalTok{(}\DecValTok{20}\NormalTok{, }\DecValTok{30}\NormalTok{) )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{findInterval}\NormalTok{( }\DecValTok{25}\NormalTok{, }\FunctionTok{c}\NormalTok{(}\DecValTok{20}\NormalTok{, }\DecValTok{30}\NormalTok{) )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{findInterval}\NormalTok{( }\DecValTok{40}\NormalTok{, }\FunctionTok{c}\NormalTok{(}\DecValTok{20}\NormalTok{, }\DecValTok{30}\NormalTok{) )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 2
\end{verbatim}

The \texttt{findInterval()} returns a \texttt{1} if the value of the first argument value is in the interval specified in the second argument.

We can apply it to check whether our estimated CI covers the true population mean:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{findInterval}\NormalTok{( }\DecValTok{4}\NormalTok{, tt}\SpecialCharTok{$}\NormalTok{conf.int )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1
\end{verbatim}

In this instance, \texttt{findInterval()} is equal to \texttt{1}, which means our CI captured the true population mean of 4.

Here is the full code for simulating data, computing the data-analysis procedure, and evaluating the result:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# make fake data}
\NormalTok{dat }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{( }\AttributeTok{n =} \DecValTok{10}\NormalTok{, }\AttributeTok{mean =} \DecValTok{4}\NormalTok{, }\AttributeTok{sd =} \DecValTok{2}\NormalTok{ )}

\CommentTok{\# conduct the test}
\NormalTok{tt }\OtherTok{\textless{}{-}} \FunctionTok{t.test}\NormalTok{( dat )}

\CommentTok{\# evaluate the results}
\FunctionTok{findInterval}\NormalTok{( }\DecValTok{4}\NormalTok{, tt}\SpecialCharTok{$}\NormalTok{conf.int ) }\SpecialCharTok{==} \DecValTok{1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] TRUE
\end{verbatim}

The above code captures the basic form of a single simulation trial: make the data, analyze the data, decide how well we did.
The code also illustrates a good way to figure out the details of a simulation: start by figuring out what a single iteration of the simulation might look like.
Starting by mucking about this way also allows us to test and develop our code in an interactive, exploratory fashion. For instance, we can play with \texttt{findInterval()} to figure out how to use it to determine whether our confidence interval captured the truth.
Once we have arrived at working code for a single iteration, we are in a good position to start writing functions to implement the actual simulation.
For now, we have generated data from a normal distribution; we will later revise the code to generate data from a non-normal population distribution.

\section{Simulating a single scenario}\label{simulating-a-single-scenario}

We can estimate the coverage of the confidence interval by repeating the above data-generating and data-analysis processes many, many times.
R's \texttt{replicate()} function is a handy way to repeatedly call a line of code.
Its first input argument is \texttt{n}, the number of times to repeat the calculation, followed by \texttt{expr}, which is one or more lines of code to be called.
We can use \texttt{replicate} to repeat our simulation process 1000 times in a row, each time generating a new sample of 10 observations from a normal distribution with mean of 4 and a standard deviation of 2.
For each replication, we store the result of using \texttt{findInterval()} to check whether the confidence interval includes the population mean of 4.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rps }\OtherTok{\textless{}{-}} \FunctionTok{replicate}\NormalTok{( }\DecValTok{1000}\NormalTok{, \{}
\NormalTok{  dat }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{( }\AttributeTok{n =} \DecValTok{10}\NormalTok{, }\AttributeTok{mean =} \DecValTok{4}\NormalTok{, }\AttributeTok{sd =} \DecValTok{2}\NormalTok{ )}
\NormalTok{  tt }\OtherTok{\textless{}{-}} \FunctionTok{t.test}\NormalTok{( dat )}
  \FunctionTok{findInterval}\NormalTok{( }\DecValTok{4}\NormalTok{, tt}\SpecialCharTok{$}\NormalTok{conf.int )}
\NormalTok{\})}

\FunctionTok{head}\NormalTok{(rps, }\DecValTok{20}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1
\end{verbatim}

To see how well we did, we can look at a table of the results stored in \texttt{rps} and calculate the proportion of replications that the interval covered the population mean:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{table}\NormalTok{( rps )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## rps
##   0   1   2 
##  27 957  16
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mean}\NormalTok{( rps }\SpecialCharTok{==} \DecValTok{1}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.957
\end{verbatim}

We got about 95\% coverage, which is good news. In 27 out of the 1000 replications, the interval was too high (so the population mean was below the interval) and in 16 out of the 1000 replications, the interval was too low (so the population mean was above the interval).

It is important to recognize that this set of simulations results, and our coverage rate of 95.7\%, itself has some uncertainty in it.
Because we only repeated the simulation 1000 times, what we really have is a \emph{sample} of 1000 independent replications, out of an infinite number of possible simulation runs.
Our coverage of 95.7\% is an \emph{estimate} of what the true coverage would be, if we ran more and more replications.
The source of uncertainty of our estimate is called \emph{Monte Carlo simulation error (MCSE)}.
We can actually assess the Monte Carlo simulation error in our simulation results using standard statistical procedures for independent and identically distributed data.
Here we use a proportion test to check whether the estimated coverage rate is consistent with a true coverage rate of 95\%:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{covered }\OtherTok{\textless{}{-}} \FunctionTok{as.numeric}\NormalTok{( rps }\SpecialCharTok{==} \DecValTok{1}\NormalTok{ )}
\FunctionTok{prop.test}\NormalTok{( }\FunctionTok{sum}\NormalTok{(covered), }\FunctionTok{length}\NormalTok{(covered), }\AttributeTok{p =} \FloatTok{0.95}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  1-sample proportions test with continuity
##  correction
## 
## data:  sum(covered) out of length(covered), null probability 0.95
## X-squared = 0.88947, df = 1, p-value =
## 0.3456
## alternative hypothesis: true p is not equal to 0.95
## 95 percent confidence interval:
##  0.9420144 0.9683505
## sample estimates:
##     p 
## 0.957
\end{verbatim}

The test indicates that our estimate is consistent with the possibility that the true coverage rate is 95\%, just as it should be.
Things working out should hardly be surprising.
Mathematical theory tells us that the \(t\)-test is exact for normally distributed population variables, and we generated data from a normal distribution.
In other words, all we have found so far is that the confidence intervals follow theory when the assumptions of the method are met.

\section{A non-normal population distribution}\label{a-non-normal-population-distribution}

To see what happens when the normality assumption is violated, let us now look at a scenario where the population variable follows a geometric distribution.
The geometric distribution is usually written in terms of a probability parameter \(p\), so that the distribution has a mean of \((1 - p) / p\).
We will use a geometric distribution with a mean of 4 by setting \(p = 1/5\).
Here is the population distribution of the variable:

\begin{center}\includegraphics[width=0.75\linewidth]{Designing-Simulations-in-R_files/figure-latex/unnamed-chunk-35-1} \end{center}

The distribution is highly right-skewed, which suggests that the normal confidence interval might not work very well.

Now let's revise our previous simulation code to use the geometric distribution:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rps }\OtherTok{\textless{}{-}} \FunctionTok{replicate}\NormalTok{( }\DecValTok{1000}\NormalTok{, \{}
\NormalTok{  dat }\OtherTok{\textless{}{-}} \FunctionTok{rgeom}\NormalTok{( }\AttributeTok{n =} \DecValTok{10}\NormalTok{, }\AttributeTok{prob =} \DecValTok{1}\SpecialCharTok{/}\DecValTok{5}\NormalTok{ )}
\NormalTok{  tt }\OtherTok{\textless{}{-}} \FunctionTok{t.test}\NormalTok{( dat )}
  \FunctionTok{findInterval}\NormalTok{( }\DecValTok{4}\NormalTok{, tt}\SpecialCharTok{$}\NormalTok{conf.int )}
\NormalTok{\})}
\FunctionTok{table}\NormalTok{( rps )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## rps
##   0   1   2 
##   8 892 100
\end{verbatim}

Our confidence interval is often entirely too low (such that the population mean is above the interval) and very rarely does our interval fall fully above the population mean.
Furthermore, our coverage rate is not the desired 95\%:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mean}\NormalTok{( rps }\SpecialCharTok{==} \DecValTok{1}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.892
\end{verbatim}

To take account of Monte Carlo error, we will again do a proportion test.
The following test result calculates a confidence interval for the true coverage rate under the scenario we are examining:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{covered }\OtherTok{\textless{}{-}} \FunctionTok{as.numeric}\NormalTok{( rps }\SpecialCharTok{==} \DecValTok{1}\NormalTok{ )}
\FunctionTok{prop.test}\NormalTok{( }\FunctionTok{sum}\NormalTok{(covered), }\FunctionTok{length}\NormalTok{(covered), }\AttributeTok{p =} \FloatTok{0.95}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  1-sample proportions test with continuity
##  correction
## 
## data:  sum(covered) out of length(covered), null probability 0.95
## X-squared = 69.605, df = 1, p-value <
## 2.2e-16
## alternative hypothesis: true p is not equal to 0.95
## 95 percent confidence interval:
##  0.8707042 0.9102180
## sample estimates:
##     p 
## 0.892
\end{verbatim}

Our coverage is \emph{too low}; the confidence interval based on the \(t\)-test misses the the true value more often than it should.
We have learned that the \(t\)-test can fail when applied to non-normal (skewed) data.

\section{Simulating across different scenarios}\label{simulating-across-different-scenarios}

So far, we have looked at coverage rates of the confidence interval under a single, specific scenario, with a sample size of 10, a population mean of 4, and a geometrically distributed variable.
We know from statistical theory (specifically, the central limit theorem) that the confidence interval should work better if the sample size is big enough.
But how big does it have to get?
One way to examine this question is to expand the simulation to look at several different scenarios involving different sample sizes.
We can think of this as a one-factor experiment, where we manipulate sample size and use simulation to estimate how confidence interval coverage rates change.

To implement such an experiment, we first write our own function that executes the full simulation process for a given sample size:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ttest\_CI\_experiment }\OtherTok{=} \ControlFlowTok{function}\NormalTok{( n ) \{}
  
\NormalTok{  rps }\OtherTok{\textless{}{-}} \FunctionTok{replicate}\NormalTok{( }\DecValTok{1000}\NormalTok{, \{}
\NormalTok{    dat }\OtherTok{\textless{}{-}} \FunctionTok{rgeom}\NormalTok{( }\AttributeTok{n =}\NormalTok{ n, }\AttributeTok{prob =} \DecValTok{1}\SpecialCharTok{/}\DecValTok{5}\NormalTok{ ) }\CommentTok{\# simulate data}
\NormalTok{    tt }\OtherTok{\textless{}{-}} \FunctionTok{t.test}\NormalTok{( dat )               }\CommentTok{\# analyze data}
    \FunctionTok{findInterval}\NormalTok{( }\DecValTok{4}\NormalTok{, tt}\SpecialCharTok{$}\NormalTok{conf.int )    }\CommentTok{\# evaluate coverage}
\NormalTok{  \})}

\NormalTok{  coverage }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{( rps }\SpecialCharTok{==} \DecValTok{1}\NormalTok{ )       }\CommentTok{\# summarize results}
  
  \FunctionTok{return}\NormalTok{(coverage)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

The code inside the body of the function is identical to what we have used above, with the sample size as a function argument, \texttt{n}, which enables us to easily run the code for different sample sizes.
With our function in hand, we can now run the simulation for a single scenario just by calling it:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ttest\_CI\_experiment}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.885
\end{verbatim}

Even though the sample size is still \texttt{n\ =\ 10}, the simulated coverage rate is a little bit different from what we found previously.
That is because there is some Monte Carlo error in each simulated coverage rate.

Our task is now to use this function for several different values of \(n\). We could just do this by copy-pasting and changing the value of \texttt{n}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ttest\_CI\_experiment}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.922
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ttest\_CI\_experiment}\NormalTok{(}\AttributeTok{n =} \DecValTok{20}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.91
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ttest\_CI\_experiment}\NormalTok{(}\AttributeTok{n =} \DecValTok{30}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.914
\end{verbatim}

However, this will quickly get cumbersome if we want to evaluate many different sample sizes.
A better approach is to use a mapping function from the \texttt{purrr} package.\footnote{Alternately, readers familiar with the \texttt{*apply()} family of functions from Base R might prefer to use \texttt{lapply()} or \texttt{sapply()}, which do essentially the same thing as \texttt{purrr::map\_dbl()}.}
The \texttt{map\_dbl()} function takes a list of values and calls a function for each value in the list.
This accomplishes the same thing as using a \texttt{for} loop to iterate through a list of items (if you happen to be familiar with these), but is more succinct.
See Appendix chapter \ref{repeating-oneself} for more on \texttt{map()}.\footnote{You can also check out \href{https://r4ds.had.co.nz/iteration.html\#the-map-functions}{Section 21.5 of R for Data Science (1st edition)}, which provides an introduction to mapping.}

To proceed, we first create a list of sample sizes to test out:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ns }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DecValTok{20}\NormalTok{, }\DecValTok{30}\NormalTok{, }\DecValTok{40}\NormalTok{, }\DecValTok{60}\NormalTok{, }\DecValTok{80}\NormalTok{, }\DecValTok{100}\NormalTok{, }\DecValTok{120}\NormalTok{, }\DecValTok{160}\NormalTok{, }\DecValTok{200}\NormalTok{, }\DecValTok{300}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Now we can use \texttt{map\_dbl()} to evaluate the coverage rate for each sample size:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{coverage\_est }\OtherTok{\textless{}{-}} \FunctionTok{map\_dbl}\NormalTok{( ns, ttest\_CI\_experiment)}
\end{Highlighting}
\end{Shaded}

This code will run our experiment for each value in \texttt{ns}, and then return a vector of the estimated coverage rates for each of the sample sizes.

We advocate for depicting simulation results graphically.
To do so, we store the simulation results in a dataset and then create a line plot using a log scale for the horizontal axis:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{res }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{( }
  \AttributeTok{n =}\NormalTok{ ns, }
  \AttributeTok{coverage =} \DecValTok{100} \SpecialCharTok{*}\NormalTok{ coverage\_est }
\NormalTok{)}

\FunctionTok{ggplot}\NormalTok{( res, }\FunctionTok{aes}\NormalTok{( }\AttributeTok{x =}\NormalTok{ n, }\AttributeTok{y =}\NormalTok{ coverage ) ) }\SpecialCharTok{+}
  \FunctionTok{geom\_hline}\NormalTok{( }\AttributeTok{yintercept=}\DecValTok{95}\NormalTok{, }\AttributeTok{col=}\StringTok{"red"}\NormalTok{ ) }\SpecialCharTok{+}  
  \CommentTok{\# A reference line for nominal coverage rate}
  \FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{geom\_point}\NormalTok{( }\AttributeTok{size =} \DecValTok{4}\NormalTok{ ) }\SpecialCharTok{+}
  \FunctionTok{scale\_x\_log10}\NormalTok{( }\AttributeTok{breaks =}\NormalTok{ ns, }\AttributeTok{minor\_breaks =} \ConstantTok{NULL}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{( }
    \AttributeTok{title=}\StringTok{"Coverage rates for t{-}test on exponential data"}\NormalTok{,}
    \AttributeTok{x =} \StringTok{"n (sample size)"}\NormalTok{, }
    \AttributeTok{y =} \StringTok{"coverage (\%)"} 
\NormalTok{  ) }\SpecialCharTok{+}
  \FunctionTok{coord\_cartesian}\NormalTok{(}\AttributeTok{xlim =} \FunctionTok{c}\NormalTok{(}\DecValTok{9}\NormalTok{,}\DecValTok{320}\NormalTok{), }\AttributeTok{ylim=}\FunctionTok{c}\NormalTok{(}\DecValTok{85}\NormalTok{,}\DecValTok{100}\NormalTok{), }\AttributeTok{expand =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{theme\_minimal}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{Designing-Simulations-in-R_files/figure-latex/ttest_result_figure-1} \end{center}

We can see from the graph that the confidence interval's coverage rate improves as sample size gets larger. For sample sizes over 100, the interval appears to have coverage quite close to the nominal 95\% level.
Although the general trend is pretty clear, the graph is still a bit messy because each point is an \emph{estimated} coverage rate, with some Monte Carlo error baked in.

\section{Extending the simulation design}\label{extending-the-simulation-design}

So far, we have executed a simple simulation to assess how well a statistical method works in a given circumstance, then expanded the simulation by running a single-factor experiment in which we varied the sample size to see how the method's performance changes.
In our example, we found that coverage is below what it should be for small sample sizes, but improves for sample sizes in the 100's.

This example captures all the major steps of a simulation study, which we outlined at the start of Chapter \ref{introduction}.
We generated some hypothetical data according to a fully-specified data-generating process: we did both a normal distribution and a geometric distribution, each with a mean of 4.
We applied a defined data-analysis procedure to the simulated data: we used a confidence interval based on the \(t\) distribution.
We assessed how well the procedure worked across replications of the data-generating and data-analysis processes: in this case we focused on the coverage rate of the confidence interval.
After creating a function to implement this whole process for a single scenario, we investigated how the performance of the confidence interval changed depending on sample size.

In simulations of more complex models and data-analysis methods, some or all of the steps in the process might have more moving pieces or entail more complex calculations.
For instance, we might want to compare the performance of different approaches to calculating a confidence interval.
We might also want to examine how coverage rates are affected by other aspects of the data-generating process, such as looking at different population mean values for the geometric distribution---or even entirely different distributions.
With such additional layers of complexity, we will need to think systematically about each of the component parts of the simulation.
In the next chapter, we introduce an abstract, general framework for simulations that is helpful for guiding simulation design and managing all the considerations involved.

\section{Exercises}\label{exercises-1}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  The simulation function we developed in this chapter runs 1000 replications of the data-generating and data-analysis process, which leads to some Monte Carlo error in the reported results. Modify the \texttt{ttest\_CI\_experiment()} function to make the number of replications an input argument, then re-run the simulation and re-create the graph of the results with \(R=10,000\) or even \(R=100,000\). Is the graph more regular than the one in the text, above?
  Use your improved results to estimate what sample size would be large enough to give coverage of at least 94\% (so only 1\% off of desired).
  Is this answer much different from if you had used the figure given in the text?
\item
  Modify the \texttt{ttest\_CI\_experiment()} function to make the \(p\) parameter an input argument. Repeat the one-factor simulation, but use \(p = 1/10\). Make sure your function is comparing coverage to the population mean of \((1 - p) / p\). How do the coverage rates change when \(p\) is so small?
\end{enumerate}

\textbf{More challenging problems}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\item
  Below is a partially completed modified version of the \texttt{ttest\_CI\_experiment()} function that should create a tibble that includes the estimated coverage rate, the average interval length, and a confidence interval for the coverage rate:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ttest\_CI\_experiment\_full }\OtherTok{=} \ControlFlowTok{function}\NormalTok{( n ) \{}

\NormalTok{  lotsa\_CIs }\OtherTok{\textless{}{-}} \FunctionTok{replicate}\NormalTok{( }\DecValTok{1000}\NormalTok{, \{}
    \CommentTok{\# simulate data}
\NormalTok{    dat }\OtherTok{\textless{}{-}} \FunctionTok{rgeom}\NormalTok{( }\AttributeTok{n =}\NormalTok{ n, }\AttributeTok{prob =} \DecValTok{1}\SpecialCharTok{/}\DecValTok{5}\NormalTok{)                            }
    \CommentTok{\# analyze data}
\NormalTok{    tt }\OtherTok{\textless{}{-}} \FunctionTok{t.test}\NormalTok{( dat )                                       }
    \CommentTok{\# return CI}
    \FunctionTok{tibble}\NormalTok{(}\AttributeTok{lower =}\NormalTok{ tt}\SpecialCharTok{$}\NormalTok{conf.int[}\DecValTok{1}\NormalTok{], }\AttributeTok{upper =}\NormalTok{ tt}\SpecialCharTok{$}\NormalTok{conf.int[}\DecValTok{2}\NormalTok{])}
\NormalTok{  \}, }\AttributeTok{simplify =} \ConstantTok{FALSE}\NormalTok{ ) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{bind\_rows}\NormalTok{()}

  \CommentTok{\# summarize results}
  \CommentTok{\# \textless{}calculate coverage\textgreater{}}
  \CommentTok{\# \textless{}calculate average interval length\textgreater{}}
  \CommentTok{\# \textless{}calculate a 95\% confidence interval for the true coverage rate\textgreater{}}

  \FunctionTok{return}\NormalTok{(coverage)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

  Complete the function by writing code to compute the estimated coverage rate and average confidence interval length. Also calculate a 95\% confidence interval for the true coverage rate (you can use \texttt{prop.test()} on your set of simulation coverage indicators to obtain this, treating your \(R\) simulation replicates as a random sample in its own right). This confidence interval captures what we call \emph{Monte Carlo Simulation Uncertainty}, which will depend on the number of simulation trials you run. Your modified function should return a one-row tibble with the coverage rate, average confidence interval length, and the lower and upper limits of a CI for the true coverage rate.
\item
  Using the prior problem, re-run the simulations to obtain a data frame with each row being a simulation scenario and columns of sample size, estimated coverage, low end of the estimate's confidence interval, high end of the interval, and average confidence interval length. You will likely want to use \texttt{map()} and then \texttt{bind\_rows()} on your list of results; see Chapter \ref{repeating-oneself} for more information about these techniques.
  Use your resulting set of results to create a graph that depicts the estimated coverage rates as a function of sample size. Make your graph include the 95\% confidence intervals also, so that the Monte Carlo simulation error in the estimated coverage rates is represented in the graph. We recommend using the \texttt{ggplot2} function \texttt{geom\_pointrange()} to plot the confidence intervals.
\item
  Modify \texttt{ttest\_CI\_experiment()} so that the user can specify the population mean of the data-generating process. Also let the user specify the number of replications to use. Here is a function skeleton to use as a starting point:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ttest\_CI\_experiment }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{( n, pop\_mean, reps) \{}

\NormalTok{  pop\_prob }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{/}\NormalTok{ (pop\_mean }\SpecialCharTok{+} \DecValTok{1}\NormalTok{)}

  \CommentTok{\# \textless{}fill in the rest\textgreater{}}

  \FunctionTok{return}\NormalTok{(coverage)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}
\item
  Using the modified function from the previous problem, implement a two-factor simulation study for several different values of \texttt{n} and several different population means.
  One way to do this is to run a few one-factor simulations, each with a different population mean. You can store them in a series of datasets, \texttt{res1}, \texttt{res2}, \texttt{res3}, etc.
  Then use \texttt{bind\_rows(\ size1\ =\ res1,\ ...,\ .id\ =\ "mean"\ )} to combine the datasets into a single dataset.
  Make a plot of your results, with \texttt{n} on the x-axis, coverage on the \(y\)-axis, and different lines for different population means.
\end{enumerate}

\part{Structure and Mechanics of a Simulation Study}\label{part-structure-and-mechanics-of-a-simulation-study}

\chapter{Structure of a simulation study}\label{simulation-structure}

Monte Carlo simulation is a very flexible tool that researchers use to study a vast array of different models and topics.
Within the realm of methodological research, most simulations share a common structure, nearly always involving the same set of steps or component pieces.
In learning to design your own simulations, it is very useful to recognize the core components that most simulation studies share.
Identifying these components will help you to organize your work and structure the coding tasks involved in writing a simulation.

In this chapter, we outline the component structure of a methodological simulation study, highlighting the four steps involved in a simulation of a single scenario and the three additional steps involved in multifactor simulations.
We then describe a strategy for implementing simulations that mirrors the same component structure, where each step in the simulation is represented by a separate function or object.
We call this strategy \textbf{\emph{tidy, modular simulation}}.
Finally, we show how the tidy, modular simulation strategy informs the structure and organization of code for a simulation study, walking through basic code skeletons (cf.~\ref{function-skeletons}) for each of the steps in a single-scenario simulation.

\section{General structure of a simulation}\label{general-structure-of-a-simulation}

The four main steps involved in a simulation study, introduced in Chapter \ref{introduction}, are summarized in the top portion of Table \ref{tab:simulation}.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4667}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4333}}@{}}
\caption{\label{tab:simulation} Steps in the Simulation Process}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Step
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Step
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & \textbf{Generate} & Generate a sample of artificial data based on a specific statistical model or data-generating process. \\
2 & \textbf{Analyze} & Apply one or more data-analysis procedures, estimators, or workflows to the artificial data. \\
3 & \textbf{Repeat} & Repeat steps (1) \& (2) \(R\) times, recording \(R\) sets of results. \\
4 & \textbf{Summarize} & Assess the performance of the procedure across the \(R\) replications. \\
5 & \textbf{Design} & Specify a set of conditions to examine \\
6 & \textbf{Execute} & Run the simulation for each condition in the design. \\
7 & \textbf{Synthesize} & Compare performance across conditions. \\
\end{longtable}

In the simple \(t\)-test example presented in Chapter \ref{t-test-simulation}, we put each of these steps into action with R code:

\begin{itemize}
\tightlist
\item
  We used the geometric distribution as a data-generating process;
\item
  We used the confidence interval from a one-sample \(t\)-test as the data-analysis procedure;
\item
  We repeatedly simulated the confidence intervals with R's \texttt{replicate()} function; and
\item
  We summarized the results by estimating the fraction of the intervals that covered the population mean.
\end{itemize}

We also saw that it was helpful to wrap all of these steps up into a single function, so that we could run the simulation across multiple sample sizes.

These four initial steps are common and shared across nearly all simulations.
In our first example, each of the steps was fairly simple, sometimes involving only a single line of code.
More generally, each of the steps might be quite a bit more complex. The data-generating process might involve a more complex model with multiple variables or multilevel structure.
The data analysis procedure might involve solving a multidimensional optimization problem to get parameter estimates, or might involve a data analysis workflow with multiple steps or contingencies.
Further, we might use more than one metric for summarizing the results across replications and describing the performance of the data analysis procedure.
Because each of the four steps involves its own set of choices, it will useful to recognize them as distinct from one another.

In methodological research projects, we usually want to examine simulation results across an array of different conditions that differ not only in terms of sample size, but also in other parameters or features of the data-generating process.
Running a simulation study across multiple conditions entails several further steps, which are summarized in the bottom portion of Table \ref{tab:simulation}. We will first need to specify the factors and specific conditions to be examined in our experimental design. We will then need to execute the simulation for each of the conditions and store all the results for further analysis. Finally, we will need to find ways to synthesize or make sense of the main patterns in the results across all of the conditions in the design.

Just as with the first four steps, it is useful to recognize these further steps as distinct from one another, each involving its own set of choices and techniques.
The design step requires choosing which parameters and features of the data-generating process to vary, as well as which specific values to use for each factor that is varied.
Executing a simulation might require a lot of computing power, especially if the simulation design has many conditions or the data analysis procedure takes a long time to compute.
How to effectively implement the execution step will therefore depend on the computing requirements and available resources.
Finally, many different techniques can be used to synthesizing findings from a multifactor simulation.
Which ones are most useful will depend on your research questions and the choices you make in each of the preceeding steps.

\section{Tidy, modular simulations}\label{tidy-modular-simulations}

It is apparent from Table \ref{tab:simulation} that writing a simulation in R involves a large space of possibilities and will requiring making a bunch of decisions.
Considering the number of choices to be made, it is critical to stay organized and to approach the process systematically.
Recognizing the components of a simulation is the starting point.
Next is to see how to translate the components into R code.

In our own methodological work, we have found it very useful to always follow the same approach to writing code for a simulation.
We call this approach \emph{tidy, modular simulation}. It involves two simple principles:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Implement each component of a simulation as a distinct function or object.
\item
  Store all results in rectangular data sets.
\end{enumerate}

Writing separate functions for each component step of a simulation has several benefits.
The first is the practical benefit of turning the coding process from a monolithic (and potentially daunting) activity into a set of smaller, discrete tasks.
This lets us focus on one task at a time and makes it easier to see progress.
Second, following this principle makes for code that is easier to read, test, and debug.
Rather than having to scan through an entire code file to understand how the data analysis procedure is implemented, we can quickly identify the function that implements it, then focus on understanding the workings of that function.
Likewise, if another researcher wanted to test out the data analysis procedure on a dataset of their own, they could do so by running the corresponding function rather than having to dissect an entire script.
Third, writing separate code for each component makes it possible to tweak the code or swap out pieces of the simulation, such as by adding additional estimation methods or trying out a data-generating process involving different distributional assumptions.
We already saw this in Chapter \ref{t-test-simulation}, where we modified our initial data-generating process to use a geometric distribution rather than a normal distribution.
In short, following the first principle makes for simpler, more robust code that is easier to navigate, easier to test, and easier to extend.

The second principle of tidy, modular simulation is to store all results in rectangular datasets, such as the base R \texttt{data.frame} object or the tidyverse \texttt{tibble} object.\footnote{\citet{Wickham2014tidydata} provides a broader introduction to the concept of tidy data in the context of data-analysis tasks.}
This principle applies to any and all output, including the simulated data from Step 1, the results of data analysis procedures from Step 2, full sets of replicated simulation results from Step 3, and summarized results from Step 4.
A primary benefit of following this principle is that it facilitates working with the output of each stage in the simulation process.
If you are comfortable using R to analyze real data, you you can use the same skills and tools to examine simulation output as long as it is in tabular form.
Many of the data processing and data analysis tools available in R work with---or even require---rectangular datasets.
Thus, using rectangular datasets makes it easier to inspect, summarize, and visualize the output.

\section{Skeleton of a simulation study}\label{skeleton-of-a-simulation-study}

The principles of tidy simulation imply that code for a simulation study should usually follow the same broad outline and organization of Table \ref{tab:simulation}, with custom functions for each step in process.
We will describe the outlines of simulation code using function skeletons to illustrate the inputs and outputs of each component.
These skeletons skip over all the specific details, so that we can see the structure more clearly.
We will first examine the structure of the code for simulating one specific scenario, then consider how to extend the code to systematically explore a variety of scenarios, as in a multifactor simulation.

In code skeletons, the structure of the first four steps in a simulation looks like this:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Generate (data{-}generating process)}

\NormalTok{generate\_data }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{( model\_params ) \{}
  \CommentTok{\# stuff}
  \FunctionTok{return}\NormalTok{(data)}
\NormalTok{\}}

\CommentTok{\# Analyze (data{-}analysis procedure)}

\NormalTok{analyze }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{( data ) \{}
  \CommentTok{\# stuff}
  \FunctionTok{return}\NormalTok{(one\_result)}
\NormalTok{\}}

\CommentTok{\# Repeat}

\NormalTok{one\_run }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{( model\_params ) \{}
\NormalTok{  dat }\OtherTok{\textless{}{-}} \FunctionTok{generate\_data}\NormalTok{( model\_params )}
\NormalTok{  one\_result }\OtherTok{\textless{}{-}} \FunctionTok{analyze}\NormalTok{(dat)}
  \FunctionTok{return}\NormalTok{(one\_result)}
\NormalTok{\}}

\NormalTok{results }\OtherTok{\textless{}{-}} \FunctionTok{replicate}\NormalTok{(R, }\FunctionTok{one\_run}\NormalTok{( params ))}

\CommentTok{\# Summarize (calculate performance measures)}

\NormalTok{assess\_performance }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{( results, model\_params ) \{}
  \CommentTok{\# stuff}
  \FunctionTok{return}\NormalTok{(performance\_measures)}
\NormalTok{\}}

\FunctionTok{assess\_performance}\NormalTok{(results, model\_params)}
\end{Highlighting}
\end{Shaded}

The code above shows the full skeleton of a simulation.
It involves four functions, where the outputs of one function get used as inputs in subsequent functions.
We will now look at the inputs and outputs of each function to see how they align with the four steps in the simulation process.
Subsequent chapters examine each piece in much greater detail---putting meat on the bones of each function skeleton, to torture our metaphor---and discuss specific statistical issues and programming techniques that are useful for designing each component.

Besides illustrating the skeletal framework of a simulation, readers might find it useful to use it as a template from which to start writing their own code.
The \texttt{simhelpers} package includes the function \texttt{create\_skeleton()}, which will open a new R script that contains a template for a simulation study, with sections corresponding to each component:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{simhelpers}\SpecialCharTok{::}\FunctionTok{create\_skeleton}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

The template that appears is a slightly more elaborate version of the code above, with the main difference being that it also includes some additional lines of code to wire the pieces together for a multifactor simulation.
Starting from this template, you will already be on the road to writing a tidy, modular simulation.

\subsection{Data-Generating Process}\label{data-generating-process}

The first step in a simulation is specifying a data-generating process. This is a hypothetical model for how data might arise, involving measurements or observations of one or more variables. The bare-bones skeleton of a data-generating function looks like the following:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{generate\_data }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{( model\_params ) \{}
  \CommentTok{\# stuff}
  \FunctionTok{return}\NormalTok{(data)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

The function takes as input a set of model parameter values, denoted here as \texttt{model\_params}.
Based on those model parameters, the function generates a hypothetical dataset as output.
Generating our own data based on a model allows us to know what the answer is (e.g., the true population mean or the true average effect of an intervention), so that we have benchmark against which to compare the results of a data analysis procedure that generates noisy estimates of the true value.

In practice, \texttt{model\_params} will usually not be just one input but rather multiple arguments.
These arguments might include inputs such as the population mean for a variable, the standard deviation of a distribution of treatment effects, or a parameter controlling the degree of skewness in the population distribution.
Many data-generating processes involving multiple variables, such as the response variable and predictor variables in a regression model.
In such instances, the inputs of \texttt{generate\_data()} might also include parameters that determine the degree of dependence or correlation between variables.
Further, the \texttt{generate\_data()} inputs will also usually include arguments relating to the sample size and structure of the hypothetical dataset.
For instance, in a simulation of a multilevel dataset where individuals are nested within clusters, the inputs might include an arguments to specify the total number of clusters and the average number of individuals per cluster.
We discuss the inputs and form of the data-generating function further in Chapter \ref{data-generating-processes}.

\subsection{Data Analysis Procedure}\label{data-analysis-procedure}

The second step in a simulation is specifying a data analysis procedure or set of procedures.
The bare-bones skeleton of a data-generating function looks like the following:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{analyze }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{( data ) \{}
  \CommentTok{\# stuff}
  \FunctionTok{return}\NormalTok{(one\_result)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

The function should take a single dataset as input and produce a set of estimates or results (e.g., point estimates, standard errors, confidence intervals, p-values, predictions, etc.).
Because we will be using the function to analyze hypothetical datasets simulated from the data-generating process, the \texttt{analyze()} function needs to work with \texttt{data} inputs that are produced by the \texttt{generate\_data()} function.
Thus, the code in the body of \texttt{analyze()} can assume that \texttt{data} will include relevant variables with specific names.

Inside the body of the function, \texttt{analyze()} includes code to carry out a data analysis procedure.
This might involve generating a confidence interval, as in the example from Chapter \ref{t-test-simulation}.
In another context, it might involve estimating an average growth rate along with a standard error, given a dataset with longitudinal repeated measurements from multiple individuals.
In still another context, it might involve generating individual-level predictions from a machine learning algorithm.
In simulations that involve comparing multiple analysis methods, we might write an \texttt{analyze()} function for each of the methods of interest, or (generally less preferred because it is less modular) we might write one function that does the calculations for all of the methods together.

A well-written estimation method should, in principle, work not only on a simulated, hypothetical dataset but also on a real empirical dataset that has the same format (i.e., appropriate variable names and structure).
Because of this, the inputs of the \texttt{analyze()} function should not typically include any information about the parameters of the data-generating process.
To be realistic, the code for our simulated data-analysis procedure should not make use of anything that the analyst could not know when analyzing a real dataset.
Thus, \texttt{analyze()} has an argument for the sample dataset but not for \texttt{model\_params}.
We discuss the form and content of the data analysis function further in Chapter \ref{data-analysis-procedures}.

\subsection{Repetition}\label{repetition}

The third step in a simulation is to repeatedly evaluate the data-generating process and data analysis procedure.
In practice, this amounts to repeatedly calling \texttt{generate\_data()} and then calling \texttt{analyze()} on the result.
Here is the skeleton from our simulation template:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{one\_run }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{( model\_params ) \{}
\NormalTok{  dat }\OtherTok{\textless{}{-}} \FunctionTok{generate\_data}\NormalTok{( model\_params )}
\NormalTok{  one\_result }\OtherTok{\textless{}{-}} \FunctionTok{analyze}\NormalTok{(dat)}
  \FunctionTok{return}\NormalTok{(one\_result)}
\NormalTok{\}}

\NormalTok{results }\OtherTok{\textless{}{-}}\NormalTok{ simhelpers}\SpecialCharTok{::}\FunctionTok{repeat\_and\_stack}\NormalTok{(R, }\FunctionTok{one\_run}\NormalTok{( params ))}
\end{Highlighting}
\end{Shaded}

We first create a helper function called \texttt{one\_run()}, which takes \texttt{model\_params} as input.
Inside the body of the function, we call the \texttt{generate\_data()} function to simulate a hypothetical dataset.
We pass this dataset to \texttt{analyze()} and return a small dataset containing the results of the data-analysis procedure.
The \texttt{one\_run()} method is like a coordinator or dispatcher of the system: it generates the data, calls all the evaluation methods we want to call, combines all the results, and hands them back for recording.
Making a helper method such as \texttt{one\_run()} can be useful because it facilitates debugging.

Once we have the \texttt{one\_run()} helper function, we need a way to call it repeatedly. As with many things in R, there are a variety of different ways to do something over and over.
In the above skeleton, we use the \texttt{repeat\_and\_stack()} function from \texttt{simhelpers}.\footnote{In the example from Chapter \ref{t-test-simulation}, we used the \texttt{replicate()} function from base R to repeat the process of generating and analyzing data.
  This function is a fine alternative to the \texttt{repeat\_and\_stack()} approach demonstrated in the skeleton.
  The only drawback is that it requires some further work to combine the results across replications.
  Here is a different version of the skeleton, which uses \texttt{replicate()} instead of \texttt{repeat\_and\_stack()}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{results\_list }\OtherTok{\textless{}{-}} \FunctionTok{replicate}\NormalTok{(}\AttributeTok{n =}\NormalTok{ R, }\AttributeTok{expr =}\NormalTok{ \{}
\NormalTok{  dat }\OtherTok{\textless{}{-}} \FunctionTok{generate\_data}\NormalTok{( params )}
\NormalTok{  one\_result }\OtherTok{\textless{}{-}} \FunctionTok{analyze}\NormalTok{(dat)}
  \FunctionTok{return}\NormalTok{(one\_result)}
\NormalTok{\}, }\AttributeTok{simplify =} \ConstantTok{FALSE}\NormalTok{)}

\NormalTok{results }\OtherTok{\textless{}{-}}\NormalTok{ purrr}\SpecialCharTok{::}\FunctionTok{list\_rbind}\NormalTok{(results\_list)}
\end{Highlighting}
\end{Shaded}

  This version of the skeleton does not create a \texttt{one\_run()} helper function, but instead puts the code from the body of \texttt{one\_run()} directly into the \texttt{expr} argument of \texttt{replicate()}. To learn about other ways of repeatedly evaluating the simulation process, see Appendix \ref{repeating-oneself}.}
In the first argument, we specify the number of times to repeat the process.
In the second argument, we specify an expression that evaluates \texttt{one\_run()} for specified values of the model parameters stored in \texttt{params}.
The \texttt{repeat\_and\_stack()} function then evaluates the expression repeatedly, \texttt{R} times in all, and then stacks up all of the replications into a big dataset, with one or more rows per replication.\footnote{If you would prefer the output as a list rather than a stacked dataset, set \texttt{repeat\_and\_stack()}'s optional argument \texttt{stack\ =\ FALSE}.}

We go into further detail about how to approach running the simulation process in Chapter \ref{running-the-simulation-process}.
Among other things, we will illustrate how to use the \texttt{bundle\_sim()} function from the \texttt{simhelpers} package to automate the process of coding this step, thereby avoiding the need to write a \texttt{one\_run()} helper function.

\subsection{Performance summaries}\label{performance-summaries}

The fourth step in a simulation is to summarize the distribution of simulation results across replications.
Here is the skeleton from our simulation template:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{assess\_performance }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{( results, model\_params ) \{}
  \CommentTok{\# stuff}
  \FunctionTok{return}\NormalTok{(performance\_measures)}
\NormalTok{\}}

\FunctionTok{assess\_performance}\NormalTok{(results, params)}
\end{Highlighting}
\end{Shaded}

The \texttt{assess\_performance()} function takes \texttt{results} as input.
\texttt{results} should be a dataset containing all of the replications of the data-generating and analysis process.
In contrast to the \texttt{analyze()} function, \texttt{assess\_performance()} also needs to know the true parameter values of the data-generating process, so it needs to have \texttt{model\_params} as its other input.
The function then uses both of these inputs to calculate performance measures and returns a summary of the performance measures in a dataset.

Performance measures are the metrics or criteria used to assess the performance of a statistical method across repeated samples from the data-generating process.
For example, we might want to know how close an estimator gets to the target parameter, on average.
We might want to know if a confidence interval captures the true parameter the right proportion of the time, as in the simulation from Chapter \ref{t-test-simulation}.
Performance is defined in terms of the sampling distribution of estimators or analysis results, across an infinite number of replications of the data-generating process.
In practice, we use many replications of the process, but still only a finite number. Consequently, we actually \emph{estimate} the performance measures and need to attend to the Monte Carlo error in the estimates.
We discuss the specifics of different performance measures and assessment of Monte Carlo error in Chapter \ref{performance-criteria}.

\subsection{Multifactor simulations}\label{multifactor-simulations}

Thus far, we have sketched out the structure of a modular, tidy simulation for a single context.
In our \(t\)-test case study, for example, we might ask how well the \(t\)-test works when we have \(n=100\) units and the observations follow geometric distribution.
However, we rarely want to examine a method only in a single context.
Typically, we want to explore how well a procedure works across a range of different contexts.
If we choose conditions in a structured and thoughtful manner, we will be able to examine broad trends and potentially make more general claims about the behaviors of the data-analysis procedures under investigation.
Thus, it is helpful to think of simulations as akin to a designed experiment: in seeking to understand the properties of one or more procedures, we test them under a variety of different scenarios to see how they perform, then seek to identify more general patterns that hold beyond the specific scenarios examined.
This is the heart of simulation for methodological evaluation.

To implement a multifactor simulation, we will follows the same principles of modular, tidy simulation.
In particular, we will take the code developed for simulating a single context and bundle it into a function that can be evaluated for any and all scenarios of interest.
Simulation studies often follow a full factorial design, in which each level of a factor (something we vary, such as sample size, true treatment effect, or residual variance) is crossed with every other level.
The experimental design then consists of sets of parameter values (including design parameters, such as sample sizes), and these too can be represented in an object, distinct from the other components of the simulation.
We will discuss multiple-scenario simulations in Part III (starting with Chapter \ref{exp-design}), after we more fully develop the core concepts and techniques involved in simulating a single context.

\section{Exercises}\label{exercises-2}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Look back at the \(t\)-test simulation presented in Chapter \ref{t-test-simulation}. The code presented there did not entirely follow the formal structure outlined in this chapter. Revise the code by creating separate functions for each of four components in the simulation skeleton. Using the functions, re-run the simulation and recreate one or more graphs from the exercises in the previous chapter.
\end{enumerate}

\chapter{Case Study: Heteroskedastic ANOVA and Welch}\label{case-ANOVA}

In this chapter, we present another detailed example of a simulation study to demonstrate how to put the principles of tidy, modular simulation into practice.
To illustrate the process of programming a simulation, we reconstruct the simulations from \citet{brown1974SmallSampleBehavior}.
We will also use this case study as a recurring example in some of the following chapters.

\citet{brown1974SmallSampleBehavior} studied methods for null hypothesis testing in studies that measure a characteristic \(X\) on samples from each of several groups. They consider a population consisting of \(G\) separate groups, with population means \(\mu_1,...,\mu_G\) and population variances \(\sigma_1^2,...,\sigma_G^2\) for the characteristic \(X\).
A researcher obtains samples of size \(n_1,...,n_G\) from each of the groups and takes measurements of the characteristic for each sampled unit.
Let \(x_{ig}\) denote the measurement from unit \(i\) in group \(g\), for \(i = 1,...,n_g\) for each \(j = 1,..., G\).
The researcher's goal is to use the sample data to test the hypothesis that the population means are all equal
\[
H_0: \mu_1 = \mu_2 = \cdots = \mu_G.
\]

If the population \emph{variances} were all equal (so \(\sigma_1^2 = \sigma_2^2 = \cdots = \sigma_G^2\)), we could use a conventional one-way analysis of variance (ANOVA) to conduct this test.
However, one-way ANOVA might not work well if the variances are not equal.
The question is then: what are best practices for testing the null of equal group means, allowing for the possibility that variances could differ across groups?

To tackle this question, Brown and Forsythe evaluated two different hypothesis testing procedures, developed by \citet{james1951ComparisonSeveralGroups} and \citet{welch1951ComparisonSeveralMean}, both of which avoid the assumption that all groups have equal equality of variances.
Brown and Forsythe also evaluated the conventional one-way ANOVA F-test as a benchmark, even though this procedure maintains the assumption of equal variances.
They also proposed and evaluated a new procedure of their own devising.\footnote{This latter piece makes Brown and Forsythe's study a prototypical example of a statistical methodology paper: find some problem that current procedures do not perfectly solve, invent something to do a better job, and then do simulations and/or math to build a case that the new procedure is better.}
Their simulation involved comparing the performance of these different hypothesis testing procedures (the methods) under a range of conditions (different data generating processes) with different sample sizes and different degrees of heteroskedasticity.
They looked at the different scenarios shown as Table \ref{tab:BF-Scenarios}, varying number of groups, group size, and amount of variation within each group.
In all, there are a total of 20 scenarios, covering conditions with between 10 and 6 groups.

\begin{table}
\centering
\caption{\label{tab:BF-Scenarios}Simulation scenarios explored by Brown and Forsythe (1974)}
\centering
\begin{tabular}[t]{l|l|l|l}
\hline
Scenario & Groups & Sample Sizes & Standard Deviations\\
\hline
A & 4 & 4,4,4,4 & 1,1,1,1\\
\hline
B & 4 &  & 1,2,2,3\\
\hline
C & 4 & 4,8,10,12 & 1,1,1,1\\
\hline
D & 4 &  & 1,2,2,3\\
\hline
E & 4 &  & 3,2,2,1\\
\hline
F & 4 & 11,11,11,11 & 1,1,1,1\\
\hline
G & 4 &  & 1,2,2,3\\
\hline
H & 4 & 11,16,16,21 & 1,1,1,1\\
\hline
I & 4 &  & 3,2,2,1\\
\hline
J & 4 &  & 1,2,2,3\\
\hline
K & 6 & 4,4,4,4,4,4 & 1,1,1,1,1,1\\
\hline
L & 6 &  & 1,1,2,2,3,3\\
\hline
M & 6 & 4,6,6,8,10,12 & 1,1,1,1,1,1\\
\hline
N & 6 &  & 1,1,2,2,3,3\\
\hline
O & 6 &  & 3,3,2,2,1,1\\
\hline
P & 6 & 6,6,6,6,6,6 & 1,1,2,2,3,3\\
\hline
Q & 6 & 11,11,11,11,11,11 & 1,1,2,2,3,3\\
\hline
R & 6 & 16,16,16,16,16,16 & 1,1,2,2,3,3\\
\hline
S & 6 & 21,21,21,21,21,21 & 1,1,2,2,3,3\\
\hline
T & 10 & 20,20,20,20,20,20,20,20,20,20 & 1,1,1.5,1.5,2,2,2.5,2.5,3,3\\
\hline
\end{tabular}
\end{table}

When evaluating hypothesis testing procedures, there are two main performance metrics of interest: type-I error rate and power.
The type-I error rate is the rate at which a test rejects the null hypothesis when the null hypothesis is actually true.
To apply a hypothesis testing procedure, one has to specify a desired, or nominal, type-I error rate, often denoted as the \(\alpha\)-level.
For a specified \(\alpha\), a valid or well-calibrated test should have an actual type-I error rate less than or equal to the nominal level, and ideally should be very close to nominal.
Power is how often a test correctly rejects the null when it is indeed false.
It is a measure of how sensitive a method is to violations of the null.

Brown and Forsythe estimated error rates and power for nominal \(\alpha\)-levels of 1\%, 5\%, and 10\%.
Table 1 of their paper reports the simulation results for type-I error (labeled as ``size'').
Our Table \ref{tab:BF-table1} reports some of their results with respect to Type I error.
For a well-calibrated hypothesis testing method, the reported numbers should be very close to the desired alpha levels, as listed at the top of the table.
We can compare the four tests to each other across each row, where each row is a specific scenario defined by a specific data generating process.
Looking at ANOVA, for example, we see some scenarios with very elevated rates. For instance, in Scenario E, the ANOVA F-test has 21.9\% rejection when it should only have 10\%.
In contrast, the ANOVA F works fine under scenario A, which is what one would expect because all the groups have the same variance.
Brown and Forsythe's choice of scenarios here illustrates a broader design principle: to provide a full picture of the performance of a method or set of methods, it is wise to always evaluate them under conditions where we expect things to work, as well as conditions where we expect them to not work well.

\begin{table}
\centering
\caption{\label{tab:BF-table1}Portion of "Table 1" reproduced from Brown and Forsythe (1974)}
\centering
\begin{tabular}[t]{l|r|r|r|r|r|r|r|r|r|r|r|r}
\hline
\multicolumn{1}{c|}{ } & \multicolumn{3}{c|}{ANOVA F test} & \multicolumn{3}{c|}{B \& F's F* test} & \multicolumn{3}{c|}{Welch's test} & \multicolumn{3}{c}{James' test} \\
\cline{2-4} \cline{5-7} \cline{8-10} \cline{11-13}
Scenario & 10\% & 5\% & 1\% & 10\% & 5\% & 1\% & 10\% & 5\% & 1\% & 10\% & 5\% & 1\%\\
\hline
A & 10.2 & 4.9 & 0.9 & 7.8 & 3.4 & 0.5 & 9.6 & 4.5 & 0.8 & 13.3 & 7.9 & 2.4\\
\hline
B & 12.0 & 6.7 & 1.7 & 8.9 & 4.1 & 0.7 & 10.3 & 4.7 & 0.8 & 13.8 & 8.1 & 2.7\\
\hline
C & 9.9 & 5.1 & 1.1 & 9.5 & 4.8 & 1.0 & 10.8 & 5.7 & 1.6 & 12.1 & 6.7 & 2.1\\
\hline
D & 5.9 & 3.0 & 0.6 & 10.3 & 5.7 & 1.4 & 9.8 & 4.9 & 0.9 & 10.8 & 5.6 & 1.3\\
\hline
E & 21.9 & 14.4 & 5.6 & 11.0 & 6.2 & 1.8 & 11.3 & 6.5 & 2.0 & 12.9 & 7.7 & 2.9\\
\hline
F & 10.1 & 5.1 & 1.0 & 9.8 & 5.7 & 1.5 & 10.0 & 5.0 & 0.9 & 10.6 & 5.5 & 1.1\\
\hline
G & 11.4 & 6.3 & 1.8 & 10.7 & 5.7 & 1.5 & 10.1 & 5.0 & 1.1 & 10.6 & 5.4 & 1.3\\
\hline
H & 10.3 & 4.9 & 1.1 & 10.3 & 5.1 & 1.0 & 10.2 & 5.0 & 1.0 & 10.5 & 5.3 & 1.2\\
\hline
I & 17.3 & 10.8 & 3.9 & 11.1 & 6.2 & 1.8 & 10.5 & 5.5 & 1.2 & 10.9 & 5.8 & 1.3\\
\hline
J & 7.3 & 4.0 & 1.0 & 11.5 & 6.5 & 1.8 & 10.6 & 5.4 & 1.1 & 10.9 & 5.6 & 1.1\\
\hline
K & 9.6 & 4.9 & 1.0 & 7.3 & 3.4 & 0.4 & 11.4 & 6.1 & 1.4 & 14.7 & 9.5 & 3.8\\
\hline
\end{tabular}
\end{table}

To replicate the Brown and Forsythe simulation, we will first write functions to generate data for a specified scenario and evaluate data of a given structure.
We will then use these functions to evaluate the hypothesis testing procedures in a specific scenario with a particular set of parameters (e.g., sample sizes, number of groups, and so forth). We will then scale up to execute the simulations for a range of scenarios that vary the parameters of the data-generating model, just as reported in Brown and Forsythe's paper.

\section{The data-generating model}\label{case-anova-DGP}

In the heteroskedastic one-way ANOVA simulation, there are three sets of parameter values: population means, population variances, and sample sizes.
Rather than attempting to write a general data-generating function immediately, it is often easier to write code for a specific case first and then use that code as a starting point for developing a function.
For example, say that we have four groups with means of 1, 2, 5, 6; variances of 3, 2, 5, 1; and sample sizes of 3, 6, 2, 4:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mu }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{6}\NormalTok{)}
\NormalTok{sigma\_sq }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{sample\_size }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{4}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Following \citet{brown1974SmallSampleBehavior}, we will assume that the measurements are normally distributed within each sub-group of the population. The following code generates a vector of group id's and a vector of simulated measurements:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{N }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(sample\_size) }\CommentTok{\# total sample size}
\NormalTok{G }\OtherTok{\textless{}{-}} \FunctionTok{length}\NormalTok{(sample\_size) }\CommentTok{\# number of groups}

\CommentTok{\# group id factor}
\NormalTok{group }\OtherTok{\textless{}{-}} \FunctionTok{factor}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{G, }\AttributeTok{times =}\NormalTok{ sample\_size))}

\CommentTok{\# mean for each unit of the sample}
\NormalTok{mu\_long }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(mu, }\AttributeTok{times =}\NormalTok{ sample\_size) }

\CommentTok{\# sd for each unit of the sample}
\NormalTok{sigma\_long }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(}\FunctionTok{sqrt}\NormalTok{(sigma\_sq), }\AttributeTok{times =}\NormalTok{ sample\_size) }

\CommentTok{\# See what we have?}
\FunctionTok{tibble}\NormalTok{( }\AttributeTok{group =}\NormalTok{ group, }\AttributeTok{mu =}\NormalTok{ mu\_long, }\AttributeTok{sigma =}\NormalTok{ sigma\_long )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 15 x 3
##    group    mu sigma
##    <fct> <dbl> <dbl>
##  1 1         1  1.73
##  2 1         1  1.73
##  3 1         1  1.73
##  4 2         2  1.41
##  5 2         2  1.41
##  6 2         2  1.41
##  7 2         2  1.41
##  8 2         2  1.41
##  9 2         2  1.41
## 10 3         5  2.24
## 11 3         5  2.24
## 12 4         6  1   
## 13 4         6  1   
## 14 4         6  1   
## 15 4         6  1
\end{verbatim}

Now we have the pieces needed to generate a small dataset consisting of group memberships and the measured characteristic:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Now make our data}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(N, }\AttributeTok{mean =}\NormalTok{ mu\_long, }\AttributeTok{sd =}\NormalTok{ sigma\_long)}
\NormalTok{dat }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}\AttributeTok{group =}\NormalTok{ group, }\AttributeTok{x =}\NormalTok{ x)}
\NormalTok{dat}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 15 x 2
##    group      x
##    <fct>  <dbl>
##  1 1      1.24 
##  2 1      3.07 
##  3 1     -0.681
##  4 2      2.43 
##  5 2      2.50 
##  6 2      2.15 
##  7 2      0.612
##  8 2      0.860
##  9 2      2.09 
## 10 3      1.56 
## 11 3      5.08 
## 12 4      5.68 
## 13 4      5.66 
## 14 4      5.92 
## 15 4      4.38
\end{verbatim}

We have followed the strategy of first constructing a dataset with parameters for each observation in each group, making heavy use of base R's \texttt{rep()} function to repeat values in a list.
We then called \texttt{rnorm()} to generate \texttt{N} observations in all.
This works because \texttt{rnorm()} is \emph{vectorized}; if you give it a vector (or vectors) of parameter values, it will generate each subsequent observation according to the next entry in the vector. As a result, the first \texttt{x} value is simulated from a normal distribution with mean \texttt{mu\_long{[}1{]}} and standard deviation \texttt{sd\_long{[}1{]}}, the second \texttt{x} is simulated using \texttt{mu\_long{[}2{]}} and \texttt{sd\_long{[}2{]}}, and so on.

As usual, there are many different and legitimate ways of doing this in R.
For instance, instead of using \texttt{rep()} to do it all at once, we could generate observations for each group separately, then stack the observations into a dataset.
Do not worry about trying to writing code the ``best'' way---especially when you are initially putting a simulation together.
If you can find a way to accomplish your task at all, then that's often enough (and you should feel good about it!).

\subsection{Now make a function}\label{now-make-a-function}

Because we will need to generate datasets over and over, we will wrap our code in a function.
The inputs to the function will be the parameters of the model that we specified at the very beginning: the set of population means \texttt{mu}, the population variances \texttt{sigma\_sq}, and sample sizes \texttt{sample\_size}. We make these quantities arguments of the data-generating function so that we can make datasets of different sizes and shapes:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{generate\_ANOVA\_data }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(mu, sigma\_sq, sample\_size) \{}
  
\NormalTok{  N }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(sample\_size)}
\NormalTok{  G }\OtherTok{\textless{}{-}} \FunctionTok{length}\NormalTok{(sample\_size)}
  
\NormalTok{  group }\OtherTok{\textless{}{-}} \FunctionTok{factor}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{G, }\AttributeTok{times =}\NormalTok{ sample\_size))}
\NormalTok{  mu\_long }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(mu, }\AttributeTok{times =}\NormalTok{ sample\_size)}
\NormalTok{  sigma\_long }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(}\FunctionTok{sqrt}\NormalTok{(sigma\_sq), }\AttributeTok{times =}\NormalTok{ sample\_size)}
  
\NormalTok{  x }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(N, }\AttributeTok{mean =}\NormalTok{ mu\_long, }\AttributeTok{sd =}\NormalTok{ sigma\_long)}
\NormalTok{  sim\_data }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}\AttributeTok{group =}\NormalTok{ group, }\AttributeTok{x =}\NormalTok{ x)}
  
  \FunctionTok{return}\NormalTok{(sim\_data)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

The function is simply the code we built previously, all bundled up.
We developed the function by first writing code to make the data-generating process to work once, the way we want, and only then turning the final code into a function for later reuse.

Once we have turned the code into a function, we can call it to get a new set of simulated data.
For example, to generate a dataset with the same parameters as before, we can do:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sim\_data }\OtherTok{\textless{}{-}} \FunctionTok{generate\_ANOVA\_data}\NormalTok{(}
  \AttributeTok{mu =}\NormalTok{ mu, }
  \AttributeTok{sigma\_sq =}\NormalTok{ sigma\_sq,}
  \AttributeTok{sample\_size =}\NormalTok{ sample\_size}
\NormalTok{)}

\FunctionTok{str}\NormalTok{(sim\_data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## tibble [15 x 2] (S3: tbl_df/tbl/data.frame)
##  $ group: Factor w/ 4 levels "1","2","3","4": 1 1 1 2 2 2 2 2 2 3 ...
##  $ x    : num [1:15] 0.777 2.115 1.31 1.848 3.041 ...
\end{verbatim}

To generate one with population means of zero in all the groups, but the same group variances and sample sizes as before, we can do:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sim\_data\_null }\OtherTok{\textless{}{-}} \FunctionTok{generate\_ANOVA\_data}\NormalTok{(}
  \AttributeTok{mu =} \FunctionTok{c}\NormalTok{( }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{ ),}
  \AttributeTok{sigma\_sq =}\NormalTok{ sigma\_sq, }
  \AttributeTok{sample\_size =}\NormalTok{ sample\_size}
\NormalTok{)}

\FunctionTok{str}\NormalTok{(sim\_data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## tibble [15 x 2] (S3: tbl_df/tbl/data.frame)
##  $ group: Factor w/ 4 levels "1","2","3","4": 1 1 1 2 2 2 2 2 2 3 ...
##  $ x    : num [1:15] 0.777 2.115 1.31 1.848 3.041 ...
\end{verbatim}

Following the principles of tidy, modular simulation, we have written a function that returns a rectangular dataset for further analysis.
Also note that the dataset returned by \texttt{generate\_ANOVA\_data()} only includes the variables \texttt{group} and \texttt{x}, but not \texttt{mu\_long} or \texttt{sd\_long}.
This is by design.
Including \texttt{mu\_long} or \texttt{sd\_long} would amount to making the population parameters available for use in the data analysis procedures, which is not something that happens when analyzing real data.

\subsection{Cautious coding}\label{cautious-coding}

In the above, we built some sample code, and then bundled it into a function by literally cutting and pasting the initial work we did into a function skeleton.
In the process, we shifted from having variables in our workspace with different names to using those variable names as parameters in our function call.

Developing code in this way is not without hazards.
In particular, after we have created our function, our workspace is left with a variable \texttt{mu} in it and our function also has a parameter named \texttt{mu}.
Inside the function, R will use the parameter \texttt{mu} first, but this is potentially confusing.
Another potential source of confusion are lines such as \texttt{mu\ =\ mu}, which means ``set the function's parameter called \texttt{mu} to the variable called \texttt{mu}.''
These are different things (with the same name).

Once you have built a function, one way to check that it is working properly is to comment out the initial code (or delete it), clear out the workspace (or restart R), and then re-run the code that uses the function.
If things still work, then you can be somewhat confident that you have successfully bundled your code into the function.
Once you bundle your code, you can also do a search and replace to change the variable names inside your function to something more generic, to better clarify the distinction betwen object names and argument names.

\section{The hypothesis testing procedures}\label{the-hypothesis-testing-procedures}

Brown and Forsythe considered four different hypothesis testing procedures for heteroskedastic ANOVA, but we will focus on just two of the tests for now.
We start with the conventional one-way ANOVA that mistakenly assumes homoskedasticity.
R's \texttt{oneway.test} function will calculate this test automatically:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sim\_data }\OtherTok{\textless{}{-}} \FunctionTok{generate\_ANOVA\_data}\NormalTok{(}
  \AttributeTok{mu =}\NormalTok{ mu, }
  \AttributeTok{sigma\_sq =}\NormalTok{ sigma\_sq,}
  \AttributeTok{sample\_size =}\NormalTok{ sample\_size}
\NormalTok{)}

\NormalTok{anova\_F }\OtherTok{\textless{}{-}} \FunctionTok{oneway.test}\NormalTok{(x }\SpecialCharTok{\textasciitilde{}}\NormalTok{ group, }\AttributeTok{data =}\NormalTok{ sim\_data, }\AttributeTok{var.equal =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{anova\_F}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  One-way analysis of means
## 
## data:  x and group
## F = 8.9503, num df = 3, denom df = 11,
## p-value = 0.002738
\end{verbatim}

We can use the same function to calculate Welch's test by setting \texttt{var.equal\ =\ FALSE}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Welch\_F }\OtherTok{\textless{}{-}} \FunctionTok{oneway.test}\NormalTok{(x }\SpecialCharTok{\textasciitilde{}}\NormalTok{ group, }\AttributeTok{data =}\NormalTok{ sim\_data, }\AttributeTok{var.equal =} \ConstantTok{FALSE}\NormalTok{)}
\NormalTok{Welch\_F}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  One-way analysis of means (not assuming
##  equal variances)
## 
## data:  x and group
## F = 22.321, num df = 3.0000, denom df =
## 3.0622, p-value = 0.01399
\end{verbatim}

The main results we need here are the \(p\)-values of the tests, which will let us assess Type-I error and power for a given nominal \(\alpha\)-level. The following function takes simulated data as input and returns as output the \(p\)-values from the one-way ANOVA test and Welch test:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ANOVA\_Welch\_F }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(data) \{}
\NormalTok{  anova\_F }\OtherTok{\textless{}{-}} \FunctionTok{oneway.test}\NormalTok{(x }\SpecialCharTok{\textasciitilde{}}\NormalTok{ group, }\AttributeTok{data =}\NormalTok{ data, }\AttributeTok{var.equal =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{  Welch\_F }\OtherTok{\textless{}{-}} \FunctionTok{oneway.test}\NormalTok{(x }\SpecialCharTok{\textasciitilde{}}\NormalTok{ group, }\AttributeTok{data =}\NormalTok{ data, }\AttributeTok{var.equal =} \ConstantTok{FALSE}\NormalTok{)}
  
\NormalTok{  result }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}
    \AttributeTok{ANOVA =}\NormalTok{ anova\_F}\SpecialCharTok{$}\NormalTok{p.value,}
    \AttributeTok{Welch =}\NormalTok{ Welch\_F}\SpecialCharTok{$}\NormalTok{p.value}
\NormalTok{  )}
  
  \FunctionTok{return}\NormalTok{(result)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ANOVA\_Welch\_F}\NormalTok{(sim\_data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1 x 2
##     ANOVA  Welch
##     <dbl>  <dbl>
## 1 0.00274 0.0140
\end{verbatim}

Following our tidy, modular simulation principles, this function returns a small dataset with the p-values from both tests.
Eventually, we might want to use this function on some real data.
Our estimation function does not care if the data are simulated or not; we call the input \texttt{data} rather than \texttt{sim\_data} to reflect this.

As an alternative to this function, we could instead write code to implement the ANOVA and Welch tests ourselves.
This has some potential advantages, such as avoiding any extraneous calculations that \texttt{oneway.test} does, which take time and slow down our simulation.
However, there are also drawbacks to doing so, including that writing our own code takes \emph{our} time and opens up the possibility of errors in our code.
For further discussion of the trade-offs, see Chapter \ref{optimize-code}, where we do implement these tests by hand and see what kind of speed-ups we can obtain.

\section{Running the simulation}\label{running-the-simulation}

We now have functions that implement steps 2 and 3 of the simulation.
Given some parameters, \texttt{generate\_ANOVA\_data} produces a simulated dataset and, given some data, \texttt{ANOVA\_Welch\_F} calculates \(p\)-values two different ways.
We now want to know which way is better, and by how much.
To answer this question, we will need to repeat the chain of generate-and-analyze calculations a bunch of times.
To facilitate repetition, we first put the components together into a single function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{one\_run }\OtherTok{=} \ControlFlowTok{function}\NormalTok{( mu, sigma\_sq, sample\_size ) \{}
\NormalTok{  sim\_data }\OtherTok{\textless{}{-}} \FunctionTok{generate\_ANOVA\_data}\NormalTok{( }\AttributeTok{mu =}\NormalTok{ mu, }\AttributeTok{sigma\_sq =}\NormalTok{ sigma\_sq, }\AttributeTok{sample\_size =}\NormalTok{ sample\_size )}
  \FunctionTok{ANOVA\_Welch\_F}\NormalTok{(sim\_data)}
\NormalTok{\}}

\FunctionTok{one\_run}\NormalTok{( }\AttributeTok{mu =}\NormalTok{ mu, }\AttributeTok{sigma\_sq =}\NormalTok{ sigma\_sq, }\AttributeTok{sample\_size =}\NormalTok{ sample\_size )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1 x 2
##    ANOVA Welch
##    <dbl> <dbl>
## 1 0.0167 0.107
\end{verbatim}

This function implements a single simulation trial by generating artificial data and then analyzing the data, ending with a tidy dataset that has results for the single run.

We next call \texttt{one\_run()} over and over; see Appendix \ref{repeating-oneself} for some discussion of options.
The following uses \texttt{repeat\_and\_stack()} from \texttt{simhelpers} to evaluate \texttt{one\_run()} 4 times and then stack the results into a single dataset:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(simhelpers)}

\NormalTok{sim\_data }\OtherTok{\textless{}{-}} \FunctionTok{repeat\_and\_stack}\NormalTok{(}\DecValTok{4}\NormalTok{, }
  \FunctionTok{one\_run}\NormalTok{( }\AttributeTok{mu =}\NormalTok{ mu, }\AttributeTok{sigma\_sq =}\NormalTok{ sigma\_sq, }\AttributeTok{sample\_size =}\NormalTok{ sample\_size)}
\NormalTok{)}
\NormalTok{sim\_data}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 4 x 2
##     ANOVA  Welch
##     <dbl>  <dbl>
## 1 0.0262  0.0125
## 2 0.00451 0.0698
## 3 0.00229 0.0380
## 4 0.0108  0.0423
\end{verbatim}

Voila! We have simulated \(p\)-values!

\section{Summarizing test performance}\label{summarizing-test-performance}

We now have all the pieces in place to reproduce the results from Brown and Forsythe (1974).
We first focus on calculating the actual type-I error rate of these tests---that is, the proportion of the time that they reject the null hypothesis of equal means when that null is actually true---for an \(\alpha\)-level of .05.
To evaluate the type-I error rate, we need to simulate data from a process where the population means are indeed all equal.
Arbitrarily, let's start with \(G = 4\) groups and set all of the means equal to zero:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mu }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{4}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

In the fifth row of Table 1 (Scenario E in our Table \ref{tab:BF-Scenarios}), Brown and Forsythe examine performance for the following parameter values for sample size and population variance:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sample\_size }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{4}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{12}\NormalTok{)}
\NormalTok{sigma\_sq }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{)}\SpecialCharTok{\^{}}\DecValTok{2}
\end{Highlighting}
\end{Shaded}

With these parameter values, we can use \texttt{map\_dfr} to simulate 10,000 \(p\)-values:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p\_vals }\OtherTok{\textless{}{-}} \FunctionTok{repeat\_and\_stack}\NormalTok{(}\DecValTok{10000}\NormalTok{, }
  \FunctionTok{one\_run}\NormalTok{(}
    \AttributeTok{mu =}\NormalTok{ mu,}
    \AttributeTok{sigma\_sq =}\NormalTok{ sigma\_sq,}
    \AttributeTok{sample\_size =}\NormalTok{ sample\_size}
\NormalTok{  ) }
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We can estimate the rejection rates by summarizing across these replicated p-values.
The rule is that the null is rejected if the \(p\)-value is less than \(\alpha\).
To get the rejection rate, we calculate the proportion of replications where the null is rejected:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sum}\NormalTok{(p\_vals}\SpecialCharTok{$}\NormalTok{ANOVA }\SpecialCharTok{\textless{}} \FloatTok{0.05}\NormalTok{) }\SpecialCharTok{/} \DecValTok{10000}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.1391
\end{verbatim}

This is equivalent to taking the mean of the logical conditions:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mean}\NormalTok{(p\_vals}\SpecialCharTok{$}\NormalTok{ANOVA }\SpecialCharTok{\textless{}} \FloatTok{0.05}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.1391
\end{verbatim}

We get a rejection rate that is much larger than \(\alpha = .05\).
We have learned that the ANOVA F-test does not adequately control Type-I error under this set of conditions.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mean}\NormalTok{(p\_vals}\SpecialCharTok{$}\NormalTok{Welch }\SpecialCharTok{\textless{}} \FloatTok{0.05}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.0697
\end{verbatim}

The Welch test does much better, although it appears to be a little bit in excess of 0.05.

Note that these two numbers are quite close (though not quite identical) to the corresponding entries in Table 1 of Brown and Forsythe (1974). The difference is due to the fact that both Table 1 and are results are actually \emph{estimated} rejection rates, because we have not actually simulated an infinite number of replications. The estimation error arising from using a finite number of replications is called \emph{simulation error} (or \emph{Monte Carlo error}).
In Chapter \ref{performance-criteria}, we will look more at how to estimate and control the Monte Carlo simulation error in performance measures.

So there you have it! Each part of the simulation is a distinct block of code, and together we have a modular simulation that can be easily extended to other scenarios or other tests.
The exercises at the end of this chapter ask you to extend the framework further.
In working through them, you will get to experience first-hand how the modular code that we have started to develop is easier to work with than a single, monolithic block of code.

\section{Exercises}\label{exAnovaExercises}

The following exercises involve exploring and tweaking the above simulation code we have developed to replicate the results of Brown and Forsythe (1974).

\subsection{\texorpdfstring{Other \(\alpha\)'s}{Other \textbackslash alpha's}}\label{BF-other-alphas}

Table 1 from Brown and Forsythe reported rejection rates for \(\alpha = .01\) and \(\alpha = .10\) in addition to \(\alpha = .05\). Calculate the rejection rates of the ANOVA F and Welch tests for all three \(\alpha\)-levels and compare to the table.

\subsection{Compare results}\label{BF-compare-results}

Try simulating the Type-I error rates for the parameter values in the first two rows of Table 1 of the original paper. Use 10,000 replications. How do your results compare to the report results?

\subsection{Power}\label{BF-power}

In the primary paper, Table 1 is about Type I error and Table 2 is about power. A portion of Table 2 follows:

\begin{table}

\caption{\label{tab:BF-power}Portion of "Table 2" reproduced from Brown and Forsythe (1974)}
\centering
\begin{tabular}[t]{l|l|r|r|r}
\hline
Variances & Means & Brown's F & B \& F's F* & Welch's W\\
\hline
1,1,1,1 & 0,0,0,0 & 4.9 & 5.1 & 5.0\\
\hline
 & 1,0,0,0 & 68.6 & 67.6 & 65.0\\
\hline
3,2,2,1 & 0,0,0,0 & NA & 6.2 & 5.5\\
\hline
 & 1.3,0,0,1.3 & NA & 42.4 & 68.2\\
\hline
\end{tabular}
\end{table}

In the table, the sizes of the four groups are 11, 16, 16, and 21, for all the scenarios.
Try simulating the \textbf{power levels} for a couple of sets of parameter values from Table \ref{tab:BF-power}.
Use 10,000 replications.
How do your results compare to the results reported in the Table?

\subsection{Wide or long?}\label{BF-wide-long}

Instead of making \texttt{ANOVA\_Welch\_F} return a single row with the columns for the \(p\)-values, one could instead return a dataset with one row for each test. The ``long'' approach is often nicer when evaluating more than two methods, or when each method returns not just a \(p\)-value but other quantities of interest. For our current simulation, we might also want to store the \(F\) statistic, for example. The resulting dataset would then look like the following:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ANOVA\_Welch\_F\_long}\NormalTok{(sim\_data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 2 x 3
##   method Fstat  pvalue
##   <chr>  <dbl>   <dbl>
## 1 ANOVA   8.46 0.00338
## 2 Welch  14.3  0.0241
\end{verbatim}

Modify \texttt{ANOVA\_Welch\_F()} to return output in this format, update your simulation code, and then use \texttt{group\_by()} plus \texttt{summarise()} to calculate rejection rates of both tests.
\texttt{group\_by()} is a method for dividing your data into distinct groups and conducting an operation on each.
The classic form of this would be something like the following:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sres }\OtherTok{\textless{}{-}} 
\NormalTok{  res }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{( method ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{( }\AttributeTok{rejection\_rate =} \FunctionTok{mean}\NormalTok{( pvalue }\SpecialCharTok{\textless{}} \FloatTok{0.05}\NormalTok{ ) )}
\end{Highlighting}
\end{Shaded}

\subsection{Other tests}\label{BF-other-tests}

The \texttt{onewaytests} package in R includes functions for calculating Brown and Forsythe's \(F^*\) test and James' test for differences in population means. Modify the data analysis function \texttt{ANOVA\_Welch\_F} (or, better yet, \texttt{ANOVA\_Welch\_F\_long} from Exercise \ref{BF-wide-long}) to also include results from these hypothesis tests. Re-run the simulation to estimate the type-I error rate of all four tests under Scenarios A and B of Table \ref{tab:BF-Scenarios}.

\subsection{Methodological extensions}\label{methodological-extensions}

What other methodological questions might you want to investigate about the one-way ANOVA problem? List two or three questions that could be investigated with further simulations.

\subsection{Power analysis}\label{power-analysis}

Suppose you were conducting an experimental study involving several groups (perhaps \(G = 3\) or \(4\) or \(5\)). Because of logistical constraints, one of the groups will need to include at least 40\% of the full sample, and you anticipate that this group might have more variable outcomes than the other groups. Your collaborators want to know how large a total sample they will need to recruit in order to have a high probability of detecting significant differences between groups.
How could you use the functions you've developed to answer this question? List out the steps in your approach, and make note of any further information or assumptions you will need in order to answer the question.

\chapter{Data-generating processes}\label{data-generating-processes}

As we saw in Chapter \ref{simulation-structure}, the first step of a simulation is creating artificial data based on some process where we know (and can control) the truth.
This step is what we call the data generating process (DGP).
Think of it as a recipe for cooking up artificial data, which can be applied over and over, any time we're hungry for a new dataset.
Like a good recipe, a good DGP needs to be complete---it cannot be missing ingredients and it cannot omit any steps.
Unlike cooking or baking, however, DGPs are usually specified in terms of a statistical model, or a set of equations involving constants, parameter values, and random variables.
More complex DGPs, such as those for hierarchical data or other latent variable models, will often involve a series of several equations that describe different dimensions or levels of the model, which need to be followed in sequence to produce an artificial dataset.

In this chapter, we will look at how to instantiate a DGP as an R function (or perhaps a set of functions).
Designing DGPs and implementing them in R code involves making choices about what aspects of the model we want to be able to control and how to set up the parameters of the model.
We start by providing a high-level overview of DGPs and discussing some of the choices and challenges involved in designing them.
We then demonstrate how to write R functions for DGPs.
In the remainder, we present detailed examples involving a hierarchical DGP for generating data on students nested within schools and an item response theory DGP for generating data on responses to individual test items.

\section{Examples}\label{DGP-examples}

Before diving in, it is helpful to consider a few examples that we will return to throughout this and subsequent chapters.

\subsection{Example 1: One-way analysis of variance}\label{ANOVA-example}

We have already seen one example of a DGP in the ANOVA example from Chapter \ref{case-ANOVA}. Here, we consider observations on some variable \(X\) drawn from a population consisting of \(G\) groups, where group \(g\) has population mean \(\mu_g\) and population variance \(\sigma_g^2\) for \(g = 1,...,G\).
A simulated dataset consists of \(n_g\) observations from each group \(g = 1,...,G\), where \(X_{ig}\) is the measurement for observation \(i\) in group \(g\).
The statistical model for these data can be written as follows:
\[ 
X_{ig} = \mu_g + \epsilon_{ig}, \quad \mbox{with} \quad \epsilon_{ig} \sim N( 0, \sigma^2_g )
\]
for \(i = 1,...,n_g\) and \(g = 1,...,G\).
Alternately, we could write the model as
\[
X_{ig} \sim N( \mu_g, \sigma_g^2 ).
\]

\subsection{Example 2: Bivariate Poisson model}\label{BVPois-example}

As a second example, suppose that we want to understand how the usual Pearson sample correlation coefficient behaves with non-normal data and also to investigate how the Pearson correlation relates to Spearman's rank correlation coefficient.
To look into such questions, one DGP we might entertain is a bivariate Poisson model, which is a distribution for a pair of counts, \(C_1,C_2\), where each count follows a Poisson distribution and where the pair of counts may be correlated.
We will denote the expected values of the counts as \(\mu_1\) and \(\mu_2\) and the Pearson correlation between the counts as \(\rho\).

To simulate a dataset based on this model, we would first need to choose how many observations to generate. Call this sample size \(N\).
One way to generate data following a bivariate Poisson model is to generate \emph{three} independent Poisson random variables for each of the \(N\) observations:
\[
\begin{aligned}
Z_0 &\sim Pois\left( \rho \sqrt{\mu_1 \mu_2}\right) \\
Z_1 &\sim Pois\left(\mu_1 - \rho \sqrt{\mu_1 \mu_2}\right) \\
Z_2 &\sim Pois\left(\mu_2 - \rho \sqrt{\mu_1 \mu_2}\right)
\end{aligned}
\]
and then combine the pieces to create two dependent observations:
\[
\begin{aligned}
C_1 &= Z_0 + Z_1 \\
C_2 &= Z_0 + Z_2.
\end{aligned}
\]
An interesting feature of this model is that the range of possible correlations is constrained: only positive correlations are possible and, because each of the independent pieces must have a non-negative mean, the maximum possible correlation is \(\sqrt{\frac{\min\{\mu_1,\mu_2\}}{\max\{\mu_1,\mu_2\}}}\).

\subsection{Example 3: Hierarchical linear model for a cluster-randomized trial}\label{CRT-example}

Cluster-randomized trials are randomized experiments where the unit of randomization is a \emph{group} of individuals, rather than the individuals themselves.
For example, suppose we have a collection of schools and the students within them.
A cluster-randomized trial involves randomizing the \emph{schools} into treatment or control conditions and then measuring an outcome such as academic performance on the multiple students within the schools.
Typically, researchers will be interested in the extent to which average outcomes differ across schools assigned to different conditions, which captures the impact of the treatment relative to the control condition.
We will index the schools using \(j = 1,...,J\) and let \(n_j\) denote the number of students observed in school \(j\).
Say that \(Y_{ij}\) is the outcome measure for student \(i\) in school \(j\), for \(1 = 1,...,n_j\) and \(j = 1,...,J\), and let \(Z_j\) be an indicator equal to 1 if school \(j\) is assigned to the treatment condition and otherwise equal to 0.

A widely used approach for estimating impacts from cluster-randomized trials is heirarchical linear modeling (HLM).
One way to write an HLM is in two parts.
First, we consider a regression model that describes the distribution of the outcomes across students within school \(j\):
\[
Y_{ij} = \beta_{0j} + \epsilon_{ij}, \qquad \epsilon_{ij} \sim N(0, \sigma_{\epsilon}^2),
\]
where \(\beta_{0j}\) is the average outcome across students in school \(j\).
Second, we allow that the school-level average outcomes differ by a treatment effect \(\gamma_{1}\) and that, for schools within each condition, the average outcomes follow a normal distribution with variance \(\sigma_u^2\).
We can write these relationships as a regression equation for the school-specific average outcome:
\[
\beta_{0j} = \gamma_{0} + \gamma_{10} Z_j + u_{0j}, \quad u_{0j} \sim N(0, \tau^2),
\]
where \(\gamma_{0}\) is the average outcome among schools in the control condition.

If we only consider the first stage of this model, it looks a bit like the one-way ANOVA model from the previous example:
in both cases, we have multiple observations from each of several groups.\\
The main distinction is that the ANOVA model treats the \(G\) groups as a fixed set, whereas the HLM treats the set of \(J\) schools as sampled from a larger population of schools and includes a regression model describing the variation in the school-level average outcomes.

\section{Components of a DGP}\label{components-of-a-dgp}

A DGP involves a statistical model with parameters and random variables, but it also often includes further details as well, beyond those that we would consider to be part of the model as we would use it for analyzing real data.
In statistical analysis of real data, we often use models that describe only \emph{part} of the distribution of the data, rather than its full, multivariate distribution.
For instance, when conducting a regression analysis, we are analyzing the distribution of an outcome or response variable, conditional on a set of predictor variables.
In other words, we take the predictor variables as \emph{given} or \emph{fixed}, rather than modeling their distribution.
When using an item response theory (IRT) model, we use responses to a set of items to estimate individual ability levels, given the set of items on the test.
We do not (usually) model the items themselves.
In contrast, if we are going to \emph{generate} data for simulating a regression model or IRT model, we need to specify distributions for these additional features (the predictors in a regression model, the items in an IRT model); we can no longer just take them as given.

In designing and discussing DGPs, it is helpful to draw distinctions between the components of the focal statistical model and the remaining components of the DGP that are taken as given when analyzing real data. A first relevant distinction is between structural features, covariates, and outcomes (or more generally, endogenous quantities):

\begin{itemize}
\tightlist
\item
  \textbf{Structural features} are quantities, such as the per-group sample sizes in the one-way ANOVA example, that describe the structure of a dataset but do not enter directly into the focal statistical model.
  When analyzing real data, we usually take the structural features as they come, but when simulating data, we will need to make choices about the structural features.
  For instance, in the HLM example involving students nested within schools, the number of students in each school is a structural feature.
  To simulate data based on HLM, we will need to make choices about the number of schools and the distribution of the number of students in each school (e.g., we might specify that school sizes are uniformly distributed between specified minimum and maximum sizes), even though we do not have to consider these quantities when estimating a hierarchical model on real data.
\item
  \textbf{Covariates} are variables in a dataset that we typically take as given when analyzing real data.
  For instance, in the one-way ANOVA example, the group assignments of each observation is a covariate.
  In the HLM example, covariates would include the treatment indicators \(Z_1,...,Z_J\). In a more elaborate version of the HLM, they might also include variables such as student demographic information, measures of past academic performance, or school-level characteristics such as the school's geographic region or treatment assignment.
  When analyzing real data, we condition on these quantities, but when specifying a DGP, we will need to make choices about how they are distributed (e.g., we might specify that students' past academic performance is normally distributed).
\item
  \textbf{Outcomes and endogenous quantities} are the variables whose distribution is described by the focal statistical model.
  In the one-way ANOVA example, the outcome variable consists of the measurements \(X_{ig}\) for \(i = 1,...,n_g\) and \(g = 1,...,G\).
  In the bivariate Poisson model, the outcomes consist of the component variables \(Z_1,Z_2,Z_3\) and the observed counts \(C_1,C_2\) because all of these quantities follow distributions that are specified as part of the focal model.
  The focal statistical model specifies the distribution of these variables, and we will be interested in estimating the parameters controlling their distribution.
\end{itemize}

Note that the focal statistical model only determines this third component of the DGP. The focal model consists of the equations describing what we would aim to estimate when analyzing real data.
In contrast, the full statistical model also includes additional elements specify how to generate the structural features and covariates---the pieces that are taken as given when analyzing real data.
Table \ref{tab:real-vs-sim} contrasts the role of structural features, covariates, and outcomes in real data analysis versus in simulations.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1594}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4203}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4203}}@{}}
\caption{\label{tab:real-vs-sim} Real Data Analysis versus Simulation}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Component
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Real world
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Simulation world
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Component
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Real world
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Simulation world
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Structural features & We obtain data of a given sample size, sizes of clusters, etc. & We specify sample sizes, we specify how to generate cluster sizes \\
Covariates & Data come with covariates & We specify how to generate covariates \\
Outcomes & Data come with outcome variables & We generate outcome data based on a focal model \\
Parameter estimation & We estimate a statistical model to learn about the unknown parameters & We estimate a statistical model and compare the results to the true parameters \\
\end{longtable}

For a given DGP, the full statistical model might involve distributions for structural features, distributions for covariates, and distributions for outcomes given the covariates.
Each of these distributions will involve parameters that control the properties of the distribution (such as the average and degree of variation in a variable).
We think of these parameters as falling into one of three categories: focal, auxiliary, or design.

\begin{itemize}
\item
  \textbf{Focal} parameters are the quantities that we care about and seek to estimate in real data analysis.
  These are typically parts of the focal statistical model, such as the population means \(\mu_1,...,\mu_G\) in the one-way ANOVA model, the correlation between counts \(\rho\) in the bivariate Poisson model, or the treatment effect \(\gamma_{1}\) in the HLM example.
\item
  \textbf{Auxiliary} parameters are the other quantities that go into the focal statistical model or some other part of the DGP, which we might not be substantively interested in when analyzing real data but which nonetheless affect the analysis.
  For instance, in the one-way ANOVA model, we would consider the population variances \(\sigma_1^2,...,\sigma_G^2\) to be auxiliary if we are not interested in investigating how they vary from group to group.
  In the bivariate Poisson model we might consider the average counts \(\mu_1\) and \(\mu_2\) to be auxiliary parameters.
\item
  \textbf{Design} parameters are the quantities that control how we generate structural features of the data.
  For instance, in a cluster-randomized trial, the fraction of schools assigned to treatment is a design parameter that can be directly controlled by the researchers.
  Additional design parameters might include the minimum and maximum number of students per school.
  Typically, in a real data analysis, we would not directly estimate such parameters because we take the distribution of structural features as given.
\end{itemize}

It is evident from this discussion that DGPs can involve \emph{many} moving parts.
One of the central challenges in specifying DGPs is that the performance of estimation methods will generally be affected by the \emph{full} statistical model---including the design parameters and distribution of structural features and covariates---even though they are not part of the focal model.

\section{A statistical model is a recipe for data generation}\label{DGP-functions}

Once we have decided on a full statistical model and written it down in mathematical terms, we need to translate it into code.
A function that implements a data-generating model should have the following form:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{generate\_data }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{( }
\NormalTok{  focal\_parameters, auxiliary\_parameters, design\_parameters}
\NormalTok{) \{}
  
  \CommentTok{\# generate pseudo{-}random numbers and use those to make some data}
  
  \FunctionTok{return}\NormalTok{(sim\_data)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

The function takes a set of parameter values as input, simulates random numbers and does calculations, and produces as output a set of simulated data.
Typically, the inputs will consist of multiple parameters, and these will include not only the focal model parameters, but also the auxiliary parameters, sample sizes, and other design parameters.
The output will typically be a dataset, mimicking what one would see in an analysis of real data.
In some cases, the output data might be augmented with some other latent quantities (normally unobserved in the real world) that can be used later to assess whether an estimation procedure produces results that are close to the truth.

We have already seen an example of a complete DGP function in the case study on one-way ANOVA (see Section \ref{case-anova-DGP}).
In this case study, we developed the following function to generate data for a single outcome from a set of \(G\) groups:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{generate\_ANOVA\_data }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(mu, sigma\_sq, sample\_size) \{}
  
\NormalTok{  N }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(sample\_size)}
\NormalTok{  G }\OtherTok{\textless{}{-}} \FunctionTok{length}\NormalTok{(sample\_size)}
  
\NormalTok{  group }\OtherTok{\textless{}{-}} \FunctionTok{factor}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{G, }\AttributeTok{times =}\NormalTok{ sample\_size))}
\NormalTok{  mu\_long }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(mu, }\AttributeTok{times =}\NormalTok{ sample\_size)}
\NormalTok{  sigma\_long }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(}\FunctionTok{sqrt}\NormalTok{(sigma\_sq), }\AttributeTok{times =}\NormalTok{ sample\_size)}
  
\NormalTok{  x }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(N, }\AttributeTok{mean =}\NormalTok{ mu\_long, }\AttributeTok{sd =}\NormalTok{ sigma\_long)}
\NormalTok{  sim\_data }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}\AttributeTok{group =}\NormalTok{ group, }\AttributeTok{x =}\NormalTok{ x)}
  
  \FunctionTok{return}\NormalTok{(sim\_data)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

This function takes both the focal model parameters (\texttt{mu}, \texttt{sigma\_sq}) and other design parameters that one might not think of as parameters per-se (\texttt{sample\_size}).
When simulating, we have to specify quantities that we take for granted when analyzing real data.

How would we write a DGP function for the bivariate Poisson model? The equations in Section \ref{DGP-examples} give us the recipe, so it just a matter of re-expressing them in code.
For this model, the only design parameter is the sample size, \(N\);
the sole focal parameter is the correlation between the variates, \(\rho\); and
the auxiliary parameters are the expected counts \(\mu_1\) and \(\mu_2\).
Our function should have all four of these quantities as inputs and should produce as output a dataset with two variables, \(C_1\) and \(C_2\).
Here is one way to implement the model:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{r\_bivariate\_Poisson }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(N, mu1, mu2, }\AttributeTok{rho =} \DecValTok{0}\NormalTok{) \{}
  
  \CommentTok{\# covariance term, equal to E(Z\_3)}
\NormalTok{  EZ3 }\OtherTok{\textless{}{-}}\NormalTok{ rho }\SpecialCharTok{*} \FunctionTok{sqrt}\NormalTok{(mu1 }\SpecialCharTok{*}\NormalTok{ mu2) }
  
  \CommentTok{\# Generate independent components}
\NormalTok{  Z1 }\OtherTok{\textless{}{-}} \FunctionTok{rpois}\NormalTok{(N, }\AttributeTok{lambda =}\NormalTok{ mu1 }\SpecialCharTok{{-}}\NormalTok{ EZ3)}
\NormalTok{  Z2 }\OtherTok{\textless{}{-}} \FunctionTok{rpois}\NormalTok{(N, }\AttributeTok{lambda =}\NormalTok{ mu2 }\SpecialCharTok{{-}}\NormalTok{ EZ3)}
\NormalTok{  Z3 }\OtherTok{\textless{}{-}} \FunctionTok{rpois}\NormalTok{(N, }\AttributeTok{lambda =}\NormalTok{ EZ3)}
  
  \CommentTok{\# Assemble components}
\NormalTok{  dat }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
    \AttributeTok{C1 =}\NormalTok{ Z1 }\SpecialCharTok{+}\NormalTok{ Z3,}
    \AttributeTok{C2 =}\NormalTok{ Z2 }\SpecialCharTok{+}\NormalTok{ Z3}
\NormalTok{  )}
  
  \FunctionTok{return}\NormalTok{(dat)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Here we generate 5 observations from the bivariate Poisson with \(\rho = 0.5\) and \(\mu_1 = \mu_2 = 4\):

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{r\_bivariate\_Poisson}\NormalTok{(}\DecValTok{5}\NormalTok{, }\AttributeTok{rho =} \FloatTok{0.5}\NormalTok{, }\AttributeTok{mu1 =} \DecValTok{4}\NormalTok{, }\AttributeTok{mu2 =} \DecValTok{4}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   C1 C2
## 1  4  4
## 2  2  2
## 3  2  1
## 4  5  6
## 5  2  3
\end{verbatim}

\section{Plot the artificial data}\label{DGP-plotting}

The whole purpose of writing a DGP is to produce something that can be treated just as if it were real data.
Considering that is our goal, we should act like it and engage in data analysis processes that we would apply whenever we analyze real data.
In particular, it is worthwhile to create one or more plots of the data generated by a DGP, just as we would if we were exploring a new real dataset for the first time.
This exercise can be very helpful for catching problems in the DGP function (about which more below).
Beyond just debugging, constructing graphic visualizations can be a very effective way to \emph{study} a model and strengthen your understanding of how to interpret its parameters.

In the one-way ANOVA example, it would be conventional to visualize the data with box plots or some other summary statistics for the data from each group.
For exploratory graphics, we prefer plots that include representations of the raw data points, not just summary statistics.
The figure below uses a density ridge-plot, filled in with points for each observation in each group.
The plot is based on a simulated dataset with 50 observations in each of five groups.

\begin{figure}

{\centering \includegraphics[width=0.75\linewidth]{Designing-Simulations-in-R_files/figure-latex/ANOVA-bee-swarm-1} 

}

\caption{Densities of five heteroskedastic groups for the one-way ANOVA example.}\label{fig:ANOVA-bee-swarm}
\end{figure}

Here is a plot of 30 observations from the bivariate Poisson distribution with means \(\mu_1 = 10, \mu_2 = 7\) and correlation \(\rho = .65\) (points are jittered slightly to avoid over-plotting):

\begin{figure}

{\centering \includegraphics[width=0.75\linewidth]{Designing-Simulations-in-R_files/figure-latex/bivariate-Poisson-scatter-1} 

}

\caption{$N = 30$ observations from the bivariate Poisson distribution with $\mu_1 = 10, \mu_2 = 7, \rho = .65$.}\label{fig:bivariate-Poisson-scatter}
\end{figure}

Plots like these are useful for building intuitions about a model. For instance, we can inspect \ref{fig:bivariate-Poisson-scatter} to get a sense of the order of magnitude and range of the observations, as well as the likelihood of obtaining multiple observations with identical counts.
Depending on the analysis procedures we will apply to the dataset, we might even create plots of transformations of the dataset, such as a histogram of the differences \(C_2 - C_1\) or a scatterplot of the rank transformations of \(C_1\) and \(C_2\).

\section{Check the data-generating function}\label{check-the-data-generating-function}

An important part of programming in R---especially when writing custom functions---is finding ways to test and check the correctness of your code. Just writing a data-generating function is not enough. It is also \emph{critical} to test whether the output it produces is correct. How best to do this will depend on the particulars of the DGP being implemented.

For many DGPs, a broadly useful strategy is to generate a very large sample of data---one so large that the sample distribution should very closely resemble the population distribution.
One can then test whether features of the sample distribution closely align with corresponding parameters of the population model.

For the heteroskedastic ANOVA problem, one basic thing we can do is check that the simulated data from each group follows a normal distribution.
In the following code, we simulate very large samples from each of the four groups, and check that the means and variances agree with the input parameters:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mu }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{6}\NormalTok{)}
\NormalTok{sigma\_sq }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{1}\NormalTok{)}

\NormalTok{check\_data }\OtherTok{\textless{}{-}} \FunctionTok{generate\_ANOVA\_data}\NormalTok{( }
  \AttributeTok{mu =}\NormalTok{ mu, }
  \AttributeTok{sigma\_sq =}\NormalTok{ sigma\_sq,}
  \AttributeTok{sample\_size =} \FunctionTok{rep}\NormalTok{(}\DecValTok{10000}\NormalTok{, }\DecValTok{4}\NormalTok{) }
\NormalTok{)}

\NormalTok{chk }\OtherTok{\textless{}{-}} 
\NormalTok{  check\_data }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{( group ) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{summarise}\NormalTok{(}
    \AttributeTok{n =} \FunctionTok{n}\NormalTok{(),}
    \AttributeTok{mean =} \FunctionTok{mean}\NormalTok{(x),}
    \AttributeTok{var =} \FunctionTok{var}\NormalTok{(x)}
\NormalTok{  ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{mu =}\NormalTok{ mu, }\AttributeTok{sigma2 =}\NormalTok{ sigma\_sq) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{relocate}\NormalTok{( group, n, mean, mu, var, sigma2 )}
\NormalTok{chk}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 4 x 6
##   group     n  mean    mu   var sigma2
##   <fct> <int> <dbl> <dbl> <dbl>  <dbl>
## 1 1     10000 0.988     1 2.97       3
## 2 2     10000 1.99      2 1.99       2
## 3 3     10000 5.02      5 5.00       5
## 4 4     10000 5.98      6 0.993      1
\end{verbatim}

It seems we are recovering our parameters.

We can also make some diagnostic plots to assess whether we have normal data (using QQ plots, where we expect a straight line if the data are normal):

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{( check\_data ) }\SpecialCharTok{+}
  \FunctionTok{aes}\NormalTok{( }\AttributeTok{sample =}\NormalTok{ x, }\AttributeTok{color =}\NormalTok{ group ) }\SpecialCharTok{+} 
  \FunctionTok{facet\_wrap}\NormalTok{( }\SpecialCharTok{\textasciitilde{}}\NormalTok{ group ) }\SpecialCharTok{+}
  \FunctionTok{stat\_qq}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{stat\_qq\_line}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{Designing-Simulations-in-R_files/figure-latex/unnamed-chunk-77-1} \end{center}

This diagnostic looks good too.
Here, these checks may seem a bit silly, but most bugs are silly---at least once you find them!
In models that are even a little bit more complex, it is quite easy for small things such as a sign error to slip into your code.
Even simple checks such as these can be quite helpful in catching such bugs.

\section{Example: Simulating clustered data}\label{case-cluster}

Writing code for a complicated DGP can feel like a daunting task, but if you first focus on a recipe for how the data is generated, it is often not too bad to then convert that recipe into code.
We now illustrate this process with a detailed case study involving a more complex data-generating process
Recent literature on multisite trials (where, for example, students are randomized to treatment or control within each of a series of sites) has explored how variation in the strength of effects across sites can affect how different data-analysis procedures behave \citep[e.g.,][]{miratrix2021applied, Bloom2016using}.
In this example, we are going to extend this work to explore best practices for estimating treatment effects in cluster randomized trials.
In particular, we will investigate what happens when the treatment impact for each school is related to the size of the school.

\subsection{A design decision: What do we want to manipulate?}\label{a-design-decision-what-do-we-want-to-manipulate}

In designing a simulation study, we need to find a DGP that will allow us to address the specific questions we are interested in investigating.
For instance, in the one-way ANOVA example, we wanted to see how different degrees of within-group variation impacted the performance of several hypothesis-testing procedures.
We therefore needed a data generation process that allowed us to control the extent of within-group variation.

To figure out what DGP to use for simulating data from a cluster-randomized trial, we need to consider how we are going to use those data in our simulation study.
Because we are interested in understanding what happens when school-specific effects are related to school size, we will need data with the following features:

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\tightlist
\item
  observations for students in each of several schools;
\item
  schools are different sizes and have different mean outcomes;
\item
  school-specific treatment effects correlate with school size; and
\item
  schools are assigned to different treatment conditions.
\end{enumerate}

A given dataset will consist of observations for individual students in schools, with each student having a school id, a treatment assignment (shared for all in the school), and an outcome.
A good starting point for building a DGP is to first sketch out what a simulated dataset should look like.
For this example, we need data like the following:

\begin{longtable}[]{@{}rrrrr@{}}
\toprule\noalign{}
schoolID & Z & size & studentID & Y \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & 1 & 24 & 1 & 3.6 \\
1 & 1 & 24 & 3 & 1.0 \\
1 & etc & etc & etc & etc \\
1 & 1 & 24 & 24 & 2.0 \\
2 & 0 & 32 & 1 & 0.5 \\
2 & 0 & 32 & 2 & 1.5 \\
2 & 0 & 32 & 3 & 1.2 \\
etc & etc & etc & etc & etc \\
\end{longtable}

When running simulations, it is good practice to look at simple scenarios along with complex ones.
This lets us not only identify conditions where some aspect of the DGP is important, but also verify that the feature does \emph{not} matter under scenarios where we know it should not.
Given this principle, we land on the following points:

\begin{itemize}
\tightlist
\item
  We need a DGP that lets us generate schools that are all the same size or that are all different sizes.
\item
  Our DGP should allow for variation in the school-specific treatment effects.
\item
  We should have the option to generate school-specific effects that are related or unrelated to school size.
\end{itemize}

\subsection{A model for a cluster RCT}\label{a-model-for-a-cluster-rct}

DGPs are expressed and communicated using mathematical models.
In developing a DGP, we often start by considering the model for the outcomes (along with its focal parameters), which covers some but not all of the steps in the full recipe for generating data.
It is helpful to write down the equations for the outcome model and then note what further quantities need to be generated (such as structural features and covariates).
Then we can consider how to generate these quantities with auxiliary models.

Section \ref{CRT-example} introduced a basic HLM for a cluster-randomized trial.
This model had two parts, starting with a model for our student outcome:
\[ Y_{ij} = \beta_{0j} + \epsilon_{ij} \mbox{ with } \epsilon_{ij} \sim N( 0, \sigma^2_\epsilon ) \]
where \(Y_{ij}\) is the outcome for student \(i\) in site \(j\), \(\beta_{0j}\) is the average outcome in site \(j\), and \(\epsilon_{ij}\) is the residual error for student \(i\) in site \(j\).
The model was completed by specifying how the site-specific outcomes vary as a function of treatment assignment:
\[ \beta_{0j} = \gamma_{0} + \gamma_{1} Z_j + u_j, \quad u_j \sim N( 0, \sigma^2_u ).\]
This model has a constant treatment effect: if a school is assigned to treatment, then all outcomes in the cluster are raised by the amount \(\gamma_{1}\).
But we also want to allow the size of impact to vary by school size.
This suggests we will need to elaborate the model to include a treatment-by-size interaction term.

One approach for allowing the school-specific impacts to depend on school size is to introduce school size as a predictor, as in
\[
\beta_{0j} = \gamma_{0} + \gamma_{1} Z_j + \gamma_{2} \left(Z_j \times n_j\right) + u_j.
\]
A drawback of this approach is that changing the average size of the schools will change the average treatment impact.
A more interpretable approach is to allow treatment effects to depend on the \emph{relative} school sizes.
To do this, we can define a covariate that describes the deviation in the school size relative to the average size.
Thus, let
\[ S_j = \frac{n_j - \bar{n}}{ \bar{n} }, \]
where \(\bar{n}\) is the overall average school size.
Using this covariate, we then revise our equation for our site \(j\) to:
\[ \beta_{0j} = \gamma_{0} + \gamma_{1} Z_j + \gamma_{2} \left( Z_j \times S_j\right) + u_j. \]
If \(\gamma_{2}\) is positive, then bigger schools will have larger treatment impacts.
Because \(S_j\) is centered at 0, the overall average impact across schools will be simply \(\gamma_{1}\).
(If \(S_j\) was not centered at zero, then the overall average impact would be some function of \(\gamma_{1}\) and \(\gamma_{2}\).)

Putting all of the above together, we now have an HLM to describe the distribution of outcomes conditional on the covariates and structural features:
\[
\begin{aligned}
Y_{ij} &= \beta_{0j} + \epsilon_{ij} \quad &\epsilon_{ij} &\sim N( 0, \sigma^2_\epsilon ) \\
\beta_{0j} &= \gamma_{0} + \gamma_{1} Z_j + \gamma_{2} Z_j S_j + u_j \quad & u_j &\sim N( 0, \sigma^2_u )
\end{aligned}
\]
Substituting the second equation into the first leads to a single equation for generating the student-level outcomes (or what is called the reduced form of the HLM):
\[ Y_{ij} = \gamma_{0} + \gamma_{1} Z_j + \gamma_{2} Z_j S_j  + u_j + \epsilon_{ij}\]
The parameters of this focal model are the mean outcome among control schools (\(\gamma_{0}\)), the average treatment impact (\(\gamma_{1}\)), the site-size by treatment interaction term (\(\gamma_{2}\)), the amount of school-level variation (\(\sigma^2_u\)), and the amount of within-school variation (\(\sigma^2_\epsilon\)).

There are several ways that we could elaborate this model further.
For one, we might want to include a main effect for \(S_j\), so that average outcomes in the absence of treatment are also dependent on school size.
For another, we might revise the model to allow for school-to-school variation in treatment impacts that is not explained by school size.
For simplicity, we do not build in these further features, but see the exercises at the end of the chapter.

So far we have a mathematical model analogous to what we would write if we were \emph{analyzing} the data.
To \emph{generate} data, we also need a way to generate the structural features and covariates involved in the model.
First, we need to know the number of clusters (\(J\)) and the sizes of the clusters (\(n_j\), for \(j = 1, ..., J\)).
For illustrative purposes, we will generate size sizes from a uniform distribution with average school size \(\bar{n}\) and a fixed parameter \(\alpha\) that controls the degree of variation in school size. Mathematically,
\[ n_j \sim \text{Unif}\left[ (1-\alpha)\bar{n}, (1+\alpha)\bar{n} \right].\]
Equivalently, we could generate site sizes by taking
\[n_j = \bar{n}(1 + \alpha U_j), \quad U_j \sim unif(-1, 1).\]
For instance, if \(\bar{n} = 100\) and \(\alpha = 0.25\) then schools would range in size from 75 to 125.
This specification is nice because it is simple, with just two parameters, both of which are easy to interpret: \(\bar{n}\) is the average school size and \(\alpha\) is the degree of variation in school size.

To round out the model, we also need to define how to generate the treatment indicator, \(Z_j\).
To allow for different treatment allocations, we will specify a proportion \(p\) of clusters assigned to treatment.
Because we are simulating a cluster-randomized trial, we do this by drawing a simple random sample (without replacement) of \(p \times J\) schools out of the total sample of \(J\) schools, then setting \(Z_j = 1\) for these schools and \(Z_j = 0\) for the remaining schools.
We will denote this process as \(Z_1,...,Z_J \sim SRS(p, J)\), where SRS stands for simple random sample.

Now that we have an auxiliary model for school sizes, let us look again at our treatment impact heterogeneity term:
\[ \gamma_{2} Z_j S_j = \gamma_{2} Z_j \left(\frac{n_j - \bar{n}}{\bar{n}}\right) = \gamma_{2}  \alpha Z_j U_j, \]
where \(U_j \sim \text{Unif}(-1,1)\) is the uniform variable used to generate \(n_j\).
Because we have standardized by average school size, the importance of the covariate does not change as a function of average school size, but rather as a function of the relative variation parameter \(\alpha\).
Setting up a DGP with standardized quantities will make it easier to interpret simulation results, especially if we are looking at results from multiple scenarios with different parameter values.
To the extent feasible, we want the parameters of the DGP to change only one feature of the data, so that it is easier to isolate the influence of each parameter.

\subsection{From equations to code}\label{from-equations-to-code}

When sketching out the equations for the DGP, we worked from the lowest level of the model (the students) to the higher level (schools) and then to the auxiliary models for covariates and structural features.
For writing code to based on the DGP, we will proceed in the opposite direction, from auxiliary to focal and from the highest level to the lowest.
First, we will generate the sites and their features:

\begin{itemize}
\tightlist
\item
  Generate school sizes
\item
  Generate school-level covariates
\item
  Generate school-level random effects
\end{itemize}

Then we will generate the students inside the sites:

\begin{itemize}
\tightlist
\item
  Generate student residuals
\item
  Add everything up to generate student outcomes
\end{itemize}

The mathematical model gives us the details we need to execute with each of these steps.

Here is the skeleton of a DGP function with arguments for each of the parameters we might want to control, including defaults for each (see \ref{default-arguments} for more on function defaults):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gen\_cluster\_RCT }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(}
    \AttributeTok{J =} \DecValTok{30}\NormalTok{,}
    \AttributeTok{n\_bar =} \DecValTok{10}\NormalTok{,}
    \AttributeTok{alpha =} \DecValTok{0}\NormalTok{,}
    \AttributeTok{p =} \FloatTok{0.5}\NormalTok{,}
    \AttributeTok{gamma\_0 =} \DecValTok{0}\NormalTok{, }\AttributeTok{gamma\_1 =} \DecValTok{0}\NormalTok{, }\AttributeTok{gamma\_2 =} \DecValTok{0}\NormalTok{,}
    \AttributeTok{sigma2\_u =} \DecValTok{0}\NormalTok{, }\AttributeTok{sigma2\_e =} \DecValTok{1}
\NormalTok{) \{}
  
  \CommentTok{\# generate schools sizes }
  \CommentTok{\# Code (see below) goes here}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Note that the inputs to this function are a mix of \emph{model parameters} (\texttt{gamma\_0}, \texttt{gamma\_1}, \texttt{gamma\_2}, representing coefficients in regressions), \emph{auxilary parameters} (\texttt{sigma2\_u}, \texttt{sigma2\_e}, \texttt{alpha}, \texttt{n\_bar}), and \emph{design parameters} (\texttt{J}, \texttt{p}) that directly inform data generation.
We set default arguments (e.g., \texttt{gamma\_0=0}) so that we can ignore aspects of the DGP that we do not care about for the moment.

Inside the model, we will have a block of code to generate the variables pertaining to schools, and then another to generate the variables pertaining to students.

We first make the schools:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{  n\_min }\OtherTok{\textless{}{-}} \FunctionTok{round}\NormalTok{( n\_bar }\SpecialCharTok{*}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ alpha) )}
\NormalTok{  n\_max }\OtherTok{\textless{}{-}} \FunctionTok{round}\NormalTok{( n\_bar }\SpecialCharTok{*}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{+}\NormalTok{ alpha) )}
\NormalTok{  nj }\OtherTok{\textless{}{-}} \FunctionTok{sample}\NormalTok{( n\_min}\SpecialCharTok{:}\NormalTok{n\_max, J, }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{ )}
  
  \CommentTok{\# Generate average control outcome for all schools}
  \CommentTok{\# (the random effects)}
\NormalTok{  u0j }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{( J, }\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \FunctionTok{sqrt}\NormalTok{(sigma2\_u) )}
  
  \CommentTok{\# randomize schools (proportion p to treatment)}
\NormalTok{  Zj }\OtherTok{\textless{}{-}} \FunctionTok{ifelse}\NormalTok{( }\FunctionTok{sample}\NormalTok{( }\DecValTok{1}\SpecialCharTok{:}\NormalTok{J ) }\SpecialCharTok{\textless{}=}\NormalTok{ J }\SpecialCharTok{*}\NormalTok{ p, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)}
  
  \CommentTok{\# Calculate schools intercepts}
\NormalTok{  S\_j }\OtherTok{\textless{}{-}}\NormalTok{ (nj }\SpecialCharTok{{-}}\NormalTok{ n\_bar) }\SpecialCharTok{/}\NormalTok{ n\_bar}
\NormalTok{  beta\_0j }\OtherTok{\textless{}{-}}\NormalTok{ gamma\_0 }\SpecialCharTok{+}\NormalTok{ gamma\_1 }\SpecialCharTok{*}\NormalTok{ Zj }\SpecialCharTok{+}\NormalTok{ gamma\_2 }\SpecialCharTok{*}\NormalTok{ Zj }\SpecialCharTok{*}\NormalTok{ S\_j }\SpecialCharTok{+}\NormalTok{ u0j}
\end{Highlighting}
\end{Shaded}

The code is a literal translation of the math we did before.
Note the line with \texttt{sample(1:J)\ \textless{}=\ J*p}; this is a simple trick to generate a 0/1 indicator for control and treatment conditions.

There is also a serious error in the above code (serious in that the code will run and look fine in many cases, but not always do what we want); we leave it as an exercise (see below) to find and fix it.

Next, we use the site characteristics to generate the individual-level variables:

\begin{Shaded}
\begin{Highlighting}[]
  \CommentTok{\# Make individual site membership}
\NormalTok{  sid }\OtherTok{\textless{}{-}} \FunctionTok{as.factor}\NormalTok{( }\FunctionTok{rep}\NormalTok{( }\DecValTok{1}\SpecialCharTok{:}\NormalTok{J, nj ) )}
  
  \CommentTok{\# Generate the individual{-}level errors and outcome}
\NormalTok{  N }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{( nj )}
\NormalTok{  e }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{( N, }\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \FunctionTok{sqrt}\NormalTok{(sigma2\_e) )}
\NormalTok{  Y }\OtherTok{\textless{}{-}}\NormalTok{ beta\_0j[sid] }\SpecialCharTok{+}\NormalTok{ e}
  
  \CommentTok{\# Bundle into a dataset}
\NormalTok{  dd }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{( }
    \AttributeTok{sid =}\NormalTok{ sid,}
    \AttributeTok{Z =}\NormalTok{ Zj[ sid ],}
    \AttributeTok{Yobs =}\NormalTok{ Y}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

A key piece here is the \texttt{rep()} function that takes a list and repeats each element of the list a specified number of times.
In particular, \texttt{rep()} repeats each number (\(1, 2, \ldots, J\)), the corresponding number of times as listed in \texttt{nj}.
Putting the code above into the function skeleton will produce a complete DGP function (view the \href{/case_study_code/gen_cluster_RCT.R}{complete function here}).
We can then call the function as so:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dat }\OtherTok{\textless{}{-}} \FunctionTok{gen\_cluster\_RCT}\NormalTok{( }
  \AttributeTok{J=}\DecValTok{3}\NormalTok{, }\AttributeTok{n\_bar =} \DecValTok{5}\NormalTok{, }\AttributeTok{alpha =} \FloatTok{0.5}\NormalTok{, }\AttributeTok{p =} \FloatTok{0.5}\NormalTok{, }
  \AttributeTok{gamma\_0 =} \DecValTok{0}\NormalTok{, }\AttributeTok{gamma\_1 =} \FloatTok{0.2}\NormalTok{, }\AttributeTok{gamma\_2 =} \FloatTok{0.2}\NormalTok{,}
  \AttributeTok{sigma2\_u =} \FloatTok{0.4}\NormalTok{, }\AttributeTok{sigma2\_e =} \DecValTok{1}
\NormalTok{)}

\NormalTok{dat}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    sid Z       Yobs
## 1    1 1  2.3263686
## 2    1 1  2.0202277
## 3    1 1  3.5850632
## 4    1 1  0.9581332
## 5    1 1  2.6183761
## 6    1 1  0.3198559
## 7    2 0 -2.2305376
## 8    2 0  1.0479261
## 9    2 0 -0.6256389
## 10   2 0  1.0891353
## 11   2 0 -0.6051252
## 12   2 0 -0.3099363
## 13   2 0 -0.9828624
## 14   2 0 -0.5571326
## 15   3 0  0.1203995
## 16   3 0  1.7086978
## 17   3 0  0.1213685
\end{verbatim}

With this function, we can control the average size of the clusters (\texttt{n}), the number of clusters (\texttt{J}), the proportion treated (\texttt{p}), the average outcome in the control group (\texttt{gamma\_0}), the average treatment effect (\texttt{gamma\_1}), the site size by treatment interaction (\texttt{gamma\_2}), the amount of cross site variation (\texttt{sigma2\_u}), the residual variation (\texttt{sigma2\_e}), and the amount of site size variation (\texttt{alpha}).
The next step is to test the code, making sure it is doing what we think it is.
In fact, it is not--there is a subtle bug that only appears under some specifications of the parameters; see the exercises for more on diagnosing and repairing this error.

\subsection{Standardization in the DGP}\label{DGP-standardization}

One difficulty with the current implementation of the model is that the magnitude of the different parameters are inter-connected.
For instance, raising or lowering the within-school variance (\(\sigma^2_u\)) will increase the overall variation in \(Y\), and therefore affect our the interpretation of the treatment effect parameters, because a given value of \(\gamma_{1}\) will be less consequential if there is more overall variation.
We can fix this issue by standardizing the model parameters.
Standardization will allow us to reduce the set of parameters we might want to manipulate and will ensure that varying the remaining parameters only affects one aspect of the DGP.

For a continuous, normally distributed outcome variable, a common approach to scaling is to constrain the overall variance of the outcome to a fixed value, such as 1 or 100.
The magnitude of the other parameters of the model can then be interpreted relative to this scale.
Often, we can also constrain the mean of the outcome to a fixed value, such as setting \(\gamma_0 = 0\) without affecting the interpretation of the other parameters.

With the current model, the variance of the outcome across students in the control condition is
\[ 
\begin{aligned}
\text{Var}( Y_{ij} | Z_j = 0) &= \text{Var}( \beta_{0j} + \epsilon_{ij} | Z_j = 0) \\
&= \text{Var}( \gamma_{0} + \gamma_{1} Z_j + \gamma_{2} Z_j S_j + u_j + \epsilon_{ij} | Z_j = 0) \\
&= \text{Var}( \gamma_{0} + u_j + \epsilon_{ij} ) \\
&= \sigma^2_u + \sigma^2_\epsilon.
\end{aligned}
\]
To ensure that the total variance is held constant, we can redefine the variance parameters in terms of the intra-class correlation (ICC).
The ICC is defined as
\[ ICC = \frac{ \sigma^2_u }{ \sigma^2_u + \sigma^2_\epsilon }.\]
The ICC measures the degree of between-group variation as a proportion of the total variation of the outcome.
It plays an important role in power calculations for cluster-randomized trials.
If we want the total variance of the outcome to be 1, we need to set \(\sigma^2_u + \sigma^2_{\epsilon} = 1\), which the implies that \(ICC = \sigma^2_u\), and \(\sigma^2_\epsilon = 1 - ICC\).
Thus, we can call our DGP function as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ICC }\OtherTok{\textless{}{-}} \FloatTok{0.3}
\NormalTok{dat }\OtherTok{\textless{}{-}} \FunctionTok{gen\_cluster\_RCT}\NormalTok{( }
  \AttributeTok{J =} \DecValTok{30}\NormalTok{, }\AttributeTok{n\_bar =} \DecValTok{20}\NormalTok{, }\AttributeTok{alpha =} \FloatTok{0.5}\NormalTok{, }\AttributeTok{p =} \FloatTok{0.5}\NormalTok{,}
  \AttributeTok{gamma\_0 =} \DecValTok{0}\NormalTok{, }\AttributeTok{gamma\_1 =} \FloatTok{0.3}\NormalTok{, }\AttributeTok{gamma\_2 =} \FloatTok{0.2}\NormalTok{,}
  \AttributeTok{sigma2\_u =}\NormalTok{ ICC, }\AttributeTok{sigma2\_e =} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ ICC}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Manipulating the ICC rather than separately manipulating \(\sigma^2_u\) and \(\sigma^2_\epsilon\) will let us change the degree of between-group variation without affecting the overall scale of the outcome.

A further consequence of setting the overall scale of the outcome to 1 is that the parameters controlling the treatment impact can now be interpreted as standardized mean difference effect sizes.
The standardized mean difference for a treatment impact is defined as the average impact over the standard deviation of the outcome among control observations.\footnote{An alternative definition is based on the pooled standard deviation, but this is usually a bad choice if one suspects treatment variation. More treatment variation should not reduce the effect size for the same absolute average impact.}
Letting \(\delta\) denote the standardized mean difference parameter,
\[ \delta = \frac{E(Y | Z_j = 1) - E(Y | Z_j = 0)}{SD( Y | Z_j = 0 )} = \frac{\gamma_1}{\sqrt{ \sigma^2_u + \sigma^2_\epsilon } } \]
Because we have constrained the total variance, \(\gamma_1\) is equivalent to \(\delta\). This equivalence holds for any value of \(\gamma_0\), so we do not have to worry about manipulating \(\gamma_0\) in the simulations---we can simply leave it at its default value.

\section{Sometimes a DGP is all you need}\label{three-parameter-IRT}

We have introduced the data-generating process as only the first step in developing a simulation study. Indeed, there are many more considerations to come, which we will describe in subsequent chapters.
However, this first step is still very useful in its own right, even apart from the other components of a simulation.
Sometimes, a data-generating function is all you need to learn about a statistical model.

Writing data-generating functions is a very effective way to \emph{study} a statistical model, such as a model that you might be learning about in a course or through self-study.
Writing code based on a model is a much more \emph{active} process than listening to a lecture or reading a book or paper.
Coding requires you to make the mathematical notation tangible and can help you to notice details of the model that might be easily missed through listening or reading alone.

Suppose we are taking a first course psychometrics and have just been introduced to item response theory (IRT) models for binary response items.
Our instructor has just laid out a bunch of notation:

\begin{itemize}
\tightlist
\item
  We have data from a sample of \(N\) individuals, each of whom responds to a set of \(M\) test items.
\item
  We let \(X_{im} = 1\) if respondent \(i\) answers item \(m\) correctly, with \(X_{im} = 0\) otherwise, for \(i = 1,...,N\) and \(m = 1,...,M\).
\item
  We imagine that each respondent has a latent ability \(\theta_i\) on whatever domain the test measures.
\end{itemize}

Now our instructor starts dropping models on us, and puts up a slide showing the equation for a three-parameter IRT model:
\[
Pr(X_{im} = 1) = \gamma_m + (1 - \gamma_m) g\left( \alpha_m [\theta_i - \beta_m]\right),
\]
where \(g(x)\) is the cumulative logistic curve: \(g(x) = e^x / (1 + e^x)\).
The instructor explains that \(\alpha_m\) is discrimination parameter that can take any real value, \(\beta_m\) is a difficulty parameter that has to be greater than zero, and \(\gamma_m\) is a guessing parameter between 0 and 1.
They also explain that the guessing parameter is often hard to estimate and so might get treated as fixed and known, based on the number of response options on the item.
For instance, if all the items have four options, then we might take \(\gamma_m = \frac{1}{4}\) for \(m = 1,...,M\).
Finally, they explain that the ability parameters are assumed to follow a standard normal distribution, so \(\theta_i \sim N(0, 1)\) in the population.

What is this madness? It certainly is a lot of notation to follow.
To make sense of all the moving pieces, let's try simulating from the model using more-or-less arbitrary parameters.
To begin, we will need to pick a test length \(M\) and a sample size \(N\).
Let's use \(N = 7\) participants and \(M = 4\) items for starters:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{N }\OtherTok{\textless{}{-}} \DecValTok{7}
\NormalTok{M }\OtherTok{\textless{}{-}} \DecValTok{4}
\end{Highlighting}
\end{Shaded}

The \(\theta_i\) distribution seems like the next-simplest part of the model, so let's generate some ability parameters:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{thetas }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(N)}
\end{Highlighting}
\end{Shaded}

Now we need sets of parameters \(\alpha_m, \beta_m, \gamma_m\) for every item \(m = 1,...,M\).
Where do we get these?

For a particular fixed-length test, the set of item parameters would depend on the features of the actual test questions.
But we are not (yet) dealing with actual testing data, so we will need to make up an auxiliary model for these parameters.
Perhaps we could just simulate some values?
Arbitrarily, let's draw the difficulty parameters from a normal distribution with mean \(\mu_\alpha = 0\) and standard deviation \(\tau_\alpha = 1\).
The discrimination parameters have to be greater than zero, and values near \(\alpha_m = 1\) make the model simplify (in other words, if \(\alpha_1 = 1\) then we can drop the parameter from the model), so let's draw them from a gamma distribution with mean \(\mu_\alpha = 1\) and standard deviation \(\tau_\alpha = 0.2\).
This decision requires a bit of work: gamma distributions are usually parameterized in terms of shape and rate, not mean and standard deviation.
A bit of poking on Wikipedia gives us the answer, however:
shape is equal to \(\mu_\alpha^2 \tau_\alpha^2 = 0.2^2\) and rate is equal to \(\mu_\alpha \tau_\alpha^2 = 0.2^2\).
Finally, we imagine that all the test questions have four possible responses, and therefore set \(\gamma_m = \frac{1}{4}\) for all the items, just like the instructor suggested.
Each item requires three numbers; the easiest way to generate them is to let them all be independent of each other, so we do that.
With that, let's make up some item parameters:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{betas }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(M, }\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \FloatTok{1.5}\NormalTok{)           }\CommentTok{\# difficulty parameters}
\NormalTok{alphas }\OtherTok{\textless{}{-}} \FunctionTok{rgamma}\NormalTok{(M, }\AttributeTok{shape =} \FloatTok{0.2}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{, }\AttributeTok{rate =} \FloatTok{0.2}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)  }\CommentTok{\# discrimination parameters}
\NormalTok{gammas }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(}\DecValTok{1} \SpecialCharTok{/} \DecValTok{4}\NormalTok{, M)                          }\CommentTok{\# guessing parameters}
\end{Highlighting}
\end{Shaded}

A three-parameter IRT model describes the probability that a given respondent with ability \(\theta_i\) answers each of the items on the test correctly.
To generate data based on the model, we need to produce scores on every item for every respondent, leading to a matrix of \(N\) respondents by \(M\) items.
To simplify this calculation, we write a function to compute the item probabilities for a single respondent:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{r\_scores }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(theta\_i, alphas, betas, gammas, M) \{}
\NormalTok{  pi\_i }\OtherTok{\textless{}{-}}\NormalTok{ gammas }\SpecialCharTok{+}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ gammas) }\SpecialCharTok{*} \FunctionTok{plogis}\NormalTok{(alphas }\SpecialCharTok{*}\NormalTok{ (theta\_i }\SpecialCharTok{{-}}\NormalTok{ betas))}
\NormalTok{  scores }\OtherTok{\textless{}{-}} \FunctionTok{rbinom}\NormalTok{(M, }\AttributeTok{size =} \DecValTok{1}\NormalTok{, }\AttributeTok{prob =}\NormalTok{ pi\_i)}
  \FunctionTok{names}\NormalTok{(scores) }\OtherTok{\textless{}{-}} \FunctionTok{paste0}\NormalTok{(}\StringTok{"q"}\NormalTok{,}\DecValTok{1}\SpecialCharTok{:}\NormalTok{M)}
  \FunctionTok{return}\NormalTok{(scores)}
\NormalTok{\}}

\FunctionTok{r\_scores}\NormalTok{(thetas[}\DecValTok{1}\NormalTok{], }\AttributeTok{alphas =}\NormalTok{ alphas, }\AttributeTok{betas =}\NormalTok{ betas, }
         \AttributeTok{gammas =}\NormalTok{ gammas, }\AttributeTok{M =}\NormalTok{ M)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## q1 q2 q3 q4 
##  1  1  1  1
\end{verbatim}

The first line of the function calculates the probability of correctly answering all \(M\) items, and stores the result in a vector \texttt{pi\_i}.
In the next line, we simulate binary outcomes by flipping coins with the specified vector of probabilities.
Finally, we assign names to each item so that we can keep track of which score is for which item.
Now, using the \texttt{map()} function, we can run the function for every one of our respondents:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{map\_dfr}\NormalTok{(}
\NormalTok{  thetas, }\AttributeTok{.f =}\NormalTok{ r\_scores, }
  \AttributeTok{alphas =}\NormalTok{ alphas, }\AttributeTok{betas =}\NormalTok{ betas, }\AttributeTok{gammas =}\NormalTok{ gammas, }\AttributeTok{M =}\NormalTok{ M}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 7 x 4
##      q1    q2    q3    q4
##   <int> <int> <int> <int>
## 1     1     0     0     1
## 2     1     1     1     0
## 3     0     0     1     0
## 4     1     1     1     1
## 5     1     0     1     1
## 6     0     0     1     1
## 7     1     1     1     1
\end{verbatim}

To make it easier to generate datasets with different characteristics, we bundle the above code chunks into a single data-generating function.
To do so, we have to decide what the input parameters should be.
We know we need to at least specify \(N\) and \(M\).
We made assumptions about the item parameters, but we had to specify means and standard deviations for the difficulty and discrimination parameter distributions, so it is natural to allow those to be inputs also.
Our data-generating function will then be

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{r\_3PL\_IRT }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(}
\NormalTok{    N, }\AttributeTok{M =} \DecValTok{4}\NormalTok{,}
    \AttributeTok{diff\_M =} \DecValTok{0}\NormalTok{, }\AttributeTok{diff\_SD =} \DecValTok{1}\NormalTok{,}
    \AttributeTok{disc\_M =} \DecValTok{1}\NormalTok{, }\AttributeTok{disc\_SD =} \FloatTok{0.2}\NormalTok{,}
    \AttributeTok{item\_options =} \DecValTok{4}
\NormalTok{) \{}
  \CommentTok{\# generate ability parameters}
\NormalTok{  thetas }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(N)}
  
  \CommentTok{\# generate item parameters}

\NormalTok{  alphas }\OtherTok{\textless{}{-}} \FunctionTok{rgamma}\NormalTok{(}
\NormalTok{    M, }
    \AttributeTok{shape =}\NormalTok{ disc\_M}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{*}\NormalTok{ disc\_SD}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{, }
    \AttributeTok{rate =}\NormalTok{ disc\_M }\SpecialCharTok{*}\NormalTok{ disc\_SD}\SpecialCharTok{\^{}}\DecValTok{2}
\NormalTok{  )}
\NormalTok{  betas }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(M, }\AttributeTok{mean =}\NormalTok{ diff\_M, }\AttributeTok{sd =}\NormalTok{ diff\_SD)}
\NormalTok{  gammas }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(}\DecValTok{1} \SpecialCharTok{/}\NormalTok{ item\_options, M)}

  \CommentTok{\# simulate item responses}
\NormalTok{  test\_scores }\OtherTok{\textless{}{-}} \FunctionTok{map\_dfr}\NormalTok{(}
\NormalTok{   thetas, }\AttributeTok{.f =}\NormalTok{ r\_scores, }
    \AttributeTok{alphas =}\NormalTok{ alphas, }\AttributeTok{betas =}\NormalTok{ betas, }\AttributeTok{gammas =}\NormalTok{ gammas, }\AttributeTok{M =}\NormalTok{ M}
\NormalTok{  )}
  
  \CommentTok{\# calculate total score}
\NormalTok{  test\_scores}\SpecialCharTok{$}\NormalTok{total }\OtherTok{\textless{}{-}} \FunctionTok{rowSums}\NormalTok{(test\_scores)}
  
  \FunctionTok{return}\NormalTok{(test\_scores)}
\NormalTok{\}}

\FunctionTok{r\_3PL\_IRT}\NormalTok{(}\AttributeTok{N =} \DecValTok{7}\NormalTok{, }\AttributeTok{M =} \DecValTok{4}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 7 x 5
##      q1    q2    q3    q4 total
##   <int> <int> <int> <int> <dbl>
## 1     1     0     0     0     1
## 2     1     1     1     0     3
## 3     0     0     1     1     2
## 4     0     1     1     1     3
## 5     1     1     1     1     4
## 6     1     0     0     1     2
## 7     1     1     1     1     4
\end{verbatim}

Now let's look at a much larger sample of participants, with a longer test that includes \(M = 12\) items:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{test\_scores }\OtherTok{\textless{}{-}} \FunctionTok{r\_3PL\_IRT}\NormalTok{(}\AttributeTok{N =} \DecValTok{10000}\NormalTok{, }\AttributeTok{M =} \DecValTok{12}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Having written a function for the 3-parameter logistic IRT model makes it easy to explore properties of the model.
For instance, we can easily visualize the distribution of the total scores on the test:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(test\_scores, }\FunctionTok{aes}\NormalTok{(total)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_bar}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{scale\_x\_continuous}\NormalTok{(}\AttributeTok{limits =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\FloatTok{12.5}\NormalTok{), }\AttributeTok{breaks =} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{12}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{Designing-Simulations-in-R_files/figure-latex/unnamed-chunk-91-1} \end{center}

Looking at item variation, we can examine the probability of correctly responding to each of the items by computing sample means for each item:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{test\_scores }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{(}\FunctionTok{across}\NormalTok{(}\FunctionTok{starts\_with}\NormalTok{(}\StringTok{"q"}\NormalTok{), mean))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1 x 12
##      q1    q2    q3    q4    q5    q6    q7    q8
##   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>
## 1 0.626 0.619 0.498 0.627 0.595 0.633 0.621 0.628
## # i 4 more variables: q9 <dbl>, q10 <dbl>,
## #   q11 <dbl>, q12 <dbl>
\end{verbatim}

The percentage of correct responses varies from 49.8\% to 63.3.
What are the correlations between individual items and the total score?
Let's check:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{test\_scores }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{(}\FunctionTok{across}\NormalTok{(}\FunctionTok{starts\_with}\NormalTok{(}\StringTok{"q"}\NormalTok{), }\SpecialCharTok{\textasciitilde{}} \FunctionTok{cor}\NormalTok{(.x, total)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1 x 12
##      q1    q2    q3    q4    q5    q6    q7    q8
##   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>
## 1 0.275 0.293 0.298 0.295 0.290 0.275 0.288 0.301
## # i 4 more variables: q9 <dbl>, q10 <dbl>,
## #   q11 <dbl>, q12 <dbl>
\end{verbatim}

Note that simulating a new dataset will produce different results for these summaries because generating each dataset entails sampling a new set of items (with different difficulty and discrimination).

Writing a DGP function makes it possible to study a model through exploration, simply by examining datasets generated by wiggling the model parameters up or down.
For instance, we could look at what happens to the distribution of total scores if the items covered a much broader range of difficulties (higher \(\tau_\alpha\)) or are mis-calibrated to be too difficult (\(\mu_\tau > 0\)).
What if the items had varying numbers of response options, so the guessing parameters differ?
Are there item parameter values that will produce highly unrealistic distributions of total scores?
Exercises \ref{IRT-DGP-parameters}, \ref{IRT-DGP-checking}, and \ref{IRT-DGP-breaking} ask you to further explore these questions.

Overall, implementing the model as a data-generating function has allowed us to explore the model in a more active way than simply reading about it or listening to a lecture.
We have a more visceral feel about how the different parameters would create variation in our data---and it would be easy to tweak those parameters to see how things changed to learn even more.

\section{More to explore}\label{more-to-explore}

This chapter has introduced the core components of a data-generating process and demonstrated how to move from formulating a full data-generating model to implementing the model in R code.
Our main aim has been to provide enough scaffolding for you to get started with simulating from and exploring a variety of models.
The exercises below will give you further practice in developing, testing, and extending DGP functions.
Of course, we have not covered every consideration and challenge that arises when writing DGPs for simulations---there is \emph{much} more to explore!

We will cover some further challenges in subsequent chapters.
Yet more challenges will surely arise as you apply simulations in your own work.
Even though such challenges might not align with the examples or contexts that we have covered here, the concepts and principles that we have introduced should allow you to reason about potential solutions.
In doing so, we encourage you adopt the attitude of a chef in a test kitchen by trying out your ideas with code.
In the kitchen, there is often no way to know how a dish will turn out until you actually bake it and taste it.
Likewise, in developing a DGP, the best way to understand and evaluate a DGP is to write it out in code and explore the datasets that you can produce with it.

\section{Exercises}\label{exercises-3}

\subsection{\texorpdfstring{The Welch test on a shifted-and-scaled \(t\) distribution}{The Welch test on a shifted-and-scaled t distribution}}\label{Welch-t-dgp}

The shifted-and-scaled \(t\)-distribution has parameters \(\mu\) (mean), \(\sigma\) (scale), and \(\nu\) (degrees of freedom).
If \(T\) follows a student's \(t\)-distribution with \(\nu\) degrees of freedom, then \(S = \mu + \sigma T\) follows a shifted-and-scaled \(t\)-distribution.

The following function will generate random draws from this distribution.
We additionally scale by \((\nu-2)/\nu\) to achieve the target standard deviation (a \(t\)-distribution has a variance of \(\nu/(\nu-2)\)).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{r\_tss }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(n, mean, sd, df) \{}
\NormalTok{  mean }\SpecialCharTok{+}\NormalTok{ sd }\SpecialCharTok{*} \FunctionTok{sqrt}\NormalTok{( (df}\DecValTok{{-}2}\NormalTok{)}\SpecialCharTok{/}\NormalTok{df ) }\SpecialCharTok{*} \FunctionTok{rt}\NormalTok{(}\AttributeTok{n =}\NormalTok{ n, }\AttributeTok{df =}\NormalTok{ df)}
\NormalTok{\}}

\FunctionTok{r\_tss}\NormalTok{(}\AttributeTok{n =} \DecValTok{8}\NormalTok{, }\AttributeTok{mean =} \DecValTok{3}\NormalTok{, }\AttributeTok{sd =} \DecValTok{2}\NormalTok{, }\AttributeTok{df =} \DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  5.9087993  2.7234790  1.7515113 -0.1839347
## [5]  3.8703590  4.5137675  3.0244741 -0.2626592
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Modify the Welch simulation's \texttt{simulate\_data()} function to generate data from shifted-and-scaled \(t\)-distributions rather than from normal distributions. Include the degrees of freedom as an input argument.
  Simulate a dataset with low degrees of freedom and plot it to see if you see a few outliers.
\item
  Now generate more data and calculate the means and standard deviations to see if they are correctly calibrated (i.e., generate a big dataset to ensure you get reliable mean and standard deviation estimates). Check \texttt{df} equal to 500, 5, 3, and 2.
\item
  Once you are satisfied you have a correct DGP function, re-run the Type-I error rate calculations from the prior exercises in Section \ref{exAnovaExercises} using a \(t\)-distribution with 5 degrees of freedom.
  Do the results change substantially?
\end{enumerate}

\subsection{Plot the bivariate Poisson}\label{plot-the-bivariate-poisson}

In Section \ref{DGP-functions}, we provided an example of a DGP function for the bivariate Poisson model.
We demonstrated a plot of data simulated from this function in \ref{DGP-plotting}.
Create a similar plot but for a much larger sample size of \(N = 1000\).

With such a large dataset, it will likely be hard to distinguish individual observations because of over-plotting.
Create a better visual representation of the same simulated dataset, such as a heatmap or a contour plot.

\subsection{Check the bivariate Poisson function}\label{BVP-check}

Although we presented a DGP function for the bivariate Poisson model, we have not demonstrated how to check that the function is correct---we're leaving that to you!
Write some code to verify that the function \texttt{r\_bivariate\_Poisson()} is working properly.
Do this by generating a very large sample (say \(N = 10^4\) or \(10^5\)) and verifying the following:

\begin{itemize}
\tightlist
\item
  The sample means of \(C_1\) and \(C_2\) align with the specified population means.
\item
  The sample variances of \(C_1\) and \(C_2\) are close to the specified population means (because for a Poisson distribution \(\mathbb{E}(C_p) = \mathbb{V}(C_p)\) for \(p = 1,2\)).
\item
  The sample correlation aligns with the specified population correlation.
\item
  The observed counts \(C_1\) and \(C_2\) follow Poisson distributions.
\end{itemize}

\subsection{Add error-catching to the bivariate Poisson function}\label{BVP-error}

In Section \ref{DGP-examples}, we noted that the bivariate Poisson function as we described it can only produce a constrained range of correlations, which a maximum value that depends on the ratio of \(\mu_1\) to \(\mu_2\).
Our current implementation of the model does not handle this aspect of the model very well:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{r\_bivariate\_Poisson}\NormalTok{(}\DecValTok{5}\NormalTok{, }\AttributeTok{rho =} \FloatTok{0.6}\NormalTok{, }\AttributeTok{mu1 =} \DecValTok{4}\NormalTok{, }\AttributeTok{mu2 =} \DecValTok{12}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning in rpois(N, lambda = mu1 - EZ3): NAs
## produced
\end{verbatim}

\begin{verbatim}
##   C1 C2
## 1 NA  8
## 2 NA 13
## 3 NA 19
## 4 NA 17
## 5 NA  9
\end{verbatim}

For this combination of parameter values, \(\rho \times \sqrt{\mu_1 \mu_2}\) is larger than \(\mu_1\), which leads to simulated values for \(C_1\) that are all missing.
That makes it pretty hard to compute the correlation between \(C_1\) and \(C_2\).

Please help us fix this issue! Revise \texttt{r\_bivariate\_Poisson()} so that it checks for allowable values of \(\rho\). If the user specifies a combination of parameters that does not make sense, make the function throw an error (using R's \texttt{stop()} function).

\subsection{A bivariate negative binomial distribution}\label{BVNB1}

One potential limitation of the bivariate Poisson distribution described above is that the variances of the counts are necessarily equal to the means (i.e., unit dispersion).
This limitation is inherited from the univariate Poisson distributions that each variate follows.
Just as with the corresponding univariate distributions, one way to relax this limitation is to consider distributions with marginals that are negative binomial rather than Poisson, thereby allowing for overdispersion.
\citet{Cho2023bivariate} describes one type of bivariate negative binomial distribution and provides a method for constructing a bivariate negative binomial distribution by using latent, gamma-distributed components.
Their algorithm involves first generating components from gamma distributions with specified shape and scale parameters:
\[
\begin{aligned}
Z_0 &\sim \Gamma\left( \alpha_0, \beta\right) \\
Z_1 &\sim \Gamma\left( \alpha_1, \beta\right) \\
Z_2 &\sim \Gamma\left( \alpha_2, \beta\right)
\end{aligned}
\]
for \(\alpha_0,\alpha_1,\alpha_2 > 0\) and \(\beta > 0\).
They then simulate independent Poisson random variables as
\[
\begin{aligned}
C_1 &\sim Pois\left( Z_0 + Z_1 \right) \\
C_2 &\sim Pois\left( \delta(Z_0 + Z_2) \right).
\end{aligned}
\]
The resulting count variables follow marginal negative binomial distributions with moments
\[
\begin{aligned}
\mathbb{E}(C_1) &= (\alpha_0 + \alpha_1) \beta & \mathbb{V}(C_1) &= (\alpha_0 + \alpha_1) \beta (\beta + 1) \\
\mathbb{E}(C_2) &= (\alpha_0 + \alpha_2) \beta \delta & \mathbb{V}(C_2) &= (\alpha_0 + \alpha_2) \beta \delta (\beta \delta + 1) \\
& & \text{Cov}(C_1, C_2) &= \alpha_0 \beta^2 \delta.
\end{aligned}
\]
The correlation between \(C_1\) and \(C_2\) is thus
\[
\text{cor}(C_1, C_2) = \frac{\alpha_0}{\sqrt{(\alpha_0 + \alpha_1)(\alpha_0 + \alpha_2)}} \frac{\beta \sqrt{\delta}}{\sqrt{(\beta + 1)(\beta \delta + 1)}}.
\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Write a DGP function that implements this distribution.
\item
  Write some code to check that the function produces data where each variate follows a negative binomial distribution and where the correlation agrees with the formula given above.
\item
  Consider parameter values that produce \(\mathbb{E}(C_1) = \mathbb{E}(C_2) = 10\) and \(\mathbb{V}(C_1) = \mathbb{V}(C_2) = 15\). What are the minimum and maximum possible correlations between \(C_1\) and \(C_2\)?
\end{enumerate}

\subsection{Another bivariate negative binomial distribution}\label{BVNB2}

Another model for generating bivariate counts with negative binomial marginal distributions is by using Gaussian copulas. Here is a mathematical recipe for this distribution, which will produce counts with marginal means \(\mu_1\) and \(\mu_2\) and marginal variances \(\mu_1 + \mu_1^2 / p_1\) and \(\mu_2 + \mu_2^2 / p_2\). Start by generating variates from a bivariate standard normal distribution with correlation \(\rho\):
\[
\left(\begin{array}{c}Z_1 \\ Z_2 \end{array}\right) \sim N\left(\left[\begin{array}{c}0 \\ 0\end{array}\right], \ \left[\begin{array}{cc}1  & \rho \\ \rho & 1\end{array}\right]\right)
\]
Now find \(U_1 = \Phi(Z_1)\) and \(U_2 = \Phi(Z_2)\), where \(\Phi()\) is the standard normal cumulative distribution function (called \texttt{pnorm()} in R).
Then generate the counts by evaluating \(U_1\) and \(U_2\) with the negative binomial quantile function, \(F_{NB}^{-1}(x | \mu, p)\) with mean parameters \(\mu\) and size parameter \(p\) (this function is called \texttt{qnbinom()} in R):
\[
C_1 = F_{NB}^{-1}(U_1 | \mu_1, p_1) \qquad C_2 = F_{NB}^{-1}(U_2 |  \mu_2, p_2).
\]
The resulting counts will be correlated, but the correlation will not be equal to \(\rho\).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Write a DGP function that implements this distribution.
\item
  Write some code to check that the function produces data where each variate follows a negative binomial distribution
\item
  Use the function to create a graph showing the population correlation between the observed counts as a function of \(\rho\). Use \(\mu_1 = \mu_2 = 10\) and \(p_1 = p_2 = 20\). How does the range of correlations compare to the range from Exercise \ref{BVNB1}?
\end{enumerate}

\subsection{Plot the data from a cluster-randomized trial}\label{cluster-RCT-plot}

Run \texttt{gen\_cluster\_RCT()} with parameter values of your choice to produce data from a simulated cluster-randomized trial. Create a plot of the data that illustrates how student-level observations are nested within schools and how schools are assigned to different treatment conditions.

\subsection{Checking the Cluster RCT DGP}\label{cluster-RCT-checks}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  What is the variance of the outcomes generated by the model for the cluster-randomized trial if there are no treatment effects? (Try simulating data to check!) What other quick checks can you run on this DGP to make sure it is working correctly?
\item
  In \texttt{gen\_cluster\_RCT()} we have the following line of code to generate the number of individuals per site.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nj }\OtherTok{\textless{}{-}} \FunctionTok{sample}\NormalTok{( n\_min}\SpecialCharTok{:}\NormalTok{n\_max, J, }\AttributeTok{replace=}\ConstantTok{TRUE}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

  This code has an error. Generate a variety of datasets where you vary \texttt{n\_min}, \texttt{n\_max} and \texttt{J} to discover the error. Then repair the code.
  Checking your data generating process across a range of scenarios is extremely important.
\end{enumerate}

\subsection{More school-level variation}\label{cluster-RCT-heterogeneity}

The DGP for the cluster-randomized trial allows for school-level treatment impact variation, but only to the extent that the variation is explained by school size. How could you modify your simulation to allow for school-level treatment impact variation that is not related to school size? Implement this change and generate some data to show how it works.

\subsection{Cluster-randomized trial with baseline predictors}\label{cluster-RCT-baseline}

Extend the DGP for the cluster-randomized trial to include an individual-level covariate \(X\) that is correlated with the outcome.
Do this by modifying the model for student-level outcomes as
\[ 
Y_{ij} = \beta_{0j} + \beta_{1} X_{ij} + \epsilon_{ij}. 
\]
Keep the same \(\beta_1\) for all sites.
To implement this model as a DGP, you will have to decide how to generate \(X_{ij}\).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Use words and equations to explain your auxiliary model for \(X_{ij}\).
\item
  Implement the model by modifying \texttt{gen\_cluster\_RCT()} accordingly.
\item
  Use your implementation to find the unconditional variance of the outcome, \(\text{Var}(Y | Z_j = 0)\), when \(\beta_1 = 0.6\).
\end{enumerate}

\subsection{3-parameter IRT datasets}\label{IRT-DGP-parameters}

A challenge that arises with the IRT model described in Section \ref{three-parameter-IRT} is that the data are generated using \emph{random parameters} (the person ability parameters \(\theta_1,...,\theta_N\) and item characteristics \(\alpha_m, \beta_m, \gamma_m\) for \(m = 1,...,M\)) that we simulated from auxiliary models.
When applying IRT models to actual test data, the analyst's goal will usually involve estimating at least some of these parameters: either using the model for \emph{scoring} by estimating latent ability parameters or for \emph{calibration} by estimating the item characteristics.
But each time we generate a new dataset with \texttt{r\_3PL\_IRT()}, those latent parameters will change.

In order to understand how well an estimation procedure will perform on data generated by our function, we will need to keep track of the parameter values, even though those values will not be known in real data analysis contexts.
Suppose that our main interest is in understanding how well we can recover the item characteristics.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Modify \texttt{r\_3PL\_IRT()} to return a list containing two datasets. The first entry in the list should be a dataset with the person-by-item responses, just as in the original function. The second entry in the list should be an \(M \times 3\) dataset containing the item parameters.
\item
  An alternative strategy for implementing the three-parameter IRT DGP is to write two functions instead of one: a function to simulate a set of \(M\) item parameters and a function to simulate the person-by-item responses. Complete the following function skeletons to implement this strategy. Demonstrate how to use the functions to simulate a dataset.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{r\_3PL\_items }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(}
\NormalTok{  M, }
  \AttributeTok{diff\_M =} \DecValTok{0}\NormalTok{, }\AttributeTok{diff\_SD =} \DecValTok{1}\NormalTok{,}
  \AttributeTok{disc\_M =} \DecValTok{1}\NormalTok{, }\AttributeTok{disc\_SD =} \FloatTok{0.2}\NormalTok{,}
  \AttributeTok{item\_options =} \DecValTok{4}
\NormalTok{) \{}
  \CommentTok{\# Code here}
  \FunctionTok{return}\NormalTok{(item\_parameter\_data)}
\NormalTok{\}}

\NormalTok{r\_3PL\_data }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{( N, item\_data ) \{}
  \CommentTok{\# Generate item responses}
  \FunctionTok{return}\NormalTok{(response\_data)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}
\item
  Describe the benefits and limitations of the two approaches you have implemented.
\item
  For your preferred strategy, modify your function(s) to also return the latent person ability parameters.
\end{enumerate}

\subsection{Check the 3-parameter IRT DGP}\label{IRT-DGP-checking}

For one of the DGPs you wrote in Exercise \ref{IRT-DGP-parameters}, write code to check that the function is working properly. What features of the model will you check?

\subsection{Explore the 3-parameter IRT model}\label{IRT-DGP-breaking}

Use the modified DGP function(s) you wrote for Exercise \ref{IRT-DGP-parameters} to explore the model. Simulate data and create visualizations to answer the following questions.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  What happens to the distribution of total scores if the items covered a very broader range of difficulties (higher \(\tau_\alpha\))?
\item
  What happens if the items are mis-calibrated to be too difficult (\(\mu_\tau > 0\))?
\item
  What if the items had varying numbers of response options, so the guessing parameters differ?
\item
  Find at least one combination of parameter values that will produce very extreme or unrealistic distributions of total scores.
\end{enumerate}

\subsection{Random effects meta-regression}\label{meta-regression-DGP}

Meta-analysis involves working with quantitative findings reported by previous studies on a particular topic, in the form of effect size estimates and their standard errors, to generate integrative summaries and identify patterns in the findings that might not be evident from any previous study considered in isolation.
One model that is widely used in meta-analysis is the random effects meta-regression, which relates the effect sizes to known characteristics of the studies that are encoded in the form of predictors.
Consider a collection of \(K\) studies. Let \(T_i\) denote an effect size estimate and \(s_i\) denote its standard error for study \(i\).
Let \(x_{i}\) be a quantitative predictor variable that represents some characteristic of study \(i\) (such as its year of publication).
The random effects meta-regression model assumes
\[
T_i = \beta_0 + \beta_1 x_i + u_i + e_i,
\]
where \(u_i \sim N(0, \tau^2)\) and \(e_i \sim N(0, s_i^2)\).
In this model, \(\beta_0\) corresponds to the expected effect size when \(x_i = 0\) and \(\beta_1\) describes the expected difference in effect size per one unit difference in \(x_i\).
The first error \(u_i\) corresponds to the heterogeneity in the effect sizes above and beyond the variation explained by \(x_i\), and the second error \(e_i\) corresponds to the sampling error, which we assume has known variance \(s_i^2\).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Of the variables in the random effects meta-regression model, which are structural features, which are covariates, and which are outcomes? Of the parameters described above, which would you classify as focal, which as auxiliary, and which as design parameters?
\item
  In addition to the focal model described above, what further assumptions or auxiliary models will be needed in order to simulate data for a random effects meta-regression? Propose an auxiliary model for any needed quantities.
\item
  Using your proposed auxiliary model, write a function to generate data for a random effects meta-regression. Demonstrate your function by creating a plot based on a simulated dataset with \(K = 30\) effect sizes.
\item
  Write code to check the properties of your data-generating function.
\end{enumerate}

\subsection{Meta-regression with selective reporting}\label{Vevea-Hedges-DGP}

\citet{vevea1995general} proposed a meta-regression model that allows for the possibility that not all primary study results are published.
They assume that primary study results are generated according to the random effects meta-regression model described in Exercise \ref{meta-regression-DGP}, but then only a subset of results are observed, where the probability of being included is a function of the result's one-sided \(p\)-value for the null hypothesis \(H_0: \delta \leq 0\) against alternative \(H_A: \delta > 0\).
Let \(p_i = 1 - \Phi(T_i / s_i)\) be the one-sided \(p\)-value for study result \(i\), where \(Phi()\) is the standard normal cumulative distribution (\texttt{pnorm()} in R).
In one version of the selection model, the selection probability follows a piece-wise constant distribution with
\[
\text{Pr}(T_i \text{ is observed}) = \begin{cases} 
1 & \text{if} \quad 0 \leq p_i < .025 \\ 
\lambda_1 & \text{if} \quad .025 \leq p_i < .500 \\  
\lambda_2 & \text{if} \quad .500 \leq  p_i < 1
\end{cases}
\]
for selection probabilities \(0 \leq \lambda_1 \leq 1\) and \(0 \leq \lambda_2 \leq 1\).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Modify your data-generating function from Exercise \ref{meta-regression-DGP} to follow the Vevea-Hedges selection model. Ensure that the new data-generating function returns a total of \(K\) primary study results.
\item
  Use the new function to generate a large number of effect size estimates, all with \(s_i = 0.35\), using parameters \(\beta_0 = 0.1\), \(\beta_1 = 0.0\), \(\tau = 0.1\), \(\lambda_1 = 0.5\), and \(\lambda_2 = 0.2\). Plot the distribution of observed effect size estimates.
\item
  Create several further plots using different values for \(\lambda_1\) and \(\lambda_2\). How do these parameters affect the shape of the distribution?
\item
  Use the \texttt{selmodel()} function from the \texttt{metafor} package to estimate the Vevea-Hedges selection model based on one of your simulated datasets. (See Exercise \ref{Vevea-Hedges-estimation} for example syntax.) How do the estimates compare to the model parameters you've specified?
\end{enumerate}

\chapter{Data analysis procedures}\label{data-analysis-procedures}

The overall aims of many simulation studies have to do with understand how a particular data-analysis procedure works or comparing the performance of multiple, competing procedures.
Thus, the data-analysis procedure or procedures are the central object of study.
Depending on the research question, the data-analysis procedure might be very simple---as simple as just computing a sample correlation--or it might involve a combination of several components.
For example, the procedure might entail first computing a diagnostic test for heteroskedasticity and then, depending on the outcome of the test, applying either a conventional formula or a heteroskedasticity-robust formula for standard errors.
As another example, a data-analysis procedure might involve using multiple imputation for missingness on key variables, then fitting a statistical model, and then generating predicted values based on the model.
Also depending on the research question, we might need to create \emph{several} functions that implement different estimation procedures to be compared.

In this chapter, we demonstrate how to implement data-analysis procedures in the form of R functions, which we call \emph{estimation functions}, so that their performance can eventually be evaluated by repeatedly applying them to artificial data.
We start by describing the high-level design of an estimation function, and illustrate with some simple examples.
We then discuss approaches for writing simulations that compare multiple data analysis procedures.
Next, we describe strategies for validating the coded-up estimation functions before running a full simulation.
Finally, we examine methods for handling common computational problems with estimation functions, such as handling non-convergence when using maximum likelihood estimation.

\section{Writing estimation functions}\label{estimation-functions}

In the abstract, a function that implements an estimation procedure should have the following form:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{estimate }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(data) \{}

  \CommentTok{\# calculations/model{-}fitting/estimation procedures}
  
  \FunctionTok{return}\NormalTok{(estimates)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

The function takes a dataset as input, fits a model or otherwise calculates an estimate, possibly with associated standard errors and so forth, and returns these quantities as output.
The estimates could be point estimates of parameters, standard errors, confidence intervals, p-values, predictions, or other quantities.
The calculations in the body of the function should be set up to use datasets that have the same structure (i.e., same dimensions, same variable names) as the output of the corresponding function for generating simulated data.
However, in principle, we should also be able to run the estimation function on real data as well.

In Chapter \ref{case-ANOVA} we wrote a function called \texttt{ANOVA\_Welch\_F()} for computing \(p\)-values from two different procedures for testing equality of means in a heteroskedastic ANOVA:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ANOVA\_Welch\_F }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(data) \{}
\NormalTok{  anova\_F }\OtherTok{\textless{}{-}} \FunctionTok{oneway.test}\NormalTok{(x }\SpecialCharTok{\textasciitilde{}}\NormalTok{ group, }\AttributeTok{data =}\NormalTok{ data, }\AttributeTok{var.equal =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{  Welch\_F }\OtherTok{\textless{}{-}} \FunctionTok{oneway.test}\NormalTok{(x }\SpecialCharTok{\textasciitilde{}}\NormalTok{ group, }\AttributeTok{data =}\NormalTok{ data, }\AttributeTok{var.equal =} \ConstantTok{FALSE}\NormalTok{)}
  
\NormalTok{  result }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}
    \AttributeTok{ANOVA =}\NormalTok{ anova\_F}\SpecialCharTok{$}\NormalTok{p.value,}
    \AttributeTok{Welch =}\NormalTok{ Welch\_F}\SpecialCharTok{$}\NormalTok{p.value}
\NormalTok{  )}
  
  \FunctionTok{return}\NormalTok{(result)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Apply this function to a simulated dataset returns two p-values, one for the usual ANOVA \(F\) test (which assumes homoskedasticity) and one for Welch's heteroskedastic \(F\) test:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sim\_data }\OtherTok{\textless{}{-}} \FunctionTok{generate\_ANOVA\_data}\NormalTok{(}
  \AttributeTok{mu =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{6}\NormalTok{), }
  \AttributeTok{sigma\_sq =} \FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{1}\NormalTok{),}
  \AttributeTok{sample\_size =} \FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{4}\NormalTok{)}
\NormalTok{)}
\FunctionTok{ANOVA\_Welch\_F}\NormalTok{(sim\_data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1 x 2
##      ANOVA  Welch
##      <dbl>  <dbl>
## 1 0.000293 0.0179
\end{verbatim}

Our \texttt{ANOVA\_Welch\_F()} function is designed to work with the output of \texttt{generate\_ANOVA\_data()} in that it assumes that the grouping variable is called \texttt{group} and the outcome is called \texttt{x}.
Relying on this assumption would be a poor choice if we were designing a function as part of an R package or for general-purpose use.
However, because the primary use of the function is for simulation, it is reasonable to assume that the input data will always have appropriate variable names.

In Chapter \ref{data-generating-processes}, we looked at a data-generating function for a bivariate Poisson distribution, an example of a non-normal bivariate distribution.
We might use such a distribution to understand the behavior of Pearson's sample correlation coefficient and its normalizing transformation, known as Fisher's \(z\)-transformation, which is equivalent to the hyperbolic arc-tangent function (\texttt{atanh()} in R).
When the sample measurements follow a bivariate normal distribution, Fisher's \(z\)-transformed correlation is very close to normally distributed and its standard error is simply \(1 / \sqrt{N - 3}\), and thus independent of the correlation.
This makes \(z\)-transformation very useful for computing confidence intervals, which can then be back-transformed to the Pearson-\(r\) scale.

In this problem, a simple estimation function would take a dataset with two variables as input and compute the sample correlation and its \(z\)-transformation, compute confidence intervals for \(z\), and then back-transform the confidence interval end-points.
Here is an implementation of these calculations:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{r\_and\_z }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(data) \{}
  
\NormalTok{  r }\OtherTok{\textless{}{-}} \FunctionTok{cor}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{C1, data}\SpecialCharTok{$}\NormalTok{C2)}
\NormalTok{  z }\OtherTok{\textless{}{-}} \FunctionTok{atanh}\NormalTok{(r)}
\NormalTok{  se\_z }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{/} \FunctionTok{sqrt}\NormalTok{(}\FunctionTok{nrow}\NormalTok{(data) }\SpecialCharTok{{-}} \DecValTok{3}\NormalTok{)}
\NormalTok{  ci\_z }\OtherTok{\textless{}{-}}\NormalTok{ z }\SpecialCharTok{+} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{) }\SpecialCharTok{*} \FunctionTok{qnorm}\NormalTok{(.}\DecValTok{975}\NormalTok{) }\SpecialCharTok{*}\NormalTok{ se\_z}
\NormalTok{  ci\_r }\OtherTok{\textless{}{-}} \FunctionTok{tanh}\NormalTok{(ci\_z)}
  
  \FunctionTok{data.frame}\NormalTok{( }\AttributeTok{r =}\NormalTok{ r, }\AttributeTok{z =}\NormalTok{ z, }\AttributeTok{CI\_lo =}\NormalTok{ ci\_r[}\DecValTok{1}\NormalTok{], }\AttributeTok{CI\_hi =}\NormalTok{ ci\_r[}\DecValTok{2}\NormalTok{] )}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

To check that the function returns a result of the expected form, we generate a small dataset using the \texttt{r\_bivariate\_Poisson()} function developed in the last chapter, then apply our estimation function to the result:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Pois\_dat }\OtherTok{\textless{}{-}} \FunctionTok{r\_bivariate\_Poisson}\NormalTok{(}\DecValTok{40}\NormalTok{, }\AttributeTok{rho =} \FloatTok{0.5}\NormalTok{, }\AttributeTok{mu1 =} \DecValTok{4}\NormalTok{, }\AttributeTok{mu2 =} \DecValTok{4}\NormalTok{)}
\FunctionTok{r\_and\_z}\NormalTok{(Pois\_dat)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           r         z     CI_lo     CI_hi
## 1 0.5565357 0.6278003 0.2964146 0.7397905
\end{verbatim}

Although it is a little cumbersome to do so, we could also apply the estimation function to a real dataset.
Here is an example, which calculates the correlation between ratings of judicial integrity and familiarity with the law from the \texttt{USJudgeRatings} dataset (which is included in base R).
For the function to work on this dataset, we first need to rename the relevant variables.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(USJudgeRatings)}

\NormalTok{USJudgeRatings }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(}\AttributeTok{C1 =}\NormalTok{ INTG, }\AttributeTok{C2 =}\NormalTok{ FAMI) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{r\_and\_z}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##          r        z     CI_lo     CI_hi
## 1 0.868858 1.328401 0.7692563 0.9272343
\end{verbatim}

The function returns a valid result---a quite strong correlation!

It is a good practice to test out a newly-developed estimation function on real data as a check that it is working as intended.
This type of test ensures that the estimation function is not using information outside of the dataset, such as by using known parameter values to construct an estimate.
Applying the function to a real dataset demonstrates that the function implements a procedure that could actually be applied in real data analysis contexts.

\section{Including Multiple Data Analysis Procedures}\label{multiple-estimation-procedures}

Many simulations involve head-to-head comparisons between more than one data-analysis procedure.
As a design principle, we generally recommend writing different functions for each estimation method one is planning on evaluating.
Doing so makes it easier to add in additional methods as desired or to focus on just a subset of methods.
Writing separate function also leads to a code base that is flexible and useful for other purposes (such as analyzing real data).
Finally (repeating one of our favorite mantras), separating functions makes debugging easier because it lets you focus attention on one thing at a time, without worrying about how errors in one area might propagate to others.

To see how this works in practice, we will return to the case study from Section \ref{case-cluster}, where we developed a data-generating function for simulating a cluster-randomized trial with student-level outcomes but school-level treatment assignment.
Our data-generating process allowed for varying school sizes and heterogeneous treatment effects, which might be correlated with school size.
Several different procedures might be used to estimate an overall average effect from a clustered experiment, including:

\begin{itemize}
\tightlist
\item
  Estimating a multi-level regression model (also known as a hierarchical linear model),
\item
  Estimating an ordinary least squares (OLS) regression model and applying cluster-robust standard errors, or
\item
  Averaging the outcomes by school, then estimating a linear regression model on the mean outcomes.
\end{itemize}

All three of these methods are widely used and have some theoretical guarantees supporting their use.
Education researchers tend to be more comfortable using multi-level regression models, whereas economists tend to use OLS with clustered standard errors.

We next develop estimation functions for each of these procedures.
We analyze as we expect would be done in practice; even though we generated data with a school size covariate, we do not include it in our estimation functions.
Each function needs to produce a point estimate, standard error, and \(p\)-value for the average treatment effect.
To have data to practice on, we generate a sample dataset using \href{/case_study_code/gen_cluster_RCT_rev.R}{a revised version of \texttt{gen\_cluster\_RCT()}}, which corrects the bug discussed in Exercise \ref{cluster-RCT-checks}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dat }\OtherTok{\textless{}{-}} \FunctionTok{gen\_cluster\_RCT}\NormalTok{( }
  \AttributeTok{J=}\DecValTok{16}\NormalTok{, }\AttributeTok{n\_bar =} \DecValTok{30}\NormalTok{, }\AttributeTok{alpha =} \FloatTok{0.8}\NormalTok{, }\AttributeTok{p =} \FloatTok{0.5}\NormalTok{, }
  \AttributeTok{gamma\_0 =} \DecValTok{0}\NormalTok{, }\AttributeTok{gamma\_1 =} \FloatTok{0.2}\NormalTok{, }\AttributeTok{gamma\_2 =} \FloatTok{0.2}\NormalTok{,}
  \AttributeTok{sigma2\_u =} \FloatTok{0.4}\NormalTok{, }\AttributeTok{sigma2\_e =} \FloatTok{0.6}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

For the multi-level modeling strategy, there are several different existing packages that we could use.
We will implement an estimator using the popular \texttt{lme4} package, along with the \texttt{lmerTest} function for computing a \(p\)-value for the average effect.
Here is a basic implementation:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{analysis\_MLM }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{( dat ) \{}
  
\NormalTok{  M1 }\OtherTok{\textless{}{-}}\NormalTok{ lme4}\SpecialCharTok{::}\FunctionTok{lmer}\NormalTok{( Yobs }\SpecialCharTok{\textasciitilde{}} \DecValTok{1} \SpecialCharTok{+}\NormalTok{ Z }\SpecialCharTok{+}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{|}\NormalTok{ sid), }\AttributeTok{data =}\NormalTok{ dat )}
\NormalTok{  M1\_test }\OtherTok{\textless{}{-}}\NormalTok{ lmerTest}\SpecialCharTok{::}\FunctionTok{as\_lmerModLmerTest}\NormalTok{(M1)}
\NormalTok{  M1\_summary }\OtherTok{\textless{}{-}} \FunctionTok{summary}\NormalTok{(M1\_test)}\SpecialCharTok{$}\NormalTok{coefficients}
  
  \FunctionTok{tibble}\NormalTok{( }
    \AttributeTok{ATE\_hat =}\NormalTok{ M1\_summary[}\StringTok{"Z"}\NormalTok{,}\StringTok{"Estimate"}\NormalTok{], }
    \AttributeTok{SE\_hat =}\NormalTok{ M1\_summary[}\StringTok{"Z"}\NormalTok{,}\StringTok{"Std. Error"}\NormalTok{], }
    \AttributeTok{p\_value =}\NormalTok{ M1\_summary[}\StringTok{"Z"}\NormalTok{, }\StringTok{"Pr(\textgreater{}|t|)"}\NormalTok{] }
\NormalTok{  )}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

The function fits a multi-level model with a fixed coefficient for the treatment indicator and random intercepts for each school.
To get a p-value for the treatment coefficient, we have to convert the model into an \texttt{lmerModLmerTest} object and then pass it through \texttt{summary()}.
The function outputs only the statistics in which we are interested.

Our function makes use of the \texttt{lme4} and \texttt{lmerTest} packages.
Rather than assuming that these packages will be loaded, we call relevant functions using the package name as a prefix, as in \texttt{lme4::lmer()}.
This way, we can run the function even if we have not loaded the packages in the global environment.
This approach is also preferable to loading packages inside the function itself (e.g., with \texttt{require(lme4)}) because calling the function does not change which packages are loaded in the global environment.

Here is a function implementing OLS regression with cluster-robust standard errors:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{analysis\_OLS }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{( dat, }\AttributeTok{se\_type =} \StringTok{"CR2"}\NormalTok{ ) \{}
  
\NormalTok{  M2 }\OtherTok{\textless{}{-}}\NormalTok{ estimatr}\SpecialCharTok{::}\FunctionTok{lm\_robust}\NormalTok{( }
\NormalTok{    Yobs }\SpecialCharTok{\textasciitilde{}} \DecValTok{1} \SpecialCharTok{+}\NormalTok{ Z, }\AttributeTok{data =}\NormalTok{ dat, }
    \AttributeTok{clusters =}\NormalTok{ sid,  }\AttributeTok{se\_type =}\NormalTok{ se\_type}
\NormalTok{  )}
  
  \FunctionTok{tibble}\NormalTok{( }
    \AttributeTok{ATE\_hat =}\NormalTok{ M2}\SpecialCharTok{$}\NormalTok{coefficients[[}\StringTok{"Z"}\NormalTok{]], }
    \AttributeTok{SE\_hat =}\NormalTok{ M2}\SpecialCharTok{$}\NormalTok{std.error[[}\StringTok{"Z"}\NormalTok{]], }
    \AttributeTok{p\_value =}\NormalTok{ M2}\SpecialCharTok{$}\NormalTok{p.value[[}\StringTok{"Z"}\NormalTok{]] }
\NormalTok{  )}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

To get cluster-robust standard errors, we use the \texttt{lm\_robust()} function from the \texttt{estimatr()} package, again calling only the relevant function using the package prefix rather than loading the whole package.
A novel aspect of this estimation function is that it includes an additional intput argument, \texttt{se\_type}, which allows us to control the type of standard error calculated by \texttt{lm\_robust()}.
Adding this option would let us use the same function to compute (and compare) different types of clustered standard errors for the average treatment effect estimate.
We set a default option of \texttt{"CR2"}, just like the default of \texttt{lm\_robust()}.

Sometimes an analytic procedure involves multiple steps.
For example, aggregation estimator first involves collapsing the data to a school-level dataset, and then analyzing at the school level.
This is fine: we just wrap all the steps in a single estimation function: from the point of view of \emph{using} the function, it is a single call, no matter how complicated the process inside.
Here is the code for the aggregate-then-analyze approach:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{analysis\_agg }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{( dat, }\AttributeTok{se\_type =} \StringTok{"HC2"}\NormalTok{ ) \{}
  
\NormalTok{  datagg }\OtherTok{\textless{}{-}}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{summarise}\NormalTok{( }
\NormalTok{    dat,}
    \AttributeTok{Ybar =} \FunctionTok{mean}\NormalTok{( Yobs ),}
    \AttributeTok{n =} \FunctionTok{n}\NormalTok{(),}
    \AttributeTok{.by =} \FunctionTok{c}\NormalTok{(sid, Z)}
\NormalTok{  )}
  
  \FunctionTok{stopifnot}\NormalTok{( }\FunctionTok{nrow}\NormalTok{( datagg ) }\SpecialCharTok{==} \FunctionTok{length}\NormalTok{(}\FunctionTok{unique}\NormalTok{(dat}\SpecialCharTok{$}\NormalTok{sid) ) )}
  
\NormalTok{  M3 }\OtherTok{\textless{}{-}}\NormalTok{ estimatr}\SpecialCharTok{::}\FunctionTok{lm\_robust}\NormalTok{( }
\NormalTok{    Ybar }\SpecialCharTok{\textasciitilde{}} \DecValTok{1} \SpecialCharTok{+}\NormalTok{ Z, }\AttributeTok{data =}\NormalTok{ datagg, }
    \AttributeTok{se\_type =}\NormalTok{ se\_type }
\NormalTok{  )}
  
  \FunctionTok{tibble}\NormalTok{( }
    \AttributeTok{ATE\_hat =}\NormalTok{ M3}\SpecialCharTok{$}\NormalTok{coefficients[[}\StringTok{"Z"}\NormalTok{]], }
    \AttributeTok{SE\_hat =}\NormalTok{ M3}\SpecialCharTok{$}\NormalTok{std.error[[}\StringTok{"Z"}\NormalTok{]], }
    \AttributeTok{p\_value =}\NormalTok{ M3}\SpecialCharTok{$}\NormalTok{p.value[[}\StringTok{"Z"}\NormalTok{]] }
\NormalTok{  )}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Note the \texttt{stopifnot} command: it will throw an error if the condition is not true.
This \texttt{stopifnot} ensures we do not have both treatment and control students within a single school--if we did, we would have more aggregated values than school ids due to the grouping!
Putting \emph{assert statements} in your code like this is a good way to guarantee you are not introducing weird and hard-to-track errors.
A \texttt{stopifnot} statement halts your code as soon as something goes wrong, rather than letting that initial wrongness flow on to further work, creating odd results that are hard to understand.
Here we are protecting ourselves from strange results if, for example, we messed up our DGP code to have treatment not nested within school, or we were using data that did not actually come from a cluster randomized experiment.
See Section \ref{about-stopifnot} for more.

All of our functions produce output in the same format:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{analysis\_MLM}\NormalTok{( dat )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1 x 3
##   ATE_hat SE_hat p_value
##     <dbl>  <dbl>   <dbl>
## 1  -0.111  0.323   0.737
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{analysis\_OLS}\NormalTok{( dat )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1 x 3
##   ATE_hat SE_hat p_value
##     <dbl>  <dbl>   <dbl>
## 1  -0.177  0.307   0.576
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{analysis\_agg}\NormalTok{( dat )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1 x 3
##   ATE_hat SE_hat p_value
##     <dbl>  <dbl>   <dbl>
## 1 -0.0818  0.339   0.813
\end{verbatim}

Ensuring that the output of all the functions is structured in the same way will make it easy to keep the results organized once we start running multiple iterations of the simulation.
If each estimation method returns a dataset with the same variables, we can simply stack the results on top of each other.
Here is a function that bundles all the estimation procedures together:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{estimate\_Tx\_Fx }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(}
\NormalTok{    data, }
    \AttributeTok{CR\_se\_type =} \StringTok{"CR2"}\NormalTok{, }\AttributeTok{agg\_se\_type =} \StringTok{"HC2"}
\NormalTok{) \{}
  
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{bind\_rows}\NormalTok{(}
    \AttributeTok{MLM =} \FunctionTok{analysis\_MLM}\NormalTok{( data ),}
    \AttributeTok{OLS =} \FunctionTok{analysis\_OLS}\NormalTok{( data, }\AttributeTok{se\_type =}\NormalTok{ CR\_se\_type),}
    \AttributeTok{agg =} \FunctionTok{analysis\_agg}\NormalTok{( data, }\AttributeTok{se\_type =}\NormalTok{ agg\_se\_type),}
    \AttributeTok{.id =} \StringTok{"estimator"}
\NormalTok{  )}
  
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{estimate\_Tx\_Fx}\NormalTok{(dat)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 3 x 4
##   estimator ATE_hat SE_hat p_value
##   <chr>       <dbl>  <dbl>   <dbl>
## 1 MLM       -0.111   0.323   0.737
## 2 OLS       -0.177   0.307   0.576
## 3 agg       -0.0818  0.339   0.813
\end{verbatim}

This is a common coding pattern for simulations that involve multiple estimation procedures.
Each procedure is expressed in its own function, then these are assembled together in a single function so that they can all easily be applied to the same dataset.
Stacking the results row-wise will make it easy to compute performance measures for all methods at once.
The benefit of stacking will become even more evident once we are working across multiple replications of the simulation process, as we will in Chapter \ref{running-the-simulation-process}.

\section{Validating an Estimation Function}\label{validating-an-estimation-function}

Just as with data-generating functions, it is critical to verify the accuracy of an implemented estimation function.
If an estimation function involves a known procedure that has been implemented in R or one of its contributed packages, then a straightforward way to do this is to compare your implementation to another existing implementation.
For estimation functions that involve multi-step procedures or novel methods, other approaches to verification may be needed, which rely more on statistical theory.

\subsection{Checking against existing implementations}\label{checking-against-existing-implementations}

For our Welch test function, we can check the output of \texttt{ANOVA\_Welch\_F()} against the built-in \texttt{oneway.test} function. Let's do that with a fresh set of data:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sim\_data }\OtherTok{\textless{}{-}} \FunctionTok{generate\_ANOVA\_data}\NormalTok{(}
  \AttributeTok{mu =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{6}\NormalTok{), }
  \AttributeTok{sigma\_sq =} \FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{1}\NormalTok{),}
  \AttributeTok{sample\_size =} \FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{4}\NormalTok{)}
\NormalTok{)}

\NormalTok{aov\_results }\OtherTok{\textless{}{-}} \FunctionTok{oneway.test}\NormalTok{(x }\SpecialCharTok{\textasciitilde{}} \FunctionTok{factor}\NormalTok{(group), }\AttributeTok{data =}\NormalTok{ sim\_data, }
                           \AttributeTok{var.equal =} \ConstantTok{FALSE}\NormalTok{)}
\NormalTok{aov\_results}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  One-way analysis of means (not assuming
##  equal variances)
## 
## data:  x and factor(group)
## F = 28.367, num df = 3.0000, denom df =
## 3.3253, p-value = 0.007427
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Welch\_results }\OtherTok{\textless{}{-}} \FunctionTok{ANOVA\_Welch\_F}\NormalTok{(sim\_data)}
\FunctionTok{all.equal}\NormalTok{(aov\_results}\SpecialCharTok{$}\NormalTok{p.value, Welch\_results}\SpecialCharTok{$}\NormalTok{Welch)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] TRUE
\end{verbatim}

We use \texttt{all.equal()} because it will check equality up to a tolerance in R, which can avoid some perplexing errors due to rounding.

For the bivariate correlation example, we can check the output of \texttt{r\_and\_z()} against R's built-in \texttt{cor.test()} function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Pois\_dat }\OtherTok{\textless{}{-}} \FunctionTok{r\_bivariate\_Poisson}\NormalTok{(}\DecValTok{15}\NormalTok{, }\AttributeTok{rho =} \FloatTok{0.6}\NormalTok{, }\AttributeTok{mu1 =} \DecValTok{14}\NormalTok{, }\AttributeTok{mu2 =} \DecValTok{8}\NormalTok{)}

\NormalTok{my\_result }\OtherTok{\textless{}{-}} \FunctionTok{r\_and\_z}\NormalTok{(Pois\_dat) }\SpecialCharTok{|\textgreater{}} \FunctionTok{subset}\NormalTok{(}\AttributeTok{select =} \SpecialCharTok{{-}}\NormalTok{z)}
\NormalTok{my\_result}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           r       CI_lo     CI_hi
## 1 0.5029796 -0.01250535 0.8072486
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{R\_result }\OtherTok{\textless{}{-}} \FunctionTok{cor.test}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{ C1 }\SpecialCharTok{+}\NormalTok{ C2, }\AttributeTok{data =}\NormalTok{ Pois\_dat)}
\NormalTok{R\_result }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}\AttributeTok{r =}\NormalTok{ R\_result}\SpecialCharTok{$}\NormalTok{estimate[[}\StringTok{"cor"}\NormalTok{]], }
                   \AttributeTok{CI\_lo =}\NormalTok{ R\_result}\SpecialCharTok{$}\NormalTok{conf.int[}\DecValTok{1}\NormalTok{], }
                   \AttributeTok{CI\_hi =}\NormalTok{ R\_result}\SpecialCharTok{$}\NormalTok{conf.int[}\DecValTok{2}\NormalTok{])}
\NormalTok{R\_result}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1 x 3
##       r   CI_lo CI_hi
##   <dbl>   <dbl> <dbl>
## 1 0.503 -0.0125 0.807
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{all.equal}\NormalTok{(R\_result, my\_result)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Attributes: < Component \"class\": Lengths (3, 1) differ (string compare on first 1) >"
## [2] "Attributes: < Component \"class\": 1 string mismatch >"
\end{verbatim}

This type of test is even more useful here because \texttt{r\_and\_z()} uses our own implementation of the confidence interval calculations, rather than relying on R's built-in functions as we did with \texttt{ANOVA\_Welch\_F()}.

\subsection{Checking novel procedures}\label{checking-novel-procedures}

Simulations are usually an integral part of projects to develop novel statistical methods.
Checking estimation functions in such projects presents a challenge: if an estimation procedure truly is new, how do you check that your code is correct?
Effective methods for doing so will vary from problem to problem, but an over-arching strategy is to use theoretical results about the performance of the estimator to check that your implementation works as expected.
For instance, we might work out the algebraic properties of an estimator for a special case and then check that the result of the estimation function agrees with our algebra.
For some estimation problems, we might be able to identify theoretical properties of an estimator when applied to a very large sample of data and when the model is correctly specified.
If we can find results about large-sample behavior, then we can test an estimation function by applying it to a very large sample and checking whether the resulting estimates are very close to specified parameter values.
We illustrate each of these approaches using our functions for estimating treatment effects from cluster-randomized trials.

We start by testing an algebraic property.
With each of the three methods we have implemented, the treatment effect estimator is a difference between the weighted average of the outcomes from students in each treatment condition;
the only difference between the estimators is in what weights are used.
In the special case where all schools have the same number of students, the weights used by all three methods end up being the same: all three methods allocate equal weight to each school.
Therefore, we know that there should be no difference between the three point estimates.
Furthermore, a bit of algebra will show that the cluster-robust standard error from the OLS approach will end up being identical to the robust standard error from the aggregation approach.
If there are also equal numbers of schools assigned to both conditions, then the standard error from the multilevel model will also be identical to the other standard errors.

Let's verify that our estimation functions produce results that are consistent with these theoretical properties.
To do so, we will need to generate a dataset with equal cluster sizes, setting \(\alpha = 0\):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dat }\OtherTok{\textless{}{-}} \FunctionTok{gen\_cluster\_RCT}\NormalTok{( }
  \AttributeTok{J=}\DecValTok{12}\NormalTok{, }\AttributeTok{n\_bar =} \DecValTok{30}\NormalTok{, }\AttributeTok{alpha =} \DecValTok{0}\NormalTok{, }\AttributeTok{p =} \FloatTok{0.5}\NormalTok{, }
  \AttributeTok{gamma\_0 =} \DecValTok{0}\NormalTok{, }\AttributeTok{gamma\_1 =} \FloatTok{0.2}\NormalTok{, }\AttributeTok{gamma\_2 =} \FloatTok{0.2}\NormalTok{,}
  \AttributeTok{sigma2\_u =} \FloatTok{0.4}\NormalTok{, }\AttributeTok{sigma2\_e =} \FloatTok{0.6}
\NormalTok{)}
\FunctionTok{table}\NormalTok{(dat}\SpecialCharTok{$}\NormalTok{sid) }\CommentTok{\# verify equal{-}sized clusters}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  1  2  3  4  5  6  7  8  9 10 11 12 
## 30 30 30 30 30 30 30 30 30 30 30 30
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{estimate\_Tx\_Fx}\NormalTok{(dat)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 3 x 4
##   estimator ATE_hat SE_hat p_value
##   <chr>       <dbl>  <dbl>   <dbl>
## 1 MLM        -0.525  0.366   0.183
## 2 OLS        -0.525  0.366   0.183
## 3 agg        -0.525  0.366   0.183
\end{verbatim}

All three methods yield identical results.
Now let's try equal school sizes but unequal allocation to treatment:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dat }\OtherTok{\textless{}{-}} \FunctionTok{gen\_cluster\_RCT}\NormalTok{( }
  \AttributeTok{J=}\DecValTok{12}\NormalTok{, }\AttributeTok{n\_bar =} \DecValTok{30}\NormalTok{, }\AttributeTok{alpha =} \DecValTok{0}\NormalTok{, }\AttributeTok{p =} \DecValTok{2} \SpecialCharTok{/} \DecValTok{3}\NormalTok{, }
  \AttributeTok{gamma\_0 =} \DecValTok{0}\NormalTok{, }\AttributeTok{gamma\_1 =} \FloatTok{0.2}\NormalTok{, }\AttributeTok{gamma\_2 =} \FloatTok{0.2}\NormalTok{,}
  \AttributeTok{sigma2\_u =} \FloatTok{0.4}\NormalTok{, }\AttributeTok{sigma2\_e =} \FloatTok{0.6}
\NormalTok{)}
\FunctionTok{estimate\_Tx\_Fx}\NormalTok{(dat)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 3 x 4
##   estimator ATE_hat SE_hat p_value
##   <chr>       <dbl>  <dbl>   <dbl>
## 1 MLM         0.189  0.547   0.737
## 2 OLS         0.189  0.413   0.663
## 3 agg         0.189  0.413   0.657
\end{verbatim}

As expected, all three point estimators match, but the SE from the multilevel model is a little bit discrepant from the others.

We can also use large-sample theory to check the multilevel modeling estimator.
If the model is correctly specified, then \emph{all} the parameters of the model should be accurately estimated if the model is fit to a very large sample of data.
To check this property, we will need access to the full model output, not just the selected results returned by \texttt{analysis\_MLM()}.
One way to handle this is to make a small tweak to the estimation function, adding an option to control whether to return the entire model or just selected results.
Here is the tweaked function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{analysis\_MLM }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{( dat, }\AttributeTok{all\_results =} \ConstantTok{FALSE}\NormalTok{) \{}
  
\NormalTok{  M1 }\OtherTok{\textless{}{-}}\NormalTok{ lme4}\SpecialCharTok{::}\FunctionTok{lmer}\NormalTok{( Yobs }\SpecialCharTok{\textasciitilde{}} \DecValTok{1} \SpecialCharTok{+}\NormalTok{ Z }\SpecialCharTok{+}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{|}\NormalTok{ sid), }\AttributeTok{data =}\NormalTok{ dat )}
\NormalTok{  M1\_test }\OtherTok{\textless{}{-}}\NormalTok{ lmerTest}\SpecialCharTok{::}\FunctionTok{as\_lmerModLmerTest}\NormalTok{(M1)}
  
  \ControlFlowTok{if}\NormalTok{ (all\_results) \{}
    \FunctionTok{return}\NormalTok{(}\FunctionTok{summary}\NormalTok{(M1\_test))}
\NormalTok{  \} }
  
\NormalTok{  M1\_summary }\OtherTok{\textless{}{-}} \FunctionTok{summary}\NormalTok{(M1\_test)}\SpecialCharTok{$}\NormalTok{coefficients}
  
  \FunctionTok{tibble}\NormalTok{( }
    \AttributeTok{ATE\_hat =}\NormalTok{ M1\_summary[}\StringTok{"Z"}\NormalTok{,}\StringTok{"Estimate"}\NormalTok{], }
    \AttributeTok{SE\_hat =}\NormalTok{ M1\_summary[}\StringTok{"Z"}\NormalTok{,}\StringTok{"Std. Error"}\NormalTok{], }
    \AttributeTok{p\_value =}\NormalTok{ M1\_summary[}\StringTok{"Z"}\NormalTok{, }\StringTok{"Pr(\textgreater{}|t|)"}\NormalTok{] }
\NormalTok{  )}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Setting \texttt{all\_results} to \texttt{TRUE} will return the entire function; keeping it at the default value of \texttt{FALSE} will return the same output as the other functions.
Now let's apply the estimation function to a very large dataset, with variation in cluster sizes.
We set \texttt{gamma\_2\ =\ 0} so that the estimation model is correctly specified:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dat }\OtherTok{\textless{}{-}} \FunctionTok{gen\_cluster\_RCT}\NormalTok{( }
  \AttributeTok{J=}\DecValTok{5000}\NormalTok{, }\AttributeTok{n\_bar =} \DecValTok{20}\NormalTok{, }\AttributeTok{alpha =} \FloatTok{0.9}\NormalTok{, }\AttributeTok{p =} \DecValTok{2} \SpecialCharTok{/} \DecValTok{3}\NormalTok{, }
  \AttributeTok{gamma\_0 =} \DecValTok{2}\NormalTok{, }\AttributeTok{gamma\_1 =} \FloatTok{0.30}\NormalTok{, }\AttributeTok{gamma\_2 =} \DecValTok{0}\NormalTok{,}
  \AttributeTok{sigma2\_u =} \FloatTok{0.4}\NormalTok{, }\AttributeTok{sigma2\_e =} \FloatTok{0.6}
\NormalTok{)}

\FunctionTok{analysis\_MLM}\NormalTok{(dat, }\AttributeTok{all\_results =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Linear mixed model fit by REML. t-tests use
##   Satterthwaite's method [lmerModLmerTest]
## Formula: Yobs ~ 1 + Z + (1 | sid)
##    Data: dat
## 
## REML criterion at convergence: 242678
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -4.3080 -0.6581  0.0007  0.6594  4.2134 
## 
## Random effects:
##  Groups   Name        Variance Std.Dev.
##  sid      (Intercept) 0.3972   0.6302  
##  Residual             0.6030   0.7765  
## Number of obs: 98772, groups:  sid, 5000
## 
## Fixed effects:
##              Estimate Std. Error        df
## (Intercept) 2.017e+00  1.636e-02 4.969e+03
## Z           2.838e-01  2.003e-02 4.964e+03
##             t value Pr(>|t|)    
## (Intercept)  123.29   <2e-16 ***
## Z             14.17   <2e-16 ***
## ---
## Signif. codes:  
## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Correlation of Fixed Effects:
##   (Intr)
## Z -0.817
\end{verbatim}

The intercept and treatment effect coefficient estimates are very close to their true parameter values, as are the estimated school-level variance and student-level residual variance.
This all gives some assurance that the \texttt{analysis\_MLM()} function is working properly.

Of course, it is important to bear in mind that these tests are only partial verifications.
With the algebraic test, we have only checked that the functions seem to be working properly for scenarios with equal school sizes, but they still might have errors that only appear when school sizes vary.
Likewise, \texttt{analysis\_MLM()} seems to be working properly for very large datasets, but our test does not rule out the possibility of bugs that only crawl out when \(J\) is small.
Our large-sample test also relies on the correctness of the \texttt{gen\_cluster\_RCT()} function;
if we had seen a discrepancy between parameters and estimates from the multilevel model, it could have been because of a problem with the data-generation function rather than with the estimation function.

These limitations are typical of what can be accomplished through tests based on theoretical results, because theoretical results typically only hold under specific conditions.
After all, if we had comprehensive theoretical results, we would not need to simulate anything in the first place!
Nonetheless, it is good to work through such tests to the extent that relevant theory is available for the problem you are studying.

\subsection{Checking with simulations}\label{checking-with-simulations}

Checking, debugging, and revising should not be limited to when you are initially developing estimation functions.
It often happens that later steps in the process of conducting a simulation will reveal problems with the code for earlier steps.
For instance, once you have run the data-generating and estimation steps repeatedly, calculated performance summaries, and created some graphs of the results, you might find an unusual or anomolous pattern in the performance of an estimator.
This might be a legitimate result---perhaps the estimator really does behave weirdly or not work well---or it might be due to a problem in how you implemented the estimator or data-generating process.
When faced with an unusual pattern, we recommend revisiting the estimation code to double check for bugs and also thinking further about what might lead to the anomoly.
Further exploration might lead you to a deeper understanding of how a method works and perhaps even an idea for how to improve the estimator or refine the data-generating process.

A good illustration of this process comes from one of Luke's past research projects (see \citet{pashley2024improving}), in which he and other co-authors were working on a way to improve Instrumental Variable (IV) estimation using post-stratification.
The method they studied involved grouping units based on a covariate that predicts compliance status, then calculating estimates within each group, then summarizing the estimates across groups.
They used simulations to see whether this method would improve the accuracy of the overall summary effect estimate.
In the first simulation, the estimates were full of NAs and odd results because the estimation function failed to account for what happens in groups of observations where the number of compliers was estimated to be zero.
After repairing that problem and re-running everything, the simulation results still indicated serious and unexpected bias, which turned out to be due to an error in how the estimation function implemented the step of summarizing estimates across groups.
After again correcting and re-running, the simulation results showed that the gains in accuracy from this new method were minimal, even when the groups were formed based on a variable that was almost perfectly predictive of compliance status.
Eventually, we understood that the groups with very few compliers produced such unstable estimates that they spoiled the overall average estimate.
This inspired us to revise our estimation strategy and introduce a method that dropped or down-weighted strata with few compliers, which ultimately helped us to strengthen the contribution of our work.

As this experience highlights, simulations seldom follow a single, well-defined trajectory.
The point of conducting simulations is to help us, as researchers, learn about estimation methods so that we can analyze real data better.
What we learn from simulation gives us a better understanding of the methods (potentially including a better understanding of theoretical results), leading to ideas about better methods to create new scenarios to explore in further simulations.
Of course, at some point one needs to step off this merry-go-round, write up the findings, cook dinner, and clean the bathroom.
But, just like many other research endeavors, simulations follow a highly iterative process.

\section{Handling errors, warnings, and other hiccups}\label{handling-errors-warnings-and-other-hiccups}

Especially when working with more advanced estimation methods, it is possible that your estimation function will fail, throw an error, or return something uninterpretable for certain input datasets.
For instance, maximum likelihood estimation often requires iterative, numerical optimization algorithms that sometimes fail to converge.
This might happen rarely enough that it takes a while to even notice that it is a problem, but even quite rare things can occur when you run simulations with many thousands of repetitions.
Less dire but still annoying, your estimation function might generate warnings, which can pile up if you are running many repetitions.
In some cases, such warnings might also signal that the estimator produced a bad result, and
it may not be clear whether we should retain this result (or include it in overall performance assessments).
After all, the function tried to warn us that something is off!

Errors and warnings in estimation functions pose two problems, one purely technical and one conceptual.
On a technical level, R functions stop running if they hit errors (though not warnings), so we need ways to handle the errors in order to get our simulations up and running.
On a conceptual level, we need to decide how to use the information contained in errors and warnings, whether that be by further elaborating the estimation procedures to address different contingencies or by evaluating the performance of the estimators in a way that appropriately accounts for errors.
We consider each of the problems here, then revisit the conceptual considerations in Chapter \ref{performance-criteria}.

\subsection{Capturing errors and warnings}\label{capturing-errors-and-warnings}

Some estimation functions will require complicated or stochastic calculations that can sometimes produce errors.
Intermittent errors can really be annoying and time-consuming if not addressed.
To protect yourself, it is good practice to anticipate potential errors, preventing them from stopping code execution and allowing your simulations to keep running.
We will demonstrate some techniques for error-handling using tools from the \texttt{purrr} package.

For illustrative purposes, consider the following error-prone function that sometimes returns what we want, sometimes returns \texttt{NaN} due to taking the square root of a negative number, and sometimes crashes completely because \texttt{broken\_code()} does not exist:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_complex\_function }\OtherTok{=} \ControlFlowTok{function}\NormalTok{( param ) \{}
    
\NormalTok{    vals }\OtherTok{=} \FunctionTok{rnorm}\NormalTok{( param, }\AttributeTok{mean =} \FloatTok{0.5}\NormalTok{ )}
    \ControlFlowTok{if}\NormalTok{ ( }\FunctionTok{sum}\NormalTok{( vals ) }\SpecialCharTok{\textgreater{}} \DecValTok{5}\NormalTok{ ) \{}
        \FunctionTok{broken\_code}\NormalTok{( }\DecValTok{4}\NormalTok{ )}
\NormalTok{    \} }\ControlFlowTok{else}\NormalTok{ \{}
        \FunctionTok{sqrt}\NormalTok{( }\FunctionTok{sum}\NormalTok{( vals ) }\SpecialCharTok{*} \FunctionTok{sign}\NormalTok{( vals )[[}\DecValTok{1}\NormalTok{]] )}
\NormalTok{    \}}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Running it produces some results and an occasional warning, and some errors:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{3}\NormalTok{)}
\FunctionTok{my\_complex\_function}\NormalTok{( }\DecValTok{1}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.6796568
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{my\_complex\_function}\NormalTok{( }\DecValTok{10}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 2.132087
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{my\_complex\_function}\NormalTok{( }\DecValTok{5}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning in sqrt(sum(vals) * sign(vals)[[1]]):
## NaNs produced
\end{verbatim}

\begin{verbatim}
## [1] NaN
\end{verbatim}

Running it many times produces warnings, then an error:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{resu }\OtherTok{\textless{}{-}} \FunctionTok{replicate}\NormalTok{(}\DecValTok{20}\NormalTok{, }\FunctionTok{my\_complex\_function}\NormalTok{( }\DecValTok{7}\NormalTok{ ))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning in sqrt(sum(vals) * sign(vals)[[1]]):
## NaNs produced
## Warning in sqrt(sum(vals) * sign(vals)[[1]]):
## NaNs produced
\end{verbatim}

\begin{verbatim}
## Error in broken_code(4): could not find function "broken_code"
\end{verbatim}

The \texttt{purrr} package includes a function called \texttt{safely} that makes it easy to trap errors.
To use it, we feed the estimation function into \texttt{safely()} to create a new version:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_safe\_function }\OtherTok{\textless{}{-}} \FunctionTok{safely}\NormalTok{( my\_complex\_function, }\AttributeTok{otherwise =} \ConstantTok{NA}\NormalTok{ )}
\FunctionTok{my\_safe\_function}\NormalTok{( }\DecValTok{7}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $result
## [1] 2.175561
## 
## $error
## NULL
\end{verbatim}

The safe version of the function returns a list with two entries: the result (or NULL if there was an error), and the error message (or NULL if there was no error).
\texttt{safely()} is an example of a \emph{functional} (or an \emph{abverb}), which takes a function and returns a new function that does something slightly different.
We include \texttt{otherwise\ =\ NA} so we always get a result, rather than a \texttt{NULL} when there is an error.

We can use the safe function repeatedly and it will always return a result:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{resu }\OtherTok{\textless{}{-}} \FunctionTok{replicate}\NormalTok{(}\DecValTok{20}\NormalTok{, }\FunctionTok{my\_safe\_function}\NormalTok{( }\DecValTok{7}\NormalTok{ ), }\AttributeTok{simplify =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning in sqrt(sum(vals) * sign(vals)[[1]]):
## NaNs produced
## Warning in sqrt(sum(vals) * sign(vals)[[1]]):
## NaNs produced
## Warning in sqrt(sum(vals) * sign(vals)[[1]]):
## NaNs produced
## Warning in sqrt(sum(vals) * sign(vals)[[1]]):
## NaNs produced
## Warning in sqrt(sum(vals) * sign(vals)[[1]]):
## NaNs produced
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{resu }\OtherTok{\textless{}{-}} \FunctionTok{transpose}\NormalTok{( resu )}
\FunctionTok{unlist}\NormalTok{(resu}\SpecialCharTok{$}\NormalTok{result)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] 1.3870195 0.5638654        NA 1.6995292
##  [5]        NA        NA       NaN 2.2121710
##  [9]        NA       NaN       NaN 1.8801925
## [13]       NaN 1.9154618       NaN        NA
## [17] 2.2245636        NA        NA 0.8854747
\end{verbatim}

The \texttt{transpose()} function takes a list of lists, and reorganizes them to give you a list of all the first elements, a list of all the second elements, etc.
This is very powerful for wrangling data, because then we can make a tibble with list columns as so:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tb }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{( }\AttributeTok{result =} \FunctionTok{unlist}\NormalTok{( resu}\SpecialCharTok{$}\NormalTok{result ), }\AttributeTok{error =}\NormalTok{ resu}\SpecialCharTok{$}\NormalTok{error )}
\FunctionTok{head}\NormalTok{( tb, }\AttributeTok{n =} \DecValTok{4}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 4 x 2
##   result error     
##    <dbl> <list>    
## 1  1.39  <NULL>    
## 2  0.564 <NULL>    
## 3 NA     <smplErrr>
## 4  1.70  <NULL>
\end{verbatim}

The \texttt{purrr} package includes several other functionals that are useful for handling errors and warnings.
The \texttt{possibly()} wrapper will try to run a function and will return a specified value in the event of an error:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_possible\_function }\OtherTok{\textless{}{-}} \FunctionTok{possibly}\NormalTok{( my\_complex\_function, }\AttributeTok{otherwise =} \ConstantTok{NA}\NormalTok{ )}
\FunctionTok{my\_possible\_function}\NormalTok{( }\DecValTok{7}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1.506734
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rs }\OtherTok{\textless{}{-}} \FunctionTok{replicate}\NormalTok{(}\DecValTok{20}\NormalTok{, }\FunctionTok{my\_possible\_function}\NormalTok{( }\DecValTok{7}\NormalTok{ ))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning in sqrt(sum(vals) * sign(vals)[[1]]):
## NaNs produced
## Warning in sqrt(sum(vals) * sign(vals)[[1]]):
## NaNs produced
## Warning in sqrt(sum(vals) * sign(vals)[[1]]):
## NaNs produced
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rs}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1]        NA 0.7023915 0.9495728        NA
##  [5]        NA 1.8784947 0.9838187        NA
##  [9]        NA 1.7676572 1.4809897        NA
## [13] 2.1019082        NA       NaN       NaN
## [17] 1.8629289 1.3467997       NaN 1.3017348
\end{verbatim}

It works as a simpler version of \texttt{safely()}, which does not record error messages.

The \texttt{quietly} functional leads to results that are bundled together with any console output, warnings, and messages, rather than printing anything to the console:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_quiet\_function }\OtherTok{\textless{}{-}} \FunctionTok{quietly}\NormalTok{( my\_complex\_function )}

\FunctionTok{my\_quiet\_function}\NormalTok{( }\DecValTok{1}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $result
## [1] 0.1724504
## 
## $output
## [1] ""
## 
## $warnings
## character(0)
## 
## $messages
## character(0)
\end{verbatim}

This can be especially useful to reduce extraneous printing in a simulation, which can slow down code execution more than you might expect.
However, \texttt{quietly()} does not trap errors:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rs }\OtherTok{\textless{}{-}} \FunctionTok{replicate}\NormalTok{(}\DecValTok{20}\NormalTok{, }\FunctionTok{my\_quiet\_function}\NormalTok{( }\DecValTok{7}\NormalTok{ ))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Error in broken_code(4): could not find function "broken_code"
\end{verbatim}

Double-wrapping your function will handle both errors and warnings, but the structure it produces gets a bit complicated:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_safe\_quiet\_function }\OtherTok{\textless{}{-}} \FunctionTok{quietly}\NormalTok{( }\FunctionTok{safely}\NormalTok{( my\_complex\_function, }\AttributeTok{otherwise =} \ConstantTok{NA}\NormalTok{ ) )}
\FunctionTok{my\_safe\_quiet\_function}\NormalTok{(}\DecValTok{7}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $result
## $result$result
## [1] NA
## 
## $result$error
## <simpleError in broken_code(4): could not find function "broken_code">
## 
## 
## $output
## [1] ""
## 
## $warnings
## character(0)
## 
## $messages
## character(0)
\end{verbatim}

Even though the result is a bit of a mess, this structure provides all the pieces that we need to do further calculations on the result (when available), along with errors, warnings, and other output.

To see how this works in practice, we will adapt our \texttt{analysis\_MLM()} function, which makes use of \texttt{lmer()} for fitting a multilevel model.
Currently, the estimation function sometimes prints messages to the console:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{101012}\NormalTok{)  }\CommentTok{\# (I picked this to show a warning.)}
\NormalTok{dat }\OtherTok{\textless{}{-}} \FunctionTok{gen\_cluster\_RCT}\NormalTok{( }\AttributeTok{J =} \DecValTok{50}\NormalTok{, }\AttributeTok{n\_bar =} \DecValTok{100}\NormalTok{, }\AttributeTok{sigma2\_u =} \DecValTok{0}\NormalTok{ )}
\NormalTok{mod }\OtherTok{\textless{}{-}} \FunctionTok{analysis\_MLM}\NormalTok{(dat)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## boundary (singular) fit: see help('isSingular')
\end{verbatim}

Wrapping \texttt{lmer()} with \texttt{quietly()} makes it possible to catch such output and store it along with other results, as in the following:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{quiet\_lmer }\OtherTok{\textless{}{-}} \FunctionTok{quietly}\NormalTok{(lme4}\SpecialCharTok{::}\NormalTok{lmer)}
\NormalTok{qmod }\OtherTok{\textless{}{-}} \FunctionTok{quiet\_lmer}\NormalTok{( Yobs }\SpecialCharTok{\textasciitilde{}} \DecValTok{1} \SpecialCharTok{+}\NormalTok{ Z }\SpecialCharTok{+}\NormalTok{ (}\DecValTok{1}\SpecialCharTok{|}\NormalTok{sid), }\AttributeTok{data=}\NormalTok{dat )}
\NormalTok{qmod}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $result
## Linear mixed model fit by REML ['lmerMod']
## Formula: Yobs ~ 1 + Z + (1 | sid)
##    Data: ..2
## REML criterion at convergence: 14026.44
## Random effects:
##  Groups   Name        Std.Dev.
##  sid      (Intercept) 0.0000  
##  Residual             0.9828  
## Number of obs: 5000, groups:  sid, 50
## Fixed Effects:
## (Intercept)            Z  
##   -0.013930    -0.008804  
## optimizer (nloptwrap) convergence code: 0 (OK) ; 0 optimizer warnings; 1 lme4 warnings 
## 
## $output
## [1] ""
## 
## $warnings
## character(0)
## 
## $messages
## [1] "boundary (singular) fit: see help('isSingular')\n"
\end{verbatim}

However, the \texttt{lmerTest} package does not like the structure of the results, and produces an error:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lmerTest}\SpecialCharTok{::}\FunctionTok{as\_lmerModLmerTest}\NormalTok{(qmod}\SpecialCharTok{$}\NormalTok{result)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Error in lmerTest::as_lmerModLmerTest(qmod$result): Unable to extract deviance function from model fit
\end{verbatim}

We can side-step this by combining \texttt{as\_lmerModLmerTest()} and \texttt{lmer()} into a single function.
While we are at it, we also layer on \texttt{summary()}.
To do so, we use the \texttt{compose()} functional from \texttt{purrr}, which takes a list of functions and wraps them into one:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lmer\_with\_test }\OtherTok{\textless{}{-}}\NormalTok{ purrr}\SpecialCharTok{::}\FunctionTok{compose}\NormalTok{(}
\NormalTok{  summary,}
\NormalTok{  lmerTest}\SpecialCharTok{::}\NormalTok{as\_lmerModLmerTest, }
\NormalTok{  lme4}\SpecialCharTok{::}\NormalTok{lmer}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The resulting \texttt{lmer\_with\_test()} function acts as if we were calling \texttt{lmer()}, then feeding the result into \texttt{as\_lmerModLmerTest()}, then feeding the result into \texttt{summary()}.
We then wrap the combination function with \texttt{safely()} and \texttt{quietly()}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{quiet\_safe\_lmer }\OtherTok{\textless{}{-}}\NormalTok{ purrr}\SpecialCharTok{::}\FunctionTok{quietly}\NormalTok{(purrr}\SpecialCharTok{::}\FunctionTok{safely}\NormalTok{(lmer\_with\_test))}
\end{Highlighting}
\end{Shaded}

Now we can use our suitably quieted and safe function in a new version of the estimation function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{analysis\_MLM\_safe }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{( dat, }\AttributeTok{all\_results =} \ConstantTok{FALSE}\NormalTok{ ) \{}
  
\NormalTok{  M1 }\OtherTok{\textless{}{-}} \FunctionTok{quiet\_safe\_lmer}\NormalTok{( Yobs }\SpecialCharTok{\textasciitilde{}} \DecValTok{1} \SpecialCharTok{+}\NormalTok{ Z }\SpecialCharTok{+}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{|}\NormalTok{ sid), }\AttributeTok{data=}\NormalTok{dat )}
  
  \ControlFlowTok{if}\NormalTok{ (all\_results) \{}
    \FunctionTok{return}\NormalTok{(M1)}
\NormalTok{  \} }
  
\NormalTok{  message }\OtherTok{\textless{}{-}} \FunctionTok{ifelse}\NormalTok{( }\FunctionTok{length}\NormalTok{( M1}\SpecialCharTok{$}\NormalTok{message ) }\SpecialCharTok{\textgreater{}} \DecValTok{0}\NormalTok{, M1}\SpecialCharTok{$}\NormalTok{message, }\ConstantTok{NA\_character\_}\NormalTok{ )}
\NormalTok{  warning }\OtherTok{\textless{}{-}} \FunctionTok{ifelse}\NormalTok{( }\FunctionTok{length}\NormalTok{( M1}\SpecialCharTok{$}\NormalTok{warning ) }\SpecialCharTok{\textgreater{}} \DecValTok{0}\NormalTok{, M1}\SpecialCharTok{$}\NormalTok{warning, }\ConstantTok{NA\_character\_}\NormalTok{ )}
\NormalTok{  error }\OtherTok{\textless{}{-}} \FunctionTok{ifelse}\NormalTok{( }\FunctionTok{length}\NormalTok{( M1}\SpecialCharTok{$}\NormalTok{result}\SpecialCharTok{$}\NormalTok{error) }\SpecialCharTok{\textgreater{}} \DecValTok{0}\NormalTok{, M1}\SpecialCharTok{$}\NormalTok{result}\SpecialCharTok{$}\NormalTok{error}\SpecialCharTok{$}\NormalTok{message, }\ConstantTok{NA\_character\_}\NormalTok{ )}
  
  \FunctionTok{tibble}\NormalTok{( }
    \AttributeTok{ATE\_hat =}\NormalTok{ M1}\SpecialCharTok{$}\NormalTok{result}\SpecialCharTok{$}\NormalTok{result}\SpecialCharTok{$}\NormalTok{coefficients[}\StringTok{"Z"}\NormalTok{,}\StringTok{"Estimate"}\NormalTok{], }
    \AttributeTok{SE\_hat =}\NormalTok{ M1}\SpecialCharTok{$}\NormalTok{result}\SpecialCharTok{$}\NormalTok{result}\SpecialCharTok{$}\NormalTok{coefficients[}\StringTok{"Z"}\NormalTok{,}\StringTok{"Std. Error"}\NormalTok{], }
    \AttributeTok{p\_value =}\NormalTok{ M1}\SpecialCharTok{$}\NormalTok{result}\SpecialCharTok{$}\NormalTok{result}\SpecialCharTok{$}\NormalTok{coefficients[}\StringTok{"Z"}\NormalTok{, }\StringTok{"Pr(\textgreater{}|t|)"}\NormalTok{],}
    \AttributeTok{message =}\NormalTok{ message,}
    \AttributeTok{warning =}\NormalTok{ warning,}
    \AttributeTok{error =}\NormalTok{ error}
\NormalTok{  )}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

This quiet version runs without extraneous messages:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mod }\OtherTok{\textless{}{-}} \FunctionTok{analysis\_MLM\_safe}\NormalTok{(dat)}
\NormalTok{mod}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1 x 6
##    ATE_hat SE_hat p_value message    warning error
##      <dbl>  <dbl>   <dbl> <chr>      <chr>   <chr>
## 1 -0.00880 0.0278   0.751 "boundary~ <NA>    <NA>
\end{verbatim}

Now we have the estimation results along with any diagnostic information from messages or warnings.
Storing this information will let us evaluate what proportion of the time there was a warning or message, run additional analyses on the subset of replications where there was no such warning, or even modify the estimation procedure to take the diagnostics into account.

\subsection{Adapting estimation procedures for errors and warnings}\label{adapting-for-errors}

So far, we have seen techniques for handling technical hiccups that occur when data analysis procedures do not always produce results.
But how do we account for the absence of results in a simulation?
In Chapter \ref{performance-criteria}, we will delve into the conceptual issues with summarizing the performance of methods that do not always provide an answer.
One of the best solutions to such problems still concerns the formulation of estimation functions, and so we introduce it here.
That solution is to \emph{re-define the estimator} to include contingencies for handling lack of results.

Consider a data analyst who was planning to apply a fancy statistical model to their data, but then finds that the model does not converge.
What would that analyst do in practice (besides cussing and taking a snack break)?
Rather than giving up entirely, they would probably think of an alternative analysis and attempt to apply it, perhaps by simplifying the model in some way.
To the extent that we can anticipate such possibilities, we can build these error-contingent alternative analyses into our estimation function.

To illustrate, let's look at an error (a not-particularly-subtle one) that can crop up in the cluster-randomized trial example when clusters are very small:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{65842}\NormalTok{)}
\NormalTok{tiny\_dat }\OtherTok{\textless{}{-}} \FunctionTok{gen\_cluster\_RCT}\NormalTok{( }\AttributeTok{J =} \DecValTok{10}\NormalTok{, }\AttributeTok{n\_bar =} \DecValTok{2}\NormalTok{, }\AttributeTok{alpha =} \FloatTok{0.5}\NormalTok{)}
\FunctionTok{analysis\_MLM\_safe}\NormalTok{(tiny\_dat)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1 x 3
##   message warning error                           
##   <chr>   <chr>   <chr>                           
## 1 <NA>    <NA>    number of levels of each groupi~
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{table}\NormalTok{(tiny\_dat}\SpecialCharTok{$}\NormalTok{sid)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  1  2  3  4  5  6  7  8  9 10 
##  1  1  1  1  1  1  1  1  1  1
\end{verbatim}

The error occurs because all 10 simulated schools include a single student, making it impossible to estimate a random-intercepts multilevel model.
A natural fall-back analysis here would be to estimate an ordinary least squares regression analysis.

Suppose that our imaginary analyst is not especially into nuance, and so will fall back onto ordinary least squares whenever the multilevel model produces an error.
We can express this logic in our estimation function by first catching the error thrown by \texttt{lmer()} and then running an OLS regression in the event an error is thrown:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{analysis\_MLM\_contingent }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{( dat, }\AttributeTok{all\_results =} \ConstantTok{FALSE}\NormalTok{ ) \{}
  
\NormalTok{  M1 }\OtherTok{\textless{}{-}} \FunctionTok{quiet\_safe\_lmer}\NormalTok{( Yobs }\SpecialCharTok{\textasciitilde{}} \DecValTok{1} \SpecialCharTok{+}\NormalTok{ Z }\SpecialCharTok{+}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{|}\NormalTok{ sid), }\AttributeTok{data=}\NormalTok{dat )}
  
  \ControlFlowTok{if}\NormalTok{ (all\_results) \{}
    \FunctionTok{return}\NormalTok{(M1)}
\NormalTok{  \} }
  
  \ControlFlowTok{if}\NormalTok{ (}\SpecialCharTok{!}\FunctionTok{is.null}\NormalTok{(M1}\SpecialCharTok{$}\NormalTok{result}\SpecialCharTok{$}\NormalTok{result)) \{ }
    \CommentTok{\# If lmer() returns a result}
\NormalTok{    res }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{( }
      \AttributeTok{ATE\_hat =}\NormalTok{ M1}\SpecialCharTok{$}\NormalTok{result}\SpecialCharTok{$}\NormalTok{result}\SpecialCharTok{$}\NormalTok{coefficients[}\StringTok{"Z"}\NormalTok{,}\StringTok{"Estimate"}\NormalTok{], }
      \AttributeTok{SE\_hat =}\NormalTok{ M1}\SpecialCharTok{$}\NormalTok{result}\SpecialCharTok{$}\NormalTok{result}\SpecialCharTok{$}\NormalTok{coefficients[}\StringTok{"Z"}\NormalTok{,}\StringTok{"Std. Error"}\NormalTok{], }
      \AttributeTok{p\_value =}\NormalTok{ M1}\SpecialCharTok{$}\NormalTok{result}\SpecialCharTok{$}\NormalTok{result}\SpecialCharTok{$}\NormalTok{coefficients[}\StringTok{"Z"}\NormalTok{, }\StringTok{"Pr(\textgreater{}|t|)"}\NormalTok{],}
\NormalTok{    )}
\NormalTok{  \} }\ControlFlowTok{else}\NormalTok{ \{}
    \CommentTok{\# If lmer() errors, fall back on OLS}
\NormalTok{    M\_ols }\OtherTok{\textless{}{-}} \FunctionTok{summary}\NormalTok{(}\FunctionTok{lm}\NormalTok{(Yobs }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Z, }\AttributeTok{data =}\NormalTok{ dat))}
\NormalTok{    res }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{( }
      \AttributeTok{ATE\_hat =}\NormalTok{ M\_ols}\SpecialCharTok{$}\NormalTok{coefficients[}\StringTok{"Z"}\NormalTok{,}\StringTok{"Estimate"}\NormalTok{], }
      \AttributeTok{SE\_hat =}\NormalTok{ M\_ols}\SpecialCharTok{$}\NormalTok{coefficients[}\StringTok{"Z"}\NormalTok{, }\StringTok{"Std. Error"}\NormalTok{], }
      \AttributeTok{p\_value =}\NormalTok{ M\_ols}\SpecialCharTok{$}\NormalTok{coefficients[}\StringTok{"Z"}\NormalTok{,}\StringTok{"Pr(\textgreater{}|t|)"}\NormalTok{]}
\NormalTok{    )}
\NormalTok{  \}}

  \CommentTok{\# Store original messages, warnings, errors  }
\NormalTok{  res}\SpecialCharTok{$}\NormalTok{message }\OtherTok{\textless{}{-}} \FunctionTok{ifelse}\NormalTok{( }\FunctionTok{length}\NormalTok{( M1}\SpecialCharTok{$}\NormalTok{message ) }\SpecialCharTok{\textgreater{}} \DecValTok{0}\NormalTok{, M1}\SpecialCharTok{$}\NormalTok{message, }\ConstantTok{NA\_character\_}\NormalTok{ )}
\NormalTok{  res}\SpecialCharTok{$}\NormalTok{warning }\OtherTok{\textless{}{-}} \FunctionTok{ifelse}\NormalTok{( }\FunctionTok{length}\NormalTok{( M1}\SpecialCharTok{$}\NormalTok{warning ) }\SpecialCharTok{\textgreater{}} \DecValTok{0}\NormalTok{, M1}\SpecialCharTok{$}\NormalTok{warning, }\ConstantTok{NA\_character\_}\NormalTok{ )}
\NormalTok{  res}\SpecialCharTok{$}\NormalTok{error }\OtherTok{\textless{}{-}} \FunctionTok{ifelse}\NormalTok{( }\FunctionTok{length}\NormalTok{( M1}\SpecialCharTok{$}\NormalTok{result}\SpecialCharTok{$}\NormalTok{error) }\SpecialCharTok{\textgreater{}} \DecValTok{0}\NormalTok{, M1}\SpecialCharTok{$}\NormalTok{result}\SpecialCharTok{$}\NormalTok{error}\SpecialCharTok{$}\NormalTok{message, }\ConstantTok{NA\_character\_}\NormalTok{ )}
  
  \FunctionTok{return}\NormalTok{(res)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

We still store the messages, warnings, and errors from the initial \texttt{lmer()} fit so that we can keep track of how often errors occur.
The function now returns an treatment effect estimate even if \texttt{lmer()} errors:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{analysis\_MLM\_contingent}\NormalTok{(tiny\_dat)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1 x 6
##   ATE_hat SE_hat p_value message warning error    
##     <dbl>  <dbl>   <dbl> <chr>   <chr>   <chr>    
## 1   0.400  0.603   0.525 <NA>    <NA>    number o~
\end{verbatim}

Of course, we can easily anticipate the conditions under which this particular error will occur: all we need to do is check whether all the clusters are single observations.
Because it is easily anticipated, a better strategy for handling this error is to check \emph{before} fitting the multilevel model and proceeding accordingly in the event that the clusters are all singletons.
Exercise \ref{contingent-estimator-processing} asks you to implement this approach and further refine this contingent analysis strategy.

Adapting estimation functions to address errors can be an effective---and often very interesting---strategy for studying the performance of estimation methods.
Rather than studying the performance of a data-analysis method that is only sometimes well-defined, we shift to studying a stylized cognitive model for the analyst's decision-making process, which handles contingencies that might crop up whether analyzing simulated data or real empirical data.
Of course, studying such a model is only interesting to the extent that the decision-making process it implements is a plausible representation of what an analyst might actually do in practice.

The adaptive estimation approach does lead to more complex estimation functions, which entail implementing multiple estimation methods and a set of decision rules for applying them.
Often, the set of contingencies that need to be handled will not be immediately obvious, so you may find that you need to build and refine the decision rules as you learn more about how they work.
Running an estimation procedure over multiple, simulated datasets is an excellent (if aggravating!) way to identify errors and edge cases.
We turn to procedures for doing so in the next chapter.

\section{Exercises}\label{exercises-4}

\subsection{More Heteroskedastic ANOVA}\label{BFFs-forever}

In the classic simulation by Brown and Forsythe (1974), they not only looked at the performance of the homoskedastic ANOVA-F test and Welch's heteroskedastic-F test, they also proposed their own new hypothesis testing procedure.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Write a function that implements the Brown-Forsythe F* test (the BFF* test!) as described on p.~130 of Brown and Forsythe (1974), using the following code skeleton:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{BF\_F }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{( data ) \{}

  \CommentTok{\# fill in the guts here}

  \FunctionTok{return}\NormalTok{(pval)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

  Run the following code to check that your function produces a result:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sim\_data }\OtherTok{\textless{}{-}} \FunctionTok{generate\_ANOVA\_data}\NormalTok{(}
  \AttributeTok{mu =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{6}\NormalTok{), }
  \AttributeTok{sigma\_sq =} \FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{1}\NormalTok{),}
  \AttributeTok{sample\_size =} \FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{4}\NormalTok{)}
\NormalTok{)}
\FunctionTok{BF\_F}\NormalTok{( sim\_data )}
\end{Highlighting}
\end{Shaded}
\item
  Try calling your \texttt{BF\_F} function on a variety of datasets of different sizes and shapes, to make sure it works. What kinds of datasets should you test out?
\item
  Add the BFF* test into the output of \texttt{Welch\_ANOVA\_F()} by calling your \texttt{BF\_F()} function inside the body of \texttt{Welch\_ANOVA\_F()}.
\item
  The \href{https://cran.r-project.org/package=onewaytests}{\texttt{onewaytests} package} implements a variety of different hypothesis testing procedures for one-way ANOVA. Validate your \texttt{Welch\_ANOVA\_F()} function by comparing the results to the output of the relevant functions from \texttt{onewaytests}.
\end{enumerate}

\subsection{Contingent testing}\label{contingent-testing}

In the one-way ANOVA problem, one approach that an analyst might think to take is to conduct a preliminary significance test for heterogeneity of variances (such as Levene's test or Bartlett's test), and then report the \(p\)-value from the homoskedastic ANOVA F test if variance heterogeneity is not detected but the \(p\)-value from the BFF* test if variance heteogeneity is detected.
Modify the \texttt{Welch\_ANOVA\_F()} function to return the \(p\)-value from this contingent BFF* test in addition to the \(p\)-values from the (non-contingent) ANOVA-F, Welch, and BFF* tests.
Include an input option that allows the user to control the \(\alpha\) level of the preliminary test for heterogeneity of variances, as in the following skeleton.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Welch\_ANOVA\_F }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{( data , }\AttributeTok{pre\_alpha =}\NormalTok{ .}\DecValTok{05}\NormalTok{) \{}
  \CommentTok{\# preliminary test for variance heterogeneity}
  \CommentTok{\# compute non{-}contingent F tests for group differences}
  \CommentTok{\# compute contingent test}
  \CommentTok{\# compile results}
  \FunctionTok{return}\NormalTok{(result)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\subsection{Check the cluster-RCT functions}\label{cross-check-CRT-estimators}

Section \ref{multiple-estimation-procedures} presented functions implementing several different strategies for estimating an average treatment effect from a cluster-randomized trial.
Write some code to validate these functions by comparing their output to the results of other tools for doing the same calculation.
Use one or more datasets simulated with \texttt{gen\_cluster\_RCT()}.
For each of these tests, you will need to figure out the appropriate syntax by reading the package documentation.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  For \texttt{analysis\_MLM()}, check the output by fitting the same model using \texttt{lme()} from the \texttt{nlme()} package or \texttt{glmmTMB()} from the package of the same name.
\item
  For \texttt{analysis\_OLS()}, check the output by fitting the linear model using the base R function \texttt{lm()}, then computing standard errors using \texttt{vcovCL()} from the \texttt{sandwich} package. Also compare the output to the results of feeding the fitted model through \texttt{coef\_test()} from the \texttt{clubSandwich} package.
\item
  For \texttt{analysis\_agg()}, check the output by aggregating the data to the school-level, fitting the linear model using \texttt{lm()}, and computing standard errors using \texttt{vcovHC()} from the \texttt{sandwich} package.
\end{enumerate}

\subsection{Extending the cluster-RCT functions}\label{CRT-ANCOVA-estimators}

Exercise \ref{cluster-RCT-baseline} from Chapter \ref{data-generating-processes} asked you to extend the data-generating function for the cluster-randomized trial to include generating a student-level covariate, \(X\), that is predictive of the outcome.
Use your modified function to generate a dataset.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Modify the estimation functions from Section \ref{multiple-estimation-procedures} to use models that include the covariate as a predictor.
\item
  Further extend the functions to include an input argument for the set of predictors to be included in the model, as in the following skeleton for the multi-level model estimator:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{analysis\_MLM }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(dat, }\AttributeTok{predictors =} \StringTok{"Z"}\NormalTok{) \{}

\NormalTok{\}}

\FunctionTok{analysis\_MLM}\NormalTok{( dat )}
\FunctionTok{analysis\_MLM}\NormalTok{( dat, }\AttributeTok{predictors =} \FunctionTok{c}\NormalTok{(}\StringTok{"Z"}\NormalTok{,}\StringTok{"X"}\NormalTok{))}
\FunctionTok{analysis\_MLM}\NormalTok{( dat, }\AttributeTok{predictors =} \FunctionTok{c}\NormalTok{(}\StringTok{"Z"}\NormalTok{,}\StringTok{"X"}\NormalTok{, }\StringTok{"X:Z"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

  Hint: Check out the \texttt{reformulate()} function, which makes it easy to build formulas for different sets of predictors.
\end{enumerate}

\subsection{Contingent estimator processing}\label{contingent-estimator-processing}

In Section \ref{adapting-for-errors} we developed a version of \texttt{analysis\_MLM()} that fell back on OLS regression in the event that \texttt{lmer()} produced any error.
The implementation that we demonstrated is not especially smart.
For one, it does not anticipate that the error will occur.
For another, if we are using this function as one of several estimation strategies, it will require fitting the OLS regression multiple times.
Can you fix these problems?

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Revise \texttt{analysis\_MLM\_contingent()} so that it checks whether all clusters are single observations \emph{before} fitting the multilevel model.
  Handle event the contingency where all clusters are singletons by skipping the model-fitting step, returning \texttt{NA} values for the point estimator, standard error, and p-value, and returning an informative message in the \texttt{error} variable. Test that the function is working as expected.
\item
  Revise \texttt{estimate\_Tx\_Fx()} to use your new version of \texttt{analysis\_MLM\_contingent()}. The revised function will sometimes return \texttt{NA} values for the \texttt{MLM} results. To implement the strategy of falling-back on OLS regression, add some code that replaces any \texttt{NA} values with corresponding results of \texttt{analysis\_OLS()}. Test that the function is working as expected.
\end{enumerate}

\subsection{Estimating 3-parameter item response theory models}\label{IRT-3PL-estimation}

Exercise \ref{IRT-DGP-parameters} asked you to write a data-generating function for the 3-parameter IRT model described in described in Section \ref{three-parameter-IRT}.
Use your function to generate a large dataset.
Using functions from the \href{https://cran.r-project.org/package=ltm}{\texttt{\{ltm\}}}, \href{https://cran.r-project.org/package=mirt}{\texttt{\{mirt\}}}, or \href{https://cran.r-project.org/package=TAM}{\texttt{\{TAM\}}} packages, \emph{estimate} the parameters of the 3-parameter IRT model based on the simulated dataset.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Write a function to \emph{estimate} a 3-parameter IRT model and return a dataset containing estimates of the item characteristics \((\alpha_m,\beta_m, \gamma_m)\).
\item
  Add an option to the function to allow the user to specify known values of \(\gamma_m\).
\item
  Create a graphic showing how the item parameter estimates compare to the true item characteristics.
\item
  Write a function or set of functions to apply 1-parameter, 2-parameter, and 3-parameter models and return datasets containing the person ability estimates \(\theta_1,...,\theta_N\) and corresponding standard errors of measurement.
\end{enumerate}

\subsection{Meta-regression with selective reporting}\label{Vevea-Hedges-estimation}

Exercise \ref{Vevea-Hedges-DGP} asked you to write a data-generating function for the \citet{vevea1995general} selection model.
The \texttt{\{metafor\}} package includes a function for fitting this model (as well as a variety of other selection models).
Here is an example of the syntax for estimating this model, using a dataset from the \texttt{\{metadat\}} package:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(metafor)}
\FunctionTok{data}\NormalTok{(}\StringTok{"dat.assink2016"}\NormalTok{, }\AttributeTok{package =} \StringTok{"metadat"}\NormalTok{)}

\CommentTok{\# rename variables and tidy up}
\NormalTok{dat }\OtherTok{\textless{}{-}} 
\NormalTok{  dat.assink2016 }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{( }\AttributeTok{si =} \FunctionTok{sqrt}\NormalTok{(vi) ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{rename}\NormalTok{( }\AttributeTok{Ti =}\NormalTok{ yi, }\AttributeTok{s\_sq =}\NormalTok{ vi, }\AttributeTok{Xi =}\NormalTok{ year )}

\CommentTok{\# fit a random effects meta{-}regression model}
\NormalTok{rma\_fit }\OtherTok{\textless{}{-}} \FunctionTok{rma.uni}\NormalTok{(}\AttributeTok{yi =}\NormalTok{ Ti, }\AttributeTok{sei =}\NormalTok{ si, }\AttributeTok{mods =} \SpecialCharTok{\textasciitilde{}}\NormalTok{ Xi, }\AttributeTok{data =}\NormalTok{ dat)}
\CommentTok{\# fit two{-}step selection model}
\FunctionTok{selmodel}\NormalTok{(rma\_fit, }\AttributeTok{type =} \StringTok{"step"}\NormalTok{, }\AttributeTok{steps =} \FunctionTok{c}\NormalTok{(.}\DecValTok{025}\NormalTok{, .}\DecValTok{500}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Mixed-Effects Model (k = 100; tau^2 estimator: ML)
## 
## tau^2 (estimated amount of residual heterogeneity): 0.2314 (SE = 0.0500)
## tau (square root of estimated tau^2 value):         0.4810
## 
## Test for Residual Heterogeneity:
## LRT(df = 1) = 349.3718, p-val < .0001
## 
## Test of Moderators (coefficient 2):
## QM(df = 1) = 42.1433, p-val < .0001
## 
## Model Results:
## 
##          estimate      se     zval    pval 
## intrcpt    0.4149  0.1013   4.0942  <.0001 
## Xi        -0.0782  0.0120  -6.4918  <.0001 
##            ci.lb    ci.ub      
## intrcpt   0.2163   0.6135  *** 
## Xi       -0.1018  -0.0546  *** 
## 
## Test for Selection Model Parameters:
## LRT(df = 2) = 1.7814, p-val = 0.4104
## 
## Selection Model Results:
## 
##                      k  estimate      se     zval 
## 0     < p <= 0.025  59    1.0000     ---      --- 
## 0.025 < p <= 0.5    23    0.6990  0.2307  -1.3049 
## 0.5   < p <= 1      18    0.5027  0.2743  -1.8132 
##                       pval   ci.lb   ci.ub    
## 0     < p <= 0.025     ---     ---     ---    
## 0.025 < p <= 0.5    0.1919  0.2470  1.1511    
## 0.5   < p <= 1      0.0698  0.0000  1.0403  . 
## 
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

The \texttt{selmodel()} function can also be used to fit selection models in which one or both of the selection parameters are fixed to user-specified values.
For example:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Fixing lambda\_1 = 0.5, lambda\_2 = 0.2}
\NormalTok{fix\_both }\OtherTok{\textless{}{-}} \FunctionTok{selmodel}\NormalTok{(rma\_fit, }\AttributeTok{type =} \StringTok{"step"}\NormalTok{, }\AttributeTok{steps =} \FunctionTok{c}\NormalTok{(.}\DecValTok{025}\NormalTok{, .}\DecValTok{500}\NormalTok{), }
                     \AttributeTok{delta =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\FloatTok{0.2}\NormalTok{))}

\CommentTok{\# Fixing lambda\_1 = 0.5, estimating lambda\_2}
\NormalTok{fix\_one }\OtherTok{\textless{}{-}} \FunctionTok{selmodel}\NormalTok{(rma\_fit, }\AttributeTok{type =} \StringTok{"step"}\NormalTok{, }\AttributeTok{steps =} \FunctionTok{c}\NormalTok{(.}\DecValTok{025}\NormalTok{, .}\DecValTok{500}\NormalTok{), }
                    \AttributeTok{delta =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\ConstantTok{NA}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Write an estimation function that fits the selection model and returns estimates, standard errors, and confidence intervals for each of the model parameters \((\beta_0,\beta_1,\tau,\lambda_1,\lambda_2)\).
\item
  The \texttt{selmodel()} fitting function sometimes returns errors, as in the following example:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dat\_sig }\OtherTok{\textless{}{-}}\NormalTok{ dat }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{filter}\NormalTok{(Ti }\SpecialCharTok{/}\NormalTok{ si }\SpecialCharTok{\textgreater{}} \DecValTok{0}\NormalTok{)}
\NormalTok{rma\_pos }\OtherTok{\textless{}{-}} \FunctionTok{rma.uni}\NormalTok{(}\AttributeTok{yi =}\NormalTok{ Ti, }\AttributeTok{sei =}\NormalTok{ si, }\AttributeTok{mods =} \SpecialCharTok{\textasciitilde{}}\NormalTok{ Xi, }\AttributeTok{data =}\NormalTok{ dat\_sig)}
\CommentTok{\# fit two{-}step selection model}
\NormalTok{sel\_fit }\OtherTok{\textless{}{-}} \FunctionTok{selmodel}\NormalTok{(rma\_pos, }\AttributeTok{type =} \StringTok{"step"}\NormalTok{, }\AttributeTok{steps =} \FunctionTok{c}\NormalTok{(.}\DecValTok{025}\NormalTok{, .}\DecValTok{500}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: One or more intervals do not contain any
## observed p-values.
\end{verbatim}

\begin{verbatim}
## Warning: One or more 'delta' estimates are (almost) equal to their lower or upper bound.
## Treat results with caution (or consider adjusting 'delta.min' and/or 'delta.max').
\end{verbatim}

  Modify your estimation function to catch and return warnings such as these.
  Write code demonstrating that the function works as expected.
\item
  The \texttt{selmodel()} throws warnings when the dataset contains no observations with \(p_i\) in one of the specified intervals.
  Modify your estimation function to set the selection probability for an interval to \(\lambda_1 = 0.1\) if there are no \(p_i\) values between .025 and .500 and to set \(\lambda_2 = 0.1\) if there are no \(p_i\) values larger than .500.
  Write code demonstrating that the function works as expected.
\end{enumerate}

\chapter{Running the Simulation Process}\label{running-the-simulation-process}

In the prior two chapters we saw how to write functions that generate data according to a particular model and functions that implement data-analysis procedures on simulated data.
The next step in a simulation involves putting these two pieces together, running the DGP function and the data-analysis function repeatedly to obtain results (in the form of point estimates, standard errors, confidence intervals, p-values, or other quantities) from many replications of the whole process.

As with most things R-related, there are many different techniques that can be used to repeat a set of calculations over and over.
In this chapter, we demonstrate several techniques for doing so.
We then explain how to ensure reproducibility of simulation results by setting the seed used by R's random number generator.

\section{Repeating oneself}\label{repeating-oneself}

Suppose that we want to simulate Pearson's correlation coefficient calculated based on a sample from the bivariate Poisson function.
We saw a DGP function for the bivariate Poisson in Section \ref{DGP-functions}, and an estimation function in Section \ref{estimation-functions}.
To produce a simulated correlation coefficient, we need to run these two functions in turn:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dat }\OtherTok{\textless{}{-}} \FunctionTok{r\_bivariate\_Poisson}\NormalTok{( }\AttributeTok{N =} \DecValTok{30}\NormalTok{, }\AttributeTok{rho =} \FloatTok{0.4}\NormalTok{, }\AttributeTok{mu1 =} \DecValTok{8}\NormalTok{, }\AttributeTok{mu2 =} \DecValTok{14}\NormalTok{ )}
\FunctionTok{r\_and\_z}\NormalTok{(dat)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           r         z     CI_lo     CI_hi
## 1 0.5332607 0.5946905 0.2141294 0.7495318
\end{verbatim}

To execute a simulation with these components, we need to repeat this set of calculations over and over.
R has many different functions for doing exactly this.
As one of many alternatives, the \texttt{simhelpers} package includes a function called \texttt{repeat\_and\_stack()}, which can be used to evaluate an arbitrary expression many times over.
We can use it to generate five replications of our correlation coefficient:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(simhelpers)}
\FunctionTok{repeat\_and\_stack}\NormalTok{(}
  \AttributeTok{n =} \DecValTok{5}\NormalTok{, }
\NormalTok{  \{}
\NormalTok{    dat }\OtherTok{\textless{}{-}} \FunctionTok{r\_bivariate\_Poisson}\NormalTok{( }\AttributeTok{N =} \DecValTok{30}\NormalTok{, }\AttributeTok{rho =} \FloatTok{0.4}\NormalTok{, }\AttributeTok{mu1 =} \DecValTok{8}\NormalTok{, }\AttributeTok{mu2 =} \DecValTok{14}\NormalTok{ )}
    \FunctionTok{r\_and\_z}\NormalTok{(dat)}
\NormalTok{  \}, }
  \AttributeTok{id =} \StringTok{"rep"}\NormalTok{, }
  \AttributeTok{stack =} \ConstantTok{TRUE}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   rep         r         z      CI_lo     CI_hi
## 1   1 0.6406898 0.7593429  0.3645711 0.8132451
## 2   2 0.2165457 0.2200290 -0.1558849 0.5350714
## 3   3 0.5664693 0.6423085  0.2590719 0.7696643
## 4   4 0.4534481 0.4890323  0.1113731 0.6994524
## 5   5 0.5044732 0.5552884  0.1762338 0.7317497
\end{verbatim}

The first argument specifies the number of times to repeat the calculation.
The second argument is an R expression that will be evaluated.
The expression is wrapped in curly braces (\texttt{\{\}}) because it involves more than a single line of code.
Including the option \texttt{id\ =\ "rep"} returns a dataset that includes a variable called \texttt{rep} to identify each replication of the process.
Setting the option \texttt{stack\ =\ TRUE} will stack up the output of each expression into a single tibble, which will facilitate later calculations on the results.
Setting this option is not necessary because it is \texttt{TRUE} by default; setting \texttt{stack\ =\ FALSE} will return the results in a list rather than a tibble (try this for yourself to see!).

There are many other functions that work very much like \texttt{repeat\_and\_stack()}, including the base-R function \texttt{replicate()} and the now-deprecated function \texttt{rerun()} from \texttt{\{purrr\}}.
The functions in the \texttt{map()} family from \texttt{\{purrr\}} can also be used to do the same thing as \texttt{repeat\_and\_stack()}.
See Appendix \ref{more-repeating-oneself} for more discussion of these alternatives.

\section{One run at a time}\label{one-run-at-a-time}

A slightly different technique for running multiple replications of a process is to first write a function that executes a single run of the simulation, and then repeatedly evaluate that single function.
For instance, here is a function that stitches together the two steps in the bivariate-Poisson correlation simulation:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{one\_bivariate\_Poisson\_r }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(N, rho, mu1, mu2) \{}
\NormalTok{  dat }\OtherTok{\textless{}{-}} \FunctionTok{r\_bivariate\_Poisson}\NormalTok{( }\AttributeTok{N =}\NormalTok{ N, }\AttributeTok{rho =}\NormalTok{ rho, }\AttributeTok{mu1 =}\NormalTok{ mu1, }\AttributeTok{mu2 =}\NormalTok{ mu2 )}
\NormalTok{  res }\OtherTok{\textless{}{-}} \FunctionTok{r\_and\_z}\NormalTok{(dat)}
  \FunctionTok{return}\NormalTok{(res)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Calling the function produces a nicely formatted set of results:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{one\_bivariate\_Poisson\_r}\NormalTok{(}\AttributeTok{N =} \DecValTok{30}\NormalTok{, }\AttributeTok{rho =} \FloatTok{0.4}\NormalTok{, }\AttributeTok{mu1 =} \DecValTok{8}\NormalTok{, }\AttributeTok{mu2 =} \DecValTok{14}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           r         z     CI_lo     CI_hi
## 1 0.5713896 0.6495836 0.2658458 0.7726132
\end{verbatim}

We can then evaluate the function over and over using \texttt{repeat\_and\_stack()}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{repeat\_and\_stack}\NormalTok{(}
  \AttributeTok{n =} \DecValTok{5}\NormalTok{, }
  \FunctionTok{one\_bivariate\_Poisson\_r}\NormalTok{( }\AttributeTok{N =} \DecValTok{30}\NormalTok{, }\AttributeTok{rho =} \FloatTok{0.4}\NormalTok{, }\AttributeTok{mu1 =} \DecValTok{8}\NormalTok{, }\AttributeTok{mu2 =} \DecValTok{14}\NormalTok{ ), }
  \AttributeTok{id =} \StringTok{"rep"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   rep         r         z       CI_lo     CI_hi
## 1   1 0.6920971 0.8519697  0.44204893 0.8423369
## 2   2 0.4639223 0.5022978  0.12445393 0.7061652
## 3   3 0.5054484 0.5565974  0.17750188 0.7323572
## 4   4 0.3102351 0.3208056 -0.05632999 0.6030973
## 5   5 0.2699476 0.2768073 -0.10005206 0.5743583
\end{verbatim}

This technique of wrapping the data-generating function and estimation function inside of another function might strike you as a bit cumbersome because the wrapper is only two lines of code and writing it requires repeating many of the function argument names when calling the data-generating function (\texttt{N\ =\ N,\ rho\ =\ rho}, etc.).
However, the wrapper technique can be useful for more complicated simulations, such as those that involve comparison of multiple estimation methods.

Consider the cluster-randomized experiment case study presented in Section \ref{case-cluster} and \ref{multiple-estimation-procedures}.
In this simulation, we are interested in comparing the performance of three different estimation methods: a multi-level model, a linear regression with clustered standard errors, and a linear regression on the data aggregated to the school level.
A single replication of the simulation entails generating a dataset and then applying three different estimation functions to it.
Here is a function that takes our simulation parameters and runs a single trial of the full process:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{one\_run }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{( }
  \AttributeTok{n\_bar =} \DecValTok{30}\NormalTok{, }\AttributeTok{J =} \DecValTok{20}\NormalTok{, }\AttributeTok{gamma\_1 =} \FloatTok{0.3}\NormalTok{, }\AttributeTok{gamma\_2 =} \FloatTok{0.5}\NormalTok{,}
  \AttributeTok{sigma2\_u =} \FloatTok{0.20}\NormalTok{, }\AttributeTok{sigma2\_e =} \FloatTok{0.80}\NormalTok{, }\AttributeTok{alpha =} \FloatTok{0.75} 
\NormalTok{) \{}
  
\NormalTok{  dat }\OtherTok{\textless{}{-}} \FunctionTok{gen\_cluster\_RCT}\NormalTok{(}
    \AttributeTok{n\_bar =}\NormalTok{ n\_bar, }\AttributeTok{J =}\NormalTok{ J, }\AttributeTok{gamma\_1 =}\NormalTok{ gamma\_1, }\AttributeTok{gamma\_2 =}\NormalTok{ gamma\_2,}
    \AttributeTok{sigma2\_u =}\NormalTok{ sigma2\_u, }\AttributeTok{sigma2\_e =}\NormalTok{ sigma2\_e, }\AttributeTok{alpha =}\NormalTok{ alpha }
\NormalTok{  )}
\NormalTok{  MLM }\OtherTok{\textless{}{-}} \FunctionTok{analysis\_MLM}\NormalTok{( dat )}
\NormalTok{  LR }\OtherTok{\textless{}{-}} \FunctionTok{analysis\_OLS}\NormalTok{( dat )}
\NormalTok{  Agg }\OtherTok{\textless{}{-}} \FunctionTok{analysis\_agg}\NormalTok{( dat )}
  
  \FunctionTok{bind\_rows}\NormalTok{( }\AttributeTok{MLM =}\NormalTok{ MLM, }\AttributeTok{LR =}\NormalTok{ LR, }\AttributeTok{Agg =}\NormalTok{ Agg, }\AttributeTok{.id =} \StringTok{"method"}\NormalTok{ )}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

We have added a bunch of defaults to our function, so that we can run it without having to remember all the various input parameters.
When we call the function, we get a nicely structured table of results:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{one\_run}\NormalTok{( }\AttributeTok{n\_bar =} \DecValTok{30}\NormalTok{, }\AttributeTok{J =} \DecValTok{20}\NormalTok{, }\AttributeTok{alpha=}\FloatTok{0.5}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 3 x 4
##   method ATE_hat SE_hat p_value
##   <chr>    <dbl>  <dbl>   <dbl>
## 1 MLM      0.506  0.255  0.0630
## 2 LR       0.474  0.261  0.0881
## 3 Agg      0.511  0.255  0.0609
\end{verbatim}

We organize the output in a tibble to make it easier to do subsequent data processing and analysis.
The results for each method are organized in separate lines.
For each method, we record the impact estimate, its estimated standard error, and a nominal \(p\)-value.
Note how the \texttt{bind\_rows()} method can take naming on the fly, and give us a column of \texttt{method}, which will be very useful for keeping track of which results come from which estimation.

Once we have a function to execute a single run, we can produce multiple results using \texttt{repeat\_and\_stack()}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{R }\OtherTok{\textless{}{-}} \DecValTok{1000}
\NormalTok{ATE }\OtherTok{\textless{}{-}} \FloatTok{0.30}
\NormalTok{runs }\OtherTok{\textless{}{-}} \FunctionTok{repeat\_and\_stack}\NormalTok{(R, }
                         \FunctionTok{one\_run}\NormalTok{( }\AttributeTok{n\_bar =} \DecValTok{30}\NormalTok{, }\AttributeTok{J=}\DecValTok{20}\NormalTok{, }\AttributeTok{gamma\_1 =}\NormalTok{ ATE ),}
                         \AttributeTok{id =} \StringTok{"runID"}\NormalTok{) }
\FunctionTok{saveRDS}\NormalTok{( runs, }\AttributeTok{file =} \StringTok{"results/cluster\_RCT\_simulation.rds"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

Setting \texttt{id\ =\ "runID"} lets us keep track of which iteration number produced which result.
Once our simulation is complete, we save our results to a file so that we can avoid having to re-run the full simulation if we want to explore the results in some future work session.

We now have results for each of our estimation methods applied to each of 1000 generated datasets.
The next step is to evaluate how well the estimators did.
For example, we will want to examine questions about bias, precision, and accuracy of the three point estimators.
In Chapter \ref{performance-criteria}, we will look systematically at ways to quantify the performance of estimation methods.

\subsection{Reparameterizing}\label{one-run-reparameterization}

In Section \ref{DGP-standardization}, we discussed how to index the DGP of the cluster-randomized experiment using an intra-class correlation (ICC) instead of using two separate variance components.
This type of re-parameterization can be handled as part of writing a wrapper function for executing the DGP and estimation procedures.
Here is a revised version of \texttt{one\_run()}, which also renames some of the more obscure model parameters using terms that are easier to interpret:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{one\_run }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{( }
  \AttributeTok{n\_bar =} \DecValTok{30}\NormalTok{, }\AttributeTok{J =} \DecValTok{20}\NormalTok{, }\AttributeTok{ATE =} \FloatTok{0.3}\NormalTok{, }\AttributeTok{size\_coef =} \FloatTok{0.5}\NormalTok{,}
  \AttributeTok{ICC =} \FloatTok{0.4}\NormalTok{, }\AttributeTok{alpha =} \FloatTok{0.75} 
\NormalTok{) \{}
  \FunctionTok{stopifnot}\NormalTok{( ICC }\SpecialCharTok{\textgreater{}=} \DecValTok{0} \SpecialCharTok{\&\&}\NormalTok{ ICC }\SpecialCharTok{\textless{}} \DecValTok{1}\NormalTok{ )}

\NormalTok{  dat }\OtherTok{\textless{}{-}} \FunctionTok{gen\_cluster\_RCT}\NormalTok{( }
    \AttributeTok{n\_bar =}\NormalTok{ n\_bar, }\AttributeTok{J=}\NormalTok{J, }\AttributeTok{gamma\_1 =}\NormalTok{ ATE, }\AttributeTok{gamma\_2 =}\NormalTok{ size\_coef,}
    \AttributeTok{sigma2\_u =}\NormalTok{ ICC, }\AttributeTok{sigma2\_e =} \DecValTok{1}\SpecialCharTok{{-}}\NormalTok{ICC, }\AttributeTok{alpha =}\NormalTok{ alpha }
\NormalTok{  )}
  
\NormalTok{  MLM }\OtherTok{=} \FunctionTok{analysis\_MLM}\NormalTok{( dat )}
\NormalTok{  LR }\OtherTok{=} \FunctionTok{analysis\_OLS}\NormalTok{( dat )}
\NormalTok{  Agg }\OtherTok{=} \FunctionTok{analysis\_agg}\NormalTok{( dat )}
  
  \FunctionTok{bind\_rows}\NormalTok{( }\AttributeTok{MLM =}\NormalTok{ MLM, }\AttributeTok{LR =}\NormalTok{ LR, }\AttributeTok{Agg =}\NormalTok{ Agg, }\AttributeTok{.id =} \StringTok{"method"}\NormalTok{ )}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Note the \texttt{stopifnot}: it is wise to ensure our parameter transforms are all reasonable, so we do not get unexplained errors or strange results later on.
It is best if your code fails as soon as possible! Otherwise debugging can be quite hard.

Controlling how we use the foundational elements such as our data generating code is a key technique for making the higher level simulations sensible and more easily interpretable.
In the revised \texttt{one\_run()} function, we transform the \texttt{ICC} input parameter into the parameters used by \texttt{gen\_cluster\_RCT()} so as to maintain the effect size interpretation of our simulation.
We have not modified \texttt{gen\_cluster\_RCT()} at all: instead, we specify the parameters of the DGP function in terms of the parameters we want to directly control in the simulation.
Here we have put our entire simulation into effect size units, and are now providing ``knobs'' to the simulation that are directly interpretable.

\section{\texorpdfstring{Bundling simulations with \texttt{simhelpers}}{Bundling simulations with simhelpers}}\label{bundle-sim-demo}

The techniques that we have demonstrated for repeating a set of calculations each involve a very specific coding pattern, which will often have the same structure even if the details of the data-generating model or the names of the input parameters are very different from the examples we have presented.
The \texttt{simhelpers} package provides a function \texttt{bundle\_sim()} that abstracts this common pattern and allows you to automatically stitch together (or ``bundle'') a DGP function and an estimation function, so that they can be run once or multiple times.
Thus, \texttt{bundle\_sim()} provides a convenient alternative to writing your own \texttt{one\_run()} function for each simulation, thereby saving a bit of typing (and avoiding an opportunity for bugs to creep into your code).

\texttt{bundle\_sim()} takes a DGP function and an estimation function as inputs and gives us back a new function that will run a simulation using whatever parameters we give it.
Here is a basic example, which creates a function for simulating Pearson correlation coefficients with a bivariate Poisson distribution:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sim\_r\_Poisson }\OtherTok{\textless{}{-}} \FunctionTok{bundle\_sim}\NormalTok{(}\AttributeTok{f\_generate =}\NormalTok{ r\_bivariate\_Poisson, }
                            \AttributeTok{f\_analyze =}\NormalTok{ r\_and\_z, }
                            \AttributeTok{id =} \StringTok{"rep"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

If we specify the optional argument \texttt{id\ =\ "rep"}, the function will include a variable called \texttt{rep} with a unique identifier for each replication of the simulation process.
We can use the newly created function like so:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sim\_r\_Poisson}\NormalTok{( }\DecValTok{4}\NormalTok{, }\AttributeTok{N =} \DecValTok{30}\NormalTok{, }\AttributeTok{rho =} \FloatTok{0.4}\NormalTok{, }\AttributeTok{mu1 =} \DecValTok{8}\NormalTok{, }\AttributeTok{mu2 =} \DecValTok{14}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   rep         r         z      CI_lo     CI_hi
## 1   1 0.4584003 0.4952841  0.1175430 0.7026316
## 2   2 0.2136916 0.2170364 -0.1588034 0.5329322
## 3   3 0.4675940 0.5069866  0.1290674 0.7085081
## 4   4 0.6562349 0.7861718  0.3876033 0.8221341
\end{verbatim}

To create this simulation function, \texttt{bundle\_sim()} examined \texttt{r\_bivariate\_Poisson()}, figured out what its input arguments are, and made sure that the simulation function includes the same input arguments.
You can see the full set of arguments for \texttt{sim\_r\_Poisson()} by evaluating it with \texttt{args()}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{args}\NormalTok{(sim\_r\_Poisson)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## function (reps, N, mu1, mu2, rho = 0, seed = NA_integer_) 
## NULL
\end{verbatim}

In addition to the expected arguments from \texttt{r\_bivariate\_Poisson()}, the function has some additional inputs.
Its first argument is \texttt{reps}, which controls the number of times that the simulation process will be evaluated.
Its last argument is \texttt{seed}, which we will discuss in Section \ref{seeds-and-pseudo-RNGs}.

The \texttt{bundle\_sim()} function requires specifying a DGP function and a \emph{single} estimation function, with the data as the first argument.
For our cluster-randomized experiment example, we would then need to use our \texttt{estimate\_Tx\_Fx()} function that organizes all of the estimators (see Chapter \ref{multiple-estimation-procedures}).
We then use \texttt{bundle\_sim()} to create a function for running an entire simulation:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sim\_cluster\_RCT }\OtherTok{\textless{}{-}} \FunctionTok{bundle\_sim}\NormalTok{( gen\_cluster\_RCT, estimate\_Tx\_Fx, }\AttributeTok{id =} \StringTok{"runID"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

We can call the newly created function like so:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sim\_cluster\_RCT}\NormalTok{( }\DecValTok{2}\NormalTok{, }
                 \AttributeTok{n\_bar =} \DecValTok{30}\NormalTok{, }\AttributeTok{J =} \DecValTok{20}\NormalTok{, }\AttributeTok{gamma\_1 =}\NormalTok{ ATE, }
                 \AttributeTok{sigma2\_u =} \FloatTok{0.3}\NormalTok{, }\AttributeTok{sigma2\_e =} \FloatTok{0.7}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   runID estimator   ATE_hat    SE_hat     p_value
## 1     1       MLM 1.0718041 0.3464600 0.009834928
## 2     1       OLS 0.8725333 0.3581403 0.041659400
## 3     1       agg 1.1574631 0.4486484 0.018880161
## 4     2       MLM 0.3975797 0.2379827 0.111564711
## 5     2       OLS 0.4656465 0.2686438 0.111156240
## 6     2       agg 0.3142312 0.2140596 0.159372293
\end{verbatim}

Again, \texttt{bundle\_sim()} produces a function with input names that exactly match the inputs of the DGP function that we give it.
It is not possible to re-parameterize or change argument names, as we did with \texttt{one\_run()} in Section \ref{one-run-reparameterization}.
See Exercise \ref{reparameterization-redux} for further discussion of this limitation.

To use the simulation function in practice, we call it by specifying the number of replications desired (which we have stored in \texttt{R}) and any relevant input parameters.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{runs }\OtherTok{\textless{}{-}} \FunctionTok{sim\_cluster\_RCT}\NormalTok{( R, }
                         \AttributeTok{n\_bar =} \DecValTok{30}\NormalTok{, }\AttributeTok{J =} \DecValTok{20}\NormalTok{, }\AttributeTok{gamma\_1 =}\NormalTok{ ATE, }
                         \AttributeTok{sigma2\_u =} \FloatTok{0.3}\NormalTok{, }\AttributeTok{sigma2\_e =} \FloatTok{0.7}\NormalTok{ )}
\FunctionTok{saveRDS}\NormalTok{( runs, }\AttributeTok{file =} \StringTok{"results/cluster\_RCT\_simulation.rds"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

The \texttt{bundle\_sim()} function is just a convenient way to create a function that pieces together the steps in the simulation process, which is especially useful when the component functions include many input parameters.
The function has several further features, which we will demonstrate in subsequent chapters.

\section{Seeds and pseudo-random number generators}\label{seeds-and-pseudo-RNGs}

In prior chapters, we have used built-in functions to generate random numbers and also written our own data-generating functions that produce artificial data following a specific random process.
With either type of function, re-running it with the exact same input parameters will produce different results.
For instance, running the \texttt{rchisq} function with the same set of inputs will produce two different sequences of \(\chi^2\) random variables:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{c1 }\OtherTok{\textless{}{-}} \FunctionTok{rchisq}\NormalTok{(}\DecValTok{4}\NormalTok{, }\AttributeTok{df =} \DecValTok{3}\NormalTok{)}
\NormalTok{c2 }\OtherTok{\textless{}{-}} \FunctionTok{rchisq}\NormalTok{(}\DecValTok{4}\NormalTok{, }\AttributeTok{df =} \DecValTok{3}\NormalTok{)}
\FunctionTok{rbind}\NormalTok{(c1, c2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##        [,1]     [,2]     [,3]     [,4]
## c1 2.859183 2.925194 9.362208 2.502835
## c2 3.495920 5.326844 1.008615 2.359409
\end{verbatim}

If you run the same code as above, you will get different results from these.
Likewise, running the bivariate Poisson function from Section \ref{DGP-functions} or the \texttt{sim\_r\_Poisson()} function from Section \ref{bundle-sim-demo} multiple times will produce different datasets:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dat\_A }\OtherTok{\textless{}{-}} \FunctionTok{r\_bivariate\_Poisson}\NormalTok{(}\DecValTok{20}\NormalTok{, }\AttributeTok{rho =} \FloatTok{0.5}\NormalTok{, }\AttributeTok{mu1 =} \DecValTok{4}\NormalTok{, }\AttributeTok{mu2 =} \DecValTok{7}\NormalTok{)}
\NormalTok{dat\_B }\OtherTok{\textless{}{-}} \FunctionTok{r\_bivariate\_Poisson}\NormalTok{(}\DecValTok{20}\NormalTok{, }\AttributeTok{rho =} \FloatTok{0.5}\NormalTok{, }\AttributeTok{mu1 =} \DecValTok{4}\NormalTok{, }\AttributeTok{mu2 =} \DecValTok{7}\NormalTok{)}
\FunctionTok{identical}\NormalTok{(dat\_A, dat\_B)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] FALSE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sim\_A }\OtherTok{\textless{}{-}} \FunctionTok{sim\_r\_Poisson}\NormalTok{( }\DecValTok{10}\NormalTok{, }\AttributeTok{N =} \DecValTok{30}\NormalTok{, }\AttributeTok{rho =} \FloatTok{0.4}\NormalTok{, }\AttributeTok{mu1 =} \DecValTok{8}\NormalTok{, }\AttributeTok{mu2 =} \DecValTok{14}\NormalTok{)}
\NormalTok{sim\_B }\OtherTok{\textless{}{-}} \FunctionTok{sim\_r\_Poisson}\NormalTok{( }\DecValTok{10}\NormalTok{, }\AttributeTok{N =} \DecValTok{30}\NormalTok{, }\AttributeTok{rho =} \FloatTok{0.4}\NormalTok{, }\AttributeTok{mu1 =} \DecValTok{8}\NormalTok{, }\AttributeTok{mu2 =} \DecValTok{14}\NormalTok{)}
\FunctionTok{identical}\NormalTok{(sim\_A, sim\_B)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] FALSE
\end{verbatim}

Of course, this is the intended behavior of these functions, but it has an important consequence that needs some care and attention.
Using functions like \texttt{rchisq()}, \texttt{r\_bivariate\_Poisson()}, or \texttt{r\_bivariate\_Poisson()} in a simulation study means that the results will not be fully reproducible.

When using DGP functions for simulations, it is useful to be able to exactly control the process of generating random numbers.
This is much more feasible than it sounds: Monte Carlo simulations are random, at least in theory, but computers are deterministic.
When we use R to generate what we have been referring to as ``random numbers,'' the functions produce what are actually \emph{pseudo-random} numbers.
Pseudo-random numbers are generated from chains of mathematical equations designed to produce sequences of numbers that appear random, but actually follow a deterministic sequence.
Each subsequent random number is a calculated by starting from the previously generated value (i.e., the current state of the random number generator), applying a complicated function, and storing the result (i.e., updating the state).
The numbers returned by the generator form a chain that, ideally, cycles through an extremely long list of values in a way that looks stochastic and unpredictable.

The state of the pseudo-random number generator is shared across different functions that produce pseudo-random numbers, so it does not matter if we are generating numbers with \texttt{rnorm()} or \texttt{rchisq()} or \texttt{r\_bivariate\_Poisson()}.
Each time we ask for a random number from the generator, its state is updated.
Functions like \texttt{rnorm()} and \texttt{rchisq()} all call the low-level generator and then transform the result to be of the correct distribution.

Because the generator is actually deterministic, we can control its output by specify a starting value or initial state,
In R, the state of the random number generator can be controlled by setting what its known as the seed.
The \texttt{set.seed()} function allows us to specify a seed value, so that we can exactly reproduce a calculation.
For example,

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{6}\NormalTok{)}
\NormalTok{c1 }\OtherTok{\textless{}{-}} \FunctionTok{rchisq}\NormalTok{(}\DecValTok{4}\NormalTok{, }\AttributeTok{df =} \DecValTok{3}\NormalTok{)}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{6}\NormalTok{)}
\NormalTok{c2 }\OtherTok{\textless{}{-}} \FunctionTok{rchisq}\NormalTok{(}\DecValTok{4}\NormalTok{, }\AttributeTok{df =} \DecValTok{3}\NormalTok{)}
\FunctionTok{rbind}\NormalTok{(c1, c2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##        [,1]    [,2]     [,3]     [,4]
## c1 2.575556 0.93847 8.062264 3.685932
## c2 2.575556 0.93847 8.062264 3.685932
\end{verbatim}

Similarly, we can set the seed and run a series of calculations involving multiple functions that make use of the random number generator:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# First time}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{6}\NormalTok{)}
\NormalTok{c1 }\OtherTok{\textless{}{-}} \FunctionTok{rchisq}\NormalTok{(}\DecValTok{4}\NormalTok{, }\AttributeTok{df =} \DecValTok{3}\NormalTok{)}
\NormalTok{dat\_A }\OtherTok{\textless{}{-}} \FunctionTok{r\_bivariate\_Poisson}\NormalTok{(}\DecValTok{20}\NormalTok{, }\AttributeTok{rho =} \FloatTok{0.5}\NormalTok{, }\AttributeTok{mu1 =} \DecValTok{4}\NormalTok{, }\AttributeTok{mu2 =} \DecValTok{7}\NormalTok{)}

\CommentTok{\# Exactly reproduce the calculations}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{6}\NormalTok{)}
\NormalTok{c2 }\OtherTok{\textless{}{-}} \FunctionTok{rchisq}\NormalTok{(}\DecValTok{4}\NormalTok{, }\AttributeTok{df =} \DecValTok{3}\NormalTok{)}
\NormalTok{dat\_B }\OtherTok{\textless{}{-}} \FunctionTok{r\_bivariate\_Poisson}\NormalTok{(}\DecValTok{20}\NormalTok{, }\AttributeTok{rho =} \FloatTok{0.5}\NormalTok{, }\AttributeTok{mu1 =} \DecValTok{4}\NormalTok{, }\AttributeTok{mu2 =} \DecValTok{7}\NormalTok{)}

\FunctionTok{bind\_rows}\NormalTok{(}\AttributeTok{A =} \FunctionTok{r\_and\_z}\NormalTok{(dat\_A), }\AttributeTok{B =} \FunctionTok{r\_and\_z}\NormalTok{(dat\_B), }\AttributeTok{.id =} \StringTok{"Rep"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   Rep         r         z      CI_lo     CI_hi
## 1   A 0.2990194 0.3084424 -0.1653856 0.6548844
## 2   B 0.2990194 0.3084424 -0.1653856 0.6548844
\end{verbatim}

The \texttt{bundle\_sim()} function demonstrated in Section \ref{bundle-sim-demo} creates a function for repeating the process of generating and analyzing data.
By default, the function it produces includes an argument \texttt{seed}, which allows the user to set a seed value before repeatedly evaluating the DGP function and estimation function.
By default, the \texttt{seed} argument is \texttt{NULL} and so the current seed is not modified.
Specifying an integer-valued seed will make the results exactly reproducible:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sim\_A }\OtherTok{\textless{}{-}} \FunctionTok{sim\_r\_Poisson}\NormalTok{( }\DecValTok{10}\NormalTok{, }\AttributeTok{N =} \DecValTok{30}\NormalTok{, }\AttributeTok{rho =} \FloatTok{0.4}\NormalTok{, }\AttributeTok{mu1 =} \DecValTok{8}\NormalTok{, }\AttributeTok{mu2 =} \DecValTok{14}\NormalTok{,}
                        \AttributeTok{seed =} \DecValTok{25}\NormalTok{)}
\NormalTok{sim\_B }\OtherTok{\textless{}{-}} \FunctionTok{sim\_r\_Poisson}\NormalTok{( }\DecValTok{10}\NormalTok{, }\AttributeTok{N =} \DecValTok{30}\NormalTok{, }\AttributeTok{rho =} \FloatTok{0.4}\NormalTok{, }\AttributeTok{mu1 =} \DecValTok{8}\NormalTok{, }\AttributeTok{mu2 =} \DecValTok{14}\NormalTok{,}
                        \AttributeTok{seed =} \DecValTok{25}\NormalTok{)}
\FunctionTok{identical}\NormalTok{(sim\_A, sim\_B)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] TRUE
\end{verbatim}

In practice, it is a good idea to always set seed values for your simulations, so that you (or someone else) can exactly reproduce the results.
Attending to reproducibility allows us to easily check if we are running the same code that generated a set of results.
For instance, try running the previous blocks of code on your machine; if you set the seed to the same value as we did, you should get identical output.

Setting seeds is also very helpful for debugging.
Suppose we had an error that showed up in one of a thousand replications, causing the simulation to crash sometimes.
If we set a seed and find that the code crashes, we can debug and then rerun the simulation.
If it now runs without error, we know we fixed the problem.
If we had not set the seed, we would not know if we were just getting (un)lucky, and avoiding the error by chance.

\section{Exercises}\label{exercises-5}

\subsection{Welch simulations}\label{Welch-simulation}

In the prior chapter's exercises, you made a new \texttt{BF\_F} function for the Welch simulation. Now incorporate the \texttt{BF\_F} function into the \texttt{one\_run()} function, and use your revised function to generate simulation results for this additional estimator.

\subsection{Compare sampling distributions of Pearson's correlation coefficients}\label{Pearson-sampling-distributions}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Use \texttt{sim\_r\_Poisson()} to generate 5000 replications of the Fisher-z-transformed correlation coefficient under a bivariate Poisson distribution with \(\rho = 0.7\) for a sample of \(N = 40\) observations. You pick the remaining parameters.
\item
  Create a bundled simulation function by combining the data-generating function from Exercise \ref{BVNB1} or \ref{BVNB2} with the \texttt{r\_and\_z()} estimation function. Run the function to generate 5000 replications of the Fisher-z-transformed correlation coefficient under a bivariate negative binomial distribution, with the same parameter values as above.
\item
  Create a plot that shows both sampling distributions, making it easy to compare the distributions.
\end{enumerate}

\subsection{Reparameterization, redux}\label{reparameterization-redux}

In Section \ref{one-run-reparameterization}, we illustrated how the \texttt{one\_run()} simulation wrapper function could be tweaked in order to reparameterize the model in terms of a single intra-class correlation rather than two variance parameters.
But what if you want to avoid having to write your own simulation wrapper and instead use \texttt{bundle\_sim()}?
Revise \texttt{gen\_cluster\_RCT()} to accomplish the reparameterization, then bundle it with \texttt{analyze\_data()}.

\subsection{Fancy clustered RCT simulations}\label{fancy-cluster-RCT-sims}

In Exercise \ref{cluster-RCT-baseline}, your task was to write a data-generating function for a cluster-randomized trial that includes a baseline covariate.
Then in Exercise \ref{CRT-ANCOVA-estimators}, your task was to create an estimation function that could be applied to data from such a trial.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Now, using your work from these exercises, create a bundled simulation function that combines the data-generating function and the estimation function. Ensure that the estimation function returns average treatment effect estimates both with and without controlling for the baseline covariate.
\item
  Use the resulting function to simulate results from 1000 replications of a cluster-randomized trial, each analyzed two ways (with and without covariate adjustment).
\item
  Create a plot that shows the sampling distribution of each average treatment effect estimator in a way that makes it easy to compare the distributions.
\end{enumerate}

\chapter{Performance metrics}\label{performance-criteria}

Once we run a simulation, we end up with a pile of results to sort through.
For example, Figure \ref{fig:CRT-ATE-hist} depicts the distribution of average treatment effect estimates from the cluster-randomized experiment simulation, which we generated in Chapter \ref{running-the-simulation-process}.
There are three different estimators, each with 1000 replications.
Each histogram is an approximation of the \emph{sampling distribution} of the estimator, meaning its distribution across repetitions of the data-generating process.
With results such as these, the question before us is now how to evaluate how well these procedures worked. And, if we are comparing several different estimators, how do we determine which ones work better or worse than others? In this chapter, we look at a variety of \textbf{performance metrics} that can answer these questions.

\begin{figure}

{\centering \includegraphics[width=0.75\linewidth]{Designing-Simulations-in-R_files/figure-latex/CRT-ATE-hist-1} 

}

\caption{Sampling distribution of average treatment effect estimates from a cluster-randomized trial with a true average treatment effect of 0.3.}\label{fig:CRT-ATE-hist}
\end{figure}

Performance metrics are summaries of a sampling distribution that describe how an estimator or data analysis procedure behaves on average if we could repeat the data-generating process an infinite number of times.
For example, the bias of an estimator is the difference between the average value of the estimator and the corresponding target parameter.
Bias measures the central tendency of the sampling distribution, capturing how far off, on average, the estimator would be from the true parameter value if we repeated the data-generating process an infinite number of times.
In Figure \ref{fig:CRT-ATE-hist}, black dashed lines mark the true average treatment effect of 0.3 and the colored vertical lines with circles at the end mark the means of the estimators.
The distance between the colored lines and the black dashed lines corresponds to the bias of the estimator.
This distance is nearly zero for the aggregation estimator and the multilevel model estimator, but larger for the linear regression estimator.

Different types of data-analysis results produce different types of information, and so the relevant set of performance metrics depends on the type of data analysis result under evaluation.
For procedures that produce point estimates or point predictions, conventional performance metrics include bias, variance, and root mean squared error.
If the point estimates come with corresponding standard errors, then we may also want to evaluate how accurately the standard errors represent the true uncertainty of the point estimators; conventional performance metrics for capturing this include the relative bias and relative root mean squared error of the variance estimator.
For procedures that produce confidence intervals or other types of interval estimates, conventional performance metrics include the coverage rate and average interval width.
Finally, for inferential procedures that involve hypothesis tests (or more generally, classification tasks), conventional performance metrics include Type I error rates and power.
We describe each of these metrics in Sections \ref{assessing-point-estimators} through \ref{assessing-inferential-procedures}.

Performance metrics are defined with respect to sampling distributions, or the results of applying a data analysis procedure to data generated according to a particular process across an infinite number of replications.
In defining specific metrics, we will use conventional statistical notation for the means, variances, and other moments of the sampling distribution.
Specifically, we will use the expectation operator \(\E()\) to denote the mean of a sampling distribution, \(\M()\) to denote the median of a sampling distribution, \(\Var()\) to denote the variance of a sampling distribution, and \(\Prob()\) to denote probabilities of specific outcomes with respect to the sampling distribution.
We will use \(\Q_p()\) to denote the \(p^{th}\) quantile of a distribution, which is the value \(x\) such that \(\Prob(T \leq x) = p\). With this notation, the median is equivalent to \(\M() = \Q_{0.5}()\).

For some simple combinations of data-generating processes and data analysis procedures, it may be possible to derive exact mathematical formulas for calculating some performance metrics (such as exact mathematical expressions for the bias and variance of the linear regression estimator).\\
But for many problems, the math is difficult or intractable---that's why we do simulations in the first place.
Simulations do not produce the \emph{exact} sampling distribution or give us \emph{exact} values of performance metrics.
Instead, simulations yield \emph{samples}---usually large samples---from the the sampling distribution, and we can use these to compute \emph{estimates} of the performance metrics of interest.
In Figure \ref{fig:CRT-ATE-hist}, we calculated the bias of each estimator by taking the mean of 1000 observations from its sampling distribution; if we were to repeat the whole set of calculations (with a different seed), then our bias results would shift slightly.

In working with simulation results, it is important to keep track of the degree of uncertainty in performance metric estimates.
We call such uncertainty \emph{Monte Carlo error} because it is the error arising from using a finite number of replications of the Monte Carlo simulation process.
One way to quantify it is with the \emph{Monte Carlo standard error (MCSE)}, or the standard error of a performance estimate based on a finite number of replications.
Just as when we analyze real data, we can apply statistical techniques to estimate the MCSE and even to generate confidence intervals for performance metrics.

The size of an MCSE is driven by how many replications we use: if we only use a few, we will have noisy estimates of performance with large MCSEs; if we use millions of replications, the MCSE will be tiny.
It is important to keep in mind that the MCSE is not measuring anything about how a data analysis procedure performs in general.
It only describes how precisely we have approximated a performance criterion, an artifact of how we conducted the simulation.
Moreover, MCSEs are under our control.
Given a desired MCSE, we can determine how many replications we would need to ensure our performance estimates have the specified level of precision.
Section \ref{MCSE} provides details about how to compute MCSEs for conventional performance measures, along with some discussion of general techniques for computing MCSE for less conventional measures.

\section{Metrics for Point Estimators}\label{assessing-point-estimators}

The most common performance measures used to assess a point estimator are bias, variance, mean squared error, and root mean squared error.
Bias compares the mean of the sampling distribution to the target parameter.
Positive bias implies that the estimator tends to systematically over-state the quantity of interest, while negative bias implies that it systematically under-shoots the quantity of interest.
If bias is zero (or nearly zero), we say that the estimator is unbiased (or approximately unbiased).
Variance (or its square root, the true standard error) describes the spread of the sampling distribution, or the extent to which it varies around its central tendency.
All else equal, we would like estimators to have low variance (or to be more precise).
Root mean squared error (RMSE) is a conventional measure of the overall accuracy of an estimator, or its average degree of error with respect to the target parameter.
For absolute assessments of performance, an estimator with low bias, low variance, and thus low RMSE is desired.
In making comparisons of several different estimators, one with lower RMSE is usually preferable to one with higher RMSE.
If two estimators have comparable RMSE, then the estimator with lower bias would usually be preferable.

To define these quantities more precisely, let's consider a generic estimator \(T\) that is targeting a parameter \(\theta\).
We call the target parameter the \emph{estimand}.
In most cases, in running our simulation we set the estimand \(\theta\) and then generate a (typically large) series of \(R\) datasets, for each of which \(\theta\) is the true target parameter.
We then analyze each dataset, obtaining a sample of estimates \(T_1,...,T_R\).
Formally, the bias, variance, and RMSE of \(T\) are defined as
\[
\begin{aligned}
\Bias(T) &= \E(T) - \theta, \\ 
\Var(T) &= \E\left[\left(T - \E (T)\right)^2 \right], \\
\RMSE(T) &= \sqrt{\E\left[\left(T - \theta\right)^2 \right]}.
\end{aligned}
\label{eq:bias-variance-RMSE}
\]
These three measures are inter-connected.
In particular, RMSE is the combination of (squared) bias and variance, as in
\[ 
\left[\RMSE(T)\right]^2 = \left[\Bias(T)\right]^2 + \Var(T). 
\label{eq:RMSE-decomposition}
\]

When conducting a simulation, we do not compute these performance measures directly but rather must estimate them using the replicates \(T_1,...,T_R\) generated from the sampling distribution.
There's nothing very surprising about how we construct estimates of the performance measures.
It is just a matter of substituting sample quantities in place of the expectations and variances.
Specifically, we estimate bias by taking
\[
\widehat{\Bias}(T) = \bar{T} - \theta, 
\label{eq:bias-estimator}
\]
where \(\bar{T}\) is the arithmetic mean of the replicates,
\[ 
\bar{T} = \frac{1}{R}\sum_{r=1}^R T_r.
\]
We estimate variance by taking the sample variance of the replicates, as
\[
S_T^2 = \frac{1}{R - 1}\sum_{r=1}^R \left(T_r - \bar{T}\right)^2. 
\label{eq:var-estimator}
\]
The square root of \(S^2_T\), \(S_T\) is an estimate of the true standard error of \(T\), or the standard deviation of the estimator across an infinite set of replications of the data-generating process.\footnote{Generally, when people say ``Standard Error'' they actually mean \emph{estimated} Standard Error, (\(\widehat{SE}\)), as we would calculate in a real data analysis (where we have only a single realization of the data-generating process). It is easy to forget that this standard error is itself an estimate of a true parameter, and thus has its own uncertainty.}
We usually prefer to work with the true SE \(S_T\) rather than the sampling variance \(S_T^2\) because the former quantity has the same units as the target parameter.

Finally, the RMSE estimate can be calculated as
\[ 
\widehat{\RMSE}(T) = \sqrt{\frac{1}{R} \sum_{r = 1}^R \left( T_r - \theta\right)^2 }.  
\label{eq:rmse-estimator}
\]
Often, people talk about the MSE (Mean Squared Error), which is just the square of RMSE.
Just like the true SE is usually easier to interpret than the sampling variance, units of RMSE are easier to interpret than the units of MSE.

It is important to recognize that the above performance measures depend on the scale of the parameter.
For example, if our estimators are measuring a treatment impact in dollars, then the bias, SE, and RMSE of the estimators are all in dollars.
(The variance and MSE would be in dollars squared, which is why we take their square roots to put them back on the more intepretable scale of dollars.)

In many simulations, the scale of the outcome is an arbitrary feature of the data-generating process, making the absolute magnitude of performance metrics less meaningful.
To ease interpretation of performance metrics, it is useful to consider their magnitude relative to the baseline level of variation in the outcome.
One way to achieve this is to generate data so the outcome has unit variance (i.e., we generate outcomes in \emph{standardized units}).
Doing so puts the bias, true standard error, and root mean-squared error on the scale of standard deviation units, which can facilitate interpretation about what constitutes a meaningfully large bias or a meaningful difference in RMSE.

In addition to understanding the scale of these performance metrics, it is also important to recognize that their magnitude depends on the scale of the parameter.
A non-linear transformation of a parameter will generally lead to changes in the magnitude of the performance metrics.
For instance, suppose that \(\theta\) measures the proportion of time that something occurs.
One natural way to transform this parameter would be to put it on the log-odds (logit) scale.
However, because the log-odds transformation is non-linear,
\[
\text{Bias}\left[\text{logit}(T)\right] \neq \text{logit}\left(\text{Bias}[T]\right), \qquad \text{RMSE}\left[\text{logit}(T)\right] \neq \text{logit}\left(\text{RMSE}[T]\right),
\]
and so on.
This is a consequence of how the performance metrics are defined.
One might see this property as a limitation on the utility of using bias and RMSE to measure the performance of an estimator, because these metrics can be quite sensitive to the scale of the parameter.

\subsection{Comparing the Performance of the Cluster RCT Estimation Procedures}\label{clusterRCTperformance}

We now demonstrate the calculation of performance metrics for the point estimators of average treatment effects in the cluster-RCT example.
In Chapter \ref{running-the-simulation-process}, we generated a large set of replications of several different treatment effect estimators.
Using these results, we can assess the bias, standard error, and RMSE of three different estimators of the ATE.
These performance metrics address the following questions:

\begin{itemize}
\tightlist
\item
  Is the estimator systematically off? (bias)
\item
  Is it precise? (standard error)
\item
  Does it predict well? (RMSE)
\end{itemize}

Let us see how the three estimators compare on these metrics.

\subsubsection*{Are the estimators biased?}\label{are-the-estimators-biased}
\addcontentsline{toc}{subsubsection}{Are the estimators biased?}

Bias is defined with respect to a target estimand.
Here we assess whether our estimates are systematically different from the \(\gamma_1\) parameter, which we defined in standardized units by setting the standard deviation of the student-level distribution of the outcome equal to one.
For these data, we generated data based on a school-level ATE parameter of 0.30 SDs.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ATE }\OtherTok{\textless{}{-}} \FloatTok{0.30}

\NormalTok{runs }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{( method ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{( }
    \AttributeTok{mean\_ATE\_hat =} \FunctionTok{mean}\NormalTok{( ATE\_hat ),}
    \AttributeTok{bias =} \FunctionTok{mean}\NormalTok{( ATE\_hat ) }\SpecialCharTok{{-}}\NormalTok{ ATE}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 3 x 3
##   method mean_ATE_hat    bias
##   <chr>         <dbl>   <dbl>
## 1 Agg           0.306 0.00561
## 2 LR            0.390 0.0899 
## 3 MLM           0.308 0.00788
\end{verbatim}

There is no indication of major bias for aggregation or multi-level modeling.
Linear regression, with a bias of about 0.09 SDs, appears about ten times as biased as the other estimators.
This is because the linear regression is targeting the person-level average average treatment effect.
The data-generating process of this simulation makes larger sites have larger effects, so the person-level average effect is going to be higher because those larger sites will count more.
In contrast, our estimand is the school-level average treatment effect, or the simple average of each school's true impact, which we have set to 0.30.
The aggregation and multi-level modeling methods target this school-level average effect.
If we had instead decided that the target estimand should be the person-level average effect, then we would find that linear regression is unbiased whereas aggregation and multi-level modeling are biased.
This example illustrates how crucial it is to think carefully about the appropriate target parameter and to assess performance with respect to a well-justified and clearly articulated target.

\subsubsection*{Which method has the smallest standard error?}\label{which-method-has-the-smallest-standard-error}
\addcontentsline{toc}{subsubsection}{Which method has the smallest standard error?}

The true standard error measures the degree of variability in a point estimator.
It reflects how stable our estimates are across replications of the data-generating process.
We calculate the standard error by taking the standard deviation of the replications of each estimator.
For purposes of interpretation, it is useful to compare the true standard errors to the variation in a benchmark estimator.
Here, we treat the linear regression estimator as the benchmark and compute the magnitude of the true SEs of each method \emph{relative} to the SE of the linear regression estimator:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{true\_SE }\OtherTok{\textless{}{-}} 
\NormalTok{  runs }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{( method ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{( }\AttributeTok{SE =} \FunctionTok{sd}\NormalTok{( ATE\_hat ) ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{( }\AttributeTok{per\_SE =}\NormalTok{ SE }\SpecialCharTok{/}\NormalTok{ SE[method}\SpecialCharTok{==}\StringTok{"LR"}\NormalTok{] )}

\NormalTok{true\_SE}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 3 x 3
##   method    SE per_SE
##   <chr>  <dbl>  <dbl>
## 1 Agg    0.168  0.916
## 2 LR     0.183  1    
## 3 MLM    0.168  0.916
\end{verbatim}

In a real data analysis, these standard errors are what we would be trying to approximate with a standard error estimator.
Aggregation and multi-level modeling have SEs about 8\% smaller than Linear Regression.
For these data-generating conditions, aggregation and multi-level modeling are preferable to linear regression because they are more precise.

\subsubsection*{Which method has the smallest Root Mean Squared Error?}\label{which-method-has-the-smallest-root-mean-squared-error}
\addcontentsline{toc}{subsubsection}{Which method has the smallest Root Mean Squared Error?}

So far linear regression is not doing well: it has more bias and a larger standard error than the other two estimators.
We can assess overall accuracy by combining these two quantities with the RMSE:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{runs }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{( method ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{( }
    \AttributeTok{bias =} \FunctionTok{mean}\NormalTok{( ATE\_hat }\SpecialCharTok{{-}}\NormalTok{ ATE ),}
    \AttributeTok{SE =} \FunctionTok{sd}\NormalTok{( ATE\_hat ),}
    \AttributeTok{RMSE =} \FunctionTok{sqrt}\NormalTok{( }\FunctionTok{mean}\NormalTok{( (ATE\_hat }\SpecialCharTok{{-}}\NormalTok{ ATE)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{ ) )}
\NormalTok{  ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{( }
    \AttributeTok{per\_RMSE =}\NormalTok{ RMSE }\SpecialCharTok{/}\NormalTok{ RMSE[method}\SpecialCharTok{==}\StringTok{"LR"}\NormalTok{]}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 3 x 5
##   method    bias    SE  RMSE per_RMSE
##   <chr>    <dbl> <dbl> <dbl>    <dbl>
## 1 Agg    0.00561 0.168 0.168    0.823
## 2 LR     0.0899  0.183 0.204    1    
## 3 MLM    0.00788 0.168 0.168    0.823
\end{verbatim}

We also include SE and bias as points of reference.

RMSE takes into account both bias and variance.
For aggregation and multi-level modeling, the RMSE is the same as the standard error, which makes sense because these estimators are not biased.
For linear regression, the combination of bias plus increased variability yields a higher RMSE, with the standard error dominating the bias term (note how RMSE and SE are more similar than RMSE and bias).
The difference between the estimators are pronounced because RMSE is the square root of the \emph{squared} bias and \emph{squared} standard errors.
Overall, aggregation and multi-level modeling have RMSEs around 17\% smaller than linear regression---a consequential difference in accuracy.

\subsection{Less Conventional Performance metrics}\label{less-conventional-performance-metrics}

Depending on the model and estimation procedures being examined, a range of different metrics might be used to assess estimator performance.
For point estimation, we have introduced bias, variance and MSE as the three core measures of performance.
However, all of these measures are sensitive to outliers in the sampling distribution.
Consider an estimator that generally does well, except for an occasional large mistake. Because conventional measures are based on arithmetic averages, they will indicate that the estimator performs very poorly overall.
Other metrics exist, such as the median bias and the median absolute deviation of \(T\), which are less sensitive to outliers in the sampling distribution compared to the conventional metrics.

Median bias is an alternative measure of the central tendency of a sampling distribution.
Positive median bias implies that more than 50\% of the sampling distribution exceeds the quantity of interest, while negative median bias implies that more than 50\% of the sampling distribution fall below the quantity of interest.
Formally,
\[ 
\text{Median-Bias}(T) = \M(T) - \theta 
\label{eq:median-bias}.
\]
An estimator of median bias is computed using the sample median of \(T_1,...,T_R\).

Another robust measure of central tendency is based on winsorizing the sampling distribution, or truncating all errors larger than a certain maximum size.
Using a winsorized distribution amounts to arguing that you don't care about errors beyond a certain size, so anything beyond a certain threshold will be treated the same as if it were exactly on the threshold.
The threshold for truncation is usually defined relative to the first and third quartiles of the sampling distribution, along with a given span of the inter-quartile range.
The thresholds for truncation are taken as
\[
L_w = \Q_{0.25}(T) - w \times (\Q_{0.75}(T) - \Q_{0.25}(T)) \quad \text{and} \quad U_w = \Q_{0.75}(T) + w \times (\Q_{0.75}(T) - \Q_{0.25}(T)),
\]
where \(\Q_{0.25}(T)\) and \(\Q_{0.75}(T)\) are the first and third quartiles of the distribution of \(T\), respectively, and \(w\) is the number of inter-quartile ranges below which an observation will be treated as an outlier.\footnote{For a normally distributed sampling distribution, the interquartile range is 1.35 SD; with \(w = 2\), the lower and upper thresholds would then fall at \(\pm 3.37\) SD, or the \(0.04^{th}\) and \(99.96^{th}\) percentiles.
  Still assuming a normal sampling distribution, taking \(w = 2.5\) will mean that the thresholds fall at the \(0.003^{th}\) and \(99.997^{th}\) percentiles.}
Let \(T^{(w)} = \min\{\max\{T, L_w\}, U_w\}\).
The winsorized bias is then defined as
\[
\text{Bias}(T^{(w)}) = \E\left(T^{(w)}\right) - \theta. 
\label{eq:winsorized-bias}
\]
Alternative measures of spread and overall accuracy can be defined along similar lines, using winsorized values in place of the raw values of \(T\).
Specifically,
\[
\begin{aligned}
\Var\left(T^{(w)}\right) &= \E\left[\left(T^{(w)} - \E (T^{(w)})\right)^2 \right], \\
\RMSE\left(T^{(w)}\right) &= \sqrt{\E\left[\left(T^{(w)} - \theta\right)^2 \right]}.
\end{aligned}
\label{eq:winsorized-variance-RMSE}
\]

To compute estimates of the winsorized performance criteria, we substitute sample quantiles in place of \(\Q_{0.25}(T)\) and \(\Q_{0.25}(T)\) to get estimated thresholds, \(\hat{L}_w\) and \(\hat{U}_w\), find \(\hat{T}_r^{(w)} = \min\{\max\{T_r, \hat{L}_w\}, \hat{U}_w\}\), and compute the sample performance metrics using Equations \eqref{eq:bias-estimator}, \eqref{eq:var-estimator}, and \eqref{eq:rmse-estimator}, but with \(\hat{T}_r^{(w)}\) in place of \(T_r\).

A further robust measure of central tendency used the \(p \times 100\%\)-trimmed mean, which ignores the estimates in the lowest and highest \(p\)-quantiles of the sampling distribution.
Formally, the trimmed-mean bias is
\[
\text{Trimmed-Bias}(T; p) = \E\left[ T \left| \Q_{p}(T) < T < \Q_{(1 - p)}(T) \right.\right] - \theta. 
\label{eq:trimmed-bias}
\]
Median bias is thus a special case of trimmed mean bias, with \(p = 0.5\).
To estimate the trimmed bias, we use sample quantiles \(\hat{Q}_p\) and \(\hat{Q}_{(1 - p)}\) and take the mean of the middle \(1 - 2p\) fraction of the distribution
\[
\widehat{\text{Trimmed-Bias}}(T; p) = \frac{1}{(1 - 2p)R} \sum_{r=1}^R T_r \times I\left(\hat{Q}_{p} < T < \hat{Q}_{(1 - p)}\right) - \theta. \label{eq:sample-trimmed-bias}
\]
For a symmetric sampling distribution, winsorized bias and trimmed-mean bias will be the same as the conventional (mean) bias, but will be less affected by outlying values (i.e., values of \(T\) very far from the center of the distribution).
However, if a sampling distribution is not symmetric, winsorized bias and trimmed-mean bias become distinct performance measures, which put less emphasis on large errors compared to the conventional bias metric.

Alternative measures of the overall accuracy of an estimator can also be defined along similar lines.
For instance, an alternative to RMSE is to use the median absolute error (MAE), defined as
\[
\text{MAE} = \M\left(\left|T - \theta\right|\right),
\label{eq:MAE}
\]
which can be estimated by taking the sample median \(|T_1 - \theta|, |T_2 - \theta|, ..., |T_R - \theta|\).
Many other robust measures of the spread of the sampling distribution are also available, including the Rosseeuw-Croux scale estimator \(Q_n\) \citep{Rousseeuw1993alternatives} and the biweight midvariance \citep{Wilcox2022introduction}.
\citet{Maronna2006robust} provide a useful introduction to these metrics and robust statistics more broadly.
The \texttt{robustbase} package \citep{robustbase} provides functions for calculating many of these robust statistics.

\section{Metrics for Standard Error Estimators}\label{metrics-for-standard-error-estimators}

Statistics is concerned not only with how to estimate things, but also with assessing how good an estimate is---that is, understanding the extent of uncertainty in estimates of target parameters.
These concerns apply for Monte Carlo simulation studies as well.
In a simulation, we can simply compute an estimator's actual properties.
When we use an estimator with real data, we need to \emph{estimate} its associated standard error and generate confidence intervals and other assessments of uncertainty.
To understand if these uncertainty assessments work in practice, we need to evaluate not only the behavior of the estimator itself, but also the behavior of these associated quantities.
In other words, we generally want to know not only whether a point estimator is doing a good job, but also whether we can obtain a good standard error for that point estimator.

Commonly used metrics for quantifying the performance of estimated standard errors include relative bias, relative standard error, and relative root mean squared error.
These metrics are defined in relative terms (rather than absolute ones) by comparing their magnitude to the \emph{true} degree of uncertainty.
Typically, performance metrics are computed for \emph{variance} estimators rather than standard error estimators.
There are a few reasons for working with variance rather than standard error.
First, in practice, so-called unbiased standard errors usually are not in fact actually unbiased.\footnote{See the delightfully titled section 11.5, ``The Joke Is on Us: The Standard Deviation Estimator is Biased after All,'' in \citet{westfall2013understanding} for further discussion.}
For linear regression, for example, the classic standard error estimator is an unbiased \emph{variance} estimator, but the standard error estimator is not exactly unbiased because
\[ 
\E[ \sqrt{ V } ] \neq \sqrt{ \E[ V ] }.
\]
Variance is also the metric that gives us the bias-variance decomposition of \(MSE = Variance + Bias^2\). Thus, if we are trying to determine whether MSE is due to instability or systematic bias, operating in this squared space may be preferable.

To make this concrete, let us consider a generic standard error estimator \(\widehat{SE}\) to go along with our generic estimator \(T\) of target parameter \(\theta\), and let \(V = \widehat{SE}^2\).
The simulation yields a large sample of standard errors, \(\widehat{SE}_1,...,\widehat{SE}_R\) and variance estimators \(V_r = \widehat{SE}_r^2\) for \(r = 1,...,R\).
Formally, the relative bias, standard error, and RMSE of \(V\) are defined as
\[
\begin{aligned}
\text{Relative Bias}(V) &= \frac{\E(V)}{\Var(T)} \\
\text{Relative SE}(V) &= \frac{\Var(V)}{\Var(T)} \\
\text{Relative RMSE}(V) &= \frac{\sqrt{\E\left[\left(V - \Var(T)\right)^2 \right]}}{\Var(T)}.
\end{aligned}
\label{eq:relative-bias-SE-RMSE}
\]
In contrast to performance metrics for \(T\), we define these metrics in relative terms because the raw magnitude of \(V\) is not a stable or interpretable parameter.
Rather, the sampling distribution of \(V\) will generally depend on many of the parameters of the data-generating process, including the sample size and any other design parameters.
Defining bias in relative terms makes for a more interpretable metric: a value of 1 corresponds to exact unbiasedness of the variance estimator.
Relative bias measures \emph{proportionate} under- or over-estimation.
For example, a relative bias of 1.12 would mean the standard error was, on average, 12\% too large.
We discuss relative performance measures further in Section \ref{sec-relative-performance}.

To estimate these relative performance measures, we proceed by substituting sample quantities in place of the expectations and variances.
In contrast to the performance metrics for \(T\), we will not generally be able to compute the true degree of uncertainty exactly.
Instead, we must estimate the target quantity \(\Var(T)\) using \(S_T^2\), the sample variance of \(T\) across replications.
Denoting the arithmetic mean of the variance estimates as
\[
\bar{V} = \frac{1}{R} \sum_{r=1}^R V_r
\]
and the sample variance as
\[
S_V^2 = \frac{1}{R - 1}\sum_{r=1}^R \left(V_r - \bar{V}\right)^2,
\]
we estimate the relative bias, standard error, and RMSE of \(V\) using
\[
\begin{aligned}
\widehat{\text{Relative Bias}}(V) &= \frac{\bar{V}}{S_T^2} \\
\widehat{\text{Relative SE}}(V) &= \frac{S_V}{S_T^2} \\
\widehat{\text{Relative RMSE}}(V) &= \frac{\sqrt{\frac{1}{R}\sum_{r=1}^R\left(V_r - S_T^2\right)^2}}{S_T^2}.
\end{aligned}
\label{eq:relative-bias-SE-RMSE-estimators}
\]
These performance measures are informative about the properties of the uncertainty estimator \(V\) (or standard error \(\widehat{SE}\)), which have implications for the performance of other uncertainty assessments such as hypothesis tests and confidence intervals.
Relative bias describes whether the central tendency of \(V\) aligns with the actual degree of uncertainty in the point estimator \(T\).
Relative bias of less than 1 implies that \(V\) tends to under-state the amount of uncertainty, which will lead to confidence intervals that are overly narrow and do not cover the true parameter value at the desired rate.
Relative bias greater than 1 implies that \(V\) tends to over-state the amount of uncertainty in the point estimator, making it seem like \(T\) is less precise than it truly is.
Relative standard errors describe the variability of \(V\) in comparison to the true degree of uncertainty in \(T\)---the lower the better.
A relative standard error of 0.5 would mean that the variance estimator has average error of 50\% of the true uncertainty, implying that \(V\) will often be off by a factor of 2 compared to the true sampling variance of \(T\).
Ideally, a variance estimator will have small relative bias, small relative standard errors, and thus small relative RMSE.

\subsection{Satterthwaite degrees of freedom}\label{satterthwaite-degrees-of-freedom}

Another more abstract measure of the stability of a variance estimator is its Satterthwaite degrees of freedom.
With some simple statistical methods such as two-sample \(t\) tests, classical analysis of variance, and linear regression with homoskedastic errors, the variance estimator is computed by taking a sum of squares of normally distributed errors.
In these cases, the sampling distribution of the variance estimator is a multiple of a chi-squared distribution, with degrees of freedom corresponding to the number of independent observations used to compute the sum of squares.
In the context of analysis of variance problems, \citet{Satterthwaite1946approximate} described a method of approximating the variability of more complex statistics, involving linear combinations of sums of squares, by using a chi-squared distribution with a certain degrees of freedom.
When applied to an arbitrary variance estimator \(V\), these degrees of freedom can be interpreted as the number of independent observations going into a sum of squares that would lead to a variance estimator that is equally precise as \(V\).
More succinctly, these degrees of freedom correspond to the amount of independent observations used to estimate \(V\).

Following \citet{Satterthwaite1946approximate}, we define the degrees of freedom of \(V\) as
\[
df = \frac{2 \left[\E(V)\right]^2}{\Var(V)}.
\label{eq:Satterthwaite-df}
\]
We can estimate the degrees of freedom by taking
\[
\widehat{df} = \frac{2 \bar{V}^2}{S_V^2}.
\label{eq:Satterthwaite-df-estimator}
\]
For simple statistical methods in which \(V\) is based on a sum-of-squares of normally distributed errors, then the Satterthwaite degrees of freedom will be constant and correspond exactly to the number of independent observations in the sum of squares.
Even with more complex methods, the degrees of freedom are interpretable: higher degrees of freedom imply that \(V\) is based on more observations, and thus will be a more precise estimate of the actual degree of uncertainty in \(T\).

\subsection{Assessing SEs for the Cluster RCT Simulation}\label{assessing-ses-for-the-cluster-rct-simulation}

Returning to the cluster RCT example, we will assess whether our estimated SEs are about right by comparing the average \emph{estimated} (squared) standard error versus the true sampling variance.
Our standard errors are \emph{inflated} if they are systematically larger than they should be, across the simulation runs.
We will also look at how stable our variance estimates are by comparing their standard deviation to the true sampling variance and by computing the Satterthwaite degrees of freedom.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{SE\_performance }\OtherTok{\textless{}{-}} 
\NormalTok{  runs }\SpecialCharTok{\%\textgreater{}\%}  
  \FunctionTok{mutate}\NormalTok{( }\AttributeTok{V =}\NormalTok{ SE\_hat}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{ ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{( method ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{( }
    \AttributeTok{SE\_sq =} \FunctionTok{var}\NormalTok{( ATE\_hat ),}
    \AttributeTok{V\_bar =} \FunctionTok{mean}\NormalTok{( V ),}
    \AttributeTok{rel\_bias =}\NormalTok{ V\_bar }\SpecialCharTok{/}\NormalTok{ SE\_sq,}
    \AttributeTok{S\_V =} \FunctionTok{sd}\NormalTok{( V ),}
    \AttributeTok{rel\_SE\_V =}\NormalTok{ S\_V }\SpecialCharTok{/}\NormalTok{ SE\_sq,}
    \AttributeTok{df =} \DecValTok{2} \SpecialCharTok{*} \FunctionTok{mean}\NormalTok{( V )}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{/} \FunctionTok{var}\NormalTok{( V )}
\NormalTok{  )}

\NormalTok{SE\_performance}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 3 x 7
##   method  SE_sq  V_bar rel_bias     S_V rel_SE_V
##   <chr>   <dbl>  <dbl>    <dbl>   <dbl>    <dbl>
## 1 Agg    0.0282 0.0304     1.08 0.00818    0.290
## 2 LR     0.0336 0.0344     1.02 0.0117     0.349
## 3 MLM    0.0282 0.0304     1.08 0.00818    0.290
## # i 1 more variable: df <dbl>
\end{verbatim}

The variance estimators for Agg and MLM appear to be a bit conservative on average, with relative bias of around 1.08, or about 8\% higher than the true sampling variance.
The column labelled \texttt{rel\_SE\_V} reports how variable the variance estimators are relative to the true sampling variances of the estimators.
The column labelled \texttt{df} reports the Satterthwaite degrees of freedom of each variance estimator.
Both of these measures indicate that the linear regression variance estimator is less stable than the other methods, with around 10 fewer degrees of freedom.
The linear regression method uses a cluster-robust variance estimator, which is known to be a bit unstable \citep{cameronPractitionerGuideClusterRobust2015}.
Overall, it is a bad day for linear regression.

\section{Metrics for Confidence Intervals}\label{metrics-for-confidence-intervals}

Some estimation procedures provide confidence intervals (or confidence sets) which are ranges of values, or interval estimators, that should include the true parameter value with a specified confidence level.
For a 95\% confidence level, the interval should include the true parameter in 95\% replications of the data-generating process.
However, with the exception of some simple methods and models, methods for constructing confidence intervals usually involve approximations and simplifying assumptions, so their actual coverage rate might deviate from the intended confidence level.

We typically measure confidence interval performance along two dimensions: \textbf{coverage rate} and \textbf{expected width}.
Suppose that the confidence interval is for the target parameter \(\theta\) and has coverage level \(\beta\) for \(0 < \beta < 1\).
Denote the lower and upper end-points of the \(\beta\)-level confidence interval as \(A\) and \(B\).
\(A\) and \(B\) are random quantities---they will differ each time we compute the interval on a different replication of the data-generating process.
The coverage rate of a \(\beta\)-level interval estimator is the probability that it covers the true parameter, formally defined as
\[
\text{Coverage}_\beta(A,B) = \Prob(A \leq \theta \leq B).
\label{eq:coverage}
\]
For a well-performing interval estimator, \(\text{Coverage}_\beta\) will at least \(\beta\) and, ideally will not exceed \(\beta\) by too much.
The expected width of a \(\beta\)-level interval estimator is the average difference between the upper and lower endpoints, formally defined as
\[
\text{Width}_\beta(A,B) = \E(B - A).
\label{eq:expected-width}
\]
Smaller expected width means that the interval tends to be narrower, on average, and thus more informative about the value of the target parameter.

In practice, we approximate the coverage and width of a confidence interval by summarizing across replications of the data-generating process.
Let \(A_r\) and \(B_r\) denote the lower and upper end-points of the confidence interval from simulation replication \(r\), and let \(W_r = B_r - A_r\), all for \(r = 1,...,R\).
The coverage rate and expected length measures can be estimated as
\[
\begin{aligned}
\widehat{\text{Coverage}}_\beta(A,B) &= \frac{1}{R}\sum_{r=1}^R I(A_r \leq \theta \leq B_r) \\
\widehat{\text{Width}}_\beta(A,B) &= \frac{1}{R} \sum_{r=1}^R W_r = \frac{1}{R} \sum_{r=1}^R \left(B_r - A_r\right).
\end{aligned}
\label{eq:coverage-width}
\]
Following a strict statistical interpretation, a confidence interval performs acceptably if it has actual coverage rate greater than or equal to \(\beta\).
If multiple tests satisfy this criterion, then the test with the lowest expected width would be preferable. Some analysts prefer to look at lower and upper coverage separately, where lower coverage is \(\Prob(A \leq \theta)\) and upper coverage is \(\Prob(\theta \leq B)\).

In many instances, confidence intervals are constructed using point estimators and
uncertainty estimators.
For example, a conventional Wald-type confidence interval is centered on a point estimator, with end-points taken to be a multiple of an estimated standard error below and above the point estimator:
\[
A = T - c \times \widehat{SE}, \quad B = T + c \times \widehat{SE}
\]
for some critical value \(c\) (e.g.,for a normal critical value with a \(\beta = 0.95\) confidence level, \(c = 1.96\)).
Because of these connections, confidence interval coverage will often be closely related to the performance of the point estimator and uncertainty estimator.
Biased point estimators will tend to have confidence intervals with coverage below the desired level because they are not centered in the right place.
Likewise, variance estimators that have relative bias below 1 will tend to produce confidence intervals that are too short, leading to coverage below the desired level.
Thus, confidence interval coverage captures multiple aspects of the performance of an estimation procedure.

\subsection{Confidence Intervals in the Cluster RCT Simulation}\label{confidence-intervals-in-the-cluster-rct-simulation}

Returning to the CRT simulation, we will examine the coverage and expected width of normal Wald-type confidence intervals for each of the estimators under consideration.
To do this, we first have to calculate the confidence intervals because we did not do so in the estimation function.
We compute a normal critical value for a \(\beta = 0.95\) confidence level using \texttt{qnorm(0.975)}, then compute the lower and upper end-points using the point estimators and estimated standard errors:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{runs\_CIs }\OtherTok{\textless{}{-}} 
\NormalTok{  runs }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{( }
    \AttributeTok{A =}\NormalTok{ ATE\_hat }\SpecialCharTok{{-}} \FunctionTok{qnorm}\NormalTok{(}\FloatTok{0.975}\NormalTok{) }\SpecialCharTok{*}\NormalTok{ SE\_hat,}
    \AttributeTok{B =}\NormalTok{ ATE\_hat }\SpecialCharTok{+} \FunctionTok{qnorm}\NormalTok{(}\FloatTok{0.975}\NormalTok{) }\SpecialCharTok{*}\NormalTok{ SE\_hat}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

Now we can estimate the coverage rate and expected width of these confidence intervals:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{runs\_CIs }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{( method ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{( }
    \AttributeTok{coverage =} \FunctionTok{mean}\NormalTok{( A }\SpecialCharTok{\textless{}=}\NormalTok{ ATE }\SpecialCharTok{\&}\NormalTok{ ATE }\SpecialCharTok{\textless{}=}\NormalTok{ B ),}
    \AttributeTok{width =} \FunctionTok{mean}\NormalTok{( B }\SpecialCharTok{{-}}\NormalTok{ A )}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 3 x 3
##   method coverage width
##   <chr>     <dbl> <dbl>
## 1 Agg       0.942 0.677
## 2 LR        0.908 0.717
## 3 MLM       0.943 0.677
\end{verbatim}

The coverage rate is close to the desired level of 0.95 for the multilevel model and aggregation estimators, but it is around 5 percentage points too low for linear regression.
The lower-than-nominal coverage level occurs because of the bias of the linear regression point estimator.
The linear regression confidence intervals are also a bit wider than the other methods due to the larger sampling variance of its point estimator and higher variability (lower degrees of freedom) of its standard error estimator.

The normal Wald-type confidence intervals we have examined here are based on fairly rough approximations.
In practice, we might want to examine more carefully constructed intervals such as ones that use critical values based on \(t\) distributions or ones constructed by profile likelihood.
Especially in scenarios with a small or moderate number of clusters, such methods might provide better intervals, with coverage closer to the desired confidence level.
See Exercise \ref{cluster-RCT-t-confidence-intervals}.

\section{Metrics for Inferential Procedures (Hypothesis Tests)}\label{assessing-inferential-procedures}

Hypothesis testing involves specifying a null hypothesis, such as that there is no difference in average outcomes between two experimental groups, collecting data, and evaluating whether the observed data is compatible with the null hypothesis.
Hypothesis testing procedures are often formulated in terms of a \(p\)-value, which measures how extreme or surprising a feature of the observed data (a test statistic) is relative to what one would expect if the null hypothesis is true.
A small \(p\)-value (such as \(p < .05\) or \(p < .01\)) indicates that the observed data would be unlikely to occur if the null is true, leading the researcher to reject the null hypothesis.
Alternately, testing procedures might be formulated by comparing a test statistic to a specified critical value; a test statistic exceeding the critical value would lead the researcher to reject the null.

Hypothesis testing procedures aim to control the level of false positives, corresponding to the probability that the null hypothesis is rejected when it holds in truth.
The level of a testing procedure is often denoted as \(\alpha\), and it has become conventional in many fields to conduct tests with a level of \(\alpha = .05\).\footnote{The convention of using \(\alpha = .05\) does not have a strong theoretical rationale. Many scholars have criticized the rote application of this convention and argued for using other \(\alpha\) levels. See \citet{Benjamin2017redefine} and \citet{Lakens2018justify} for spirited arguments about choosing \(\alpha\) levels for hypothesis testing.}
Just as in the case of confidence intervals, hypothesis testing procedures can sometimes be developed that will have false positive rates exactly equal to the intended level \(\alpha\).
However, in many other problems, hypothesis testing procedures involve approximations or assumption violations, so that the actual rate of false positives might deviate from the intended \(\alpha\)-level.
When we evaluate a hypothesis testing procedure, we are concerned with two primary measures of performance: \emph{validity} and \emph{power}.

\subsection{Validity}\label{validity}

Validity pertains to whether we erroneously reject a true null more than we should.
An \(\alpha\)-level testing procedure is valid if it has no more than an \(\alpha\) chance of rejecting the null, when the null is true.
This means that if we conducted a valid testing procedure 1000 times, where the null holds for all of those 1000 replications, we should not see more than about \(1000 \alpha\) rejections. If we were using the conventional \(\alpha = .05\) level, then we should reject the null in only 50 of the 1000 replications.

To assess validity, we will need to specify a data generating process where the null hypothesis holds (e.g., where there is no difference in average outcomes between experimental groups).
We then generate a large series of data sets with a true null, conduct the testing procedure on each dataset and record the \(p\)-value or critical value, then score whether we reject the null hypothesis.
In practice, may be interested in evaluating a testing procedure by exploring data generation processes where the null is true but other aspects of the data (such as outliers, skewed outcome distributions, or small sample size) make estimation difficult, or where auxiliary assumptions of the testing procedure are violated.
Examining such data-generating processes allows us to understand if our methods are robust to patterns that might be encountered in real data analysis.
The key to evaluating the validity of a procedure is that, for whatever data-generating process we examine, the null hypothesis must be true.

\subsection{Power}\label{power}

Power is concerned with the chance that we notice when an effect or a difference exists---that is, the probability of rejecting the null hypothesis when it does not actually hold.
Compared to validity, power is a more nuanced concept because larger effects will clearly be easier to notice than smaller ones, and more blatant violations of a null hypothesis will be easier to identify than subtle ones.
Furthermore, the rate at which we can detect violations of a null will depend on the \(\alpha\) level of the testing procedure. A lower \(\alpha\) level will make for a less sensitive test, requiring stronger evidence to rule out a null hypothesis.
Conversely, a higher \(\alpha\) level will reject more readily, leading to higher power but at a cost of increased false positives.
When assessing validity, we want the rejection rate to be at or below the specified \(\alpha\) level, and when assessing power we want the rejection rate to be as high as possible.

We find it useful to think of power as a \emph{function} rather than as a single quantity because its absolute magnitude will generally depend on the sample size of a dataset and the magnitude of the effect of interest.
Because of this, power evaluations will typically involve examining a sequence of data-generating scenarios with increasing sample size or increasing effect size.
Further, if our goal is to evaluate several different testing procedures, the absolute power of a procedure will be of less concern than the \emph{relative} performance of one procedure compared to another.

In order to evaluate the power of a testing procedure by simulation, we will need to generate data where there is something to detect.
In other words, we will need to ensure that the null hypothesis is violated (and that some specific alternative hypothesis of interest holds).
The process of evaluating the power of a testing procedure is otherwise identical to that for evaluating its validity: generate many datasets, carry out the testing procedure, and track the rate at which the null hypothesis is rejected.
The only difference is the \emph{conditions} under which the data are generated.

\subsection{The Rejection Rate}\label{the-rejection-rate}

When evaluating both validity and power, the main performance measure is the \textbf{rejection rate} of the hypothesis test. Letting \(P\) be the p-value from a procedure for testing the null hypothesis that a parameter \(\theta = 0\), generated under a data-generating process with parameter \(\theta\) (which could in truth be zero or non-zero). The rejection rate is then
\[
\rho_\alpha(\theta) = \Prob(P < \alpha)
\label{eq:rejection-rate}
\]
When data are simulated from a process in which the null hypothesis is true, then the rejection rate is equivalent to the Type-I error rate of the test, which should ideally be near the desired \(\alpha\) level.
When the data are simulated from a process in which the null hypothesis is violated, then the rejection rate is equivalent to the \textbf{power} of the test (for the given alternate hypothesis specified in the data-generating process).
Ideally, a testing procedure should have actual Type-I error equal to the nominal level \(\alpha\) (this is the definition of validity), but such exact tests are rare.

To estimate the rejection rate of a test, we calculate the proportion of replications where the test rejects the null hypothesis.
Letting \(P_1,...,P_R\) be the p-values simulated from \(R\) replications of a data-generating process with true parameter \(\theta\), we estimate the rejection rate by calculating
\[
r_\alpha(\theta) = \frac{1}{R} \sum_{r=1}^R I(P_r < \alpha).
\label{eq:rejection-rate-estimate}
\]
It may be of interest to evaluate the performance of the test at several different \(\alpha\) levels.
For instance, \citet{brown1974SmallSampleBehavior} evaluated the Type-I error rates and power of their tests using \(\alpha = .01\), \(.05\), and \(.10\).
Simulating the \(p\)-value of the test makes it easy to estimate rejection rates for multiple \(\alpha\) levels, since we simply need to apply Equation \eqref{eq:rejection-rate-estimate} for several values of \(\alpha\).
When simulating from a data-generating process where the null hypothesis holds, one can also plot the empirical cumulative distribution function of the \(p\)-values; for an exactly valid test, the \(p\)-values should follow a standard uniform distribution with a cumulative distribution falling along the \(45^\circ\) line.

There are some different perspectives on how close the actual Type-I error rate should be in order to qualify as suitable for use in practice. Following a strict statistical definition, a hypothesis testing procedure is said to be \textbf{level-\(\alpha\)} if its actual Type-I error rate is \emph{always} less than or equal to \(\alpha\).
Among a collection of level-\(\alpha\) testing procedures, we would prefer the one with highest power.
If looking only at null rejection rates, then the test with Type-I error closest to \(\alpha\) would usually be preferred.
However, some scholars prefer to use a less stringent criterion, where the Type-I error rate of a testing procedure would be considered acceptable if it is within 50\% of the desired \(\alpha\) level.
For instance, a testing procedure with \(\alpha = .05\) would be considered acceptable if its Type-I error is no more than 7.5\%; with \(\alpha = .01\), it would be considered acceptable if its Type-I error is no more than 1.5\%.

\subsection{Inference in the Cluster RCT Simulation}\label{inference-in-the-cluster-rct-simulation}

Returning to the cluster RCT simulation, we will evaluate the validity and power of hypothesis tests for the average treatment effect based on each of the three estimation methods.
The data used in previous sections of the chapter was simulated under a process with a non-null treatment effect parameter (equal to 0.3 SDs), so the null hypothesis of zero average treatment effect does not hold.
Thus, the rejection rates for this scenario correspond to estimates of power.
We compute the rejection rate for tests with an \(\alpha\) level of \(.05\):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{runs }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{( method ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{( }\AttributeTok{power =} \FunctionTok{mean}\NormalTok{( p\_value }\SpecialCharTok{\textless{}=} \FloatTok{0.05}\NormalTok{ ) )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 3 x 2
##   method power
##   <chr>  <dbl>
## 1 Agg    0.376
## 2 LR     0.503
## 3 MLM    0.383
\end{verbatim}

For this particular scenario, the power of the tests is not particularly high, and the linear regression estimator apparently has higher power than the aggregation method and the multi-level model.

To make sense of this power pattern, we need to also consider the validity of the testing procedures.
We can do so by re-running the simulation using code we constructed in Chapter \ref{running-the-simulation-process} using the \texttt{simhelpers} package.
To evaluate the Type-I error rate of the tests, we will set the average treatment effect parameter to zero by specifying \texttt{ATE\ =\ 0}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{( }\DecValTok{404044}\NormalTok{ )}
\NormalTok{runs\_val }\OtherTok{\textless{}{-}} \FunctionTok{sim\_function}\NormalTok{( R, }\AttributeTok{n\_bar =} \DecValTok{30}\NormalTok{, }\AttributeTok{J =} \DecValTok{20}\NormalTok{, }\AttributeTok{gamma\_1 =} \DecValTok{0}\NormalTok{, }\AttributeTok{gamma\_2 =} \FloatTok{0.2}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

Assessing validity involves repeating the exact same rejection rate calculations as we did for power:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{runs\_val }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{( method ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{( }\AttributeTok{power =} \FunctionTok{mean}\NormalTok{( p\_value }\SpecialCharTok{\textless{}=} \FloatTok{0.05}\NormalTok{ ) )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 3 x 2
##   method power
##   <chr>  <dbl>
## 1 Agg    0.051
## 2 LR     0.059
## 3 MLM    0.048
\end{verbatim}

The Type-I error rates of the tests for the aggregation and multi-level modeling estimators are around 0.05, as desired.
The test for the linear regression estimator has Type-I error above the specified \(\alpha\)-level, due to the upward bias of the point estimator used in constructing the test.
The elevated rejection rate might be part of the reason that the linear regression test has higher power than the other procedures.
It is not entirely fair to compare the power of these testing procedures, because one of them has Type-I error in excess of the desired level.\footnote{One approach for conducting a fair comparison in this situation is to compute the \emph{size-adjusted} power of the tests.
  Size-adjusted power involves computing the rejection rate of a test using a different threshold \(\alpha'\), selected so that the Type-I error rate of the test is equal to the desired \(\alpha\) level.
  Specifically, size adjusted power is
  \[
  \rho^{adjusted}_\alpha(\theta) = \Prob(P < \rho_\alpha(0)).
  \]
  To estimate size-adjusted power using simulation, we first need to estimate the Type-I error rate, \(r_\alpha(0)\). We can then evaluate the rejection rate of the testing procedure under scenarios with other values of \(\theta\) by computing
  \[
  r^{adjusted}_\alpha(\theta) = \frac{1}{R} \sum_{r=1}^R I(P_r < r_{\alpha}(0)).
  \]}

As discussed above, linear regression target the person-level average treatment effect. In the scenario we simulated for evaluating validity, the person-level average effect is not zero because we have specified a non-zero impact heterogeneity parameter (\(\gamma_2=0.2\)), meaning that the school-specific treatment effects vary around 0.
To see if this is why the linear regression test has an inflated Type-I error rate, we could re-run the simulation using settings where both the school-level and person-level average effects are truly zero.

\section{Selecting Relative vs.~Absolute Metrics}\label{sec-relative-performance}

We have primarily examined performance estimators for point estimators using absolute metrics, focusing on measures like bias directly on the scale of the outcome.
In contrast, for evaluation things such as estimated standard errors, which are always positive and scale-dependent, it often makes sense to use relative metrics, i.e., metrics calculated as proportions of the target parameter (\(T/\theta\)) rather than as differences (\(T - \theta\)).
We typically apply absolute metrics to point estimators and relative metrics to standard error estimators (we are setting aside, for the moment, the relative metrics of a measure from one estimation procedure to another, as we saw earlier when we compared the SEs to a baseline SE of linear regression for the cluster randomized trial simulation.
So how do we select when to use what?

As a first piece of guidance, establish whether we expect the performance (e.g., bias, standard error, or RMSE) of a point estimate to depend on the magnitude of the estimand.
For example, if we are estimating some mean \(\theta\), and we generate data where \(\theta = 100\) vs where \(\theta = 1000\) (or any other arbitrary number), we would not generally expect the value of \(\theta\) to change the magnitude of bias, variance, or MSE.
On the other hand, these different \(\theta\)s would have a large impact on the \emph{relative} bias and \emph{relative} MSE.
(Want smaller relative bias? Just add a million to the parameter!)
For these sorts of ``location parameters'' we generally use absolute measures of performance.

That being said, a more principled approach for determining whether to use absolute or relative performance metrics depends on assessing performance for \emph{multiple} values of the parameter.
In many simulation studies, replications are generated and performance metrics are calculated for several different values of a parameter, say \(\theta = \theta_1,...,\theta_p\).
Let's focus on bias for now, and say that we've estimated (from a large number of replications) the bias at each parameter value.
We present two hypothetical scenarios, A and B, in the figures below.

\begin{center}\includegraphics[width=0.75\linewidth]{Designing-Simulations-in-R_files/figure-latex/unnamed-chunk-170-1} \end{center}

If the absolute bias is roughly the same for all values of \(\theta\) (as in Scenario A), then it makes sense to report absolute bias as the summary performance criterion.
On the other hand, if the bias grows roughly in proportion to \(\theta\) (as in Scenario B), then relative bias might be a better summary criterion.

\textbf{Performance relative to a baseline estimator.}

Another relative measure, as we saw earlier, is to calculate performance relative to some baseline.
For example, if one of the estimators is the ``generic method,'' we could calculate ratios of the RMSE of our estimators to the baseline RMSE.
This can provide a way of standardizing across simulation scenarios where the overall scale of the RMSE changes radically.
This could be critical to, for example, examining trends across simulations that have different sample sizes, where we would expect all estimators' performance measures to improve as sample size grows.
This kind of relative standardization allows us to make statements such as ``Aggregation has standard errors around 8\% smaller than linear regression''--which is very interpretable, more interpretable than saying ``Aggregation has standard errors around 0.01 smaller than linear regression.''
In the latter case, we do not know if that is big or small.

While a powerful tool, standardization is not without risks: if you scale relative to something, then higher or lower ratios can either be due to the primary method of interest (the numerator) or due to the behavior of the reference method in the denominator.
These relative ratios can end up being confusing to interpret due to this tension.

They can also break when everything is on a constrained scale, like power.
If we have a power of 0.05, and we improve it to 0.10, we have doubled our power, but if it is 0.10 and we increase to 0.15, we have only increased by 50\%.
Ratios when near zero can be very deceiving.

\section{Estimands Not Represented By a Parameter}\label{implicit-estimands}

In our Cluster RCT example, we focused on the estimand of the school-level ATE, represented by the model parameter \(\gamma_1\).
What if we were instead interested in the person-level average effect?
This estimand does not correspond to any input parameter in our data generating process.
Instead, it is defined \emph{implicitly} by a combination of other parameters.
In order to compute performance characteristics such as bias and RMSE, we would need to calculate the parameter based on the inputs of the data-generating processes.
There are at least three possible ways to accomplish this.

One way is to use mathematical distribution theory to compute an implied parameter.
Our target parameter will be some function of the parameters and random variables in the data-generating process, and it may be possible to evaluate that function algebraically or numerically (i.e., using numerical integration functions such as \texttt{integrate()}).
Such an exercise can be very worthwhile if it provides insights into the relationship between the target parameter and the inputs of the data-generating process.
However, this approach requires knowledge of distribution theory, and it can get quite complicated and technical.\footnote{In the cluster-RCT example, the distribution theory is tractable. See Exercise \ref{cluster-RCT-SPATE}}
Other approaches are often feasible and more closely aligned with our focus on Monte Carlo simulation.

Another alternative approach is to simply generate a massive dataset---so large that can stand in for the entire data-generating model---and then simply calculate the target parameter of interest in this massive dataset. In the cluster-RCT example, we can apply this strategy by generating data from a very large number of clusters and then simply calculating the true person-average effect across all generated clusters.
If the dataset is big enough, then the uncertainty in this estimate will be negligible compared to the uncertainty in our simulation.

We implement this approach as follows, generating a dataset with 100,000 clusters:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dat }\OtherTok{\textless{}{-}} \FunctionTok{gen\_cluster\_RCT}\NormalTok{( }
  \AttributeTok{n\_bar =} \DecValTok{30}\NormalTok{, }\AttributeTok{J =} \DecValTok{100000}\NormalTok{, }
  \AttributeTok{gamma\_1 =} \FloatTok{0.3}\NormalTok{, }\AttributeTok{gamma\_2 =} \FloatTok{0.5}\NormalTok{,}
  \AttributeTok{sigma2\_u =} \FloatTok{0.20}\NormalTok{, }\AttributeTok{sigma2\_e =} \FloatTok{0.80}\NormalTok{,}
  \AttributeTok{alpha =} \FloatTok{0.75}  
\NormalTok{)}
\NormalTok{ATE\_person }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{( dat}\SpecialCharTok{$}\NormalTok{Yobs[dat}\SpecialCharTok{$}\NormalTok{Z}\SpecialCharTok{==}\DecValTok{1}\NormalTok{] ) }\SpecialCharTok{{-}} \FunctionTok{mean}\NormalTok{( dat}\SpecialCharTok{$}\NormalTok{Yobs[dat}\SpecialCharTok{$}\NormalTok{Z}\SpecialCharTok{==}\DecValTok{0}\NormalTok{] )}
\NormalTok{ATE\_person}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.3966183
\end{verbatim}

Note our estimate of the person-average effect of 0.4 is about what we would expect given the bias we saw earlier for the linear model.

With respect to the \texttt{ATE\_person} estimand, the bias and RMSE of our estimators will shift, although SE will stay the same as in our performance calculations for the school-level average effect:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{runs }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{( method ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{( }
    \AttributeTok{bias =} \FunctionTok{mean}\NormalTok{( ATE\_hat }\SpecialCharTok{{-}}\NormalTok{ ATE\_person ),}
    \AttributeTok{SE =} \FunctionTok{sd}\NormalTok{( ATE\_hat ),}
    \AttributeTok{RMSE =} \FunctionTok{sqrt}\NormalTok{( }\FunctionTok{mean}\NormalTok{( (ATE\_hat }\SpecialCharTok{{-}}\NormalTok{ ATE\_person)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{ ) )}
\NormalTok{  ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{( }\AttributeTok{per\_RMSE =}\NormalTok{ RMSE }\SpecialCharTok{/}\NormalTok{ RMSE[method}\SpecialCharTok{==}\StringTok{"LR"}\NormalTok{] )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 3 x 5
##   method     bias    SE  RMSE per_RMSE
##   <chr>     <dbl> <dbl> <dbl>    <dbl>
## 1 Agg    -0.0910  0.168 0.191     1.04
## 2 LR     -0.00675 0.183 0.183     1   
## 3 MLM    -0.0887  0.168 0.190     1.04
\end{verbatim}

For the person-weighted estimand, Agg and MLM are biased but LR is unbiased.
RMSE is now a tension between bias and reduced variance.
Overall, Agg and MLM are 4\% worse than LR in terms of RMSE, because they have lower SEs but higher bias.

A further approach for calculating \texttt{ATE\_person} would be to record the true person average effect of the dataset with each simulation iteration, and then average the sample-specific parameters at the end.
The overall average of the dataset-specific \texttt{ATE\_person}s corresponds to the population person-level ATE.
This approach is effectively equivalent to generating a massive dataset---we just generate it in piece.

To implement this approach, we would need to modify the data-generating function \texttt{gen\_cluster\_RCT()} to track the additional information.
We might have, for example

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tx\_effect }\OtherTok{\textless{}{-}}\NormalTok{ gamma\_1 }\SpecialCharTok{+}\NormalTok{ gamma\_2 }\SpecialCharTok{*}\NormalTok{ (nj}\SpecialCharTok{{-}}\NormalTok{n\_bar)}\SpecialCharTok{/}\NormalTok{n\_bar}
\NormalTok{beta\_0j }\OtherTok{\textless{}{-}}\NormalTok{ gamma\_0 }\SpecialCharTok{+}\NormalTok{ Zj }\SpecialCharTok{*}\NormalTok{ tx\_effect }\SpecialCharTok{+}\NormalTok{ u0j}
\end{Highlighting}
\end{Shaded}

and then we would return \texttt{tx\_effect} as well as \texttt{Yobs} and \texttt{Z} as a column in our dataset.
This approach is quite similar to directly calculating \emph{potential outcomes}, as discussed in Chapter \ref{potential-outcomes}.

After modifying the data-generating function, we will also need to modify the analysis function(s) to record the sample-specific treatment effect parameter.
We might have, for example:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{analyze\_data }\OtherTok{=} \ControlFlowTok{function}\NormalTok{( dat ) \{}
\NormalTok{  MLM }\OtherTok{\textless{}{-}} \FunctionTok{analysis\_MLM}\NormalTok{( dat )}
\NormalTok{  LR }\OtherTok{\textless{}{-}} \FunctionTok{analysis\_OLS}\NormalTok{( dat )}
\NormalTok{  Agg }\OtherTok{\textless{}{-}} \FunctionTok{analysis\_agg}\NormalTok{( dat )}
\NormalTok{  res }\OtherTok{\textless{}{-}} \FunctionTok{bind\_rows}\NormalTok{( }
    \AttributeTok{MLM =}\NormalTok{ MLM, }\AttributeTok{LR =}\NormalTok{ LR, }\AttributeTok{Agg =}\NormalTok{ Agg,}
    \AttributeTok{.id =} \StringTok{"method"} 
\NormalTok{  )}
\NormalTok{  res}\SpecialCharTok{$}\NormalTok{ATE\_person }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{( dat}\SpecialCharTok{$}\NormalTok{tx\_effect )}
  \FunctionTok{return}\NormalTok{( res )}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Now when we run our simulation, we will have a column which is the true person-level average treatment effect for each dataset.
We could then take the average of these value across replications to estimate the true person average treatment effect in the population, and then use this as the target parameter for performance calculations.

Clearly, an estimand not represented by any single input parameter is more difficult to work with, but it is not impossible.
The key is to be clear about what you are trying to estimate, since the performance of an estimator depends critically on the estimand to which the estimator is compared.

\section{Uncertainty in Performance Estimates (the Monte Carlo Standard Error)}\label{MCSE}

Our performance metrics are defined as average performance across an infinite number of trials.
Of course, in our simulations we only run a finite number of trials, and estimate the performance metrics with the sample of trials we generate.
For example, if we are assessing coverage across 100 trials, we can calculate what fraction rejected the null for that 100.
This is an \emph{estimate} of the true coverage rate.
Due to random chance, we might see a higher, or lower, proportion rejected than what we would see if we ran the simulation forever.

To account for estimation uncertainty we want associated uncertainty estimates to go with our point estimates of performance.
We want to, in other words, treat our simulation results as a dataset in its own right.
(And yes, this is quite meta!)

Once we frame the problem in these terms, it is relatively straightforward to calculate standard errors for most of the performance critera because we have an independent and identically distributed set of measurements.
We call these standard errors Monte Carlo Simulation Errors, or MCSEs.
For some of the performance metrics we have to be a bit more clever, as we will discuss below.

We list MCSE expressions for many of our straightforward performance measures on the following table.
In reading the table, recall that, for an estimator \(T\), we have \(S_T\) being the standard deviation of \(T\) across our simulation runs (i.e., our estimated true Standard Error).
We also have

\begin{itemize}
\tightlist
\item
  Sample skewness (standardized): \(\displaystyle{g_T = \frac{1}{R S_T^3}\sum_{r=1}^R \left(T_r - \bar{T}\right)^3}\)
\item
  Sample kurtosis (standardized): \(\displaystyle{k_T = \frac{1}{R S_T^4} \sum_{r=1}^R \left(T_r - \bar{T}\right)^4}\)
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.6667}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.3333}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Criterion for T
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
MCSE
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Bias (\(T-\theta\)) & \(\sqrt{S_T^2/ R}\) \\
Variance (\(S_T^2\)) & \(\displaystyle{S_T^2 \sqrt{\frac{k_T - 1}{R}}}\) \\
MSE & see below \\
MAD & - \\
Power \& Validity (\(r_\alpha\)) & \(\sqrt{ r_\alpha \left(1 - r_\alpha\right) / R}\) \\
Coverage (\(\omega_\beta\)) & \(\sqrt{\omega_\beta \left(1 - \omega_\beta\right) / R}\) \\
Average length (\(\text{E}(W)\)) & \(\sqrt{S_W^2 / R}\) \\
\end{longtable}

The MCSE for the MSE is a bit more complicated, and does not quite fit on our table:
\[ \widehat{MCSE}( \widehat{MSE} ) = \displaystyle{\sqrt{\frac{1}{R}\left[S_T^4 (k_T - 1) + 4 S_T^3 g_T\left(\bar{T} - \theta\right) + 4 S_T^2 \left(\bar{T} - \theta\right)^2\right]}} .\]

For relative quantities with respect to an estimand, simply divide the criterion by the target estimand.
E.g., for relative bias \(T / \theta\), the standard error would be
\[ SE\left( \frac{T}{\theta} \right) = \frac{1}{\theta} SE(T) = \sqrt{\frac{S_T^2}{R\theta^2}} .\]

For square rooted quantities, such as the SE for the true SE (square root of the Variance) or the RMSE (square root of MSE) we can use the Delta method.
The Delta method says (with some conditions), that if we assume \(X \sim N( \phi, V )\), then we can approximate the distribution of \(g(X)\) for some continuous function \(g(\cdot)\) as
\[ g(X) \sim N\left( g(\phi), \;\; g'(\phi)^2\cdot V \right) , \]
where \(g'(\phi)\) is the derivative of \(g(\cdot)\) evaluated at \(\phi\).
In other words,
\[ SE( g(\hat{X}) ) \approx g'(\theta)  \times SE(\hat{X}) .\]
For estimation, we plug in \(\hat{\theta}\) and our estimate of \(SE(\hat{X})\) into the above.
Back to the square root, we have \(g(x) = \sqrt(x)\) and \(g'(x) = 1/2\sqrt(x)\).
This gives, for example, the estimated MCSE of the SE as
\[ \widehat{SE}( \widehat{SE} ) = \widehat{SE}( S^2_T ) = \frac{1}{2S^2_T} \widehat{SE}( S^2_T ) = \frac{1}{2S^2_T} S_T^2 \sqrt{\frac{k_T - 1}{R}} = \frac{1}{2} \sqrt{\frac{k_T - 1}{R}} .\]

\subsection{MCSE for Relative Variance Estimators}\label{mcse-for-relative-variance-estimators}

Estimating the MCSE of the relative bias or relative MSE of a (squared) standard error estimator, i.e., of \(E( \widehat{SE^2} - SE^2 ) / SE^2 )\) or \(\widehat{MSE} / MSE\), is complicated by the appearance of an estimated quantity, \(SE^2\) or \(MSE\), in the denominator of the ratio.
This renders the simple division approach from above unusable, technically speaking.
The problem is we cannot use our clean expressions for MCSEs of relative performance measures since we are not taking the uncertainty of our denominator into account.

To properly assess the overall MCSE, we need to do something else.
One approach is to use the \emph{jackknife} technique.
Let \(\bar{V}_{(j)}\) and \(S_{T(j)}^2\) be the average squared standard error estimate and the true variance estimate calculated from the set of replicates \textbf{\emph{that excludes replicate \(j\)}}, for \(j = 1,...,R\).
The relative bias estimate, excluding replicate \(j\) would then be \(\bar{V}_{(j)} / S_{T(j)}^2\).
Calculating all \(R\) versions of this relative bias estimate and taking the variance of these \(R\) versions yields the jackknife variance estimator:

\[
MCSE\left( \frac{ \widehat{SE}^2 }{SE^2} \right) = \frac{1}{R} \sum_{j=1}^R \left(\frac{\bar{V}_{(j)}}{S_{T(j)}^2} - \frac{\bar{V}}{S_T^2}\right)^2.
\]

This would be quite time-consuming to compute if we did it by brute force. However, a few algebra tricks provide a much quicker way. The tricks come from observing that

\[
\begin{aligned}
\bar{V}_{(j)} &= \frac{1}{R - 1}\left(R \bar{V} - V_j\right) \\
S_{T(j)}^2 &= \frac{1}{R - 2} \left[(R - 1) S_T^2 - \frac{R}{R - 1}\left(T_j - \bar{T}\right)^2\right]
\end{aligned}
\]
These formulas can be used to avoid re-computing the mean and sample variance from every subsample.
Instead, you calculate the overall mean and overall variance, and then do a small adjustment with each jackknife iteration.
You can even implement this with vector processing in R!

\subsection{\texorpdfstring{Calculating MCSEs With the \texttt{simhelpers} Package}{Calculating MCSEs With the simhelpers Package}}\label{calculating-mcses-with-the-simhelpers-package}

The \texttt{simhelper} package is designed to calculate MCSEs (and the performance metrics themselves) for you.
It is easy to use: take this set of simulation runs on the Welch dataset:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{( simhelpers )}
\FunctionTok{data}\NormalTok{( welch\_res )}
\NormalTok{welch }\OtherTok{\textless{}{-}}\NormalTok{ welch\_res }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{( method }\SpecialCharTok{==} \StringTok{"t{-}test"}\NormalTok{ ) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{( }\SpecialCharTok{{-}}\NormalTok{method, }\SpecialCharTok{{-}}\NormalTok{seed, }\SpecialCharTok{{-}}\NormalTok{iterations )}

\NormalTok{welch}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 8,000 x 8
##       n1    n2 mean_diff      est    var p_val
##    <dbl> <dbl>     <dbl>    <dbl>  <dbl> <dbl>
##  1    50    50         0  0.0258  0.0954 0.934
##  2    50    50         0  0.00516 0.0848 0.986
##  3    50    50         0 -0.0798  0.0818 0.781
##  4    50    50         0 -0.0589  0.102  0.854
##  5    50    50         0  0.0251  0.118  0.942
##  6    50    50         0 -0.115   0.106  0.725
##  7    50    50         0  0.157   0.115  0.645
##  8    50    50         0 -0.213   0.121  0.543
##  9    50    50         0  0.509   0.117  0.139
## 10    50    50         0 -0.354   0.0774 0.206
## # i 7,990 more rows
## # i 2 more variables: lower_bound <dbl>,
## #   upper_bound <dbl>
\end{verbatim}

We can calculate performance metrics across all the range of scenarios.
Here is the rejection rate:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{welch\_sub }\OtherTok{=} \FunctionTok{filter}\NormalTok{( welch, n1 }\SpecialCharTok{==} \DecValTok{50}\NormalTok{, n2 }\SpecialCharTok{==} \DecValTok{50}\NormalTok{, mean\_diff}\SpecialCharTok{==}\DecValTok{0}\NormalTok{ )}
\FunctionTok{calc\_rejection}\NormalTok{(welch\_sub, p\_val)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   K_rejection rej_rate rej_rate_mcse
## 1        1000    0.048   0.006759882
\end{verbatim}

And coverage:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{calc\_coverage}\NormalTok{(welch\_sub, lower\_bound, upper\_bound, mean\_diff)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1 x 5
##   K_coverage coverage coverage_mcse width
##        <int>    <dbl>         <dbl> <dbl>
## 1       1000    0.952       0.00676  1.25
## # i 1 more variable: width_mcse <dbl>
\end{verbatim}

Using \texttt{tidyverse} it is easy to process across scenarios (more on experimental design and multiple scenarios later):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{welch }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{group\_by}\NormalTok{(n1,n2,mean\_diff) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{( }\FunctionTok{calc\_rejection}\NormalTok{( }\AttributeTok{p\_values =}\NormalTok{ p\_val ) )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 8 x 6
## # Groups:   n1, n2 [2]
##      n1    n2 mean_diff K_rejection rej_rate
##   <dbl> <dbl>     <dbl>       <int>    <dbl>
## 1    50    50       0          1000    0.048
## 2    50    50       0.5        1000    0.34 
## 3    50    50       1          1000    0.876
## 4    50    50       2          1000    1    
## 5    50    70       0          1000    0.027
## 6    50    70       0.5        1000    0.341
## 7    50    70       1          1000    0.904
## 8    50    70       2          1000    1    
## # i 1 more variable: rej_rate_mcse <dbl>
\end{verbatim}

\subsection{MCSE Calculation in our Cluster RCT Example}\label{mcse-calculation-in-our-cluster-rct-example}

We can check our MCSEs for our performance measures to see if we have enough simulation trials to give us precise enough estimates to believe the differences we reported earlier.
In particular, we have:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{( simhelpers )}
\NormalTok{runs}\SpecialCharTok{$}\NormalTok{ATE }\OtherTok{=}\NormalTok{ ATE}
\NormalTok{runs }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarise}\NormalTok{( }\FunctionTok{calc\_absolute}\NormalTok{( }\AttributeTok{estimates =}\NormalTok{ ATE\_hat,}
                            \AttributeTok{true\_param =}\NormalTok{ ATE,}
                            \AttributeTok{criteria =} \FunctionTok{c}\NormalTok{(}\StringTok{"bias"}\NormalTok{,}\StringTok{"stddev"}\NormalTok{, }\StringTok{"rmse"}\NormalTok{)) ) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{( }\SpecialCharTok{{-}}\NormalTok{K\_absolute ) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\AttributeTok{digits=}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{r|r|r|r|r|r}
\hline
bias & bias\_mcse & stddev & stddev\_mcse & rmse & rmse\_mcse\\
\hline
0.034 & 0.003 & 0.178 & 0.002 & 0.181 & 0.003\\
\hline
\end{tabular}

We see the MCSEs are quite small relative to the linear regression bias term and all the SEs (\texttt{stddev}) and RMSEs: we have simulated enough runs to see the gross trends identified.
We have \emph{not} simulated enough to for sure know if MLM and Agg are not slightly biased. Given our MCSEs, they could have true bias of around 0.01 (two MCSEs).

\section{Summary of Peformance Measures}\label{summary-of-peformance-measures}

We list most of the performance criteria we saw in this chapter in the table below, for reference:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3833}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3667}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Criterion
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Definition
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Estimate
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Bias & \(\text{E}(T) - \theta\) & \(\bar{T} - \theta\) \\
Median bias & \(\text{M}(T) - \theta\) & \(\tilde{T} - \theta\) \\
Variance & \(\text{E}\left[\left(T - \text{E}(T)\right)^2\right]\) & \(S_T^2\) \\
MSE & \(\text{E}\left[\left(T - \theta\right)^2\right]\) & \(\left(\bar{T} - \theta\right)^2 + S_T^2\) \\
MAE & \(\text{M}\left[\left|T - \theta\right|\right]\) & \(\left[\left|T - \theta\right|\right]_{R/2}\) \\
Relative bias & \(\text{E}(T) / \theta\) & \(\bar{T} / \theta\) \\
Relative median bias & \(\text{M}(T) / \theta\) & \(\tilde{T} / \theta\) \\
Relative MSE & \(\text{E}\left[\left(T - \theta\right)^2\right] / \theta^2\) & \(\frac{\left(\bar{T} - \theta\right)^2 + S_T^2}{\theta^2}\) \\
\end{longtable}

\begin{itemize}
\tightlist
\item
  Bias and median bias are measures of whether the estimator is systematically higher or lower than the target parameter.
\item
  Variance is a measure of the \textbf{precision} of the estimator---that is, how far it deviates \emph{from its average}. We might look at the square root of this, to assess the precision in the units of the original measure. This is the true SE of the estimator.
\item
  Mean-squared error is a measure of \textbf{overall accuracy}, i.e.~is a measure how far we typically are from the truth. We more frequently use the root mean-squared error, or RMSE, which is just the square root of the MSE.
\item
  The median absolute deviation (MAD) is another measure of overall accuracy that is less sensitive to outlier estimates. The RMSE can be driven up by a single bad egg. The MAD is less sensitive to this.
\end{itemize}

\section{Concluding thoughts}\label{concluding-thoughts}

In practice, many data analysis procedures produce multiple pieces of information---not just point estimates, but also standard errors and confidence intervals and p-values from null hypothesis tests---and those pieces are inter-related.
For instance, a confidence interval is usually computed from a point estimate and its standard error.
Consequently, the performance of that confidence interval will be strongly affected by whether the point estimator is biased and whether the standard error tends to understates or over-states the true uncertainty.
Likewise, the performance of a hypothesis testing procedure will often strongly depend on the properties of the point estimator and standard error used to compute the test.\\
Thus, most simulations will involve evaluating a data analysis procedure on several metrics to arrive at a holistic understanding of its performance.

Moreover, the main aim of many simulations is to compare the performance of several different estimators or to determine which of several data analysis procedures is preferable.
For such aims, we will need to use the performance metrics to understand whether a set of procedures work differently, when and how one is superior to the other, and what factors influence differences in performance.
To fully understand the advantages and trade-offs among a set of estimators, we will generally need to compare them using several performance metrics.

\section{Exercises}\label{exercises-6}

\subsection{Brown and Forsythe (1974)}\label{Brown-Forsythe-performance}

Continuing the exercises from the prior chapters, estimate rejection rates of the BFF* test for the parameter values in the fifth line of Table 1 of Brown and Forsythe (1974).

\subsection{Better confidence intervals}\label{cluster-RCT-t-confidence-intervals}

Consider the estimation functions for the cluster RCT example, as given in Section \ref{multiple-estimation-procedures}.
Modify the functions to return \textbf{both} normal Wald-type 95\% confidence intervals and robust confidence intervals based on \(t\) distributions with Satterthwaite degrees of freedom.
For the latter, use \texttt{conf\_int()} from the \texttt{clubSandwich} package, as in the following example code:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(clubSandwich)}
\NormalTok{M1 }\OtherTok{\textless{}{-}}\NormalTok{ lme4}\SpecialCharTok{::}\FunctionTok{lmer}\NormalTok{( }
\NormalTok{  Yobs }\SpecialCharTok{\textasciitilde{}} \DecValTok{1} \SpecialCharTok{+}\NormalTok{ Z }\SpecialCharTok{+}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{|}\NormalTok{ sid), }
  \AttributeTok{data =}\NormalTok{ dat }
\NormalTok{)}
\FunctionTok{conf\_int}\NormalTok{(M1, }\AttributeTok{vcov =} \StringTok{"CR2"}\NormalTok{)}

\NormalTok{M2 }\OtherTok{\textless{}{-}}\NormalTok{ estimatr}\SpecialCharTok{::}\FunctionTok{lm\_robust}\NormalTok{( }
\NormalTok{  Yobs }\SpecialCharTok{\textasciitilde{}} \DecValTok{1} \SpecialCharTok{+}\NormalTok{ Z, }\AttributeTok{data =}\NormalTok{ dat, }
  \AttributeTok{clusters =}\NormalTok{ sid,  }\AttributeTok{se\_type =}\NormalTok{ se\_type}
\NormalTok{)}
\FunctionTok{conf\_int}\NormalTok{(M2, }\AttributeTok{cluster =}\NormalTok{ dat}\SpecialCharTok{$}\NormalTok{sid, }\AttributeTok{vcov =} \StringTok{"CR2"}\NormalTok{)}

\NormalTok{M3 }\OtherTok{\textless{}{-}}\NormalTok{ estimatr}\SpecialCharTok{::}\FunctionTok{lm\_robust}\NormalTok{( }
\NormalTok{  Ybar }\SpecialCharTok{\textasciitilde{}} \DecValTok{1} \SpecialCharTok{+}\NormalTok{ Z, }\AttributeTok{data =}\NormalTok{ datagg, }
  \AttributeTok{se\_type =}\NormalTok{ se\_type }
\NormalTok{)}
\FunctionTok{conf\_int}\NormalTok{(M3, }\AttributeTok{cluster =}\NormalTok{ dat}\SpecialCharTok{$}\NormalTok{sid, }\AttributeTok{vcov =} \StringTok{"CR2"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Pick some simulation parameters and estimate the coverage and interval width of both types of confidence intervals.

\subsection{Cluster RCT simulation under a strong null hypothesis}\label{cluster-rct-simulation-under-a-strong-null-hypothesis}

\subsection{Jackknife calculation of MCSEs}\label{jackknife-MCSE}

Implement the jackknife as described above in code. Check your answers against the \texttt{simhelpers} package for the built-in \texttt{t\_res} dataset:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{( simhelpers )}
\FunctionTok{calc\_relative}\NormalTok{(}\AttributeTok{data =}\NormalTok{ t\_res, }\AttributeTok{estimates =}\NormalTok{ est, }\AttributeTok{true\_param =}\NormalTok{ true\_param)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1 x 7
##   K_relative rel_bias rel_bias_mcse rel_mse
##        <int>    <dbl>         <dbl>   <dbl>
## 1       1000     1.00        0.0128   0.163
## # i 3 more variables: rel_mse_mcse <dbl>,
## #   rel_rmse <dbl>, rel_rmse_mcse <dbl>
\end{verbatim}

\subsection{Distribution theory for person-level average treatment effects}\label{cluster-RCT-SPATE}

\subsection{Multiple scenarios}\label{multiple-scenario-performance}

As foreground to the following chapters, can you explore multiple scenarios for the cluster RCT example to see if the trends are common? First write a function that takes a parameter, runs the entire simulation, and returns the results as a small table. You pick which parameter, e.g., average treatment effect, \texttt{alpha}, or whatever you like), that you wish to vary. Here is a skeleton for the function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_simulation }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{( my\_param ) \{}
  \CommentTok{\# call the sim\_function() simulation function from the end of last}
  \CommentTok{\# chapter, setting the parameter you want to vary to my\_param}
  
  \CommentTok{\# Analyze the results, generating a table of performance metrics,}
  \CommentTok{\# e.g., bias or coverage. Make sure your analysis is a data frame,}
  \CommentTok{\# like we saw earlier this chapter.}
  
  \CommentTok{\# Return results}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Then use code like the following to generate a set of results measured as a function of a varying parameter:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{vals }\OtherTok{=} \FunctionTok{seq}\NormalTok{( start, stop, }\AttributeTok{length.out =} \DecValTok{5}\NormalTok{ )}
\NormalTok{res }\OtherTok{=} \FunctionTok{map\_df}\NormalTok{( vals, my\_simulation ) }
\end{Highlighting}
\end{Shaded}

The above code will give you a data frame of results, one column for each performance measure.
Finally, you can use this table and plot the performance measure as a function of the varying parameter.

\part{Multifactor Simulations}\label{part-multifactor-simulations}

\chapter{Designing and executing multifactor simulations}\label{exp-design}

Thus far, we have created code that will run a simulation for a single combination of parameter values.
In practice, simulation studies typically examine a range of different values, including varying the levels of the focal parameter values, auxiliary parameters, sample size, and possibly other design parameters, to explore a range of different scenarios.
We either want reassurance that our findings are general, or we want to understand what aspects of the context affect the performance of the estimator or estimators we are studying.
A single simulation gives us no hint as to either of these questions.
It is only by looking across a range of settings that we can hope to understand trade-offs, general rules, and limits.
Let's now look at the remaining piece of the simulation puzzle: the study's experimental design.

Simulation studies often take the form of \textbf{full factorial} designed experiments.
In full factorials, each factor (a particular knob a researcher might turn to change the simulation conditions) is varied across multiple levels, and the design includes \emph{every} possible combination of the levels of every factor. One way to represent such a design is as a list of the factors and levels to be explored.

For example, consider a simulation study examining the performance of confidence intervals for Pearson's correlation coefficient under a bivariate Poisson distribution.
We examined this data-generating model in Section \ref{BVPois-example}, implementing it in the function \texttt{r\_bivariate\_Poisson()}. The model has three parameters (the means of each variate, \(\mu_1, \mu_2\) and the correlation \(\rho\)) and there is one design parameter (sample size, \(N\)).
Thus, we could in principle examine up to four factors.

Using these parameters directly as factors in the simulation design will lead to considerable redundancy because of the symmetry of the model: generating data with \(\mu_1 = 10\) and \(\mu_2 = 5\) would lead to identical correlations as using \(\mu_1 = 5\) and \(\mu_2 = 10\).
It is useful to re-parameterize to reduce redundancy and simply things.
We will therefore define the simulation conditions by always treating \(\mu_1\) as the larger variate and by specifying the ratio of the smaller to the larger mean as \(\lambda = \mu_2 / \mu_1\).
We might then examine the following factors:

\begin{itemize}
\tightlist
\item
  the sample size, with values of \(N = 10, 20\), or \(30\)
\item
  the mean of the larger variate, with values of \(\mu_1 = 4, 8\), or \(12\)
\item
  the ratio of means, with values of \(\lambda = 0.5\) or \(1.0\).
\item
  the true correlation, with values ranging from \(\rho = 0.0\) to \(0.7\) in steps of \(0.1\)
\end{itemize}

The above parameters describe a \(3 \times 3 \times 2 \times 8\) factorial design, where each element is the number of levels for that factor. This is a four-factor experiment, because we have four different things we are varying.

To implement this design in code, we first save the simulation parameters as a list with one entry per factor, where each entry consists of the levels that we would like to explore.
We will run a simulation for every possible combination of these values.
Here is code that generates all of the scenarios given the above design, storing these combinations in a data frame, \texttt{params}, that represents the full experimental design:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{design\_factors }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}
  \AttributeTok{N =} \FunctionTok{c}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DecValTok{20}\NormalTok{, }\DecValTok{30}\NormalTok{),}
  \AttributeTok{mu1 =} \FunctionTok{c}\NormalTok{(}\DecValTok{4}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{12}\NormalTok{),}
  \AttributeTok{lambda =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.5}\NormalTok{, }\FloatTok{1.0}\NormalTok{),}
  \AttributeTok{rho =} \FunctionTok{seq}\NormalTok{(}\FloatTok{0.0}\NormalTok{, }\FloatTok{0.7}\NormalTok{, }\FloatTok{0.1}\NormalTok{)}
\NormalTok{)}

\FunctionTok{lengths}\NormalTok{(design\_factors)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      N    mu1 lambda    rho 
##      3      3      2      8
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{params }\OtherTok{\textless{}{-}} \FunctionTok{expand\_grid}\NormalTok{( }\SpecialCharTok{!!!}\NormalTok{design\_factors )}
\NormalTok{params}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 144 x 4
##        N   mu1 lambda   rho
##    <dbl> <dbl>  <dbl> <dbl>
##  1    10     4    0.5   0  
##  2    10     4    0.5   0.1
##  3    10     4    0.5   0.2
##  4    10     4    0.5   0.3
##  5    10     4    0.5   0.4
##  6    10     4    0.5   0.5
##  7    10     4    0.5   0.6
##  8    10     4    0.5   0.7
##  9    10     4    1     0  
## 10    10     4    1     0.1
## # i 134 more rows
\end{verbatim}

We use \texttt{expand\_grid()} from the \texttt{tidyr} package to create all possible combinations of the four factors.\footnote{\texttt{expand\_grid()} is set up to take one argument per factor of the design. A clearer example of its natural syntax is:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{params }\OtherTok{\textless{}{-}} \FunctionTok{expand\_grid}\NormalTok{(}
  \AttributeTok{N =} \FunctionTok{c}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DecValTok{20}\NormalTok{, }\DecValTok{30}\NormalTok{),}
  \AttributeTok{mu1 =} \FunctionTok{c}\NormalTok{(}\DecValTok{4}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{12}\NormalTok{),}
  \AttributeTok{lambda =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.5}\NormalTok{, }\FloatTok{1.0}\NormalTok{),}
  \AttributeTok{rho =} \FunctionTok{seq}\NormalTok{(}\FloatTok{0.0}\NormalTok{, }\FloatTok{0.7}\NormalTok{, }\FloatTok{0.1}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

  However, we generally find it useful to create a list of design factors before creating the full grid of parameter values, so we prefer to make \texttt{design\_factors} first. To use \texttt{expand\_grid()} on a list, we need to use \texttt{!!!}, the splice operator from the \texttt{rlang} package, which treats \texttt{design\_factors} as a set of arguments to be passed to \texttt{expand\_grid}. The syntax may look a bit wacky, but it is succinct and useful.}
We have a total of \(3 \times 3 \times 2 \times 8 = 144\) rows, each row corresponding to a simulation scenario to explore.
With multifactor experiments, it is easy to end up running a lot of experiments!

The multi-factor aspect of a simulation is incredible important.
It can take us from an overly narrow exploration to one that has broader significance.
As \citet{little2013praise} puts it:

\begin{quote}
Good simulation studies are not given the respect they deserve. Often the design is perfunctory and simplistic, neglecting to attempt a factorial experimental design to cover the relevant sample space, and results are over-generalized. Well designed simulation studies with realistic sample sizes are an antidote to a xation on asymptotics and a useful tool for assessing calibration.
\end{quote}

\section{Choosing parameter combinations}\label{choosing-parameter-combinations}

How do we go about choosing parameter values to examine?
Choosing which parameters to use is a central part of good simulation design because the primary limitation of simulation studies is always their \emph{generalizability}.
On the one hand, it is difficult to extrapolate findings from a simulation study beyond the set of simulation conditions that were examined. On the other hand, it is often difficult or impossible to examine the full space of all possible parameter values, except for very simple problems.
Even in the relatively straightforward Pearson correlation simulation, we have four factors and the last three could each take on an infinite number of possible levels.
How can we come up with a defensible set of levels to examine?

Broadly, you generally want to vary parameters that you believe matter, or that you think other people will believe matter.
The first is so you can learn.
The second is to build your case.
The choice of simulation conditions needs to be made in the context of the problem or model that you are studying, so it is difficult to identify valid but acontextual principles.
We nonetheless offer a few points of advice, informed by our experience conducting and reading simulation studies:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  For research simulations, it often is important to be able to relate your findings to previous research. This suggests that you should select parameter levels to make this possible, such as by looking at sample sizes similar to those examined in previous studies. That said, previous simulation studies are not always perfect (actually, there are a lot of really crummy ones out there!), and so prior work should not generally be your sole guide or justification.
\item
  Generally, it is better to err on the side of being more comprehensive. You learn more by looking at a broader range of conditions, and you can always boil down your results to a more limited set of conditions for purposes of presentation.
\item
  It is also important to explore breakdown points (e.g., what sample size is too small for a method to work?) rather than focusing only on conditions where a method might be expected to work well. Pushing the boundaries and identifying conditions where estimation methods break will help you to provide better guidance for how the methods should be used in practice.
\end{enumerate}

On point (2), comprehensiveness will generally increase the amount of computing required for a simulation. However, this can be tempered by reducing the number of replications per scenario.
For example, say you were planning on doing 1000 simulations per scenario, but then you realize there is some other factor that you do not think matters, but that you believe other researchers will worry about.
You could add in that factor, say with four levels, and then do 250 simulations per scenario.
The total work remains the same.
When analyzing the final simulation you would first verify you do not see trends along this new factor, and then marginalize out that factor in your summaries of results.
Marginalizing out a factor (i.e., averaging your performance metrics across the additional factor) is a powerful technique of making a claim about how your methods work \emph{on average} across a \emph{range} of scenarios, rather than for a specific scenario.
We will discuss it further in Chapter \ref{presentation-of-results}.

Once you have identified your parameters, you then have to decide on the levels of the parameter you will include in the simulation.
There are three strategies you might take:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Vary a parameter over as much of its range as you can.
\item
  Choose parameter levels to represent a realistic practical range. These ranges would ideally be empirically justified based on systematic reviews of prior applications. Lacking such empirical evidence, you may need to rely on informal impressions of what is realistic in practice.
\item
  Choose parameters to emulate an important known application or context.
\end{enumerate}

Of these choices, option (1) is the most general but also the most computationally intensive.
In the ideal, option (2) focuses attention on what is of practical relevance to a practitioner.
Option (3) is usually coupled with a subsequent applied data analysis, and in this case the simulation is often used to enrich that analysis.
In particular, if the simulation shows the methods work for data with the given form of the target application, people may be more willing to believe the application's findings.

Regardless of how you select your primary parameters, you should also vary nuisance parameters (at least a little) to test the sensitivity of your results to these other aspects.
While simulations will (generally) never be fully generalizable, you can certainly make them so they avoid the obvious things a critic might identify as a basis for dismissing your findings.

To recap, as you think about your parameter selection, always keep the following design principles and acknowledgements:

\begin{itemize}
\tightlist
\item
  The primary limitation of simulation studies is generalizability.
\item
  Choose conditions that allow you to relate your findings to previous work.
\item
  Err towards being comprehensive. Your goal should be to build an understanding of the major moving parts, and you can always tailor your final presentation of results to give the simplified story, once you find it.
\item
  Explore breakdown points and boundary conditions (e.g., what sample size is too small for applying a given method?).
\end{itemize}

Finally, you should fully expect to add and subtract from your set of simulation factors as you get your initial simulation results. Rarely does anyone nail the choice of parameters on the first pass.

\section{Using pmap to run multifactor simulations}\label{using-pmap-to-run-multifactor-simulations}

Once we have selected factors and levels for simulation, we now need to run the simulation code across all of our factor combinations.
Conceptually, each row of our \texttt{params} dataset represents a single simulation scenario, and we want to run our simulation code for each of these scenarios.
We would thus call our simulation function, using all the values in that row as parameters to pass to the function.

One way to call a function on each row of a dataset in this manner is by using \texttt{pmap()} from the \texttt{purrr} package.
\texttt{pmap()} marches down a set of lists, running a function on each \(p\)-tuple of elements, taking the \(i^{th}\) element from each list for iteration \(i\), and passing them as parameters to the specified function.
\texttt{pmap()} then returns the results of this sequence of function calls as a list of results.\footnote{Just like \texttt{map()} or \texttt{map2()}, \texttt{pmap()} has variants such as \texttt{\_dbl} or \texttt{\_df}.
  These variants automatically stack or convert the list of things returned into a tidier collection (for \texttt{\_dbl} it will convert to a vector of numbers, for \texttt{\_df} it will stack the results to make a large dataframe, assuming each thing returned is a little dataframe).}
Because R's \texttt{data.frame} objects are also sets of lists (where each variable is a vector, which is a simple form of list), \texttt{pmap()} also works seemlessly on \texttt{data.frame} or \texttt{tibble} objects.

Here is a small illustration of \texttt{pmap()} in action:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{some\_function }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{( a, b, theta, scale ) \{}
\NormalTok{    scale }\SpecialCharTok{*}\NormalTok{ (a }\SpecialCharTok{+}\NormalTok{ theta}\SpecialCharTok{*}\NormalTok{(b}\SpecialCharTok{{-}}\NormalTok{a))}
\NormalTok{\}}

\NormalTok{args\_data }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{( }\AttributeTok{a =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{, }\AttributeTok{b =} \DecValTok{5}\SpecialCharTok{:}\DecValTok{7}\NormalTok{, }\AttributeTok{theta =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.2}\NormalTok{, }\FloatTok{0.3}\NormalTok{, }\FloatTok{0.7}\NormalTok{) )}
\NormalTok{purrr}\SpecialCharTok{::}\FunctionTok{pmap}\NormalTok{( args\_data, }\AttributeTok{.f =}\NormalTok{ some\_function, }\AttributeTok{scale =} \DecValTok{10}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [[1]]
## [1] 18
## 
## [[2]]
## [1] 32
## 
## [[3]]
## [1] 58
\end{verbatim}

One important constraint of \texttt{pmap()} is that the variable names over which to iterate over must correspond exactly to arguments of the function to be evaluated.
In the above example, \texttt{args\_data} must have column names that correspond to the arguments of \texttt{some\_function}.
For functions with additional arguments that are not manipulated, extra parameters can be passed after the function name (as in the \texttt{scale} argument in this example).
These will also be passed to each function call, but will be the same for all calls.

Let's now implement this technique for our simulation of confidence intervals for Pearson's correlation coefficient.
In Section \ref{estimation-functions}, we developed a function called \texttt{r\_and\_z()} for computing confidence intervals for Pearson's correlation using Fisher's \(z\) transformation;
then in Section \ref{assessing-confidence-intervals}, we wrote a function called \texttt{evaluate\_CIs()} for evaluating confidence interval coverage and average width.
We can bundle \texttt{r\_bivariate\_Poisson()}, \texttt{r\_and\_z()}, and \texttt{evaluate\_CIs()} into a simulation driver function by taking

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(simhelpers)}

\NormalTok{Pearson\_sim }\OtherTok{\textless{}{-}} \FunctionTok{bundle\_sim}\NormalTok{(}
  \AttributeTok{f\_generate =}\NormalTok{ r\_bivariate\_Poisson, }\AttributeTok{f\_analyze =}\NormalTok{ r\_and\_z, }\AttributeTok{f\_summarize =}\NormalTok{ evaluate\_CIs}
\NormalTok{)}
\FunctionTok{args}\NormalTok{(Pearson\_sim)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## function (reps, N, mu1, mu2, rho = 0, seed = NA_integer_, summarize = TRUE) 
## NULL
\end{verbatim}

This function will run a simulation for a given scenario:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{Pearson\_sim}\NormalTok{(}\DecValTok{1000}\NormalTok{, }\AttributeTok{N =} \DecValTok{10}\NormalTok{, }\AttributeTok{mu1 =} \DecValTok{5}\NormalTok{, }\AttributeTok{mu2 =} \DecValTok{5}\NormalTok{, }\AttributeTok{rho =} \FloatTok{0.3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1 x 5
##   K_coverage coverage coverage_mcse width
##        <int>    <dbl>         <dbl> <dbl>
## 1       1000    0.952       0.00676  1.10
## # i 1 more variable: width_mcse <dbl>
\end{verbatim}

In order to call \texttt{Pearson\_sim()}, we will need to ensure that the columns of the \texttt{params} dataset correspond to the arguments of the function.
Because we re-parameterized the model in terms of \(\lambda\), we will first need to compute the parameter value for \(\mu_2\) and remove the \texttt{lambda} variable because it is not an argument of \texttt{Pearson\_sim()}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{params\_mod }\OtherTok{\textless{}{-}} 
\NormalTok{  params }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{mu2 =}\NormalTok{ mu1 }\SpecialCharTok{*}\NormalTok{ lambda) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{lambda)}
\end{Highlighting}
\end{Shaded}

Now we can use \texttt{pmap()} to run the simulation for all 144 parameter settings:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sim\_results }\OtherTok{\textless{}{-}}\NormalTok{ params}
\NormalTok{sim\_results}\SpecialCharTok{$}\NormalTok{res }\OtherTok{\textless{}{-}} \FunctionTok{pmap}\NormalTok{(params\_mod, Pearson\_sim, }\AttributeTok{reps =} \DecValTok{1000}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

The above code calls our \texttt{run\_alpha\_sim()} method for each row in the list of scenarios we want to explore.
Conveniently, we can store the results \textbf{as a new variable in the same dataset}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sim\_results}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 144 x 5
##        N   mu1   rho   mu2 res             
##    <dbl> <dbl> <dbl> <dbl> <list>          
##  1    10     4   0       2 <tibble [1 x 5]>
##  2    10     4   0.1     2 <tibble [1 x 5]>
##  3    10     4   0.2     2 <tibble [1 x 5]>
##  4    10     4   0.3     2 <tibble [1 x 5]>
##  5    10     4   0.4     2 <tibble [1 x 5]>
##  6    10     4   0.5     2 <tibble [1 x 5]>
##  7    10     4   0.6     2 <tibble [1 x 5]>
##  8    10     4   0.7     2 <tibble [1 x 5]>
##  9    10     4   0       4 <tibble [1 x 5]>
## 10    10     4   0.1     4 <tibble [1 x 5]>
## # i 134 more rows
\end{verbatim}

The above code may look a bit peculiar: we are storing a set of dataframes (our result) in our original dataframe.
This is actually ok in R: our results will be in what is called a \textbf{list-column}, where each element in our list column is the little summary of our simulation results for that scenario.
Here is the third scenario, for example:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sim\_results}\SpecialCharTok{$}\NormalTok{res[[}\DecValTok{3}\NormalTok{]]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1 x 5
##   K_coverage coverage coverage_mcse width
##        <int>    <dbl>         <dbl> <dbl>
## 1       1000    0.947       0.00708  1.13
## # i 1 more variable: width_mcse <dbl>
\end{verbatim}

List columns are neat, but hard to work with.
To turn the list-column into normal data, we can use \texttt{unnest()} to expand the \texttt{res} variable, replicating the values of the main variables once for each row in the nested dataset:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sim\_results }\OtherTok{\textless{}{-}} \FunctionTok{unnest}\NormalTok{(sim\_results, }\AttributeTok{cols =}\NormalTok{ res)}
\NormalTok{sim\_results}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 144 x 9
##        N   mu1   rho   mu2 K_coverage coverage
##    <dbl> <dbl> <dbl> <dbl>      <int>    <dbl>
##  1    10     4   0       2       1000    0.949
##  2    10     4   0.1     2       1000    0.954
##  3    10     4   0.2     2       1000    0.947
##  4    10     4   0.3     2       1000    0.94 
##  5    10     4   0.4     2       1000    0.953
##  6    10     4   0.5     2       1000    0.953
##  7    10     4   0.6     2       1000    0.938
##  8    10     4   0.7     2       1000    0.945
##  9    10     4   0       4       1000    0.956
## 10    10     4   0.1     4       1000    0.94 
## # i 134 more rows
## # i 3 more variables: coverage_mcse <dbl>,
## #   width <dbl>, width_mcse <dbl>
\end{verbatim}

Putting all of this together into a tidy workflow leads to the following:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sim\_results }\OtherTok{\textless{}{-}} 
\NormalTok{  params }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}
    \AttributeTok{mu2 =}\NormalTok{ mu1 }\SpecialCharTok{*}\NormalTok{ lambda,}
    \AttributeTok{reps =} \DecValTok{1000}
\NormalTok{  ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}
    \AttributeTok{res =} \FunctionTok{pmap}\NormalTok{(dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(., }\SpecialCharTok{{-}}\NormalTok{lambda), }\AttributeTok{.f =}\NormalTok{ Pearson\_sim)}
\NormalTok{  ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{unnest}\NormalTok{(}\AttributeTok{cols =}\NormalTok{ res)}
\end{Highlighting}
\end{Shaded}

If you like, you can simply use the \texttt{evaluate\_by\_row()} function from the \texttt{simhelpers} package:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sim\_results }\OtherTok{\textless{}{-}} 
\NormalTok{  params }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{( }\AttributeTok{mu2 =}\NormalTok{ mu1 }\SpecialCharTok{*}\NormalTok{ lambda ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{evaluate\_by\_row}\NormalTok{( Pearson\_sim, }\AttributeTok{reps =} \DecValTok{1000}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

An advantage of \texttt{evaluate\_by\_row()} is that the input dataset can include extra variables (such as \texttt{lambda}).
Another advantage is that it is easy to run the calculations in parallel; see Chapter \ref{parallel-processing}.

As a final step, we save our results using tidyverse's \texttt{write\_rds()}; see \href{https://r4ds.hadley.nz/data-import.html\#sec-writing-to-a-file}{R for Data Science, Section 7.5}.
We first ensure we have a directory by making one via \texttt{dir.create()} (see Chapter \ref{saving-files} for more on files):

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{dir.create}\NormalTok{( }\StringTok{"results"}\NormalTok{, }\AttributeTok{showWarnings =} \ConstantTok{FALSE}\NormalTok{ )}
\FunctionTok{write\_rds}\NormalTok{( sim\_results, }\AttributeTok{file =} \StringTok{"results/Pearson\_Poisson\_results.rds"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

We now have a complete set of simulation results for all of the scenarios we specified.

\section{When to calculate performance metrics}\label{when-to-calculate-performance-metrics}

For a single-scenario simulation, we repeatedly generate and analyze data, and then assess the performance across the repetitions.
When we extend this process to multifactor simulations, we have a choice: do we compute performance measures for each simulation scenario as we go (inside) or do we compute all of them after we get all of our individual results (outside)?
There are pros and cons to each approach.

\subsection{Aggregate as you simulate (inside)}\label{aggregate-as-you-simulate-inside}

The \emph{inside} approach runs a stand-alone simulation for each scenario of interest. For each combination of factors, we simulate data, apply our estimators, assess performance, and return a table with summary performance measures. We can then stack these tables to get a dataset with all of the results, ready for analysis.

This is the approach we illustrated above. It is straightforward and streamlined: we already have a method to run simulations for a single scenario, and we just repeat it across multiple scenarios and combine the outputs.
After calling \texttt{pmap()} (or \texttt{evaluate\_by\_row()}) and stacking the results, we end up with a dataset containing all the simulation conditions, one simulation context per row (or maybe we have sets of several rows for each simulation context, with one row for each method), with the columns consisting of the simulation factors and measured performance outcomes.
This table of performance is ideally all we need to conduct further analysis and write up the results.

The primary advantages of the inside strategy are that it is easy to modularize the simulation code and it produces a compact dataset of results, minimizing the number and size of files that need to be stored.
On the con side, calculating summary performance measures inside of the simulation driver limits our ability to add new performance measures on the fly or to examine the distribution of individual estimates.
For example, say we wanted to check if the distribution of Fisher-z estimates in a particular scenario was right-skewed, perhaps because we are worried that the estimator sometimes breaks down.
We might want to make a histogram of the point estimates, or calculate the skew of the estimates as a performance measure.
Because the individual estimates are not saved, we would have no way of investigating these questions without rerunning the simulation for that condition.
In short, the inside strategy minimizes disk space but constrains our ability to explore or revise performance calculations.

\subsection{Keep all simulation runs (outside)}\label{keep-all-simulation-runs-outside}

The \emph{outside} approach involves retaining the entire set of estimates from every replication, with each row corresponding to an estimate for a given simulated dataset.
The benefit of the outside approach is that it allows us to add or change how we calculate performance measures without re-running the entire simulation.
This is especially important if the simulation is time-intensive, such as when the estimators being evaluated are computationally expensive.
The primary disadvantage the outside approach is that it produces large amounts of data that need to be stored and further manipulated.
Thus, the outside strategy maximizes flexibility, at the cost of increased dataset size.

In our Pearson correlation simulation, we initially followed the inside strategy. To move to the outside strategy, we can set the \texttt{summarize} argument of \texttt{Pearson\_sim()} to \texttt{FALSE} so that the simulation driver returns a row for every replication:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{Pearson\_sim}\NormalTok{(}\AttributeTok{reps =} \DecValTok{4}\NormalTok{, }\AttributeTok{N =} \DecValTok{15}\NormalTok{, }\AttributeTok{mu1 =} \DecValTok{5}\NormalTok{, }\AttributeTok{mu2 =} \DecValTok{5}\NormalTok{, }\AttributeTok{rho =} \FloatTok{0.5}\NormalTok{, }\AttributeTok{summarize =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           r         z      CI_lo     CI_hi
## 1 0.6486750 0.7730077  0.2042991 0.8713839
## 2 0.6680333 0.8071830  0.2368083 0.8793686
## 3 0.3530928 0.3689727 -0.1943175 0.7328080
## 4 0.3729984 0.3919016 -0.1721595 0.7432467
\end{verbatim}

We then save the entire set of estimates, rather than the performance summaries.
This result file will have \(R\) times as many rows as the older file. In practice, these results can quickly get to be extremely large.
But disk space is cheap!
Here we run the same experiment as in Section \ref{using-pmap-to-run-multifactor-simulations}, but storing the individual replications instead of just the summarized results:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sim\_results\_full }\OtherTok{\textless{}{-}} 
\NormalTok{  params }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{( }\AttributeTok{mu2 =}\NormalTok{ mu1 }\SpecialCharTok{*}\NormalTok{ lambda ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{evaluate\_by\_row}\NormalTok{( Pearson\_sim, }\AttributeTok{reps =} \DecValTok{1000}\NormalTok{, }\AttributeTok{summarize =} \ConstantTok{FALSE}\NormalTok{ )}

\FunctionTok{write\_rds}\NormalTok{( sim\_results\_full, }\AttributeTok{file =} \StringTok{"results/Pearson\_Poisson\_results\_full.rds"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

We end up with many more rows.
Here is the number of rows for the outside vs inside approach:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{c}\NormalTok{(}\AttributeTok{inside =} \FunctionTok{nrow}\NormalTok{( sim\_results ), }\AttributeTok{outside =} \FunctionTok{nrow}\NormalTok{( sim\_results\_full ))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  inside outside 
##     144  144000
\end{verbatim}

Comparing the file sizes on the disk:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{c}\NormalTok{(}
  \AttributeTok{inside =} \FunctionTok{file.size}\NormalTok{(}\StringTok{"results/Pearson\_Poisson\_results.rds"}\NormalTok{),}
  \AttributeTok{outside =} \FunctionTok{file.size}\NormalTok{(}\StringTok{"results/Pearson\_Poisson\_results\_full.rds"}\NormalTok{)}
\NormalTok{) }\SpecialCharTok{/} \DecValTok{2}\SpecialCharTok{\^{}}\DecValTok{10} \CommentTok{\# Kb}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     inside    outside 
##   43.14551 9000.31055
\end{verbatim}

The first is several kilobytes, the second is several megabytes.

\subsection{Getting raw results ready for analysis}\label{getting-raw-results-ready-for-analysis}

If we generate raw results, we then need to do the performance calculations across replications within each simulation context so that we can explore the trends across simulation factors.

One way to do this is to use \texttt{group\_by()} and \texttt{summarize()} to carry out the performance calculations:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sim\_results\_full }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{( N, mu1, mu2, rho ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{( }
    \FunctionTok{calc\_coverage}\NormalTok{(}\AttributeTok{lower\_bound =}\NormalTok{ CI\_lo, }\AttributeTok{upper\_bound =}\NormalTok{ CI\_hi, }\AttributeTok{true\_param =}\NormalTok{ rho)}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 144 x 9
## # Groups:   N, mu1, mu2 [18]
##        N   mu1   mu2   rho K_coverage coverage
##    <dbl> <dbl> <dbl> <dbl>      <int>    <dbl>
##  1    10     4     2   0         1000    0.945
##  2    10     4     2   0.1       1000    0.936
##  3    10     4     2   0.2       1000    0.956
##  4    10     4     2   0.3       1000    0.944
##  5    10     4     2   0.4       1000    0.946
##  6    10     4     2   0.5       1000    0.935
##  7    10     4     2   0.6       1000    0.95 
##  8    10     4     2   0.7       1000    0.945
##  9    10     4     4   0         1000    0.946
## 10    10     4     4   0.1       1000    0.944
## # i 134 more rows
## # i 3 more variables: coverage_mcse <dbl>,
## #   width <dbl>, width_mcse <dbl>
\end{verbatim}

If we want to use our full performance measure function \texttt{evaluate\_CIs()} to get additional metrics such as MCSEs, we would \emph{nest} our data into a series of mini-datasets (one for each simulation), and then process each element.
As we saw above, nesting collapses a larger dataset into one where one of the variables consists of a list of datasets:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{results }\OtherTok{\textless{}{-}} 
\NormalTok{  sim\_results\_full }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{group\_by}\NormalTok{( N, mu1, mu2, rho ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{nest}\NormalTok{( }\AttributeTok{.key =} \StringTok{"res"}\NormalTok{ )}
\NormalTok{results}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 144 x 5
## # Groups:   N, mu1, mu2, rho [144]
##        N   mu1   rho   mu2 res                 
##    <dbl> <dbl> <dbl> <dbl> <list>              
##  1    10     4   0       2 <tibble [1,000 x 4]>
##  2    10     4   0.1     2 <tibble [1,000 x 4]>
##  3    10     4   0.2     2 <tibble [1,000 x 4]>
##  4    10     4   0.3     2 <tibble [1,000 x 4]>
##  5    10     4   0.4     2 <tibble [1,000 x 4]>
##  6    10     4   0.5     2 <tibble [1,000 x 4]>
##  7    10     4   0.6     2 <tibble [1,000 x 4]>
##  8    10     4   0.7     2 <tibble [1,000 x 4]>
##  9    10     4   0       4 <tibble [1,000 x 4]>
## 10    10     4   0.1     4 <tibble [1,000 x 4]>
## # i 134 more rows
\end{verbatim}

Note how each row of our nested data has a little tibble containing the results for that context, with 1000 rows each.
Once nested, we can then use \texttt{map2()} to apply a function to each element of \texttt{res}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{results\_summary }\OtherTok{\textless{}{-}} 
\NormalTok{  results }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{( }\AttributeTok{performance =} \FunctionTok{map2}\NormalTok{( res, rho, evaluate\_CIs ) ) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{( }\SpecialCharTok{{-}}\NormalTok{res ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{unnest}\NormalTok{( }\AttributeTok{cols=}\StringTok{"performance"}\NormalTok{ ) }
\NormalTok{results\_summary}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 144 x 9
## # Groups:   N, mu1, mu2, rho [144]
##        N   mu1   rho   mu2 K_coverage coverage
##    <dbl> <dbl> <dbl> <dbl>      <int>    <dbl>
##  1    10     4   0       2       1000    0.945
##  2    10     4   0.1     2       1000    0.936
##  3    10     4   0.2     2       1000    0.956
##  4    10     4   0.3     2       1000    0.944
##  5    10     4   0.4     2       1000    0.946
##  6    10     4   0.5     2       1000    0.935
##  7    10     4   0.6     2       1000    0.95 
##  8    10     4   0.7     2       1000    0.945
##  9    10     4   0       4       1000    0.946
## 10    10     4   0.1     4       1000    0.944
## # i 134 more rows
## # i 3 more variables: coverage_mcse <dbl>,
## #   width <dbl>, width_mcse <dbl>
\end{verbatim}

We have built our final performance table \emph{after} running the entire simulation, rather than running it on each simulation scenario in turn.

Now, if we want to add a performance metric, we can simply change \texttt{evaluate\_CIs} and recalculate, without having to recompute the entire simulation.
Summarizing during the simulation vs.~after, as we just did, leads to the same set of results.\footnote{In fact, if we use the same seed, we should obtain \emph{exactly} the same results.}
Allowing yourself the flexibility to re-calculate performance measures can be very advantageous, and we tend to follow this outside strategy for any simulations involving more complex estimation procedures.

\section{Summary}\label{summary}

Multifactor simulations are simply a systematically generated series of individual scenario simulations.
The overall workflow is to first identify the factors and levels to explore (which we store as a dataset of all the combinations of the factors desired).
Think of this as a menu, or checklist, of simulations to run.
The next step is to then to walk down the list, running each simulation in turn.

Each individual simulation will generate its own set of results.
These can be the raw results (individual simulation iterations) or summary results (performance measures).
We stack all of these results, connecting them to the simulation factors they came from, to get a single massive dataset.
The key question is then how to explore this full set of results; without much difficulty, the amount of results generated can be kind of overwhelming!

The next chapters dive into how to take on this final task.

\section{Case Study: A multifactor evaluation of cluster RCT estimators}\label{case-study-a-multifactor-evaluation-of-cluster-rct-estimators}

To bring the multifactor simulation to life, let us return to the case study of comparing three ways to analyze a cluster randomized trial that we presented in Section \ref{case-cluster}.
In our original setup, we wrote code to generate cluster randomized trial data where a cluster's size could be correlated its average treatment effect.
Then, in Chapter \ref{clusterRCTperformance} we looked across a variety of performance criteria so we could see how the estimators compared for any given scenario we wanted.

So far, we have only examined a single scenario at a time.
But how do our findings generalize? Under what conditions do the various estimation methods perform better or worse?
To answer these questions, we need to extend to a multifactor simulation to \emph{systematically} explore how our three estimators behave across a range of contexts.
Happily, the modular functions that we have designed make it relatively straightforward to explore a range of scenarios by calling our simulation function over and over, using the tools of this chapter.
We illustrate extending to a full multifactor simulation next, and then continue our running example to discuss how to analyze the results in Chapter \ref{building-good-visualization}.

\subsection{Choosing parameters for the Clustered RCT}\label{choosing-parameters-for-the-clustered-rct}

We begin by identifying some potential research questions suggested by our preliminary exploration.
Regarding bias, we noticed in our initial simulation that Linear Regression targets a person-weighted average effect, so it would be considered biased for the cluster-average average treatment effect.
We might then ask, how large is bias in practice, and how much does bias change as we change the cluster-size by impact relationship?
Considering precision, we saw that Linear Regression has a higher standard error than the other estimators.
But is this a general finding? If not, are there contexts where linear regression will have a lower standard error than the others?
Further, we originally thought that aggregation would lose information because smaller clusters would have the same weight as larger clusters, but be more imprecisely estimated.
Were we wrong? Or perhaps if cluster size was even more variable, aggregation might do worse and worse.
Finally, the estimated SEs for all three methods all appeared to be good, although they were rather variable, relative to the true SE.
We might then ask, are the standard errors always the right size, on average? Will the estimated SEs fall apart (i.e., be far too large or far too small) in some contexts? If so, which ones?

To answer all of these questions we need to more systematically explore the space of models.
But we have a lot of knobs to turn.
In particular, our data-generating process will produce artificial cluster-randomized experiments where we can vary any of the following features:

\begin{itemize}
\tightlist
\item
  the number of clusters;
\item
  the proportion of clusters that receive treatment;
\item
  the average cluster size and degree of variation in cluster sizes;
\item
  how much the average impact varies across clusters, and how strongly that is connected to cluster size;
\item
  how much the cluster intercepts vary (degree of cross-cluster variation); and
\item
  the degree of residual variation.
\end{itemize}

Manipulating all of these factors would lead to a huge and unwieldy number of simulation conditions to evaluate.
Before proceeding, we reflect on our research questions, speculate as to what is likely to matter, and then consider varying the following:

\begin{itemize}
\tightlist
\item
  Number of clusters: Do cluster-robust SEs work with fewer clusters?
\item
  Average cluster size: Does the number of students/cluster matter?
\item
  Variation in cluster size: Do varying cluster sizes cause bias or break things?
\item
  Correlation of cluster size and cluster impact: Will correlation cause bias?
\item
  Cross cluster variation: Does the amount of cluster variation matter?
\end{itemize}

When selecting factors to manipulate, it is important to ensure the each factor is isolated, so that changing one of them should not change other aspects of the data-generating process that might impact performance.
For example, if we simply added more cross-cluster variation by directly increasing the random effects for the clusters, the total variation in the outcome will also increase.
If we then see that the performance of an estimator deteriorates as variation increases, we have a confound: is the cross-cluster variation causing the problem, or is it the total variation?
To avoid this confound, we should vary cluster variation while holding the total variation fixed; this is why we use the ICC parameterization, as discussed in Section \ref{case-cluster}.

Given our research questions and the way we parameterize the DGP, we end up with the following factors and levels:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{crt\_design\_factors }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}
  \AttributeTok{n\_bar =} \FunctionTok{c}\NormalTok{( }\DecValTok{20}\NormalTok{, }\DecValTok{80}\NormalTok{, }\DecValTok{320}\NormalTok{ ),}
  \AttributeTok{J =} \FunctionTok{c}\NormalTok{( }\DecValTok{5}\NormalTok{, }\DecValTok{20}\NormalTok{, }\DecValTok{80}\NormalTok{ ),}
  \AttributeTok{ATE =} \FunctionTok{c}\NormalTok{( }\FloatTok{0.2}\NormalTok{ ),}
  \AttributeTok{size\_coef =} \FunctionTok{c}\NormalTok{( }\DecValTok{0}\NormalTok{, }\FloatTok{0.2}\NormalTok{ ),}
  \AttributeTok{ICC =} \FunctionTok{c}\NormalTok{( }\DecValTok{0}\NormalTok{, }\FloatTok{0.2}\NormalTok{, }\FloatTok{0.4}\NormalTok{, }\FloatTok{0.6}\NormalTok{, }\FloatTok{0.8}\NormalTok{ ),}
  \AttributeTok{alpha =} \FunctionTok{c}\NormalTok{( }\DecValTok{0}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\FloatTok{0.8}\NormalTok{ )}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The ATE factor only has one level. We could later expand this if we wanted to, but based on theoretical concerns we are fairly confident the ATE will not impact our research questions, so we leave it as it is, set at a value we would consider plausible in real life.

\subsection{Redundant factor combinations}\label{redundant-factor-combinations}

There is some redundancy in the parameter combinations that we have selected.
In particular, if \texttt{size\_coef} is nonzero, but \texttt{alpha} is 0, then the cluster size will not impact the cluster average ATE because there is no variation in cluster size.
We could drop one of the redundant conditions to save some simulation runs---in essence we are running the same simulation for \texttt{alpha=0} two times, once for each \texttt{size\_coef} value, for each level of \texttt{ICC}, \texttt{n\_bar} and \texttt{J}.
However, doing so would mean that our simulation is no longer fully crossed.
We will leave the redundant conditions in as a sanity check for our results.
We know we should get the same results and if we do not, then we either have uncontrolled uncertainty in our simulation or an error in the code.
If computation is cheap, then keeping the fully crossed structure will make for easier analysis.
Otherwise our experiment is not balanced, and so when we average performance across some factors to see the effect of others, we might end up with surprising and misleading results.

\subsection{Running the simulations}\label{running-the-simulations}

We will run our cluster RCT simulation using the same code pattern as we used with the Pearson correlation simulations.
Because we are not exactly sure which performance metrics we will want to use, we will save the individual replications and then calculate performance metrics after generating the simulation results.
That is, we will use the outside strategy.

We first make a table of scenarios:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{params }\OtherTok{\textless{}{-}} 
  \FunctionTok{expand\_grid}\NormalTok{( }\SpecialCharTok{!!!}\NormalTok{crt\_design\_factors ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}
    \AttributeTok{seed =} \DecValTok{20200320} \SpecialCharTok{+} \DecValTok{17} \SpecialCharTok{*} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{n}\NormalTok{()}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

To allow reproducibility, we specify a different seed for each scenario just to avoid anything confusing about shared randomness across scenarios (see Section \ref{seeds-and-pseudo-RNGs} for further discussion).
We then run the simulation 1000 times each for scenario, then unnest to get our final data:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{params}\SpecialCharTok{$}\NormalTok{res }\OtherTok{\textless{}{-}} \FunctionTok{pmap}\NormalTok{(params, }\AttributeTok{.f =}\NormalTok{ run\_CRT\_sim, }\AttributeTok{reps =} \DecValTok{1000}\NormalTok{ )}
\NormalTok{res }\OtherTok{\textless{}{-}} \FunctionTok{unnest}\NormalTok{(params, }\AttributeTok{cols=}\NormalTok{data)}
\FunctionTok{saveRDS}\NormalTok{( res, }\AttributeTok{file =} \StringTok{"results/simulation\_CRT.rds"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

Normally, we would speed up these calculations using parallel processing; we discuss how to do so in Chapter \ref{parallel-processing}.
Even under parallel processing, this simulation took overnight to run.
Our final results look like this:

\begin{verbatim}
## # A tibble: 810,000 x 13
##    n_bar     J   ATE size_coef   ICC alpha  reps
##    <dbl> <dbl> <dbl>     <dbl> <dbl> <dbl> <dbl>
##  1    20     5   0.2         0     0     0  1000
##  2    20     5   0.2         0     0     0  1000
##  3    20     5   0.2         0     0     0  1000
##  4    20     5   0.2         0     0     0  1000
##  5    20     5   0.2         0     0     0  1000
##  6    20     5   0.2         0     0     0  1000
##  7    20     5   0.2         0     0     0  1000
##  8    20     5   0.2         0     0     0  1000
##  9    20     5   0.2         0     0     0  1000
## 10    20     5   0.2         0     0     0  1000
## # i 809,990 more rows
## # i 6 more variables: seed <dbl>, runID <chr>,
## #   method <chr>, ATE_hat <dbl>, SE_hat <dbl>,
## #   p_value <dbl>
\end{verbatim}

We have a lot of rows of data!

\subsection{Calculating performance metrics}\label{calculating-performance-metrics}

Our next step is to group the results by the simulation factors and calculate the performance metrics across the replications of each simulation condition.
Here we calculate our primary performance measures by hand:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{res }\OtherTok{\textless{}{-}} \FunctionTok{readRDS}\NormalTok{( }\AttributeTok{file =} \StringTok{"results/simulation\_CRT.rds"}\NormalTok{ )}

\NormalTok{sres }\OtherTok{\textless{}{-}} 
\NormalTok{  res }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{( n\_bar, J, ATE, size\_coef, ICC, alpha, method ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{( }
    \AttributeTok{bias =} \FunctionTok{mean}\NormalTok{(ATE\_hat }\SpecialCharTok{{-}}\NormalTok{ ATE),}
    \AttributeTok{SE =} \FunctionTok{sd}\NormalTok{( ATE\_hat ),}
    \AttributeTok{RMSE =} \FunctionTok{sqrt}\NormalTok{( }\FunctionTok{mean}\NormalTok{( (ATE\_hat }\SpecialCharTok{{-}}\NormalTok{ ATE )}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{ ) ),}
    \AttributeTok{ESE\_hat =} \FunctionTok{sqrt}\NormalTok{( }\FunctionTok{mean}\NormalTok{( SE\_hat}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{ ) ),}
    \AttributeTok{SD\_SE\_hat =} \FunctionTok{sqrt}\NormalTok{( }\FunctionTok{sd}\NormalTok{( SE\_hat}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{ ) ),}
    \AttributeTok{power =} \FunctionTok{mean}\NormalTok{( p\_value }\SpecialCharTok{\textless{}=} \FloatTok{0.05}\NormalTok{ ),}
    \AttributeTok{R =} \FunctionTok{n}\NormalTok{(),}
    \AttributeTok{.groups =} \StringTok{"drop"}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

If we want MCSEs (as we usually do), then we could do that by hand or use the \texttt{simhelpers} package as so:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{( simhelpers )}

\NormalTok{sres }\OtherTok{\textless{}{-}} 
\NormalTok{  res }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{( n\_bar, J, ATE, size\_coef, ICC, alpha, method ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{( }
    \FunctionTok{calc\_absolute}\NormalTok{( }
      \AttributeTok{estimates =}\NormalTok{ ATE\_hat, }\AttributeTok{true\_param =}\NormalTok{ ATE, }
      \AttributeTok{criteria =} \FunctionTok{c}\NormalTok{(}\StringTok{"bias"}\NormalTok{,}\StringTok{"stddev"}\NormalTok{,}\StringTok{"rmse"}\NormalTok{)}
\NormalTok{    ),}
    \FunctionTok{calc\_relative\_var}\NormalTok{( }
      \AttributeTok{estimates =}\NormalTok{ ATE\_hat, }\AttributeTok{var\_estimates =}\NormalTok{ SE\_hat}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{,}
      \AttributeTok{criteria =} \StringTok{"relative bias"}
\NormalTok{    ) }
\NormalTok{  ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{rename}\NormalTok{( }\AttributeTok{SE =}\NormalTok{ stddev, }\AttributeTok{SE\_mcse =}\NormalTok{ stddev\_mcse ) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{( }\SpecialCharTok{{-}}\NormalTok{K\_absolute, }\SpecialCharTok{{-}}\NormalTok{K\_relvar ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ungroup}\NormalTok{()}

\FunctionTok{glimpse}\NormalTok{( sres )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Rows: 810
## Columns: 15
## $ n_bar             <dbl> 20, 20, 20, 20, 20, 20~
## $ J                 <dbl> 5, 5, 5, 5, 5, 5, 5, 5~
## $ ATE               <dbl> 0.2, 0.2, 0.2, 0.2, 0.~
## $ size_coef         <dbl> 0, 0, 0, 0, 0, 0, 0, 0~
## $ ICC               <dbl> 0.0, 0.0, 0.0, 0.0, 0.~
## $ alpha             <dbl> 0.0, 0.0, 0.0, 0.5, 0.~
## $ method            <chr> "Agg", "LR", "MLM", "A~
## $ bias              <dbl> 0.0078528924, 0.007852~
## $ bias_mcse         <dbl> 0.006512388, 0.0065123~
## $ SE                <dbl> 0.2059398, 0.2059398, ~
## $ SE_mcse           <dbl> 0.004380492, 0.0043804~
## $ rmse              <dbl> 0.2059865, 0.2059865, ~
## $ rmse_mcse         <dbl> 0.005468829, 0.0054688~
## $ rel_bias_var      <dbl> 0.9834635, 0.9834635, ~
## $ rel_bias_var_mcse <dbl> 0.05290317, 0.05290317~
\end{verbatim}

\section{Exercises}\label{exercises-7}

\subsection{Brown and Forsythe redux}\label{brown-and-forsythe-redux}

Take another look at Table \href{case-ANOVA.html\#tab:BF-Scenarios}{5.1}, which is excerpted from \citet{brown1974SmallSampleBehavior}.
Create a tibble with one row for each of the 20 scenarios that they evaluated.
Then create a function for running the full simulation process (see Exercise \ref{Welch-simulation}).
Use \texttt{pmap()} or \texttt{evaluate\_by\_row()} to run simulations of all 20 scenarios and reproduce the results in Table \href{case-ANOVA.html\#tab:BF-table1}{5.2} of Chapter \ref{case-ANOVA}.

\subsection{Meta-regression}\label{meta-regression}

Exercise \ref{meta-regression-DGP} described the random effects meta-regression model.
List the focal, auxiliary, and structural parameters of this model, and propose a set of design factors to use in a multifactor simulation of the model.
Create a list with one entry per factor, then create a dataset with one row for each simulation context that you propose to evaluate.

\subsection{Comparing the trimmed mean, median and mean}\label{exercise:trimmed-mean}

In this exercise, you will write a simulation to compare several different
estimators of a common parameter.
In particular, you will compare the mean, trimmed mean, and median as estimators of the center of a symmetric distribution (such that the mean and median parameters are identical).\\
To do this, you should break building this simulation evaluation down into functions for each component of the simulation.
This will allow you to extend the same framework to more complicated simulation studies.
This extended exercise illustrates how methodologists might compare different estimation strategies, as you might see in the ``simulation'' section of a stats paper.

As the data-generation function, use a scaled \(t\)-distribution so that the standard deviation will always be 1 but will have different fatness of tails (high chance of outliers):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gen\_scaled\_t }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{( n, mu, df0 ) \{}
\NormalTok{    mu }\SpecialCharTok{+} \FunctionTok{rt}\NormalTok{( n, }\AttributeTok{df=}\NormalTok{df0 ) }\SpecialCharTok{/} \FunctionTok{sqrt}\NormalTok{( df0 }\SpecialCharTok{/}\NormalTok{ (df0}\DecValTok{{-}2}\NormalTok{) )}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

The variance of a \(t\) distribution is \(df/(df-2)\), so when we divide our observations by the
square root of this, we standardize them so they have unit variance.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Verify that \texttt{gen\_scaled\_t()} produces data with mean \texttt{mu} and standard deviation 1 for various \texttt{df0} values.
\item
  Write a method to calculate the mean, trimmed mean, and median of a vector of data.
  The trimmed mean should trim 10\% of the data from each end.
  The method should return a data frame with the three estimates, one row per estimator.
\item
  Verify your estimation method works by analyzing a dataset generated with \texttt{gen\_scaled\_t()}.
  For example, you can generate a dataset of size 100 with \texttt{gen\_scaled\_t(100,\ 0,\ 3)} and then analyze it.
\item
  Use \texttt{bundle\_sim()} to create a simulation function that generates data and then analyzes it.
  The function should take \texttt{n} and \texttt{df0} as arguments, and return the estimates from your analysis method.
  Use \texttt{id} to give each simulation run an ID.
\item
  Run your simulation function for 1000 datasets of size 10, with \texttt{mu=0} and \texttt{df0=5}.
  Store the results in a variable called \texttt{raw\_exps}.
\item
  Write a function to calculate the RMSE, bias, and standard error for your three estimators, given the results.
\item
  Make a single function that takes \texttt{df0} and \texttt{n}, and runs a simulation and returns the performances of your three methods.
\item
  Now make a grid of \(n = 10, 50, 250, 1250\) and \(df_0 =  3, 5, 15, 30\), and generate results for your multi-factor simulation.
\item
  Make a plot showing how SE changes as a function of sample size for each estimator. Do the three estimator seem to follow the same pattern? Or do they work differently?
\end{enumerate}

\chapter{Exploring and presenting simulation results}\label{presentation-of-results}

Once we have our performance measures for each method examined for each of scenario of our study's design, the computationally challenging parts of a simulation study are complete, but several intellectually challenging tasks remain.
The goal of a simulation study is to provide evidence to address a research question or questions, but
performance measures (like numbers more generally) do not analyze themselves.
Rather, they require interpretation, analysis, and communication in order to identify findings and broad, potentially generalizable patterns of results that are relevant the research question(s).
Good analysis will provide a clear understanding of how one or more of the simulation factors influence key performance measures of interest, the circumstances where a data analysis method works well or breaks down, and---in simulations that examine multiple methods---the conditions where a method performs better or worse than alternatives.

In multi-factor simulations, the major challenge in analyzing simulation results is dealing with the multiplicity and dimensional nature of the results.
For instance, in our cluster RCT simulation, we calculated performance metrics in each of 270 different simulation scenarios, which vary along several factors.
For each scenario, we calculated a whole suite of performance measures (bias, SE, RMSE, coverage, \ldots), and we have these performance measures for each of three estimation methods under consideration.
We organized all these results as a table with 810 rows (three rows per simulation scenario, with each row corresponding to a specific method) and one column per performance metric.
Navigating all of this can feel somewhat overwhelming.
How do we understand trends in this complex, multi-factor data structure?

In this chapter, we survey three main categories of analytic tools that can be used for exploring and presenting simulation results:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Tabulation
\item
  Visualization
\item
  Modeling
\end{enumerate}

For each category of tools, we describe the logic behind how it can be applied, provide high-level examples drawn from the literature and our own work, and discuss the strengths and limitations of the approach.

In this and subsequent chapters, we assume that you will be sharing the findings from your simulations with an audience beyond yourself. Depending on your context, that might be a broad audience of researchers and data analysts, with whom you will communicate through a scholarly article in a peer-reviewed methodology journal; it might be colleagues who are evaluating your proposal for an empirical study, where the simulations serve to justify the data analysis protocol; it might be a small group of collaborators, who will use the simulations to make decisions about how to design a study; or it might be fellow students of statistics, interested to read a blog post that discusses how a particular model or method works.
These contexts differ in the format and level of formality used, but with any of them, you will probably need to create a written explanation of your findings, which summarizes what you found and presents evidence to support your assertions and interpretations.
We close the chapter with a discussion about creating such write-ups and distilling your analysis into a set of exhibits for presentation.

\section{Tabulation}\label{tabulation}

Traditionally, simulation study results are presented in big tables.
In general, we believe tables rarely make the take-aways of a simulation readily apparent.
Perhaps tables are fine if\ldots{}

\begin{itemize}
\tightlist
\item
  they involve only a few numbers, and a few targeted comparisons.
\item
  it is important to report \emph{exact} values for some quantities.
\end{itemize}

Unfortunately, simulations usually produce lots of numbers and require making many comparisons.
You are going to want to show, for example, the relative performance of alternative estimators, or the performance of your estimators under different conditions for the data-generating model.
This means a lot of rows, and a lot of dimensions.
Tables can do two dimensions; when you try to cram more than that into a table, no one is particularly well served.

Furthermore, in simulation, exact values for your bias/RMSE/type-I error, or whatever, are not usually of interest. And in fact, we rarely have them due to Monte Carlo simulation error.
The tables provide a false sense of security, unless you include uncertainty, which clutters your table even further.

Overall, tables and simulations do not particularly well mix.
In particular, if you are ever tempted into putting your table in landscape mode to get it to fit on the page, think again.
It is often more useful and insightful to present results in graphs \citep{gelman2002let}.

To illustrate, consider the following table of simulation results showing the false rejection rate, against an \(\alpha\) of \(0.10\), for an estimator of an average treatment impact.
We have two factors of interest, the treatment and control group sizes.

\begin{tabular}{r|r|r}
\hline
nT & nC & reject\\
\hline
2 & 2 & 19\\
\hline
2 & 4 & 16\\
\hline
2 & 10 & 20\\
\hline
2 & 50 & 26\\
\hline
2 & 500 & 31\\
\hline
10 & 2 & 18\\
\hline
10 & 4 & 9\\
\hline
10 & 10 & 7\\
\hline
10 & 50 & 7\\
\hline
10 & 500 & 8\\
\hline
500 & 2 & 29\\
\hline
500 & 4 & 14\\
\hline
500 & 10 & 8\\
\hline
500 & 50 & 5\\
\hline
500 & 500 & 5\\
\hline
\end{tabular}

We can see that the rejection rates are often well above 10\%, and that if there are few treatment units, the rates are all way too high.
Because of the ordering of rows, it is a bit harder to see how the number of control units impacts the rate, and understanding the joint relationship between number of treatment and number of control requires extra thinking.
But this is a classic type of table you might see in a paper: the table is a group of tables indexed by one factor, with the second varying within each group.

By contrast, a plot of these exact same numbers (this is an ``interaction plot'' showing the ``interaction'' of nT and nC) can make trends much more clear:

\begin{center}\includegraphics[width=0.75\linewidth]{Designing-Simulations-in-R_files/figure-latex/unnamed-chunk-206-1} \end{center}

Now we immediately see that only if both nC and nT are above 50 do we achieve anything close to valid tests.
Even if nC is 500, we are elevated if nT is only 10. When nT is 2, then increasing nC actually \emph{increases} the rejection rate, meaning larger samples are worse; this is not obvious from looking at the raw table results.

\subsection{Example: estimators of treatment variation}\label{example-estimators-of-treatment-variation}

Tables do have some purpose.
For example, tables can be used a bit more effectively to show average performance across a range of simulation scenarios.
In general, tables are more plausibly useful for displaying a summary of findings. Do not use them for raw results.

For example, in ongoing work, Miratrix has been studying the performance of a suite of estimators designed to estimate individual treatment effects.
To test which estimators perform better or worse than the others, we designed a series of scenarios where we varied the data-generating model by a variety of factors.
We then, for each scenario, calculated the relative performance of each estimator to the median of all estimators considered.

We can then ask, do some methods perform better than their peers on average across all scenarios considered?
The following table gives an answer to this question: we evaluate each method, averaged across all scenarios, along four metrics: relative bias, relative se, relative rmse, and \(R^2\).
To easily see who is good and who is bad, we order the methods from highest average relative RMSE to lowest:

\begin{tabular}{l|r|r|r|r|r|r|r|r}
\hline
model & bias & se & rmse & sd\_bias & sd\_se & sd\_rmse & R2 & sd\_R2\\
\hline
BART S & -6 & -28 & -17 & 13 & 14 & 12 & 0.40 & 0.25\\
\hline
CF & -10 & -2 & -10 & 14 & 34 & 11 & 0.35 & 0.19\\
\hline
CF LC & -10 & -1 & -10 & 14 & 33 & 10 & 0.34 & 0.19\\
\hline
LASSO R & 2 & -21 & -10 & 9 & 13 & 10 & 0.31 & 0.25\\
\hline
LASSO MCM EA & 2 & -20 & -10 & 9 & 13 & 10 & 0.31 & 0.25\\
\hline
LASSO MOM DR & 2 & -21 & -9 & 9 & 13 & 10 & 0.31 & 0.25\\
\hline
RF MOM DR & 3 & -12 & -8 & 15 & 16 & 8 & 0.31 & 0.20\\
\hline
LASSO T & -1 & -10 & -6 & 16 & 23 & 9 & 0.32 & 0.23\\
\hline
LASSO T INT & -4 & 19 & 5 & 16 & 55 & 17 & 0.28 & 0.20\\
\hline
LASSO MCM & 20 & 4 & 8 & 20 & 10 & 8 & 0.14 & 0.15\\
\hline
LASSO MOM IPW & 20 & 4 & 8 & 20 & 10 & 8 & 0.14 & 0.15\\
\hline
ATE & 47 & -60 & 9 & 48 & 7 & 17 & NA & NA\\
\hline
RF T & -22 & 60 & 11 & 17 & 48 & 19 & 0.29 & 0.13\\
\hline
RF MOM IPW & 15 & 27 & 12 & 31 & 20 & 13 & 0.16 & 0.12\\
\hline
OLS S & -26 & 87 & 23 & 28 & 83 & 34 & 0.31 & 0.22\\
\hline
BART T & -38 & 103 & 25 & 19 & 54 & 22 & 0.32 & 0.18\\
\hline
CDML & 9 & 71 & 31 & 20 & 99 & 46 & 0.24 & 0.20\\
\hline
\end{tabular}

We are summarizing 324 scenarios.
The first columns show relative performance.
To calculate these values we, for each method \(m\), performance metric \(Q\), and scenario \(s\), calculate \(P_{ms} = Q_{ms} / median( Q_{ms} )\), and then average the \(P_{ms}\) across the scenarios to get \(\bar{P}_m\).
Each method also has an \(R^2_{ms}\) value for each scenario; we simply take the average of these across all scenarios for the penultimate column.

The standard deviation columns show the standard deviation of the performances across the full set of scenarios: they give some sense of how much the relative performance of a method changes from scenario to scenario.
Seeing this variation more explicitly might be better done with a visualization; we explore that below.

Overall, the table does give a nice summary of the results, but we still do not feel it makes the results particularly visceral.
Visualization can make trends jump out much more clearly.
That said, the table \emph{is} showing four performance measures, one of which (the \(R^2\)) is on a different scale than the others; this is hard to do with a single visualiztion.

So much for tables.

\section{Visualization}\label{visualization}

We believe visualization to be the primary vehicle for communicating simulation results.
To illustrate some illustration principles, we next present a series of visualizations, illustrating some different themes behind visualization that we believe are important.
In the following chapters we talk about how to get to this final point by iteratively refining a series of plots.

\subsection{Example 0: RMSE in Cluster RCTs}\label{example-0-rmse-in-cluster-rcts}

Probably one of the most common visualizations found in the literature would be a line chart showing how a performance metric changes in response to some factor of interest.
For example, in our cluster RCT experiment, if we look at just those experiments with average cluster size of \(n=320\), \(J = 80\) clusters, and an ICC of 0, we might have a plot such as the following:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sres\_sub }\OtherTok{\textless{}{-}}\NormalTok{ sres }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{( n\_bar }\SpecialCharTok{==} \DecValTok{320}\NormalTok{, J}\SpecialCharTok{==}\DecValTok{80}\NormalTok{, ICC }\SpecialCharTok{==} \DecValTok{0}\NormalTok{ )}

\FunctionTok{ggplot}\NormalTok{( sres\_sub, }\FunctionTok{aes}\NormalTok{( }\FunctionTok{as.factor}\NormalTok{(alpha), RMSE, }
                       \AttributeTok{col=}\NormalTok{method, }\AttributeTok{pch=}\NormalTok{method, }\AttributeTok{group=}\NormalTok{method ) ) }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{( }\SpecialCharTok{\textasciitilde{}}\NormalTok{ size\_coef ) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{( }\AttributeTok{position =} \FunctionTok{position\_dodge}\NormalTok{(}\AttributeTok{width =} \FloatTok{0.1}\NormalTok{) ) }\SpecialCharTok{+} 
  \FunctionTok{geom\_line}\NormalTok{( }\AttributeTok{position =} \FunctionTok{position\_dodge}\NormalTok{(}\AttributeTok{width =} \FloatTok{0.1}\NormalTok{) ) }\SpecialCharTok{+}
  \FunctionTok{geom\_hline}\NormalTok{( }\AttributeTok{yintercept =} \DecValTok{0}\NormalTok{ ) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{( }\AttributeTok{x =} \StringTok{"Variation in cluster size"}\NormalTok{, }\AttributeTok{y =} \StringTok{"RMSE"}\NormalTok{ ) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{Designing-Simulations-in-R_files/figure-latex/unnamed-chunk-208-1} \end{center}

We use multiple plots of a similar form to capture more factors in our simulation.
The left facet shows scenarios with no correlation between cluster size and treatment effect, and the right facet shows the case where there is correlation.
We jitter the points so the lines are not fully overplotted (linear regression and MLM would otherwise be identical at the left).

Figures such as these clearly show how the estimators are similar or diverge.
Here, for instance, we see that, if size correlates with impact, all estimators deteriorate as size variation increases.
We also see that even when there is no correlation, the aggregation estimator deteriorates slightly.

For some examples of these sorts of plots, check out the discussion of the importance of simulation in \citet{little2013praise}, which includes an RMSE figure and a confidence interval converage figure of this type, comparing four estimators of a regression coefficient when using a calibration procedure.
Also see Figures 1 and 2 in \citet{antonakis2021ignoring}, where they run a simulation to understand what happens when random effect assumptions are ignored in multilevel modeling.

\subsection{Example 1: Biserial correlation estimation}\label{example-1-biserial-correlation-estimation}

Our first example, from \citet{pustejovsky2014converting}, shows the bias of a biserial correlation estimate from an extreme groups design.
This simulation was a 4-factor, \(96 \times 2 \times 5 \times 5\) factorial design (factors being true correlation for a range of values, cut-off type, cut-off percentile, and sample size).
The correlation, with 96 levels, forms the \(x\)-axis, giving us nice performance curves.
We use line type for the sample size, allowing us to easily see how bias collapses as sample size increases.
Finally, the facet grid gives our final factors of cut-off type and cut-off percentile.
All our factors, and nearly 5000 explored simulation scenarios, are visible in a single plot.

\begin{center}\includegraphics[width=0.75\linewidth]{Designing-Simulations-in-R_files/figure-latex/unnamed-chunk-209-1} \end{center}

To make this figure, we smoothed the lines with respect to \texttt{rho} using \texttt{geom\_smooth()}.
Smoothing is a nice tool for taking some of the simulation jitter out of an analysis to show overall trends more directly.

This style of plotting, with a bunch of small plots, is called
``many small multiples'' and is beloved by Edward Tufte, who has written extensively on best on information design (see, for example, \citet{tufte1983visual}).
Tufte likes many small multiples, in part, because in a single plot we can display many different variables: here, our facets are organized by two (\texttt{p1} and the cut-off approach), and within each facet we have three (our outcome of bias, rho (x-axis), and \(n\) (line type).
We have five variables in total; this means we can fully show all combinations of our factors along with an outcome in a four factor experiment!

\subsection{Example 2: Variance estimation and Meta-regression}\label{example-2-variance-estimation-and-meta-regression}

In our next example, from \citet{tipton2015small}, we explore Type-I error rates of small-sample corrected F-tests based on cluster-robust variance estimation in meta-regression.
The simulation aimed to compare 5 different small-sample corrections.

This was another complex experimental design, varying several factors:

\begin{itemize}
\tightlist
\item
  sample size (\(m\))
\item
  dimension of hypothesis (\(q\))
\item
  covariates tested
\item
  degree of model mis-specification
\end{itemize}

\begin{center}\includegraphics[width=0.75\linewidth]{Designing-Simulations-in-R_files/figure-latex/unnamed-chunk-210-1} \end{center}

Again using small multiples, we are able to show two of our simulation factors: sample size (\(m\)) and dimension of hypothesis (\(q\)).
The \(x\)-axis shows each of our five methods we are comparing.
The boxplots are ``holding'' the other factors, and show the Type-I error rates for the different small-sample corrections across the covariates tested and degree of model misspecification.
We add a line at the target 0.05 rejection rate to ease comparison.
The reach of the boxes shows how some methods are more or less vulnerable to different types of misspecification.
Some estimators (e.g., \(T^2_A\)) are clearly hyper-conservative, with very low rejection rates.
Other methods (e.g., EDF), have a range of very high rejection rates when \(m = 10\); the degree of rejection rate must depend on model mis-specification and number of covariates tested (the things in the boxes).

\subsection{Example 3: Heat maps of coverage}\label{example-3-heat-maps-of-coverage}

For data with many levels of two different factors, one option is to use a heat map.
For example, the visualization below shows the coverage of parametric bootstrap confidence intervals for momentary time sampling data.
In this simulation study the authors were comparing maximum likelihood estimators to posterior mode (penalized likelihood) estimators of prevalence.
We have a 2-dimensional parameter space of prevalence (19 levels) by incidence (10 levels).
We also have 12 levels of sample size.

The plot shows the combinations of prevalence and incidence as a grid for each sample size level.
We break coverage into ranges of interest, with green being ``good'' (near 95\%) and yellow being ``close'' (92.5\% or above).
Blue and purple show conservative (above 95\%) coverage.
For this kind of plotting to work, we need our MCSE to be small enough that our coverage is estimated precisely enough to show structure.
We have two plots, one for each of the methods being compared.

\begin{center}\includegraphics[width=0.75\linewidth]{Designing-Simulations-in-R_files/figure-latex/swan_example_setup-1} \end{center}

\begin{center}\includegraphics[width=0.75\linewidth]{Designing-Simulations-in-R_files/figure-latex/unnamed-chunk-211-1} \end{center}

For each plot, we can see clear trends, where coverage degrades for low incidence rates.
We are \emph{wrapping} our small multiples by sample size--if you have many levels of a factor you can wrap to show all the levels, which is good, but wrapping does not take advantage of the two-dimensional aspect of having rows and columns of plots (such as we saw with Example 1 and Example 2).

For comparing our two estimators, the prevelance of green in the bottom plot shows generally good behavior for the penalized MLE. The upper plot has less green, showing worse coverage; the improvement of the penalized MLE over the simple MLE is clear.
To see this plot in real life, see \citet{pustejovsky2015four}.

\subsection{Example 4: Relative performance of treatment effect estimators}\label{example-4-relative-performance-of-treatment-effect-estimators}

Revisiting the example of different estimators for estimating treatment variation from the table example above, we can try to plot our results.

As a starting point, we can use the same data we used for the table, and just plot the values as bars (after rescaling \texttt{R2} by 100 to put it on a similar scale to the other measures):

\begin{center}\includegraphics[width=0.75\linewidth]{Designing-Simulations-in-R_files/figure-latex/MLIV_bar_chart-1} \end{center}

Now we can more visually see how the trends of performance between the different methods correspond.
This plot does not, however, show how variation across scenarios might play out.
We can extend this plot by plotting boxplots of the actual performances across all scenarios.

In the plot below, we show the range of relative performances for each estimator vs the median, across the simulations.
We again order the methods from highest average RMSE to lowest, and plot the average performance across all the simulations as little circles (these would correspond to the bars, above).

\begin{center}\includegraphics[width=0.75\linewidth]{Designing-Simulations-in-R_files/figure-latex/unnamed-chunk-212-1} \end{center}

The simulation summarizes performance across 324 scenarios, now showing how much the relative performance can change from scenario to scenario.
We truncate extreme values to make the plot more readable and bring focus to central tendencies.
We are able to see three performance measures at the same time.
The x-axis is on a log scale, again selected to navigate long tails and highlight relative performances.
The log scaling also makes the scale of improved performance (less than x1) similar to worse performance (above x1).

We dropped R2 for these plot, since the R2 measure was in percentage points, and, due to estimation uncertainty, included negative values; this was not compatible with the log scaling.
We have lost something in order to gain something; what plot is best will often be a matter of aesthetic opinion.
Pick your plot based on whether it is clearly communicating the message you are trying to get across.
(We could also dive into complex plot management, making an R2 plot and adding it to the right of the above plot; we could do this with plot control packages such as \texttt{patchwork} or \texttt{cowplot}, but we do not do that here.)

\section{Modeling}\label{modeling}

Simulations are designed experiments, often with a full factorial structure.
The results are datasets in their own right, just as if we had collected data in the wild.
We can therefore leverage classic means for analyzing such full factorial experiments.
For example, we can regress a performance measure against our factor levels to get the ``main effects'' of how the different levels impact performance, holding the other levels constant.
This type of regression is called a ``meta regression'' \citep{kleijnen1981regression, friedman1988metamodel, gilbert2024multilevel}, as we are regressing on already processed results.
It also has ties to meta analysis \citep[see, e.g.,][]{borenstein2021introduction}, where we look for trends across sets of experiments.

In the language of a full factor experiment, we might be interested in the ``main effects'' and the ``interaction effects.''
A main effect is whether, averaging across the other factors in our experiment, a factor of interest systematically impacts performance.
When we look at a main effect, the other factors help ensure our main effect is generalizable: if we see a trend when we average over the other varying aspects, then we can state that our finding is relevant across the host of simulation contexts explored, rather than being an idiosyncratic aspect of a specific and narrow context

If we are comparing multiple methods, we would include the method itself as a factor in our regression.
Then the estimated main effects for each method will tell us if a method is, on average, higher or lower than the baseline method, averaging across all the simulation scenarios.
The main effect of the simulation factors will tell us if that factor impacts the performance measure on average across the methods considered.
We might expect, for example, that for all methods the true standard error goes down as sample size increases.

Meta-regressions would also typically include interactions between method and factor, to see if some factors impact different methods differently.
They can also include interactions between simulation factors, which allows us to explore how the impact of a factor can matter more or less, depending on other aspects of the context.

Using meta regresion can also account for simulation uncertainty in some contexts, which can be especially important when the number of iterations per scenario is low.
See \citet{gilbert2024multilevel} for more on this.

\subsection{Example 1: Biserial, revisited}\label{example-1-biserial-revisited}

In the biserial correlation example above, we saw that bias can change notably across scenarios considered, and that several factors appear to be driving these changes.
These factors also seem to have complex interactions: note how when p1 = 0.5, we get larger dips than when p1 = 1/8.
The figure gives a sense of this complex, rich story, but we might also want to summarize our results to get a sense of overall trends, so we can provide a simpler story of what is going on.
We also might want to get a sense of the relative importance of various factors and their interactions.
For example, we might ask how much the population (top row) vs.~sample (bottom row) cutoff option matters for bias, across all the simulation factors considered.
Is it a primary driver of when there is a lot of bias, or just one of many players of roughly equal import?

ANOVA helps answer these sorts of questions.
In particular, with ANOVA, we can decompose how much bias changes across scenarios into components predicted by various combinations of the simulation factors.
We can do this with the \texttt{aov()} function in R, which is a wrapper around \texttt{lm()} that is designed for ANOVA.
We first fit a model regressing bias on all interactions of our four simulation factors.
In the R formula syntax, our model is \texttt{bias\ \textasciitilde{}\ rho\ *\ p1\ *\ fixed\ *\ n}.

The sum of squares ANOVA decomposition then provides a means for identifying which factors have negligible/minor influence on the bias of an estimator, and which factors drive the variation we see.
For example, the following ``eta table'' gives the contribution of the various factors and interactions to the total amount of variation in bias across scenarios:

\begin{tabular}{r|l|r|r}
\hline
order & source & eta.sq & eta.sq.part\\
\hline
1 & p1 & 0.21 & 0.77\\
\hline
1 & fixed & 0.14 & 0.70\\
\hline
1 & n & 0.12 & 0.67\\
\hline
1 & rho & 0.02 & 0.26\\
\hline
2 & p1:n & 0.18 & 0.74\\
\hline
2 & fixed:n & 0.12 & 0.66\\
\hline
2 & rho:fixed & 0.03 & 0.33\\
\hline
2 & rho:n & 0.02 & 0.23\\
\hline
2 & rho:p1 & 0.02 & 0.20\\
\hline
2 & p1:fixed & 0.02 & 0.20\\
\hline
3 & rho:fixed:n & 0.03 & 0.30\\
\hline
3 & rho:p1:n & 0.01 & 0.18\\
\hline
3 & p1:fixed:n & 0.01 & 0.17\\
\hline
3 & rho:p1:fixed & 0.00 & 0.06\\
\hline
4 & rho:p1:fixed:n & 0.00 & 0.06\\
\hline
\end{tabular}

The table shows which factors are explaining the most variation. E.g., \texttt{p1} is explaining 21\% of the variation in bias across simulations.
The contribution of any of the three- or four-way interactions are fairly minimal, by comparison, and could be dropped to simplify our model.

Modeling summarizes overall trends, and ANOVA allows us to identify what factors are relatively more important for explaining variation in our performance measure.
We could fit a regression model or ANOVA model for each performance measure in turn, to understand what drives our results.

\subsection{Example 2: Comparing methods for cross-classified data}\label{example-2-comparing-methods-for-cross-classified-data}

\citet{lee2023comparing} were interested in evaluating how different modeling approaches perform when analyzing cross-classified data structures.
To do this they conducted a multi-factor simulation to compare three methods: a method called CCREM, two-way OLS with cluster-robust variance estimation (CRVE), and two-way fixed effects with CRVE.
The simulation was complex, involving several factors, so they fit an ANOVA model to understand which factors had the most influence on performance.
In particular, they ran \emph{four} multifactor simulations, each under a different broader context (those being assumptions met, homoscedasticity violated, exogeneity violated, and presence of random slopes).
They then used ANOVA to explore how the simulation factors impacted bias within each of these contexts.

One of their tables in the supplementary materials (Table S5.2, see \href{https://osf.io/hy73g}{here}, page 20, and reproduced below) shows the results of these four ANOVA models, with each column being a simulation context, and the rows corresponding to factors manipulated within that context.
Small, medium, and large effects are marked to make them jump out to the eye.

\textbf{ANOVA Results on Parameter Bias}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2949}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1795}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2436}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1667}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1154}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Source
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Assumptions
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Homoscedasticity
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Exogeneity
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Rand Slope
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Method & 0.000 & 0.006 & 0.995 L & 0.000 \\
Effect Size (r) & 0.131 M & 0.008 & 0.020 S & 0.142 L \\
Number of Schools (H) & 0.014 S & 0.113 M & 0.188 L & 0.001 \\
Students per School (J) & 0.016 S & 0.016 S & 0.747 L & 0.110 M \\
IUCC & 0.007 & 0.007 & 0.033 S & 0.073 M \\
method  r & 0.006 & 0.006 & 0.007 & 0.012 S \\
method  H & 0.004 & 0.002 & 0.157 L & 0.000 \\
method  J & 0.002 & 0.000 & 0.878 L & 0.010 S \\
method  IUCC & 0.012 S & 0.003 & 0.037 S & 0.000 \\
r  H & 0.103 M & 0.010 S & 0.059 S & 0.377 L \\
r  J & 0.006 & 0.024 S & 0.008 & 0.051 S \\
r  IUCC & 0.065 M & 0.025 S & 0.084 M & 0.136 M \\
H  J & 0.002 & 0.014 S & 0.062 M & 0.105 M \\
H  IUCC & 0.024 S & 0.008 & 0.034 S & 0.137 M \\
J  IUCC & 0.004 & 0.088 M & 0.013 S & 0.029 S \\
\end{longtable}

\emph{Note: (S)mall = .01, (M)edium = .06, (L)arge = .14}

We see that when model assumptions are met or only homoscedasticity is violated, choice of method (CCREM, two-way OLS-CRVE, FE-CRVE) has almost no impact on parameter bias (\(\eta^2 = 0.000\) to 0.006).
However, under an exogeneity violation, method choice has a large effect (\(\eta^2 = 0.995\)), indicating that some methods (e.g., OLS-CRVE) have much more bias than others.
Other factors such as the effect size of the parameter and the number of schools can also show moderate-to-large impacts on bias in several conditions.

The table also shows how an interaction between simulation factors can matter.
For example, interactions between method and number of schools, or students per school, can really impact bias under the Exogeniety Violated condition; this means the different methods respond differently as sample size changes.

Overall, the table shows how some aspects of the DGP matter more, and some less.

\section{Reporting}\label{reporting}

There is a difference in the results you will generate so you can understand what is going on in your simulation, and the results that you will include in an outward facing report.
Do not pummel your reader with a deluge of tables, figures, and observations.
Instead, present selected results that clearly illustrate the main findings from the study, along with anything unusual or anomalous.
Your presentation will typically be best served with a few well-chosen figures.
Then, in the text of your write-up, you might include a few specific numerical comparisons.
Do not include too many of these, and be sure to say why the numerical comparisons you include are important.

To form your final exhibits, you will likely have to generate a wide range of results that show different aspects of your simulation.
These are for you, and will help you deeply understand what is going on.
You then try to simplify the story, in a way that is honest and transparent, by curating this full set of figures to your final ones.
Some of the remainder will then become supplementary materials that contain further detail to both enrich your main narrative and demonstrate that you are not hiding anything.

Results are by definition a simplified summary of a complex thing.
The alert reader will know this, and will thus be suspicious about what you might have left out.
To give a great legitimacy bump to your work, you should also provide reproducible code so others could, if so desired, rerun the simulation and conduct your analysis themselves, or perhaps rerun your simulation under different conditions.
Even if no one touches your code, the code's existence and availability builds confidence.
People will naturally think, ``if that researcher is so willing to let me see what they actually did, then they must be fairly confident it does not contain too many horrendous mistakes.''

\chapter{Building good visualizations}\label{building-good-visualization}

Visualization should nearly always be the first step in analyzing simulation results.
In the prior chapter, we saw a variety of examples primarily taken from published work.
Those visualizations were not the initial ones created for those research projects.
In practice, getting to a good visualization often requires creating \emph{many} different graphs to look at different aspects of the data.
From that pile of graphs, you would then curate and refine those that communicate the overall results most cleanly.

In our work, we find we often generate a series of R Markdown reports with comprehensive sets of charts targeting our various research questions.
These initial documents are then discussed internally by the research team.

In this chapter we first discuss four essential tools that we frequently use to make these initial sets of graphs:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Subsetting}: Multifactor simulations can be complex and confusing. Sometimes it is easier to first explore a subset of the simulation results, such as a single factor level.
\item
  \textbf{Many small multiples}: Plot many results in a single plot, with facets to break up the results by simulation factors.
\item
  \textbf{Bundling}: Group the results by a primary factor of interest, and then plot the performance measure as a boxplot so you can see how much variation there is within that factor level.
\item
  \textbf{Aggregation}: Average the performance measure across some of the simulation factors, so you can see overall trends with respect to the remaining factors.
\end{enumerate}

To illustrate these tools, we walk through them using our running example of comparing methods for analyzing a Cluster RCT.
We will start with an investigation of bias.

As a reminder, in our Cluster RCT example, we have three methods for estimating the average treatment effect: linear regression of the student-level outcome onto treatment (with cluster-robust standard errors); aggregation, where we regress the cluster-level average outcome onto treatment (with heteroskedastic robust standard errors); and multilevel modeling with random effects.
We want to know if these methods are biased for our defined estimand, which is the cluster-average treatment effect.
We have five simulation factors: school size (\texttt{n\_bar}), number of schools (\texttt{J}), the intraclass correlation (\texttt{ICC}), the degree of variation in school size (\texttt{alpha}), and the relationship between school size and treatment effect (\texttt{size\_coef}).

Once we go through the four core tools, we continue our evaluation of our simulation to show how we can assess other performance metrics of interest (true standard error, RMSE, estimated standard errors, and coverage) using these tools.
We do not dive deeply into validity or power; see Chapter \ref{sec:power} for more on those measures.

\section{Subsetting and Many Small Multiples}\label{subsetting-and-many-small-multiples}

``Small Multiples'' is when you make a plot that is actually a bunch of related plots.
These plots can themselves be organized in a plot-like structure, with an x-axis depending on one categorical variable, and the y-axis another (this is a ``facet grid'').

Generally, with small multiples, it is relatively straightforward to look at \emph{three simulation factors}.
This is because, with a facet grid, you can easily plot five aspects of your data: two for the facet x- and y-axis arrangement, one for the within-facet x-axis, and one for the within-facet y-axis, and one for color/line type.
Of the five aspects, we usually use one of these for method (if we are comparing methods) and one for the performance metric (the outcome), giving three remaining aspects to assign to our simulation factors.
In cases with more than three factors, an easy initial approach is pick three favorite factors and then filter all the simulation results down to specific levels for the remaining factors.

For example, for our simulation, we might target the ICC of 0.20, taking it as a ``reasonable'' value that, given our substance matter knowledge, we know is frequently found in empirical data.
We might further pick \(\bar{n} = 80\), as our middle level for cluster size.
This leaves three factors (\texttt{J}, \texttt{size\_coef}, and \texttt{alpha}), allowing us to plot all of our simulation results for our subset.
We then decide to plot bias (our performance measure) as a function of \texttt{J}, with different lines for the different methods, and facets for the different levels of \texttt{size\_coef} and \texttt{alpha}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sres\_sub }\OtherTok{\textless{}{-}}\NormalTok{ sres }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{( ICC }\SpecialCharTok{==} \FloatTok{0.20}\NormalTok{, n\_bar }\SpecialCharTok{==} \DecValTok{80}\NormalTok{ )}
\FunctionTok{ggplot}\NormalTok{( sres\_sub, }\FunctionTok{aes}\NormalTok{( }\FunctionTok{as.factor}\NormalTok{(J), bias, }
                       \AttributeTok{col=}\NormalTok{method, }\AttributeTok{pch=}\NormalTok{method, }\AttributeTok{group=}\NormalTok{method ) ) }\SpecialCharTok{+}
  \FunctionTok{facet\_grid}\NormalTok{( size\_coef }\SpecialCharTok{\textasciitilde{}}\NormalTok{ alpha, }\AttributeTok{labeller =}\NormalTok{ label\_both ) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_hline}\NormalTok{( }\AttributeTok{yintercept =} \DecValTok{0}\NormalTok{ ) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{Designing-Simulations-in-R_files/figure-latex/unnamed-chunk-214-1} \end{center}

Each point is one of our methods in one of our simulation scenarios.
We are looking at the raw results.
We connect the points with lines to help us see trends within each of the small multiples.
The lines help us visually track which group of points goes with which.

We immediately see that when there is a lot of site variation (\texttt{alpha\ =\ 0.80}), and it relates to outcome (\texttt{size\_coef=0.2}), linear regression is very different from the other two methods.
We also see that when \texttt{alpha} is 0 and \texttt{size\_coef} is 0, we may also have a negative bias when \$J = 5`.
Before we get too excited about this surprising result, we add MCSEs to our plot to see if this is a real effect or just noise:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{( sres\_sub, }\FunctionTok{aes}\NormalTok{( }\FunctionTok{as.factor}\NormalTok{(J), bias, }
                       \AttributeTok{col=}\NormalTok{method, }\AttributeTok{pch=}\NormalTok{method, }\AttributeTok{group=}\NormalTok{method ) ) }\SpecialCharTok{+}
  \FunctionTok{facet\_grid}\NormalTok{( size\_coef }\SpecialCharTok{\textasciitilde{}}\NormalTok{ alpha, }\AttributeTok{labeller =}\NormalTok{ label\_both ) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{( }\AttributeTok{position =} \FunctionTok{position\_dodge}\NormalTok{( }\AttributeTok{width=}\FloatTok{0.3}\NormalTok{) ) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{(}\AttributeTok{position =} \FunctionTok{position\_dodge}\NormalTok{( }\AttributeTok{width=}\FloatTok{0.3}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{geom\_errorbar}\NormalTok{( }\FunctionTok{aes}\NormalTok{( }\AttributeTok{ymin =}\NormalTok{ bias }\SpecialCharTok{{-}} \FloatTok{1.96}\SpecialCharTok{*}\NormalTok{bias\_mcse,}
                      \AttributeTok{ymax =}\NormalTok{ bias }\SpecialCharTok{+} \FloatTok{1.96}\SpecialCharTok{*}\NormalTok{bias\_mcse ),}
                 \AttributeTok{position =} \FunctionTok{position\_dodge}\NormalTok{( }\AttributeTok{width=}\FloatTok{0.3}\NormalTok{),}
                 \AttributeTok{width =} \DecValTok{0}\NormalTok{ ) }\SpecialCharTok{+}
  \FunctionTok{geom\_hline}\NormalTok{( }\AttributeTok{yintercept =} \DecValTok{0}\NormalTok{ ) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{Designing-Simulations-in-R_files/figure-latex/unnamed-chunk-215-1} \end{center}

Our confidence intervals exclude zero!
Our excitement mounts.
We next subset our overall results again to check all the scenarios with \texttt{alpha\ =\ 0}, and \texttt{size\_coef\ =\ 0} to see if this is a real effect.
We can still plot all our data, as for that subset of scenarios we still only have three factors, \texttt{ICC}, \texttt{J\_bar}, and \texttt{n\_bar}, left to plot.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{null\_sub }\OtherTok{\textless{}{-}}\NormalTok{ sres }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{filter}\NormalTok{( alpha }\SpecialCharTok{==} \DecValTok{0}\NormalTok{, size\_coef }\SpecialCharTok{==} \DecValTok{0}\NormalTok{ )}
\FunctionTok{ggplot}\NormalTok{( null\_sub, }\FunctionTok{aes}\NormalTok{( ICC, bias, }
                       \AttributeTok{col=}\NormalTok{method, }\AttributeTok{pch=}\NormalTok{method, }\AttributeTok{group=}\NormalTok{method ) ) }\SpecialCharTok{+}
  \FunctionTok{facet\_grid}\NormalTok{( J }\SpecialCharTok{\textasciitilde{}}\NormalTok{ n\_bar, }\AttributeTok{labeller =}\NormalTok{ label\_both ) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{( }\AttributeTok{position =} \FunctionTok{position\_dodge}\NormalTok{( }\AttributeTok{width=}\FloatTok{0.03}\NormalTok{) ) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{( }\AttributeTok{position =} \FunctionTok{position\_dodge}\NormalTok{( }\AttributeTok{width=}\FloatTok{0.03}\NormalTok{) ) }\SpecialCharTok{+}
  \FunctionTok{geom\_errorbar}\NormalTok{( }\FunctionTok{aes}\NormalTok{( }\AttributeTok{ymin =}\NormalTok{ bias }\SpecialCharTok{{-}} \FloatTok{1.96}\SpecialCharTok{*}\NormalTok{bias\_mcse,}
                      \AttributeTok{ymax =}\NormalTok{ bias }\SpecialCharTok{+} \FloatTok{1.96}\SpecialCharTok{*}\NormalTok{bias\_mcse ),}
                 \AttributeTok{position =} \FunctionTok{position\_dodge}\NormalTok{( }\AttributeTok{width=}\FloatTok{0.03}\NormalTok{),}
                 \AttributeTok{width =} \DecValTok{0}\NormalTok{ ) }\SpecialCharTok{+}
  \FunctionTok{geom\_hline}\NormalTok{( }\AttributeTok{yintercept =} \DecValTok{0}\NormalTok{ ) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{Designing-Simulations-in-R_files/figure-latex/unnamed-chunk-216-1} \end{center}

It appears that our \texttt{ICC=0.20} and \texttt{n\_bar=80} scenario was just a fluke.
Almost all of our confidence intervals easily cover 0, and we are not seeing any major trends.
Overall we conclude that indeed, when there is no variation, or when the variation does not relate to impact, nothing is particularly biased.
Interestingly, we further see that all methods seem to give identical estimates when there is no site variation, regardless of ICC, J, or \texttt{n\_bar} (identical estimates is the easiest explanation of all the estimated biases and associated MCSEs across the three methods being identical).

Subsetting is a very useful tool, especially when the scope of the simulation feels overwhelming.
And as we just saw, it can also be used as a quick validity check: subset to a known context where we know nothing exciting should be happening to verify that indeed nothing is there.

Subsetting allows for a deep dive into specific context.
It also can make it easier to think through what is happening in a complex context; think of it as a flashlight, shining attention on one part of your overall simulation or another, to focus attention and reduce complexity.
Sometimes we might even just report the results for a subset in our final analysis and put the analysis of the the remaining scenarios elsewhere, such as an online supplemental appendix.
In this case, it would then be our job to verify that our reported findings on the main results indeed were echoed in the set-aside runs.

Subsetting is useful, but if you do want to look at all your simulation results at once, you need to somehow aggregate or group your results to make them all fit on the plot.
We next present bundling, a way of keeping the core idea of small multiples to show all of the raw results, but now in a semi-aggregated way.

\section{Bundling}\label{bundling}

When faced with many simulation factors, we can \emph{bundle} the simulations into groups defined by a selected primary factor of interest, and then plot each bundle with a boxplot of the distribution of a selected performance criteria.
Each boxplot shows the central measure of how well an estimator worked across a set of scenarios, along with a sense of how much that performance varied across those scenarios in the box.
If the boxes are narrow, then we know that the variation across simulations within the box did not impact performance much.
If the boxes are wide, then we know that the factors that vary within the box matter a lot for performance.

With bundling, we generally need a good number of simulation runs per scenario, so that the MCSE in the performance measures does not make our boxplots look substantially more variable (wider) than the truth.
Consider a case where all the scenarios within a box have zero \emph{true} bias; if the MCSE were large, the \emph{estimated} biases would still vary and we would see a wide boxplot when we should not.

To illustrate bundling, we replicate our small subset figure from above, but instead of each point (with a given \texttt{J\textquotesingle{},}alpha\texttt{,\ and}size\_coef\texttt{)\ just\ being\ the\ single\ scenario\ with}n\_bar=80\texttt{and}ICC = 0.20`, we plot all the scenarios in a boxplot at that location.
We put the boxes for the three methods side-by-side to directly compare them:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{( sres, }\FunctionTok{aes}\NormalTok{( }\FunctionTok{as.factor}\NormalTok{(J), bias, }\AttributeTok{col=}\NormalTok{method, }
                   \AttributeTok{group=}\FunctionTok{paste0}\NormalTok{(method, J) ) ) }\SpecialCharTok{+}
  \FunctionTok{facet\_grid}\NormalTok{( size\_coef }\SpecialCharTok{\textasciitilde{}}\NormalTok{ alpha, }\AttributeTok{labeller =}\NormalTok{ label\_both ) }\SpecialCharTok{+}
  \FunctionTok{geom\_boxplot}\NormalTok{( }\AttributeTok{coef =} \ConstantTok{Inf}\NormalTok{, }\AttributeTok{width=}\FloatTok{0.7}\NormalTok{, }\AttributeTok{fill=}\StringTok{"grey"}\NormalTok{ ) }\SpecialCharTok{+}
  \FunctionTok{geom\_hline}\NormalTok{( }\AttributeTok{yintercept =} \DecValTok{0}\NormalTok{ ) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{Designing-Simulations-in-R_files/figure-latex/clusterRCT_plot_bias_v1-1} \end{center}

All of our simulation trials are represented in this plot.
Each box is a collection of simulation trials. E.g., for \texttt{J\ =\ 5}, \texttt{size\_coef\ =\ 0}, and \texttt{alpha\ =\ 0.8} each of the three boxes contains 15 scenarios representing the varying ICC and cluster size.
Here are the 15 results in the top right box for the Aggregation method:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{filter}\NormalTok{( sres, }
\NormalTok{        J }\SpecialCharTok{==} \DecValTok{5}\NormalTok{, }
\NormalTok{        size\_coef }\SpecialCharTok{==} \DecValTok{0}\NormalTok{,}
\NormalTok{        alpha }\SpecialCharTok{==} \FloatTok{0.8}\NormalTok{, }
\NormalTok{        method}\SpecialCharTok{==}\StringTok{"Agg"}\NormalTok{ ) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{( n\_bar, J, size\_coef, ICC, alpha, bias, bias\_mcse ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{arrange}\NormalTok{( bias ) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{( }\AttributeTok{digits =} \DecValTok{2}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{r|r|r|r|r|r|r}
\hline
n\_bar & J & size\_coef & ICC & alpha & bias & bias\_mcse\\
\hline
80 & 5 & 0 & 0.6 & 0.8 & -0.03 & 0.02\\
\hline
320 & 5 & 0 & 0.8 & 0.8 & -0.02 & 0.02\\
\hline
20 & 5 & 0 & 0.6 & 0.8 & -0.02 & 0.02\\
\hline
80 & 5 & 0 & 0.8 & 0.8 & -0.02 & 0.03\\
\hline
20 & 5 & 0 & 0.4 & 0.8 & -0.01 & 0.02\\
\hline
20 & 5 & 0 & 0.2 & 0.8 & -0.01 & 0.01\\
\hline
320 & 5 & 0 & 0.2 & 0.8 & 0.00 & 0.01\\
\hline
320 & 5 & 0 & 0.4 & 0.8 & 0.00 & 0.02\\
\hline
20 & 5 & 0 & 0.0 & 0.8 & 0.00 & 0.01\\
\hline
80 & 5 & 0 & 0.4 & 0.8 & 0.00 & 0.02\\
\hline
320 & 5 & 0 & 0.0 & 0.8 & 0.00 & 0.00\\
\hline
80 & 5 & 0 & 0.0 & 0.8 & 0.00 & 0.00\\
\hline
320 & 5 & 0 & 0.6 & 0.8 & 0.01 & 0.02\\
\hline
80 & 5 & 0 & 0.2 & 0.8 & 0.01 & 0.01\\
\hline
20 & 5 & 0 & 0.8 & 0.8 & 0.07 & 0.03\\
\hline
\end{tabular}

Our bias boxplot makes some trends clear.
For example, we see that there is no bias, on average, for any method when the size coefficient is 0 and alpha is 0, especially when \(J = 80\).
When the size coefficient is 0.2, we also see LR jump out from the others when \texttt{alpha} is not 0.

The apparent outliers (long tails) for some of the boxplots suggest that the two remaining factors (ICC and cluster size) could relate to the degree of bias.
They could also be due to MCSE, and given that we primariy see these tails when \(J\) is small, this is a real concern.
MCSE aside, a long tail means that some scenario in the box had a high level of estimated bias.
We could try bundling along different aspects to see if either of the remaining factors (e.g., ICC) explains these differences.
Here we try bundling cluster size and number of clusters.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{( sres, }\FunctionTok{aes}\NormalTok{( }\FunctionTok{as.factor}\NormalTok{(alpha), bias, }\AttributeTok{col=}\NormalTok{method, }
                   \AttributeTok{group=}\FunctionTok{paste0}\NormalTok{(method, alpha) ) ) }\SpecialCharTok{+}
  \FunctionTok{facet\_grid}\NormalTok{( size\_coef }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ICC, }\AttributeTok{labeller =}\NormalTok{ label\_both ) }\SpecialCharTok{+}
  \FunctionTok{geom\_boxplot}\NormalTok{( }\AttributeTok{coef =} \ConstantTok{Inf}\NormalTok{, }\AttributeTok{width=}\FloatTok{0.7}\NormalTok{, }\AttributeTok{fill=}\StringTok{"grey"}\NormalTok{ ) }\SpecialCharTok{+}
  \FunctionTok{geom\_hline}\NormalTok{( }\AttributeTok{yintercept =} \DecValTok{0}\NormalTok{ ) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{Designing-Simulations-in-R_files/figure-latex/clusterRCT_plot_bias_v2-1} \end{center}

We have some progress now: the long tails are primarily when the ICC is high, but we also see that MLM has bias with ICC is 0, if alpha is nonzero.

We know things are more unstable in smaller samples sizes, so the tails could still be MCSE, with some of our bias estimates being large due to random chance.
Or perhaps there is still some specific combination of factors that allow for large bias (e.g., perhaps small sample sizes makes our estimators more vulnerable to bias).
In an actual analysis, we would make a note to investigate these anomalies later on.

In general, trying to group your simulation scenarios so that their boxes are generally narrow is a good idea; narrow boxes means that you have found a representation of the data where you know what is driving the variation in your performance measure, and that the factors bundled inside the boxes are less important.
This might not always be possible, if all your factors matter; in this case the width of your boxes tells you to what extent the bundled factors matter relative to the factors explicitly present in your plot.

One might wonder, with only few trials per box, whether we should instead look at the individual scenarios.
Unfortunately, that gets a bit cluttered:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{( sres, }\FunctionTok{aes}\NormalTok{( }\FunctionTok{as.factor}\NormalTok{(alpha), bias, }\AttributeTok{col=}\NormalTok{ method, }
                   \AttributeTok{group=}\FunctionTok{paste0}\NormalTok{(alpha,ICC,method) ) ) }\SpecialCharTok{+}
  \FunctionTok{facet\_grid}\NormalTok{( size\_coef }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ICC, }\AttributeTok{labeller =}\NormalTok{ label\_both ) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{( }\AttributeTok{size =} \FloatTok{0.5}\NormalTok{,}
              \AttributeTok{position =} \FunctionTok{position\_dodge}\NormalTok{(}\AttributeTok{width=}\FloatTok{0.7}\NormalTok{ ) ) }\SpecialCharTok{+}
  \FunctionTok{geom\_hline}\NormalTok{( }\AttributeTok{yintercept =} \DecValTok{0}\NormalTok{ ) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{Designing-Simulations-in-R_files/figure-latex/unnamed-chunk-218-1} \end{center}

Using boxplots, even over such a few number of points, notably clarifies a visualization.

\section{Aggregation}\label{aggregation}

Boxplots can make seeing trends more difficult, as the eye is drawn to the boxes and tails, and the range of your plot axes can be large due to needing to accommodate the full tails and outliers of your results; this can compress the mean differences between groups, making them look small.
They can also be artificially inflated, especially if the MCSEs are large.
Instead of bundling, we can therefore aggregate, where we average all the scenarios within a box to get a single number of average performance.
This will show us overall trends rather than individual simulation variation.

When we aggregate, and average over some of the factors, we collapse our simulation results down to fewer moving parts.
Aggregation across factors is better than not having varied those factors in the first place!
A performance measure averaged over a factor is a more general answer of how things work in practice than having not varied the factor at all.

For example, if we average across ICC and site variation, and see that our methods had different degrees of bias as a function of \(J\), we would know that the found trend is a general trend across a range of scenarios defined by different ICC and site variation levels, rather than a specific one tied to a single ICC and amount of site variation.
Our conclusions would then be more general: if we had not explored more scenarios, we would not have any sense of how general our found trend might be.

That said, if some of our scenarios had no bias, and some had large bias, when we aggregated we would report that there is generally a moderate amount of bias.
This would not be entirely faithful to the actual results.
But when the initial boxplots show results generally in one direction or another, then aggregation can be quite faithful to the spirit of the results.

A major advantage of aggregation over the bundling approach is we can have fewer replications per scenario.
If the number of replicates within each scenario is small, then the performance measures for each scenario is estimated with a lot of error; the aggregate, by contrast, will be an average across many more replicates and thus give a good sense of \emph{average} performance.
The averaging, in effect, gives a lot more replications per aggregated performance measure.

For our cluster RCT, we might aggregate our bias across our sample sizes as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ssres }\OtherTok{\textless{}{-}} 
\NormalTok{  sres }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{( method, size\_coef, J, alpha ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{( }\AttributeTok{bias =} \FunctionTok{mean}\NormalTok{( bias ) )}
\end{Highlighting}
\end{Shaded}

We now have a single bias estimate for each combination of size\_coef, J, and alpha; we have collapsed 15 scenarios into one overall scenario that generalizes bias across different average cluster sizes and different ICCs.
We can then plot, using many small multiples:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{( ssres, }\FunctionTok{aes}\NormalTok{( }\FunctionTok{as.factor}\NormalTok{(J), bias, }\AttributeTok{col=}\NormalTok{method, }\AttributeTok{group=}\NormalTok{method ) ) }\SpecialCharTok{+}
  \FunctionTok{facet\_grid}\NormalTok{( size\_coef }\SpecialCharTok{\textasciitilde{}}\NormalTok{  alpha, }\AttributeTok{labeller =}\NormalTok{ label\_both ) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{( }\AttributeTok{alpha=}\FloatTok{0.75}\NormalTok{ ) }\SpecialCharTok{+} 
  \FunctionTok{geom\_line}\NormalTok{( }\AttributeTok{alpha=}\FloatTok{0.75}\NormalTok{ ) }\SpecialCharTok{+}
  \FunctionTok{geom\_hline}\NormalTok{( }\AttributeTok{yintercept =} \DecValTok{0}\NormalTok{ ) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{Designing-Simulations-in-R_files/figure-latex/agg_bias_plot_clusterRCT-1} \end{center}

We now see quite clearly that as \texttt{alpha} grows, linear regression gets more biased if cluster size relates to average impact in the cluster (\texttt{size\_coef}).
Our finding makes sense given our theoretical understanding of the problem---if size is not related to treatment effect, it is hard to imagine how varying cluster sizes would cause much bias.

We are looking at an interaction between our simulation factors: we only see bias for linear regression when cluster size relates to impact and there is variation in cluster size.
We also see that all the estimators have near zero bias when there is no variation in cluster size or the cluster size does not relate to outcome, as shown by the top row and left column facets.
Finally, we see the methods all likely give the same answers when there is no cluster size variation, given the overplotted lines on the left column of the figure.

We might take this figure as still too complex.
So far we have learned that MLM does seem to react to ICC, and that LR reacts to \texttt{alpha} and \texttt{size\_coef} in combination.
More broadly, with many levels of a factor, as we have with ICC, we can let ggplot aggregate directly by taking advantage of \texttt{geom\_smooth()}.
This leads to the following:

\begin{center}\includegraphics[width=0.75\linewidth]{Designing-Simulations-in-R_files/figure-latex/unnamed-chunk-220-1} \end{center}

Our story is fairly clear now: LR is biased when alpha is large and the cluster size relates to impact.
MLM can be biased when ICC is low, if cluster size relates to impact (this is because it is driving towards person-weighting when there is little cluster variation).

Aggregation is powerful, but it can be misleading if you have scaling issues or extreme outliers.
With bias, our scale is fairly well set, so we are good.
But if we were aggregating standard errors over different sample sizes, then the larger standard errors of the smaller sample size simulations (and the greater variability in estimating those standard errors) would swamp the standard errors of the larger sample sizes.
Usually, with aggregation, we want to average over something we believe does not change massively over the marginalized-out factors.
To achieve this, we can often average over a relative measure (such as standard error divided by the standard error of some baseline method), which tend to be more invariant and comparable across scenarios.
We will see more examples of this kind of aggregation later on.

\subsubsection{Some notes on how to aggregate}\label{some-notes-on-how-to-aggregate}

Some performance measures are biased with respect to the Monte Carlo uncertainty.
The estimated standard error, for example, is biased; the variance, by contrast, is not.
The RMSE is biased, the MSE is not.

When aggregating, therefore, it is often best to aggregate the unbiased performance measures, and then calculate the biased ones from those.
For example, to estimate aggregated standard error you might do the following:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{agg\_perf }\OtherTok{\textless{}{-}}\NormalTok{ sres }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{( ICC, method, alpha, size\_coef ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{( }\AttributeTok{SE =} \FunctionTok{sqrt}\NormalTok{( }\FunctionTok{mean}\NormalTok{( SE}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{ ) ) )}
\end{Highlighting}
\end{Shaded}

Because bias is linear, you do not need to worry about the MCSE.
But if you are looking at the magnitude of bias (\(|bias|\)), then you can run into issues when the biases are close to zero, if they are measured noisily.
For example, imagine you have two scenarios with true bias of 0.0, but your MCSE is 0.02.
In one scenario, you estimate a bias of 0.017, and in the other -0.023.
If you average the estimated biases, you get -0.003, which suggests a small bias as we would wish.
Averaging the absolute biases, on the other hand, gives you 0.02, which could be deceptive.
With high MCSE and small magnitudes of bias, looking at average bias, not average \(|bias|\), is safer.

Alternatively, you can use the formula \(RMSE^2 = Bias^2 + SE^2\) to back out the average absolute bias from the RMSE and SE.

\section{Comparing true SEs with standardization}\label{comparing-true-ses-with-standardization}

We just did a deep dive into bias.
Uncertainty (standard errors) is another primary performance criterion of interest.

As an initial exploration, we plot the standard error estimates from our Cluster RCT simulation, using smoothed lines to visualize trends. We use \texttt{ggplot}'s \texttt{geom\_smooth} to aggregate over \texttt{size\_coef} and \texttt{alpha}, which we leave out of the plot.
We include individual data points to visualize variation around the smoothed estimates:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{( sres, }\FunctionTok{aes}\NormalTok{( ICC, SE, }\AttributeTok{col=}\NormalTok{method ) ) }\SpecialCharTok{+}
  \FunctionTok{facet\_grid}\NormalTok{( n\_bar }\SpecialCharTok{\textasciitilde{}}\NormalTok{  J, }\AttributeTok{labeller =}\NormalTok{ label\_both ) }\SpecialCharTok{+}
  \FunctionTok{geom\_jitter}\NormalTok{( }\AttributeTok{height =} \DecValTok{0}\NormalTok{, }\AttributeTok{width =} \FloatTok{0.05}\NormalTok{, }\AttributeTok{alpha=}\FloatTok{0.5}\NormalTok{ ) }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{( }\AttributeTok{se=}\ConstantTok{FALSE}\NormalTok{, }\AttributeTok{alpha=}\FloatTok{0.5}\NormalTok{ ) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{Designing-Simulations-in-R_files/figure-latex/unnamed-chunk-222-1} \end{center}

We observe several broad trends in the standard error behavior.
First, standard error increases with the intraclass correlation (ICC).
This is as expected: greater similarity within clusters reduces the effective sample size.
Second, standard error decreases as the number of clusters (J) increases, which reflects the benefit of having more independent units of analysis.
In contrast, increasing the cluster size (n\_bar) has relatively little effect on the standard error (the rows of our facets look about the same).
Lastly, all methods show fairly similar levels of standard error overall.

While we can extract all of these from the figure, the figure is still not ideal for \emph{comparing} our methods.
The dominant influence of design features like ICC and sample size obscures our ability to detect meaningful differences between methods.
In other words, even though SE changes across scenarios, it's difficult to tell which method is actually performing better within each scenario.

We can also view the same information by bundling over the left-out dimensions.
We put \texttt{n\_bar} in our bundles because maybe it does not matter that much:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{( sres, }\FunctionTok{aes}\NormalTok{( ICC, SE, }\AttributeTok{col=}\NormalTok{method, }\AttributeTok{group=}\FunctionTok{paste0}\NormalTok{( ICC, method ) ) ) }\SpecialCharTok{+}
  \FunctionTok{facet\_grid}\NormalTok{( . }\SpecialCharTok{\textasciitilde{}}\NormalTok{  J, }\AttributeTok{labeller =}\NormalTok{ label\_both ) }\SpecialCharTok{+}
  \FunctionTok{geom\_boxplot}\NormalTok{( }\AttributeTok{width =} \FloatTok{0.2}\NormalTok{, }\AttributeTok{coef =} \ConstantTok{Inf}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{scale\_x\_continuous}\NormalTok{( }\AttributeTok{breaks =} \FunctionTok{unique}\NormalTok{( sres}\SpecialCharTok{$}\NormalTok{ICC)) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{Designing-Simulations-in-R_files/figure-latex/unnamed-chunk-223-1} \end{center}

Our plots are still dominated by the strong effects of ICC and the number of clusters (J). When performance metrics like standard error vary systematically with design features, it becomes difficult to compare methods meaningfully across scenarios.

To address this, we shift our focus through standardization. Instead of noting that:

\begin{quote}
``All my SEs are getting smaller,''
\end{quote}

we want to conclude that:

\begin{quote}
``Estimator 1 has systematically higher SEs than Estimator 2 across scenarios.''
\end{quote}

Simulation results are often driven by broad design effects, which can obscure the specific methodological questions we care about. Standardizing helps bring those comparisons to the forefront.
Let's try that next.

One straightforward strategy for standardization is to compare each method's performance to a designated baseline. In this example, we use Linear Regression (LR) as our baseline.

We standardize by, for each simulation scenario, dividing each method's SE by the SE of LR, to produce \texttt{SE.scale}.
This relative measure, \texttt{SE.scale}, allows us to examine how much better or worse, across our scenarios, each method performs relative to a chosen reference method.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ssres }\OtherTok{\textless{}{-}} 
\NormalTok{  sres }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{( n\_bar, J, ATE, size\_coef, ICC, alpha ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{( }\AttributeTok{SE.scale =}\NormalTok{ SE }\SpecialCharTok{/}\NormalTok{ SE[method}\SpecialCharTok{==}\StringTok{"LR"}\NormalTok{]) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ungroup}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

We can then treat \texttt{SE.scale} as a measure like any other.
Here we bundle, showing how relative SE changes by J, \texttt{n\_bar} and ICC:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{( ssres, }\FunctionTok{aes}\NormalTok{( ICC, SE.scale, }\AttributeTok{col=}\NormalTok{method,}
                    \AttributeTok{group =} \FunctionTok{interaction}\NormalTok{(ICC, method) ) ) }\SpecialCharTok{+}
  \FunctionTok{facet\_grid}\NormalTok{( n\_bar }\SpecialCharTok{\textasciitilde{}}\NormalTok{  J, }\AttributeTok{labeller =}\NormalTok{ label\_both ) }\SpecialCharTok{+}
  \FunctionTok{geom\_boxplot}\NormalTok{( }\AttributeTok{position=}\StringTok{"dodge"}\NormalTok{, }\AttributeTok{width=}\FloatTok{0.1}\NormalTok{ ) }\SpecialCharTok{+}
  \FunctionTok{scale\_y\_continuous}\NormalTok{( }\AttributeTok{labels =}\NormalTok{ scales}\SpecialCharTok{::}\FunctionTok{percent\_format}\NormalTok{() )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{Designing-Simulations-in-R_files/figure-latex/unnamed-chunk-225-1} \end{center}

The figure above shows how each method compares to LR across simulation scenarios.
Aggregation clearly performs worse than LR when the Intraclass Correlation Coefficient (ICC) is zero. However, when ICC is greater than zero, Aggregation yields improved precision.
The Multilevel Model (MLM), in contrast, appears more adaptive.
It captures the benefits of aggregation when ICC is high, but avoids the precision cost when ICC is zero.
This adaptivity makes MLM appealing in practice when ICC is unknown or variable across contexts.

In looking at the plot we are seeing essentially identical rows and and fairly similar across columns.
This suggests we should bundle the \texttt{n\_bar} to get a cleaner view of the main patterns, and that we can also bundle over \texttt{J} as well.
We finally drop the \texttt{LR} results entirely, as it is the reference method and always has a relative SE of 1.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ssres }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{( method }\SpecialCharTok{!=} \StringTok{"LR"}\NormalTok{ ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{( }\FunctionTok{aes}\NormalTok{( ICC, SE.scale, }\AttributeTok{col=}\NormalTok{method, }
               \AttributeTok{group =} \FunctionTok{interaction}\NormalTok{(ICC, method) ) ) }\SpecialCharTok{+}
  \FunctionTok{facet\_grid}\NormalTok{( size\_coef }\SpecialCharTok{\textasciitilde{}}\NormalTok{ alpha, }\AttributeTok{labeller =}\NormalTok{ label\_both ) }\SpecialCharTok{+}
  \FunctionTok{geom\_hline}\NormalTok{( }\AttributeTok{yintercept =} \DecValTok{1}\NormalTok{  ) }\SpecialCharTok{+}
  \FunctionTok{geom\_boxplot}\NormalTok{( }\AttributeTok{position=}\StringTok{"dodge"}\NormalTok{, }\AttributeTok{width=}\FloatTok{0.1}\NormalTok{ ) }\SpecialCharTok{+}
  \FunctionTok{scale\_x\_continuous}\NormalTok{( }\AttributeTok{breaks=}\FunctionTok{unique}\NormalTok{( ssres}\SpecialCharTok{$}\NormalTok{ICC ) ) }\SpecialCharTok{+}
  \FunctionTok{scale\_y\_continuous}\NormalTok{( }\AttributeTok{breaks=}\FunctionTok{seq}\NormalTok{(}\DecValTok{90}\NormalTok{, }\DecValTok{125}\NormalTok{, }\AttributeTok{by=}\DecValTok{5}\NormalTok{ ) ) }
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{Designing-Simulations-in-R_files/figure-latex/unnamed-chunk-226-1} \end{center}

The pattern is clear: when ICC = 0, Aggregation performs worse than LR, and MLM performs about the same. But as ICC increases, Aggregation and MLM both improve, and perform about the same to each other.
This highlights the robustness of MLM across diverse conditions.

As a warning regarding Monte Carlo uncertainty: when standardizing results, it is important to remember that uncertainty in the baseline measure (here, LR) propagates to the standardized values.
This should be considered when interpreting variability in the scaled results.
Uncertainty for relative performance is generally tricky to assess.

To clarify the main patterns, we then aggregate our SE.scale across the bundled simulation settings---relative performance is on the same scale, so averaging is now a natural thing to do.
We have aggregated out sample sizes, and we go further and remove \texttt{size\_coef} since it does not seem to matter much, given the above plot:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{s2 }\OtherTok{\textless{}{-}} 
\NormalTok{  ssres }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{( ICC, alpha, method ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{( }\AttributeTok{SE.scale =} \FunctionTok{mean}\NormalTok{( SE.scale ) ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{( method }\SpecialCharTok{!=} \StringTok{"LR"}\NormalTok{ )}

\FunctionTok{ggplot}\NormalTok{( s2, }\FunctionTok{aes}\NormalTok{( ICC, SE.scale, }\AttributeTok{col=}\NormalTok{method ) ) }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{( }\SpecialCharTok{\textasciitilde{}}\NormalTok{ alpha, }\AttributeTok{labeller =}\NormalTok{ label\_both ) }\SpecialCharTok{+}
  \FunctionTok{geom\_hline}\NormalTok{( }\AttributeTok{yintercept =} \DecValTok{1}\NormalTok{ ) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{scale\_y\_continuous}\NormalTok{( }\AttributeTok{labels =}\NormalTok{ scales}\SpecialCharTok{::}\FunctionTok{percent\_format}\NormalTok{() ) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{( }\AttributeTok{title =} \StringTok{"Average relative SE to Linear Regression"}\NormalTok{,}
        \AttributeTok{y =} \StringTok{"Relative Standard Error"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{Designing-Simulations-in-R_files/figure-latex/unnamed-chunk-227-1} \end{center}

Our aggregated plot of the precision of aggregation and MLM relative to Linear Regression gives a simple story clearly told.
The performance of aggregation improves with ICC.
MLM also has benefits over LR, and does not pay much cost when ICC is low.

\section{The Bias-SE-RMSE plot}\label{the-bias-se-rmse-plot}

We can visualize bias and standard error together, along with RMSE, to get a rich picture of performance.
To illustrate, we subset to our scenarios where there is real bias for both LR and MLM (i.e., when ICC is 0; see findings under bias from above).
We also subset to our middle values of \texttt{n\_bar\ =\ 80} and our large \texttt{J=80}, where uncertainty is small and thus the relative role of bias may be large.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bsr }\OtherTok{\textless{}{-}}\NormalTok{ sres }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{( n\_bar }\SpecialCharTok{==} \DecValTok{80}\NormalTok{, J}\SpecialCharTok{==}\DecValTok{80}\NormalTok{, ICC }\SpecialCharTok{==} \DecValTok{0}\NormalTok{ )}

\NormalTok{bsr }\OtherTok{\textless{}{-}}\NormalTok{ bsr }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{( }\SpecialCharTok{{-}}\NormalTok{R, }\SpecialCharTok{{-}}\NormalTok{power, }\SpecialCharTok{{-}}\NormalTok{ESE\_hat, }\SpecialCharTok{{-}}\NormalTok{SD\_SE\_hat ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{pivot\_longer}\NormalTok{( }\AttributeTok{cols=}\FunctionTok{c}\NormalTok{(}\StringTok{"bias"}\NormalTok{,}\StringTok{"SE"}\NormalTok{,}\StringTok{"RMSE"}\NormalTok{),}
                \AttributeTok{names\_to =} \StringTok{"measure"}\NormalTok{,}
                \AttributeTok{values\_to =} \StringTok{"value"}\NormalTok{ ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{( measure, size\_coef, method, ICC, alpha ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{( }\AttributeTok{value =} \FunctionTok{mean}\NormalTok{( value ),}
             \AttributeTok{n =} \FunctionTok{n}\NormalTok{() )}

\NormalTok{bsr}\SpecialCharTok{$}\NormalTok{measure }\OtherTok{=} \FunctionTok{factor}\NormalTok{( bsr}\SpecialCharTok{$}\NormalTok{measure, }
                      \AttributeTok{levels=}\FunctionTok{c}\NormalTok{(}\StringTok{"bias"}\NormalTok{, }\StringTok{"SE"}\NormalTok{, }\StringTok{"RMSE"}\NormalTok{),}
                      \AttributeTok{labels =}\FunctionTok{c}\NormalTok{(}\StringTok{"bias"}\NormalTok{, }\StringTok{"SE"}\NormalTok{, }\StringTok{"RMSE"}\NormalTok{ ) )}

\FunctionTok{ggplot}\NormalTok{( bsr, }\FunctionTok{aes}\NormalTok{( }\FunctionTok{as.factor}\NormalTok{(alpha), value, }\AttributeTok{col=}\NormalTok{method,}
                  \AttributeTok{group =}\NormalTok{ method )) }\SpecialCharTok{+}
  \FunctionTok{facet\_grid}\NormalTok{( size\_coef }\SpecialCharTok{\textasciitilde{}}\NormalTok{ measure ) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{( }\AttributeTok{y =} \StringTok{""}\NormalTok{, }\AttributeTok{x =} \StringTok{"Site Variation"}\NormalTok{ )  }\SpecialCharTok{+}
  \FunctionTok{geom\_hline}\NormalTok{( }\AttributeTok{yintercept =} \DecValTok{0}\NormalTok{ ) }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{( }\AttributeTok{legend.position=}\StringTok{"bottom"}\NormalTok{,}
         \AttributeTok{legend.direction=}\StringTok{"horizontal"}\NormalTok{, }
         \AttributeTok{legend.key.width=}\FunctionTok{unit}\NormalTok{(}\DecValTok{1}\NormalTok{,}\StringTok{"cm"}\NormalTok{),}
         \AttributeTok{panel.border =} \FunctionTok{element\_blank}\NormalTok{() )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{Designing-Simulations-in-R_files/figure-latex/unnamed-chunk-228-1} \end{center}

The combination of bias, standard error, and RMSE provides a rich and informative view of estimator performance.
The top row represents settings where effect size is independent of cluster size, while the bottom row reflects a correlation between size and effect.
We see how bias, SE and RMSE grow as site variation increases (moving rightward in each panel).
Notably, when effect size is related to cluster size (bottom row), both linear regression and MLM exhibit significant bias, leading to notable increase in RMSE over SE.
In contrast, when effect size is unrelated to cluster size (top row), all methods show minimal bias, and the SEs are about the same; that said, we see aggregation paying a penalty as variation cluster size increases.
Overall, we see RMSE is primarily driven by SE.

The Bias-SE-RMSE visualization directly illustrates the canonical relationship:

\[ \text{RMSE}^2 = \text{Bias}^2 + \text{SE}^2 \]

The plot shows overall performance (RMSE) decomposed into into its two fundamental components: systematic error (bias) and variability (standard error).
Here we see how bias for LR, for example, is dominant when site variation is high.
The differences in SE across methods are small and are thus not the main reason for differences in overall estimator performance; bias is the main driver.

This is the kind of diagnostic plot we often wish were included in more applied simulation studies.

\section{Assessing the quality of the estimated SEs}\label{assessing-the-quality-of-the-estimated-ses}

So far we have examined the performance of our \emph{point estimators}.
We next look at ways to assess our \emph{estimated} standard errors.
A good first question is whether they are about the right size, on average, across all the scenarios.

When assessing estimated standard errors it is very important to see if they are \emph{reliably} the right size, making the bundling method an especially important tool here.
We first see if the average estimated SE, relative to the true SE, is usually around 1 across all scenarios:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sres }\OtherTok{\textless{}{-}}\NormalTok{ sres }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{( }\AttributeTok{inflate =}\NormalTok{ ESE\_hat }\SpecialCharTok{/}\NormalTok{ SE  ) }

\FunctionTok{ggplot}\NormalTok{( sres,}
        \FunctionTok{aes}\NormalTok{( ICC, inflate, }\AttributeTok{col=}\NormalTok{method,}
             \AttributeTok{group =} \FunctionTok{interaction}\NormalTok{(ICC,method) ) ) }\SpecialCharTok{+}
    \FunctionTok{facet\_grid}\NormalTok{( . }\SpecialCharTok{\textasciitilde{}}\NormalTok{ J, }\AttributeTok{labeller =}\NormalTok{ label\_both) }\SpecialCharTok{+}
  \FunctionTok{geom\_boxplot}\NormalTok{( }\AttributeTok{position=}\StringTok{"dodge"}\NormalTok{, }\AttributeTok{outlier.size=}\FloatTok{0.5}\NormalTok{ ) }\SpecialCharTok{+}
  \FunctionTok{geom\_hline}\NormalTok{( }\AttributeTok{yintercept=}\DecValTok{1}\NormalTok{ ) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{( }\AttributeTok{color=}\StringTok{"n"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Inflation"}\NormalTok{ ) }\SpecialCharTok{+}
  \FunctionTok{scale\_y\_continuous}\NormalTok{( }\AttributeTok{labels =}\NormalTok{ scales}\SpecialCharTok{::}\FunctionTok{percent\_format}\NormalTok{() ) }
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=1\linewidth]{Designing-Simulations-in-R_files/figure-latex/unnamed-chunk-229-1} \end{center}

We see that, for the most part, our estimated SEs are about right, on average, across all scenarios.
When the ICC is 0 and J is small, the MLM SEs are clearly too high.
We also see that when J is 5, the LR estimator tends to be a bit low.

We next start exploring to dig into why our boxplots are wide.
In particular, we want to see if other factors dictate when the SEs are biased.
We first subset to the \(J = 80\) scenarios to see if those box widths could just be due to the MCSEs.
The \texttt{simhelpers} \texttt{calc\_relative\_var()} method gives mcses for relative bias of an estimated \emph{variance} to the true \emph{variance}.
We thus square our estimated SEs to get variance estimates, and then use that function to see if the relative variance estimates are biased:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{se\_res }\OtherTok{\textless{}{-}}\NormalTok{ res }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{( n\_bar, J, ATE, size\_coef, ICC, alpha, method ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarize}\NormalTok{( }\FunctionTok{calc\_relative\_var}\NormalTok{( }\AttributeTok{estimates =}\NormalTok{ ATE\_hat,}
                                \AttributeTok{var\_estimates =}\NormalTok{ SE\_hat}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{,}
                                \AttributeTok{criteria =} \StringTok{"relative bias"}\NormalTok{ ) )}
  
\NormalTok{se\_res }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{( J }\SpecialCharTok{==} \DecValTok{80}\NormalTok{, n\_bar }\SpecialCharTok{==} \DecValTok{80}\NormalTok{ ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{( }\FunctionTok{aes}\NormalTok{( ICC, rel\_bias\_var, }\AttributeTok{col=}\NormalTok{method ) ) }\SpecialCharTok{+}
  \FunctionTok{facet\_grid}\NormalTok{( size\_coef }\SpecialCharTok{\textasciitilde{}}\NormalTok{ alpha  ) }\SpecialCharTok{+} 
  \FunctionTok{geom\_hline}\NormalTok{( }\AttributeTok{yintercept =} \DecValTok{1}\NormalTok{ ) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{( }\AttributeTok{position =} \FunctionTok{position\_dodge}\NormalTok{( }\AttributeTok{width=}\FloatTok{0.05}\NormalTok{) ) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{( }\AttributeTok{position =} \FunctionTok{position\_dodge}\NormalTok{( }\AttributeTok{width=}\FloatTok{0.05}\NormalTok{) ) }\SpecialCharTok{+}
  \FunctionTok{geom\_errorbar}\NormalTok{( }\FunctionTok{aes}\NormalTok{( }\AttributeTok{ymin =}\NormalTok{ rel\_bias\_var }\SpecialCharTok{{-}} \FloatTok{1.96}\SpecialCharTok{*}\NormalTok{rel\_bias\_var\_mcse,}
                      \AttributeTok{ymax =}\NormalTok{ rel\_bias\_var }\SpecialCharTok{+} \FloatTok{1.96}\SpecialCharTok{*}\NormalTok{rel\_bias\_var\_mcse ),}
                 \AttributeTok{position =} \FunctionTok{position\_dodge}\NormalTok{( }\AttributeTok{width=}\FloatTok{0.05}\NormalTok{ ),}
                 \AttributeTok{width =} \DecValTok{0}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{Designing-Simulations-in-R_files/figure-latex/unnamed-chunk-230-1} \end{center}

In looking at this plot, we see no real evidence of miscalibration: our confidence intervals are generally covering 1, meaning our average estimated variance is about the same as the true variance.
This makes us think the boxes for \(J=80\) in the prior plot are wide due to MCSE rather than other simulation factors driving some slight miscalibration.
We might then assume this applies to the \(J = 20\) case as well.

\subsection{Stability of estimated SEs}\label{stability-of-estimated-ses}

We can also look at how stable the estimated SEs are, relative to the actual uncertainty they are trying to capture.
We do this by calculating the standard deviation of the estimated standard errors and compare that to the standard deviation of the point estimate.
This is related to the coefficient of variation of \texttt{SE\_hat}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sres }\OtherTok{\textless{}{-}} \FunctionTok{mutate}\NormalTok{( sres,}
               \AttributeTok{SD\_SE\_hat\_rat =}\NormalTok{ SD\_SE\_hat }\SpecialCharTok{/}\NormalTok{ SE )}

\FunctionTok{ggplot}\NormalTok{( sres,}
        \FunctionTok{aes}\NormalTok{( ICC, SD\_SE\_hat\_rat, }\AttributeTok{col=}\NormalTok{method,}
             \AttributeTok{group =} \FunctionTok{interaction}\NormalTok{(ICC,method) ) ) }\SpecialCharTok{+}
    \FunctionTok{facet\_grid}\NormalTok{( . }\SpecialCharTok{\textasciitilde{}}\NormalTok{ J, }\AttributeTok{labeller =}\NormalTok{ label\_both) }\SpecialCharTok{+}
  \FunctionTok{geom\_boxplot}\NormalTok{( }\AttributeTok{position=}\StringTok{"dodge"}\NormalTok{ ) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{( }\AttributeTok{color=}\StringTok{"n"}\NormalTok{ ) }\SpecialCharTok{+}
  \FunctionTok{scale\_y\_continuous}\NormalTok{( }\AttributeTok{labels =}\NormalTok{ scales}\SpecialCharTok{::}\FunctionTok{percent\_format}\NormalTok{() ) }\SpecialCharTok{+}
  \FunctionTok{scale\_x\_continuous}\NormalTok{( }\AttributeTok{breaks =} \FunctionTok{unique}\NormalTok{( sres}\SpecialCharTok{$}\NormalTok{ICC ) )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{Designing-Simulations-in-R_files/figure-latex/unnamed-chunk-231-1} \end{center}

Overall, we have a lot of variation in the estimated SEs, relative to the actual uncertainty.
We also see that MLM has more reliably estimated SEs than other methods when ICC is small.
Aggregation has relatively more trouble estimating uncertainty when J is small.
Finally, LR's SEs are slightly more unstable, relative to the other methods, when \(J\) is larger.

Assessing the stability of standard errors is usually very in the weeds of a performance evaluation.
It is a tricky measure: if the true SE is high for a method, then the relative instability will be lower, even if the absolute instability is the same.
Thinking through what might be driving what can be difficult, and is often not central to the main purpose of an evaluation.
People often look at confidence interval coverage and confidence interval width, instead, to assess the quality of estimated SEs.

\section{Assessing confidence intervals}\label{assessing-confidence-intervals}

Coverage is a blend of how accurate (unbiased) our estimates are and how good our estimated SEs are.
To assess coverage, we first calculate confidence intervals using the estimated effect, estimated standard error, and degrees of freedom.
Once we have our calculated \(t\)-based intervals, we can average them across runs to get average width and coverage using \texttt{simhelpers}'s \texttt{calc\_coverage()} method.
A good confidence interval estimator would be one which is generally relatively short while maintaining proper coverage.

Our calculations are as so:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{res}\SpecialCharTok{$}\NormalTok{df }\OtherTok{=}\NormalTok{ res}\SpecialCharTok{$}\NormalTok{J}
\NormalTok{res }\OtherTok{\textless{}{-}} \FunctionTok{mutate}\NormalTok{( res,}
               \AttributeTok{tstar =} \FunctionTok{qt}\NormalTok{( }\FloatTok{0.975}\NormalTok{, }\AttributeTok{df=}\NormalTok{df ),}
               \AttributeTok{CI\_low =}\NormalTok{ ATE\_hat }\SpecialCharTok{{-}}\NormalTok{ tstar}\SpecialCharTok{*}\NormalTok{SE\_hat,}
               \AttributeTok{CI\_high =}\NormalTok{ ATE\_hat }\SpecialCharTok{+}\NormalTok{ tstar}\SpecialCharTok{*}\NormalTok{SE\_hat,}
               \AttributeTok{width =}\NormalTok{ CI\_high }\SpecialCharTok{{-}}\NormalTok{ CI\_low )}

\NormalTok{covres }\OtherTok{\textless{}{-}}\NormalTok{ res }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{( n\_bar, J, ICC, alpha, size\_coef, method, ATE ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{( }\FunctionTok{calc\_coverage}\NormalTok{( }\AttributeTok{lower\_bound =}\NormalTok{ CI\_low, }
                            \AttributeTok{upper\_bound =}\NormalTok{ CI\_high,}
                            \AttributeTok{true\_param =}\NormalTok{ ATE ) ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ungroup}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

We then look at those simulations with a relationship of site size and impact.
We subset to \texttt{n\_bar\ =\ 80}, \texttt{size\_coef\ =\ 0.2} and \texttt{alpha\ =\ 0.5} to simplify for our initial plot:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{c\_sub }\OtherTok{\textless{}{-}}\NormalTok{ covres }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{filter}\NormalTok{( size\_coef }\SpecialCharTok{!=} \DecValTok{0}\NormalTok{, n\_bar }\SpecialCharTok{==} \DecValTok{80}\NormalTok{, alpha }\SpecialCharTok{==} \FloatTok{0.5}\NormalTok{ )}

\FunctionTok{ggplot}\NormalTok{( c\_sub, }\FunctionTok{aes}\NormalTok{( ICC, coverage, }\AttributeTok{col=}\NormalTok{method, }\AttributeTok{group=}\NormalTok{method ) ) }\SpecialCharTok{+}
  \FunctionTok{facet\_grid}\NormalTok{( . }\SpecialCharTok{\textasciitilde{}}\NormalTok{ J, }\AttributeTok{labeller =}\NormalTok{ label\_both ) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{( }\AttributeTok{position =} \FunctionTok{position\_dodge}\NormalTok{( }\AttributeTok{width=}\FloatTok{0.05}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{( }\AttributeTok{position =} \FunctionTok{position\_dodge}\NormalTok{( }\AttributeTok{width=}\FloatTok{0.05}\NormalTok{) ) }\SpecialCharTok{+}
  \FunctionTok{geom\_errorbar}\NormalTok{( }\FunctionTok{aes}\NormalTok{( }\AttributeTok{ymax =}\NormalTok{ coverage }\SpecialCharTok{+} \DecValTok{2}\SpecialCharTok{*}\NormalTok{coverage\_mcse, }
                      \AttributeTok{ymin =}\NormalTok{ coverage }\SpecialCharTok{{-}} \DecValTok{2}\SpecialCharTok{*}\NormalTok{coverage\_mcse ), }\AttributeTok{width=}\DecValTok{0}\NormalTok{,}
                 \AttributeTok{position =} \FunctionTok{position\_dodge}\NormalTok{( }\AttributeTok{width=}\FloatTok{0.05}\NormalTok{) ) }\SpecialCharTok{+}
  \FunctionTok{geom\_hline}\NormalTok{( }\AttributeTok{yintercept =} \FloatTok{0.95}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{Designing-Simulations-in-R_files/figure-latex/unnamed-chunk-233-1} \end{center}

Generally coverage is good unless \(J\) is low or ICC is 0.
Monte Carlo standard error based confidence intervals on our performance metrics indicate that, in some settings, the observed coverage is reliably different from the nominal 95\%, suggesting issues with estimator bias, standard error estimation, or both.
We might then want to see if these results are general across the other simulation scenarios (see exercises).

For confidence interval width, we can calculate the average width relative to the width of LR across all scenarios:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{covres }\OtherTok{\textless{}{-}}\NormalTok{ covres }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{( n\_bar, J, ICC, alpha, size\_coef ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{( }\AttributeTok{width\_rel =}\NormalTok{ width }\SpecialCharTok{/}\NormalTok{ width[method}\SpecialCharTok{==}\StringTok{"LR"}\NormalTok{] ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ungroup}\NormalTok{()}

\NormalTok{c\_agg }\OtherTok{\textless{}{-}}\NormalTok{ covres }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{( ICC, J, alpha, method ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{( }\AttributeTok{coverage =} \FunctionTok{mean}\NormalTok{( coverage ),}
             \AttributeTok{coverage\_mcse =} \FunctionTok{sqrt}\NormalTok{( }\FunctionTok{mean}\NormalTok{( coverage\_mcse}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{ ) ) }\SpecialCharTok{/} \FunctionTok{n}\NormalTok{(),}
             \AttributeTok{width\_rel =} \FunctionTok{mean}\NormalTok{( width\_rel ) ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{( method }\SpecialCharTok{!=} \StringTok{"LR"}\NormalTok{ )}
  
\FunctionTok{ggplot}\NormalTok{( c\_agg, }\FunctionTok{aes}\NormalTok{( ICC, width\_rel, }\AttributeTok{col=}\NormalTok{method, }\AttributeTok{group=}\NormalTok{method ) ) }\SpecialCharTok{+}
  \FunctionTok{facet\_grid}\NormalTok{( J }\SpecialCharTok{\textasciitilde{}}\NormalTok{ alpha ) }\SpecialCharTok{+}
  \FunctionTok{geom\_hline}\NormalTok{( }\AttributeTok{yintercept =} \DecValTok{1}\NormalTok{ ) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{( }\AttributeTok{y =} \StringTok{"Relative CI Width"}\NormalTok{,}
        \AttributeTok{title =} \StringTok{"Average Relative CI Width to Linear Regression"}\NormalTok{ ) }\SpecialCharTok{+}
  \FunctionTok{scale\_y\_continuous}\NormalTok{( }\AttributeTok{labels =}\NormalTok{ scales}\SpecialCharTok{::}\FunctionTok{percent\_format}\NormalTok{() )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{Designing-Simulations-in-R_files/figure-latex/unnamed-chunk-234-1} \end{center}

Confidence interval width serves as a proxy for precision. Narrow intervals suggest more precise estimates.
We see MLM has wider intervals, relative to LR, when ICC is low. When there is site variation, both Agg and MLM have shorter intervals.
This plot essentially echos our standard error findings, as expected.
There are mild differences due to differences in how the degrees of freedom are calculated, however.

\section{Exercises}\label{exercises-8}

\subsection{Assessing uncertainty}\label{assessing-uncertainty}

Select a plot from this chapter that does not have monte carlo standard errors.
Add monte carlo standard errors to that selected plot.

\subsection{Assessing power}\label{assessing-power}

Make a plot showing how power changes as \(J\) changes.
As an extra step, add MCSEs to the plot to assess whether you have a reasonable numbers of replications in the simulation.

\subsection{Going deeper with coverage}\label{going-deeper-with-coverage}

For our cluster RCT, we saw coverage is low in some circumstances.
Explore the full set of simulation results, possibly adding your own analyses, to ascertain why coverage is low.
Is it due to estimator bias? Unreliably estimated standard errors? Something else?
Make a final plot and draft a short write-up that captures how coverage is vulnerable for the three estimators.

\subsection{Pearson correlations with a bivariate Poisson distribution}\label{pearson-correlations-with-a-bivariate-poisson-distribution}

In Section \ref{using-pmap-to-run-multifactor-simulations}, we generated results for a multifactor simulation of confidence intervals for Pearson's correlation coefficient under a bivariate Poisson data-generating process. Create a plot that depicts the coverage rate of the confidence intervals for \(\rho\) across all four simulation factors.
Write a brief explanation of how the plot is laid out and explain why you chose to construct it as you did.

\subsection{Making another plot for assessing SEs}\label{making-another-plot-for-assessing-ses}

In the main chapter we examined how SE changes as a function of various simulation factors.
Now generate a plot to see whether and when cluster size meaningfully helps precision, and explain what you find.

\chapter{Special Topics on Reporting Simulation Results}\label{special-topics-on-reporting-simulation-results}

In this chapter we cover some special topics on reporting simulation results.
We first walk through some examples of how to do regression modeling.
We then dive more deeply into what to do when you have only a few iterations per scenario, and then we discuss what to do when you are evaluating methods that sometimes fail to converge or give an answer.

\section{Using regression to analyze simulation results}\label{using-regression-to-analyze-simulation-results}

In Chapter \ref{presentation-of-results} we saw some examples of using regression and ANOVA on a set of simulation results to summarize overall patterns across scenarios.
In this chapter we will provide some further in-depth examples along with the R code for doing this sort of thing.

\subsection{Example 1: Biserial, revisited}\label{example-1-biserial-revisited-1}

As our first in depth example, we walk through the analysis that produces the final ANOVA summary table for the biserial correlation example in Chapter \ref{presentation-of-results}.
In the visualization there, we saw that several factors appeared to impact bias.
On the eta table presented later in that same chapter, we saw a table that decomposed the variance across several factors so we could see which simulation factors mattered most for bias.

To build that table, we first fit a regression model, regressing bias on all the simulation factors.
We first convert each factor to a factor variable, so that R does not assume a continuous relationship.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{options}\NormalTok{(}\AttributeTok{scipen =} \DecValTok{5}\NormalTok{)}
\NormalTok{mod }\OtherTok{=} \FunctionTok{lm}\NormalTok{( bias }\SpecialCharTok{\textasciitilde{}}\NormalTok{ fixed }\SpecialCharTok{+}\NormalTok{ rho }\SpecialCharTok{+} \FunctionTok{I}\NormalTok{(rho}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) }\SpecialCharTok{+}\NormalTok{ p1 }\SpecialCharTok{+}\NormalTok{ n, }\AttributeTok{data =}\NormalTok{ r\_F)}
\FunctionTok{summary}\NormalTok{(mod, }\AttributeTok{digits=}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = bias ~ fixed + rho + I(rho^2) + p1 + n, data = r_F)
## 
## Residuals:
##        Min         1Q     Median         3Q 
## -0.0215935 -0.0013608  0.0003823  0.0015677 
##        Max 
##  0.0081802 
## 
## Coefficients:
##                        Estimate  Std. Error
## (Intercept)         -0.00186446  0.00017971
## fixedSample cut-off -0.00363520  0.00009733
## rho                 -0.00942338  0.00069578
## I(rho^2)             0.00720857  0.00070868
## p1p1 = 1/3           0.00358696  0.00015390
## p1p1 = 1/4           0.00482709  0.00015390
## p1p1 = 1/5           0.00547657  0.00015390
## p1p1 = 1/8           0.00635532  0.00015390
## n.L                  0.00362949  0.00010882
## n.Q                 -0.00103981  0.00010882
## n.C                  0.00027941  0.00010882
## n^4                  0.00001976  0.00010882
##                     t value Pr(>|t|)    
## (Intercept)         -10.375   <2e-16 ***
## fixedSample cut-off -37.347   <2e-16 ***
## rho                 -13.544   <2e-16 ***
## I(rho^2)             10.172   <2e-16 ***
## p1p1 = 1/3           23.307   <2e-16 ***
## p1p1 = 1/4           31.365   <2e-16 ***
## p1p1 = 1/5           35.585   <2e-16 ***
## p1p1 = 1/8           41.295   <2e-16 ***
## n.L                  33.352   <2e-16 ***
## n.Q                  -9.555   <2e-16 ***
## n.C                   2.568   0.0103 *  
## n^4                   0.182   0.8559    
## ---
## Signif. codes:  
## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.003372 on 4788 degrees of freedom
## Multiple R-squared:  0.5107, Adjusted R-squared:  0.5096 
## F-statistic: 454.4 on 11 and 4788 DF,  p-value: < 2.2e-16
\end{verbatim}

The above printout gives main effects for each factor, averaged across the others.
Because \texttt{p1} and \texttt{n} are ordered factors, the \texttt{lm()} command automatically generates linear, quadratic, cubic and fourth order contrasts for them.
We smooth our \texttt{rho} factor, which has many levels of a continuous measure, with a quadratic curve.
We could instead use splines or some local linear regression if we were worried about model fit for a complex relationship.

The main effects are summaries of trends across contexts.
For example, averaged across the other contexts, the ``sample cutoff'' condition is around 0.004 lower than the population (the baseline condition).

As shown in Chapter \ref{presentation-of-results}, we can also use ANOVA to get a sense of the major sources of variation in the simulation results (e.g., identifying which factors have negligible/minor influence on the bias of an estimator).
To do this, we use \texttt{aov()} to fit an analysis of variance model:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{anova\_table }\OtherTok{\textless{}{-}} \FunctionTok{aov}\NormalTok{(bias }\SpecialCharTok{\textasciitilde{}}\NormalTok{ rho }\SpecialCharTok{*}\NormalTok{ p1 }\SpecialCharTok{*}\NormalTok{ fixed }\SpecialCharTok{*}\NormalTok{ n, }\AttributeTok{data =}\NormalTok{ r\_F)}
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{( }\FunctionTok{summary}\NormalTok{(anova\_table)[[}\DecValTok{1}\NormalTok{]],}
              \AttributeTok{digits =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{5}\NormalTok{) )}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{l|r|r|r|r|r}
\hline
  & Df & Sum Sq & Mean Sq & F value & Pr(>F)\\
\hline
rho & 1 & 0.0024 & 0.0024 & 1673.3 & 0\\
\hline
p1 & 4 & 0.0236 & 0.0059 & 4036.4 & 0\\
\hline
fixed & 1 & 0.0159 & 0.0159 & 10854.5 & 0\\
\hline
n & 4 & 0.0138 & 0.0034 & 2354.6 & 0\\
\hline
rho:p1 & 4 & 0.0017 & 0.0004 & 294.7 & 0\\
\hline
rho:fixed & 1 & 0.0034 & 0.0034 & 2354.7 & 0\\
\hline
p1:fixed & 4 & 0.0017 & 0.0004 & 288.0 & 0\\
\hline
rho:n & 4 & 0.0020 & 0.0005 & 342.3 & 0\\
\hline
p1:n & 16 & 0.0198 & 0.0012 & 847.5 & 0\\
\hline
fixed:n & 4 & 0.0134 & 0.0033 & 2286.0 & 0\\
\hline
rho:p1:fixed & 4 & 0.0005 & 0.0001 & 80.9 & 0\\
\hline
rho:p1:n & 16 & 0.0015 & 0.0001 & 62.9 & 0\\
\hline
rho:fixed:n & 4 & 0.0029 & 0.0007 & 501.2 & 0\\
\hline
p1:fixed:n & 16 & 0.0014 & 0.0001 & 61.1 & 0\\
\hline
rho:p1:fixed:n & 16 & 0.0004 & 0.0000 & 18.4 & 0\\
\hline
Residuals & 4700 & 0.0069 & 0.0000 & NA & NA\\
\hline
\end{tabular}

The advantage here is the multiple levels of our categorical factors get bundled together in our table of results, making a tidier display.
Note we are including interactions between our simulation factors.
The prior linear regression model was just estimating main effects of the factors, and not estimating these more complex relationships.

The eta table in Chapter \ref{presentation-of-results} is a summary of this anova table, which we generate as follows:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(lsr)}
\FunctionTok{etaSquared}\NormalTok{(anova\_table) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{as.data.frame}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{rownames\_to\_column}\NormalTok{(}\StringTok{"source"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{( }\AttributeTok{order =} \DecValTok{1} \SpecialCharTok{+} \FunctionTok{str\_count}\NormalTok{(source, }\StringTok{":"}\NormalTok{ ) ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{( order ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{arrange}\NormalTok{( }\SpecialCharTok{{-}}\NormalTok{eta.sq, }\AttributeTok{.by\_group =} \ConstantTok{TRUE}\NormalTok{ ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{relocate}\NormalTok{( order ) }
\end{Highlighting}
\end{Shaded}

We group the results by the order of the interaction, so that we can see the main effects first, then two-way interactions, and so on.
We then sort within each group to put the high importance factors first.
The resulting variance decomposition table shows the amount of variation explained by each combination of factors.

\subsection{Example 2: Cluster RCT example, revisited}\label{example-2-cluster-rct-example-revisited}

When we have several methods to compare, we can use meta-regression to understand how these methods change as other simulation factors change.
We next illustrate this with our running Cluster RCT example.

We first turn our simulation levels (except for ICC, which has several levels) into factors, so R does not assume that sample size, for example, should be treated as a continuous variable:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sres\_f }\OtherTok{\textless{}{-}}
\NormalTok{  sres }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{( }
    \FunctionTok{across}\NormalTok{( }\FunctionTok{c}\NormalTok{( n\_bar, J, size\_coef, alpha ), factor ),}
    \AttributeTok{ICC =} \FunctionTok{as.numeric}\NormalTok{(ICC)}
\NormalTok{  )}

\CommentTok{\# Run the regression}
\NormalTok{M }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{( bias }\SpecialCharTok{\textasciitilde{}}\NormalTok{ (n\_bar }\SpecialCharTok{+}\NormalTok{ J }\SpecialCharTok{+}\NormalTok{ size\_coef }\SpecialCharTok{+}\NormalTok{ ICC }\SpecialCharTok{+}\NormalTok{ alpha) }\SpecialCharTok{*}\NormalTok{ method, }
         \AttributeTok{data =}\NormalTok{ sres\_f )}

\CommentTok{\# View the results}
\FunctionTok{tidy}\NormalTok{( M ) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{( }\AttributeTok{digits =} \DecValTok{3}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{l|r|r|r|r}
\hline
term & estimate & std.error & statistic & p.value\\
\hline
(Intercept) & 0.002 & 0.003 & 0.872 & 0.384\\
\hline
n\_bar80 & -0.003 & 0.002 & -1.295 & 0.196\\
\hline
n\_bar320 & -0.001 & 0.002 & -0.657 & 0.511\\
\hline
J20 & -0.002 & 0.002 & -0.991 & 0.322\\
\hline
J80 & -0.002 & 0.002 & -0.885 & 0.376\\
\hline
size\_coef0.2 & 0.003 & 0.002 & 1.537 & 0.125\\
\hline
ICC & 0.001 & 0.003 & 0.272 & 0.786\\
\hline
alpha0.5 & -0.002 & 0.002 & -1.179 & 0.239\\
\hline
alpha0.8 & 0.001 & 0.002 & 0.419 & 0.676\\
\hline
methodLR & -0.012 & 0.004 & -3.060 & 0.002\\
\hline
methodMLM & 0.001 & 0.004 & 0.191 & 0.849\\
\hline
n\_bar80:methodLR & 0.000 & 0.003 & 0.037 & 0.971\\
\hline
n\_bar320:methodLR & 0.000 & 0.003 & -0.004 & 0.997\\
\hline
n\_bar80:methodMLM & 0.000 & 0.003 & -0.170 & 0.865\\
\hline
n\_bar320:methodMLM & -0.001 & 0.003 & -0.362 & 0.718\\
\hline
J20:methodLR & 0.005 & 0.003 & 1.722 & 0.085\\
\hline
J80:methodLR & 0.006 & 0.003 & 1.946 & 0.052\\
\hline
J20:methodMLM & 0.001 & 0.003 & 0.354 & 0.723\\
\hline
J80:methodMLM & 0.001 & 0.003 & 0.420 & 0.675\\
\hline
size\_coef0.2:methodLR & 0.016 & 0.002 & 6.741 & 0.000\\
\hline
size\_coef0.2:methodMLM & 0.003 & 0.002 & 1.294 & 0.196\\
\hline
ICC:methodLR & 0.000 & 0.004 & -0.062 & 0.951\\
\hline
ICC:methodMLM & -0.006 & 0.004 & -1.482 & 0.139\\
\hline
alpha0.5:methodLR & 0.006 & 0.003 & 2.210 & 0.027\\
\hline
alpha0.8:methodLR & 0.017 & 0.003 & 5.868 & 0.000\\
\hline
alpha0.5:methodMLM & 0.001 & 0.003 & 0.434 & 0.664\\
\hline
alpha0.8:methodMLM & 0.003 & 0.003 & 1.091 & 0.275\\
\hline
\end{tabular}

With even a modestly complex simulation, we can quickly generate a lot of regression coefficients, making our meta-regression somewhat hard to interpret.
The above model does not even have interactions between the simulation factors, even though the plots we have seen strongly suggest interactions among them.
That said, picking out the significant coefficients is a quick way to obtain clues as to what is driving performance.
For instance, several features interact with the LR method for bias.
The other two methods seem less impacted.

\subsubsection{Using LASSO to simplify the model}\label{using-lasso-to-simplify-the-model}

We can simplify a meta regression model using LASSO regression, to drop coefficients that are less relevant.
This requires some work to make our model matrix of dummy variables with all the interactions.
If using LASSO, we recommend fitting a separate model to each method being considered;
the set of fit LASSO models can then be compared to see which methods react to what factors, and how.

We first illustrate with LR, and then extend to all three.
To use the LASSO we have to prepare our data first by hand---this involves converting all our factors to sets of dummy variables for the regression.
We also generate all interaction terms up to the cubic level.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(modelr)}
\FunctionTok{library}\NormalTok{(glmnet)}

\NormalTok{sres\_f\_LR }\OtherTok{\textless{}{-}}\NormalTok{ sres\_f }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{( method }\SpecialCharTok{==} \StringTok{"LR"}\NormalTok{ )}

\CommentTok{\# Create model matrix}
\NormalTok{form }\OtherTok{\textless{}{-}}\NormalTok{ bias }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ( n\_bar }\SpecialCharTok{+}\NormalTok{ J }\SpecialCharTok{+}\NormalTok{ size\_coef }\SpecialCharTok{+}\NormalTok{ ICC }\SpecialCharTok{+}\NormalTok{ alpha )}\SpecialCharTok{\^{}}\DecValTok{3}
\NormalTok{X }\OtherTok{\textless{}{-}} \FunctionTok{model.matrix}\NormalTok{(form, }\AttributeTok{data =}\NormalTok{ sres\_f\_LR)[, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{]}
\CommentTok{\#          The [,{-}1] drops the intercept}
\FunctionTok{dim}\NormalTok{(X)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 270  71
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Fit LASSO}
\NormalTok{fit }\OtherTok{\textless{}{-}} \FunctionTok{cv.glmnet}\NormalTok{(X, sres\_f\_LR}\SpecialCharTok{$}\NormalTok{bias, }\AttributeTok{alpha =} \DecValTok{1}\NormalTok{)}

\CommentTok{\# Non{-}zero coefficients}
\FunctionTok{coef}\NormalTok{(fit, }\AttributeTok{s =} \StringTok{"lambda.1se"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{as.matrix}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{as.data.frame}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{rownames\_to\_column}\NormalTok{(}\StringTok{"term"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(}\FunctionTok{abs}\NormalTok{(lambda}\FloatTok{.1}\NormalTok{se) }\SpecialCharTok{\textgreater{}} \DecValTok{0}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\AttributeTok{digits =} \DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{l|r}
\hline
term & lambda.1se\\
\hline
(Intercept) & 0.004\\
\hline
size\_coef0.2 & 0.002\\
\hline
size\_coef0.2:alpha0.8 & 0.021\\
\hline
\end{tabular}

Note we have 71 covariates due to the many, many interactions and the fact that our sample sizes, etc., are all factors, not continuous.

When using regression, and especially LASSO, which levels are baseline can impact the final results.
We have our smallest sample sizes, no variation, 0 ICC, and no \texttt{size\_coef} as baseline.
We might imagine that other choices of baseline could suddenly make other factors appear with large coefficients.
One trick to avoid selecting a baseline is to give dummy variables for all the factors, and fit LASSO with the colinear terms.
Due to regularization, this would still work; we do not pursue this here, however.

We next bundle the above to make three models, one for each method.
We first rescale ICC to be on a 5 point scale to control it's relative coefficient size to the dummy variables, and then add a new feature of ``zeroICC'' as well (recalling the prior plots that showed ICC being 0 was unusual).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{meth }\OtherTok{=} \FunctionTok{c}\NormalTok{( }\StringTok{"LR"}\NormalTok{, }\StringTok{"MLM"}\NormalTok{, }\StringTok{"Agg"}\NormalTok{ )}
\NormalTok{sres\_f}\SpecialCharTok{$}\NormalTok{zeroICC }\OtherTok{=} \FunctionTok{ifelse}\NormalTok{( sres\_f}\SpecialCharTok{$}\NormalTok{ICC }\SpecialCharTok{==} \DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{ )}
\NormalTok{sres\_f}\SpecialCharTok{$}\NormalTok{ICCsc }\OtherTok{=}\NormalTok{ sres\_f}\SpecialCharTok{$}\NormalTok{ICC }\SpecialCharTok{*} \DecValTok{5} \CommentTok{\# rescale ICC to be on a 5 point scale}

\NormalTok{models }\OtherTok{\textless{}{-}} \FunctionTok{map}\NormalTok{( meth, }\ControlFlowTok{function}\NormalTok{(m) \{}
  
\NormalTok{  sres\_f\_LR }\OtherTok{\textless{}{-}}\NormalTok{ sres\_f }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{filter}\NormalTok{( method }\SpecialCharTok{==}\NormalTok{ m )}
  
\NormalTok{  form }\OtherTok{\textless{}{-}}\NormalTok{ bias }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ( n\_bar }\SpecialCharTok{+}\NormalTok{ J }\SpecialCharTok{+}\NormalTok{ size\_coef }\SpecialCharTok{+}\NormalTok{ ICCsc }\SpecialCharTok{+}\NormalTok{ alpha }\SpecialCharTok{+}\NormalTok{ zeroICC )}\SpecialCharTok{\^{}}\DecValTok{3}
\NormalTok{  X }\OtherTok{\textless{}{-}} \FunctionTok{model.matrix}\NormalTok{(form, }\AttributeTok{data =}\NormalTok{ sres\_f\_LR)[, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{]}
\NormalTok{  fit }\OtherTok{\textless{}{-}} \FunctionTok{cv.glmnet}\NormalTok{(X, sres\_f\_LR}\SpecialCharTok{$}\NormalTok{bias, }\AttributeTok{alpha =} \DecValTok{1}\NormalTok{)}
  
  \FunctionTok{coef}\NormalTok{(fit, }\AttributeTok{s =} \StringTok{"lambda.min"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{as.matrix}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{as.data.frame}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{rownames\_to\_column}\NormalTok{(}\StringTok{"term"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{rename}\NormalTok{( }\AttributeTok{estimate =}\NormalTok{ lambda.min ) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{filter}\NormalTok{(}\FunctionTok{abs}\NormalTok{(estimate) }\SpecialCharTok{\textgreater{}} \DecValTok{0}\NormalTok{) }
\NormalTok{\} )}

\NormalTok{models }\OtherTok{\textless{}{-}} 
\NormalTok{  models }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{set\_names}\NormalTok{(meth) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{bind\_rows}\NormalTok{( }\AttributeTok{.id =} \StringTok{"model"}\NormalTok{ )}

\NormalTok{m\_res }\OtherTok{\textless{}{-}}\NormalTok{ models }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{( model, term, estimate ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{pivot\_wider}\NormalTok{( }\AttributeTok{names\_from=}\StringTok{"model"}\NormalTok{, }\AttributeTok{values\_from=}\StringTok{"estimate"}\NormalTok{ ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{order =} \FunctionTok{str\_count}\NormalTok{(term, }\StringTok{":"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{arrange}\NormalTok{(order) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{relocate}\NormalTok{(order)}

\FunctionTok{options}\NormalTok{(}\AttributeTok{knitr.kable.NA =} \StringTok{\textquotesingle{}\textquotesingle{}}\NormalTok{)}
\NormalTok{m\_res }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{( }\AttributeTok{digits =} \DecValTok{3}\NormalTok{ ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{print}\NormalTok{( }\AttributeTok{na.print =} \StringTok{""}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## \begin{tabular}{r|l|r|r|r}
## \hline
## order & term & LR & MLM & Agg\\
## \hline
## 0 & (Intercept) & 0.000 & 0.001 & 0.001\\
## \hline
## 0 & n\_bar80 & -0.001 & -0.001 & \\
## \hline
## 0 & size\_coef0.2 & 0.002 & 0.002 & \\
## \hline
## 1 & J20:alpha0.8 & 0.002 &  & \\
## \hline
## 1 & size\_coef0.2:ICCsc & 0.001 &  & \\
## \hline
## 1 & size\_coef0.2:alpha0.5 & 0.008 &  & \\
## \hline
## 1 & size\_coef0.2:alpha0.8 & 0.026 & 0.003 & \\
## \hline
## 1 & ICCsc:alpha0.5 & 0.000 & -0.001 & \\
## \hline
## 1 & n\_bar80:ICCsc &  & 0.000 & \\
## \hline
## 1 & J20:ICCsc &  & 0.000 & \\
## \hline
## 2 & n\_bar80:J80:size\_coef0.2 & 0.000 &  & \\
## \hline
## 2 & n\_bar320:J20:ICCsc & -0.001 & -0.001 & \\
## \hline
## 2 & n\_bar80:J20:alpha0.8 & 0.004 & 0.005 & \\
## \hline
## 2 & n\_bar320:size\_coef0.2:alpha0.5 & 0.004 & 0.002 & \\
## \hline
## 2 & J20:size\_coef0.2:alpha0.5 & 0.001 &  & \\
## \hline
## 2 & J80:size\_coef0.2:alpha0.5 & 0.002 &  & \\
## \hline
## 2 & J20:size\_coef0.2:alpha0.8 & 0.008 &  & \\
## \hline
## 2 & J80:size\_coef0.2:alpha0.8 & 0.010 &  & \\
## \hline
## 2 & size\_coef0.2:ICCsc:alpha0.8 & 0.000 &  & \\
## \hline
## 2 & n\_bar80:size\_coef0.2:ICCsc &  & 0.000 & \\
## \hline
## 2 & J80:size\_coef0.2:zeroICC &  & 0.002 & \\
## \hline
## 2 & size\_coef0.2:alpha0.5:zeroICC &  & 0.003 & \\
## \hline
## 2 & size\_coef0.2:alpha0.8:zeroICC &  & 0.015 & \\
## \hline
## \end{tabular}
\end{verbatim}

Of course, this is table is hard to read. Better to instead plot the coefficients:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lvl }\OtherTok{=}\NormalTok{ m\_res}\SpecialCharTok{$}\NormalTok{term}
\NormalTok{m\_resL }\OtherTok{\textless{}{-}}\NormalTok{ m\_res }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{pivot\_longer}\NormalTok{( }\SpecialCharTok{{-}}\FunctionTok{c}\NormalTok{( order, term ), }
                \AttributeTok{names\_to =} \StringTok{"model"}\NormalTok{, }\AttributeTok{values\_to =} \StringTok{"estimate"}\NormalTok{ ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{( }\AttributeTok{term =} \FunctionTok{factor}\NormalTok{(term, }\AttributeTok{levels =} \FunctionTok{rev}\NormalTok{(lvl) ) )}

\FunctionTok{ggplot}\NormalTok{( m\_resL,}
        \FunctionTok{aes}\NormalTok{( }\AttributeTok{x =}\NormalTok{ term, }\AttributeTok{y =}\NormalTok{ estimate, }
             \AttributeTok{fill =}\NormalTok{ model, }\AttributeTok{group =}\NormalTok{ model ) ) }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{( }\SpecialCharTok{\textasciitilde{}}\NormalTok{ model ) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{( }\AttributeTok{stat =} \StringTok{"identity"}\NormalTok{, }\AttributeTok{position =} \StringTok{"dodge"}\NormalTok{ ) }\SpecialCharTok{+}
  \FunctionTok{geom\_hline}\NormalTok{(}\AttributeTok{yintercept =} \DecValTok{0}\NormalTok{ ) }\SpecialCharTok{+}
  \FunctionTok{coord\_flip}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: Removed 37 rows containing missing values or
## values outside the scale range (`geom_bar()`).
\end{verbatim}

\begin{center}\includegraphics[width=0.75\linewidth]{Designing-Simulations-in-R_files/figure-latex/unnamed-chunk-240-1} \end{center}

Here we see how LR stands out, but also how MLM stands out under different simulation factor combinations (see, e.g., the interaction of zeroICC, alpha being 0.8, and size\_coef being 0.2).
This aggregate plot provides some understanding of how the methods are similar, and dissimilar.

For another example we turn to the standard error.
Here we regress \(log(SE)\) onto the coefficients.
We then exponentiate the estimated coefficients to get the relative change in SE as a function of the factors.
We can interpret an exponentiated coefficient of, for example, 0.64 for MLM for \texttt{n\_bar80} as a 36\% reduction of the standard error when we increase n\_bar from the baseline of 20 to 80.
We use ordinary least squares and include all interactions up to three way interactions.
We will then simply drop all the tiny coefficients, rather than use the full LASSO machinery, to simplify our output.
This results in a plot similar to the above:

\begin{center}\includegraphics[width=0.75\linewidth]{Designing-Simulations-in-R_files/figure-latex/unnamed-chunk-241-1} \end{center}

Our plot clearly shows that the three methods are basically the same in terms of uncertainty estimation, with a few differences when alpha is 0.8.
We also see some interesting trends, such as the impact of n\_bar declines when ICC is higher (see the positive interaction terms at right of plot).

\section{Using regression trees to find important factors}\label{using-regression-trees-to-find-important-factors}

With more complex experiments, where the various factors are interacting with each other in strange ways, it can be a bit tricky to decipher which factors are important and what patterns are stable.
Another exploration approach we might use is regression trees.

We wrote a utility method, a wrapper to the \texttt{rpart} package, to do this (\href{code/create_analysis_tree.R}{script here}).
Here, for example, we see what predicts larger bias amounts:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{source}\NormalTok{( here}\SpecialCharTok{::}\FunctionTok{here}\NormalTok{( }\StringTok{"code/create\_analysis\_tree.R"}\NormalTok{ ) )}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{12411}\NormalTok{)}
\FunctionTok{create\_analysis\_tree}\NormalTok{( sres\_f,}
                      \AttributeTok{outcome =} \StringTok{"bias"}\NormalTok{,}
                      \AttributeTok{predictor\_vars =} \FunctionTok{c}\NormalTok{(}\StringTok{"method"}\NormalTok{, }\StringTok{"n\_bar"}\NormalTok{, }\StringTok{"J"}\NormalTok{,}
                                         \StringTok{"size\_coef"}\NormalTok{, }\StringTok{"ICC"}\NormalTok{, }\StringTok{"alpha"}\NormalTok{),}
                      \AttributeTok{tree\_title =} \StringTok{"Cluster RCT Bias Analysis Tree"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{Designing-Simulations-in-R_files/figure-latex/unnamed-chunk-242-1} \end{center}

The default pruning is based on a cross-fitting evaluation, but our sample size is not too terribly high (just the number of simulation scenarios fit) so this is quite unstable.
Rerunning the code with a different seed will generally give a different tree.
We find that it is often worth forcibly simplifying the tree.
Trees are built greedily, so forcibly trimming often leaves you only with the big things.
For example:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{create\_analysis\_tree}\NormalTok{( sres\_f,}
                      \AttributeTok{outcome =} \StringTok{"bias"}\NormalTok{,}
                      \AttributeTok{predictor\_vars =} \FunctionTok{c}\NormalTok{(}\StringTok{"method"}\NormalTok{, }\StringTok{"n\_bar"}\NormalTok{, }\StringTok{"J"}\NormalTok{,}
                                         \StringTok{"size\_coef"}\NormalTok{, }\StringTok{"ICC"}\NormalTok{, }\StringTok{"alpha"}\NormalTok{),}
                      \AttributeTok{tree\_title =} \StringTok{"Smaller Cluster RCT Bias Analysis Tree"}\NormalTok{, }
                      \AttributeTok{min\_leaves =} \DecValTok{5}\NormalTok{, }\AttributeTok{max\_leaves =} \DecValTok{10}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{Designing-Simulations-in-R_files/figure-latex/unnamed-chunk-243-1} \end{center}

This tree gives a very straightforward story: if \texttt{size\_coef} is not 0 and we are using LR, then alpha drives bias.

We can also zero in on specific methods to understand how they engage with the simulation factors, like so:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{create\_analysis\_tree}\NormalTok{( }\FunctionTok{filter}\NormalTok{( sres\_f, method}\SpecialCharTok{==}\StringTok{"LR"}\NormalTok{ ),}
                      \AttributeTok{outcome =} \StringTok{"bias"}\NormalTok{,}
                      \AttributeTok{min\_leaves =} \DecValTok{4}\NormalTok{,}
                      \AttributeTok{predictor\_vars =} \FunctionTok{c}\NormalTok{(}\StringTok{"n\_bar"}\NormalTok{, }\StringTok{"J"}\NormalTok{,}
                                         \StringTok{"size\_coef"}\NormalTok{, }\StringTok{"ICC"}\NormalTok{, }\StringTok{"alpha"}\NormalTok{),}
                      \AttributeTok{tree\_title =} \StringTok{"Drivers of Bias for LR method"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{Designing-Simulations-in-R_files/figure-latex/unnamed-chunk-244-1} \end{center}

We force more leaves to get at some more nuance.
We again immediately see, for the LR method, that bias is large when we have non-zero size coefficient \emph{and} a large alpha value.
Then, when \(J\) is small, bias is even larger.

Generally we would not use a tree like this for a final reporting of results, but they can be important tools for \emph{understanding} your results, which leads to how to make and select more conventional figures for an outward facing document.

\section{Analyzing results with few iterations per scenario}\label{analyzing-results-with-few-iterations-per-scenario}

When each simulation iteration is expensive to run (e.g., if fitting your model takes several minutes), then running thousands of iterations for many scenarios may not be computationally feasible.
But running simulations with only a small number of iterations will yield very noisy estimates of estimator performance for that scenario.

Now, if the methods being evaluated are substantially different, then differences in performance might still be evident even with only a few iterations.
More generally, however, the Monte Carlo Standard Errors (MCSEs) may be so large that you will have a hard time discriminating between systematic patterns and noise.

One tool to handle few iterations is aggregation: if you average across scenarios, those averages will have more precise estimates of (average) performance than the estimates of performance within the scenarios.
Do not, by contrast, trust the bundling approach--the MCSEs will make your boxes wider, and give the impression that there is more variation across scenarios than there really is.

Meta regression approaches such as we saw above can be particularly useful: a regression will effectively average performance across scenario, and give summaries of overall trends.
You can even fit random effects regression, specifically accounting for the noise in the scenario-specific performance measures.
For more on using random effects for your meta regression see \citet{gilbert2024multilevel}.

\subsection{Example: ClusterRCT with only 100 replicates per scenario}\label{example-clusterrct-with-only-100-replicates-per-scenario}

In the prior chapter we analyzed the results of our cluster RCT simulation with 1000 iterations per scenario.
But say we only had 25 per scenario.
Using the prior chapter as a guide, we next recreate some of the plots to show how MCSE can distort the picture of what is going on.

First, we look at our single plot of the raw results.
Before we plot, however, we calculate MCSEs and add them to the plot as error bars.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sres\_sub }\OtherTok{\textless{}{-}} 
\NormalTok{  ssres }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{( n\_bar }\SpecialCharTok{==} \DecValTok{320}\NormalTok{, J }\SpecialCharTok{==} \DecValTok{20}\NormalTok{ ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{( }\AttributeTok{bias.mcse =}\NormalTok{ SE }\SpecialCharTok{/} \FunctionTok{sqrt}\NormalTok{( R ) )}

\NormalTok{dodge }\OtherTok{\textless{}{-}} \FunctionTok{position\_dodge}\NormalTok{(}\AttributeTok{width =} \FloatTok{0.35}\NormalTok{)}
\FunctionTok{ggplot}\NormalTok{( sres\_sub, }\FunctionTok{aes}\NormalTok{( }\FunctionTok{as.factor}\NormalTok{(alpha), bias, }
                       \AttributeTok{col=}\NormalTok{method, }\AttributeTok{pch=}\NormalTok{method, }\AttributeTok{group=}\NormalTok{method ) ) }\SpecialCharTok{+}
  \FunctionTok{facet\_grid}\NormalTok{( size\_coef }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ICC, }\AttributeTok{labeller =}\NormalTok{ label\_both ) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{( }\AttributeTok{position =}\NormalTok{ dodge ) }\SpecialCharTok{+} 
  \FunctionTok{geom\_errorbar}\NormalTok{( }\FunctionTok{aes}\NormalTok{( }\AttributeTok{ymin =}\NormalTok{ bias }\SpecialCharTok{{-}} \DecValTok{2}\SpecialCharTok{*}\NormalTok{bias.mcse, }
                      \AttributeTok{ymax =}\NormalTok{ bias }\SpecialCharTok{+} \DecValTok{2}\SpecialCharTok{*}\NormalTok{bias.mcse ), }
                 \AttributeTok{width =} \DecValTok{0}\NormalTok{,}
                 \AttributeTok{position =}\NormalTok{ dodge ) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{( }\AttributeTok{position =}\NormalTok{ dodge ) }\SpecialCharTok{+}
  \FunctionTok{geom\_hline}\NormalTok{( }\AttributeTok{yintercept =} \DecValTok{0}\NormalTok{ ) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{coord\_cartesian}\NormalTok{( }\AttributeTok{ylim =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{0.10}\NormalTok{,}\FloatTok{0.10}\NormalTok{) )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{Designing-Simulations-in-R_files/figure-latex/unnamed-chunk-246-1} \end{center}

Our uncertainty is much less when ICC is 0; this is because our estimators are far more precise due to not having cluster variation to contend with.
Other than the ICC = 0 case, we see substantial amounts of uncertainty, making it very hard to tell the different estimators apart.
In the top row, second plot from left, we see that the three estimators are co-dependent: they all react similarly to the same datasets, so if we end up with datasets that randomly lead to large estimates, all three will give large estimates.
The shape we are seeing is not a systematic bias, but rather a shared random variation.

Here is the same plot with the full 1000 replicates, with the 100 replicate results overlaid in light color for comparison:

\begin{center}\includegraphics[width=0.75\linewidth]{Designing-Simulations-in-R_files/figure-latex/unnamed-chunk-247-1} \end{center}

The MCSEs have shrunk by around \(1/\sqrt{10} = 0.32\), as we would expect (generally the MCSEs will be on the order of \(1/\sqrt{R}\), where \(R\) is the number of replicates, so to halve the MCSE you need to quadruple the number of replicates).
Also note the ICC=0.2 top facet has shifted to a flat, slightly elevated line: we do not yet know if the elevation is real, just as we did not know if the dip in the prior plot was real.
Our confidence intervals are still including 0: it is possible there is no bias at all when the size coefficient is 0 (in fact we are fairly sure it is indeed the case).

Moving back to our ``small replicates'' simulation, we can use aggregation to smooth out some of our uncertainty.
For example, if we aggregate across 9 scenarios, our number of replicates goes from 100 to 900; our MCSEs should then be about a third the size.
To calculate an aggregated MCSE, we aggregate our scenario-specific MCSEs as follows:
\[ MCSE_{agg} = \sqrt{ \frac{1}{K^2} \sum_{k=1}^{K} MCSE_k^2 } \]

where \(MCSE_k\) is the Monte Carlo Standard Error for scenario \(k\), and \(K\) is the number of scenarios being averaged.
Assuming a collection of estimates are independent, the overall \(SE^2\) of an average is the average \(SE^2\) divided by \(K\).
In code we have:

Recall that the \texttt{SE} variable is simply the standard deviation of the estimates.

We can then make our aggregated bias plot, aggregating across \texttt{n\_bar} and \texttt{J}:

\begin{center}\includegraphics[width=0.75\linewidth]{Designing-Simulations-in-R_files/figure-latex/unnamed-chunk-250-1} \end{center}

Even with the additional replicates per point, we see noticeable noise in our plot: look at the top-right ICC of 0.8 facet, for example.
Also note how our three methods continue to track each other up and down in top row, giving a sense of a shared error.
This is because all methods are analyzing the same set of datasets; they have shared uncertainty.
This uncertainty can be deceptive.
It can also be a boon: if we are explicitly comparing the performance of one method vs another, the shared uncertainty can be subtracted out, similar to what happens in a blocked experiment \citep{gilbert2024multilevel}.

One way to take advantage of this is to fit a multilevel regression model to our raw simulation results with a random effect for dataset.
We next fit such a model, taking advantage of the fact that bias is simply the average of the error across replicates.
We first make a unique ID for each scenario and dataset, and then fit the model with a random effect for both.
The first random effect allows for specific scenarios to have more or less bias beyond what our model predicts.
The second random effect allows for a given dataset to have a larger or smaller error than expected, shared across the three estimators.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(lme4)}
\NormalTok{res\_small }\OtherTok{\textless{}{-}}\NormalTok{ res\_small }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}
    \AttributeTok{error =}\NormalTok{ ATE\_hat }\SpecialCharTok{{-}}\NormalTok{ ATE,}
    \AttributeTok{simID =} \FunctionTok{paste}\NormalTok{(n\_bar, J, size\_coef, ICC, alpha, }\AttributeTok{sep =} \StringTok{"\_"}\NormalTok{),}
    \AttributeTok{dataID =} \FunctionTok{paste}\NormalTok{( simID, runID, }\AttributeTok{sep=}\StringTok{"\_"}\NormalTok{ ),}
    \AttributeTok{J =} \FunctionTok{as.factor}\NormalTok{(J),}
    \AttributeTok{n\_bar =} \FunctionTok{as.factor}\NormalTok{(n\_bar),}
    \AttributeTok{alpha =} \FunctionTok{as.factor}\NormalTok{(alpha),}
    \AttributeTok{size\_coef =} \FunctionTok{as.factor}\NormalTok{(size\_coef)}
\NormalTok{  )}

\NormalTok{M }\OtherTok{\textless{}{-}} \FunctionTok{lmer}\NormalTok{( }
\NormalTok{  error }\SpecialCharTok{\textasciitilde{}}\NormalTok{ method }\SpecialCharTok{+}\NormalTok{ (}\DecValTok{1}\SpecialCharTok{|}\NormalTok{dataID) }\SpecialCharTok{+}\NormalTok{ (}\DecValTok{1}\SpecialCharTok{|}\NormalTok{simID),}
  \AttributeTok{data =}\NormalTok{ res\_small }
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning in checkConv(attr(opt, "derivs"),
## opt$par, ctrl = control$checkConv, : Model failed
## to converge with max|grad| = 0.00462697 (tol =
## 0.002, component 1)
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{arm}\SpecialCharTok{::}\FunctionTok{display}\NormalTok{(M)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## lmer(formula = error ~ method + (1 | dataID) + (1 | simID), data = res_small)
##             coef.est coef.se
## (Intercept) 0.00     0.00   
## methodLR    0.01     0.00   
## methodMLM   0.00     0.00   
## 
## Error terms:
##  Groups   Name        Std.Dev.
##  dataID   (Intercept) 0.39    
##  simID    (Intercept) 0.01    
##  Residual             0.06    
## ---
## number of obs: 81000, groups: dataID, 27000; simID, 270
## AIC = -95344.2, DIC = -95430.3
## deviance = -95393.3
\end{verbatim}

We can look at how much each source of variation explains the overall error:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ranef\_vars }\OtherTok{\textless{}{-}}
  \FunctionTok{as.data.frame}\NormalTok{(}\FunctionTok{VarCorr}\NormalTok{(M)) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(}\AttributeTok{grp =}\NormalTok{ grp, }\AttributeTok{sd =}\NormalTok{ vcov) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{( }\AttributeTok{sd =} \FunctionTok{sqrt}\NormalTok{(sd),}
          \AttributeTok{ICC =}\NormalTok{ sd}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{/} \FunctionTok{sum}\NormalTok{(sd}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{ ) )}

\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(ranef\_vars, }\AttributeTok{digits =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{l|r|r}
\hline
grp & sd & ICC\\
\hline
dataID & 0.39 & 0.98\\
\hline
simID & 0.01 & 0.00\\
\hline
Residual & 0.06 & 0.02\\
\hline
\end{tabular}

The random variation for \texttt{simID} captures unexplained variation due to the interactions of the simulation factors.
It appears to be a trivial amount; almost all the variation is due to the dataset.
This makes sense: each datasets is unbalanced due to random assignment, and that estimation error is part of the dataset random effect.

So far we have not included any simulation factors: we are pushing variation across simulation into the random effect terms. We can instead include the simulation factors as fixed effects, to see how they impact bias.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{M2 }\OtherTok{\textless{}{-}} \FunctionTok{lmer}\NormalTok{( }
\NormalTok{  error }\SpecialCharTok{\textasciitilde{}}\NormalTok{ method}\SpecialCharTok{*}\NormalTok{(J }\SpecialCharTok{+}\NormalTok{ n\_bar }\SpecialCharTok{+}\NormalTok{ ICC }\SpecialCharTok{+}\NormalTok{ alpha }\SpecialCharTok{+}\NormalTok{ size\_coef) }\SpecialCharTok{+}\NormalTok{ (}\DecValTok{1}\SpecialCharTok{|}\NormalTok{dataID) }\SpecialCharTok{+}\NormalTok{ (}\DecValTok{1}\SpecialCharTok{|}\NormalTok{simID),}
  \AttributeTok{data =}\NormalTok{ res\_small }
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning in checkConv(attr(opt, "derivs"),
## opt$par, ctrl = control$checkConv, : Model failed
## to converge with max|grad| = 0.0169698 (tol =
## 0.002, component 1)
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{texreg}\SpecialCharTok{::}\FunctionTok{screenreg}\NormalTok{(M2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## ======================================
##                          Model 1      
## --------------------------------------
## (Intercept)                   0.00    
##                              (0.01)   
## methodLR                     -0.01 ***
##                              (0.00)   
## methodMLM                     0.00    
##                              (0.00)   
## J20                          -0.01    
##                              (0.01)   
## J80                          -0.01    
##                              (0.01)   
## n_bar80                      -0.00    
##                              (0.01)   
## n_bar320                      0.00    
##                              (0.01)   
## ICC                           0.00    
##                              (0.01)   
## alpha0.5                     -0.00    
##                              (0.01)   
## alpha0.8                      0.01    
##                              (0.01)   
## size_coef0.2                  0.01    
##                              (0.00)   
## methodLR:J20                  0.01 ***
##                              (0.00)   
## methodMLM:J20                 0.00    
##                              (0.00)   
## methodLR:J80                  0.01 ***
##                              (0.00)   
## methodMLM:J80                 0.00    
##                              (0.00)   
## methodLR:n_bar80              0.00 *  
##                              (0.00)   
## methodMLM:n_bar80            -0.00    
##                              (0.00)   
## methodLR:n_bar320             0.00    
##                              (0.00)   
## methodMLM:n_bar320           -0.00    
##                              (0.00)   
## methodLR:ICC                 -0.00    
##                              (0.00)   
## methodMLM:ICC                -0.01 ***
##                              (0.00)   
## methodLR:alpha0.5             0.01 ***
##                              (0.00)   
## methodMLM:alpha0.5            0.00    
##                              (0.00)   
## methodLR:alpha0.8             0.02 ***
##                              (0.00)   
## methodMLM:alpha0.8            0.00 *  
##                              (0.00)   
## methodLR:size_coef0.2         0.02 ***
##                              (0.00)   
## methodMLM:size_coef0.2        0.00 ** 
##                              (0.00)   
## --------------------------------------
## AIC                      -95700.20    
## BIC                      -95421.14    
## Log Likelihood            47880.10    
## Num. obs.                 81000       
## Num. groups: dataID       27000       
## Num. groups: simID          270       
## Var: dataID (Intercept)       0.15    
## Var: simID (Intercept)        0.00    
## Var: Residual                 0.00    
## ======================================
## *** p < 0.001; ** p < 0.01; * p < 0.05
\end{verbatim}

The above models allow us to estimate how bias varies with method and simulation factor, while accounting for the uncertainty in the simulation.

Finally, we can see how much variation has been explained by comparing the random effect variances:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ranef\_vars1 }\OtherTok{\textless{}{-}}
  \FunctionTok{as.data.frame}\NormalTok{(}\FunctionTok{VarCorr}\NormalTok{(M)) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(}\AttributeTok{grp =}\NormalTok{ grp, }\AttributeTok{sd =}\NormalTok{ vcov) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{( }\AttributeTok{sd =} \FunctionTok{sqrt}\NormalTok{(sd),}
          \AttributeTok{ICC =}\NormalTok{ sd}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{/} \FunctionTok{sum}\NormalTok{(sd}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{ ) )}
\NormalTok{ranef\_vars2 }\OtherTok{\textless{}{-}}
  \FunctionTok{as.data.frame}\NormalTok{(}\FunctionTok{VarCorr}\NormalTok{(M2)) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(}\AttributeTok{grp =}\NormalTok{ grp, }\AttributeTok{sd =}\NormalTok{ vcov) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{( }\AttributeTok{sd =} \FunctionTok{sqrt}\NormalTok{(sd),}
          \AttributeTok{ICC =}\NormalTok{ sd}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{/} \FunctionTok{sum}\NormalTok{(sd}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{ ) )}
\NormalTok{rr }\OtherTok{=} \FunctionTok{left\_join}\NormalTok{( ranef\_vars1, ranef\_vars2, }\AttributeTok{by =} \StringTok{"grp"}\NormalTok{, }
                \AttributeTok{suffix =} \FunctionTok{c}\NormalTok{(}\StringTok{".null"}\NormalTok{, }\StringTok{".full"}\NormalTok{) )}
\NormalTok{rr }\OtherTok{\textless{}{-}}\NormalTok{ rr }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{( }\AttributeTok{sd.red =}\NormalTok{ sd.full }\SpecialCharTok{/}\NormalTok{ sd.null )}
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(rr, }\AttributeTok{digits =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{l|r|r|r|r|r}
\hline
grp & sd.null & ICC.null & sd.full & ICC.full & sd.red\\
\hline
dataID & 0.39 & 0.98 & 0.39 & 0.98 & 1.00\\
\hline
simID & 0.01 & 0.00 & 0.01 & 0.00 & 0.89\\
\hline
Residual & 0.06 & 0.02 & 0.06 & 0.02 & 0.99\\
\hline
\end{tabular}

\section{What to do with warnings in simulations}\label{what-to-do-with-warnings-in-simulations}

Sometimes our analytic strategy might give some sort of warning (or fail altogether).
For example, from the cluster randomized experiment case study we have:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{101012}\NormalTok{)  }\CommentTok{\# (I picked this to show a warning.)}
\NormalTok{dat }\OtherTok{\textless{}{-}} \FunctionTok{gen\_cluster\_RCT}\NormalTok{( }\AttributeTok{J =} \DecValTok{50}\NormalTok{, }\AttributeTok{n\_bar =} \DecValTok{100}\NormalTok{, }\AttributeTok{sigma2\_u =} \DecValTok{0}\NormalTok{ )}
\NormalTok{mod }\OtherTok{\textless{}{-}} \FunctionTok{lmer}\NormalTok{( Yobs }\SpecialCharTok{\textasciitilde{}} \DecValTok{1} \SpecialCharTok{+}\NormalTok{ Z }\SpecialCharTok{+}\NormalTok{ (}\DecValTok{1}\SpecialCharTok{|}\NormalTok{sid), }\AttributeTok{data=}\NormalTok{dat )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## boundary (singular) fit: see help('isSingular')
\end{verbatim}

We have to make a deliberate decision as to what to do about this:

\begin{itemize}
\tightlist
\item
  Keep these ``weird'' trials?
\item
  Drop them?
\end{itemize}

Generally, when a method fails or gives a warning is something to investigate in its own right.
Ideally, failure would not be too common, meaning we could drop those trials, or keep them, without really impacting our overall results.
But one should at least know what one is ignoring.

If you decide to drop them, you should drop the entire simulation iteration including the other estimators, even if they worked fine!
If there is something particularly unusual about the dataset, then dropping for one estimator, and keeping for the others that maybe didn't give a warning, but did struggle to estimate the estimand, would be unfair: in the final performance measures the estimators that did not give a warning could be being held to a higher standard, making the comparisons between estimators biased.

If your estimators generate warnings, you should calculate the rate of errors or warning messages as a performance measure.
Especially if you drop some trials, it is important to see how often things are acting pecularly.

As discussed earlier, the main tool for doing this is the \texttt{quietly()} function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{quiet\_lmer }\OtherTok{=} \FunctionTok{quietly}\NormalTok{( lmer )}
\NormalTok{qmod }\OtherTok{\textless{}{-}} \FunctionTok{quiet\_lmer}\NormalTok{( Yobs }\SpecialCharTok{\textasciitilde{}} \DecValTok{1} \SpecialCharTok{+}\NormalTok{ Z }\SpecialCharTok{+}\NormalTok{ (}\DecValTok{1}\SpecialCharTok{|}\NormalTok{sid), }\AttributeTok{data=}\NormalTok{dat )}
\NormalTok{qmod}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $result
## Linear mixed model fit by REML ['lmerModLmerTest']
## Formula: ..1
##    Data: ..2
## REML criterion at convergence: 14026.44
## Random effects:
##  Groups   Name        Std.Dev.
##  sid      (Intercept) 0.0000  
##  Residual             0.9828  
## Number of obs: 5000, groups:  sid, 50
## Fixed Effects:
## (Intercept)            Z  
##   -0.013930    -0.008804  
## optimizer (nloptwrap) convergence code: 0 (OK) ; 0 optimizer warnings; 1 lme4 warnings 
## 
## $output
## [1] ""
## 
## $warnings
## character(0)
## 
## $messages
## [1] "boundary (singular) fit: see help('isSingular')\n"
\end{verbatim}

You then might have, in your analyzing code:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{analyze\_data }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{( dat ) \{}
    
\NormalTok{    M1 }\OtherTok{\textless{}{-}} \FunctionTok{quiet\_lmer}\NormalTok{( Yobs }\SpecialCharTok{\textasciitilde{}} \DecValTok{1} \SpecialCharTok{+}\NormalTok{ Z }\SpecialCharTok{+}\NormalTok{ (}\DecValTok{1}\SpecialCharTok{|}\NormalTok{sid), }\AttributeTok{data=}\NormalTok{dat )}
\NormalTok{    message1 }\OtherTok{=} \FunctionTok{ifelse}\NormalTok{( }\FunctionTok{length}\NormalTok{( M1}\SpecialCharTok{$}\NormalTok{message ) }\SpecialCharTok{\textgreater{}} \DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{ )}
\NormalTok{    warning1 }\OtherTok{=} \FunctionTok{ifelse}\NormalTok{( }\FunctionTok{length}\NormalTok{( M1}\SpecialCharTok{$}\NormalTok{warning ) }\SpecialCharTok{\textgreater{}} \DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{ )}

    \CommentTok{\# Compile our results}
    \FunctionTok{tibble}\NormalTok{( }\AttributeTok{ATE\_hat =} \FunctionTok{coef}\NormalTok{(M1)[}\StringTok{"Z"}\NormalTok{],}
            \AttributeTok{SE\_hat =} \FunctionTok{se.coef}\NormalTok{(M1)[}\StringTok{"Z"}\NormalTok{],}
            \AttributeTok{message =}\NormalTok{ message1,}
            \AttributeTok{warning =}\NormalTok{ warning1 )}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Now you have your primary estimates, and also flags for whether there was a convergence issue.
In the analysis section you can then evaluate what proportion of the time there was a warning or message, and then do subset analyses to those simulation trials where there was no such warning.

For example, in our cluster RCT running example, we know that ICC is an important driver of when these convergence issues might occur, so we can explore how often we get a convergence message by ICC level:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{res }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{( method, ICC ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{( }\AttributeTok{message =} \FunctionTok{mean}\NormalTok{( message ) ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{pivot\_wider}\NormalTok{( }\AttributeTok{names\_from =} \StringTok{"method"}\NormalTok{, }\AttributeTok{values\_from=}\StringTok{"message"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: There were 15 warnings in `summarise()`.
## The first warning was:
## i In argument: `message = mean(message)`.
## i In group 1: `method = "Agg"` `ICC = 0`.
## Caused by warning in `mean.default()`:
## ! argument is not numeric or logical: returning NA
## i Run `dplyr::last_dplyr_warnings()` to see the
##   14 remaining warnings.
\end{verbatim}

\begin{verbatim}
## # A tibble: 5 x 4
##     ICC   Agg    LR   MLM
##   <dbl> <dbl> <dbl> <dbl>
## 1   0      NA    NA    NA
## 2   0.2    NA    NA    NA
## 3   0.4    NA    NA    NA
## 4   0.6    NA    NA    NA
## 5   0.8    NA    NA    NA
\end{verbatim}

We see that when the ICC is 0 we get a lot of convergence issues, but as soon as we pull away from 0 it drops off considerably.
At this point we might decide to drop those runs with a message or keep them.
In this case, we decide to keep.
It should not matter much, except possibly when ICC = 0, and we know the convergence issues are driven by trying to estimate a 0 variance, and thus is in some sense expected.
Furthermore, we know people using these methods would likely ignore these messages, and thus we are faithfully capturing how these methods would be used in practice.
We might eventually, however, want to do a separate analysis of the ICC = 0 context to see if the MLM approach is actually falling apart, or if it is just throwing warnings.

\chapter{Case study: Comparing different estimators}\label{case-study-comparing-different-estimators}

In a previous exercise (See Exercise \ref{exercise:trimmed-mean}), you wrote a simulation to compare the mean, median, and trimmed mean for estimating the same thing.
This exercise is analogous to what we often are doing in a paper: pretend we have ``invented'' the trimmed mean and want to demonstrate its utility.

In this chapter, we walk through a deeper analysis of the results from such a simulation.
In this simulation we have three factors: sample size, a degrees of freedom to control the thickness of the tails (thicker tails means higher chance of outliers), and a degree of skew (how much the tails tend to the right vs.~the left).
For our data-generation function we use a scaled skew \(t\)-distribution parameterized so the standard deviation will always be 1 and the mean will always be 0, with parameters to control the skew and tail thickness.
See the attached code file for the simulation.
Our final multi-factor simulation results look like this (we ran 1000 trials per scenario):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{results }\OtherTok{\textless{}{-}} \FunctionTok{read\_rds}\NormalTok{( here}\SpecialCharTok{::}\FunctionTok{here}\NormalTok{( }\StringTok{"results/skewed\_t\_simulation.rds"}\NormalTok{ ) )}
\NormalTok{results}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 144 x 8
##        n   df0  skew     seed estimator  RMSE
##    <dbl> <dbl> <dbl>    <dbl> <chr>     <dbl>
##  1    10     3     0 42242459 mean      0.323
##  2    10     3     0 42242459 median    0.246
##  3    10     3     0 42242459 trim.mean 0.246
##  4    10     3    10 42242476 mean      0.333
##  5    10     3    10 42242476 median    0.303
##  6    10     3    10 42242476 trim.mean 0.246
##  7    10     5     0 42242493 mean      0.320
##  8    10     5     0 42242493 median    0.304
##  9    10     5     0 42242493 trim.mean 0.290
## 10    10     5    10 42242510 mean      0.327
## # i 134 more rows
## # i 2 more variables: bias <dbl>, SE <dbl>
\end{verbatim}

Our first question is how sample size impacts our different estimators' precision.
We plot:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ns }\OtherTok{\textless{}{-}} \FunctionTok{unique}\NormalTok{( results}\SpecialCharTok{$}\NormalTok{n )}

\FunctionTok{ggplot}\NormalTok{(results) }\SpecialCharTok{+} 
  \FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{n, }\AttributeTok{y=}\NormalTok{SE, }\AttributeTok{col=}\NormalTok{estimator) }\SpecialCharTok{+}
  \FunctionTok{facet\_grid}\NormalTok{( skew }\SpecialCharTok{\textasciitilde{}}\NormalTok{ df0 , }\AttributeTok{labeller =} \StringTok{"label\_both"}\NormalTok{ ) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{scale\_x\_log10}\NormalTok{( }\AttributeTok{breaks=}\NormalTok{ns )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{Designing-Simulations-in-R_files/figure-latex/unnamed-chunk-262-1} \end{center}

The above doesn't show differences clearly because all the SEs goes to zero as \(n\) increases.
One move when we see strong trends like this is to log our outcome or otherwise re-scale the variables.
Using log-10 scales for both axes makes it clear that relative differences in precision are nearly constant across sample sizes.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{( results ) }\SpecialCharTok{+}
  \FunctionTok{aes}\NormalTok{( }\AttributeTok{x=}\NormalTok{n, }\AttributeTok{y=}\NormalTok{SE, }\AttributeTok{col=}\NormalTok{estimator ) }\SpecialCharTok{+}
  \FunctionTok{facet\_grid}\NormalTok{( skew }\SpecialCharTok{\textasciitilde{}}\NormalTok{ df0 , }\AttributeTok{labeller =} \StringTok{"label\_both"}\NormalTok{ ) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{scale\_x\_log10}\NormalTok{( }\AttributeTok{breaks=}\NormalTok{ns ) }\SpecialCharTok{+}
  \FunctionTok{scale\_y\_log10}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{Designing-Simulations-in-R_files/figure-latex/unnamed-chunk-263-1} \end{center}

One step better would be to re-scale based on our knowledge of standard errors.
If we scale by the square root of sample size, we should get horizontal lines.
We now clearly see the trends.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{results }\OtherTok{\textless{}{-}} \FunctionTok{mutate}\NormalTok{( results, }\AttributeTok{scaleSE =}\NormalTok{ SE }\SpecialCharTok{*} \FunctionTok{sqrt}\NormalTok{(n) )}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{( results ) }\SpecialCharTok{+}  
  \FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{n, }\AttributeTok{y=}\NormalTok{scaleSE, }\AttributeTok{col=}\NormalTok{estimator) }\SpecialCharTok{+}
  \FunctionTok{facet\_grid}\NormalTok{( skew }\SpecialCharTok{\textasciitilde{}}\NormalTok{ df0 , }\AttributeTok{labeller =} \StringTok{"label\_both"}\NormalTok{ ) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{scale\_x\_log10}\NormalTok{( }\AttributeTok{breaks=}\NormalTok{ns )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{Designing-Simulations-in-R_files/figure-latex/unnamed-chunk-265-1} \end{center}

Overall, the scaled error of the mean is stable across the different distributions.
The trimmed mean is advantageous when the degrees of freedom are small (note the blue line is far below the red line on the left hand plot): we are cropping outliers that destabilize our estimate, which leads to great wins.
As the distribution grows more normal (i.e., as the degrees of freedom increases), this is no longer an advantage and the trimmed mean gets closer to the mean in terms of
performance.
We might think we should be penalized slightly by having dropped 10\% of our data, making the standard errors slightly larger: if this is the case, then it is not large as the MCSE swamps it (the red and blue line are basically overlapping).

The median is not able to take advantage of the nuances of the individual observations in the data because it is entirely determined by the middle value.
When outliers cause real concern, this cost is minimal. When outliers are not a concern, the median is just worse.

Overall, we see for precision, the trimmed mean seems an excellent choice: in the presence of
outliers it is far more stable than the mean, and when there are no outliers
the cost of using it is small.
Considering the lessons for how we analyze simulation results, our final figure nicely illustrates how
visual displays of simulation results can tell very clear stories.
Eschew complicated tables with lots of numbers.

\section{Bias-variance tradeoffs}\label{bias-variance-tradeoffs}

We just looked at the precision of our three estimators, but we did not take into account bias.
In our data generating processes, the median is not the same as the mean.
To see this more clearly, we can generate large datasets for each value of our simulation parameters, then compare the mean and median:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{source}\NormalTok{( }\StringTok{"case\_study\_code/trimmed\_mean\_simulation.R"}\NormalTok{ )}

\NormalTok{vals }\OtherTok{\textless{}{-}} \FunctionTok{expand\_grid}\NormalTok{( }
  \AttributeTok{df0 =} \FunctionTok{unique}\NormalTok{( results}\SpecialCharTok{$}\NormalTok{df0 ),}
  \AttributeTok{skew =} \FunctionTok{unique}\NormalTok{( results}\SpecialCharTok{$}\NormalTok{skew ) }
\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}
    \AttributeTok{n =} \FloatTok{1e6}
\NormalTok{  )}

\NormalTok{res }\OtherTok{\textless{}{-}}
\NormalTok{  vals }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}
    \AttributeTok{data =} \FunctionTok{pmap}\NormalTok{(., gen.data),}
    \FunctionTok{map\_df}\NormalTok{(data, }\SpecialCharTok{\textasciitilde{}} \FunctionTok{tibble}\NormalTok{(}\AttributeTok{mean =} \FunctionTok{mean}\NormalTok{(.x), }\AttributeTok{median =} \FunctionTok{median}\NormalTok{(.x), }\AttributeTok{sd =} \FunctionTok{sd}\NormalTok{(.x)))}
\NormalTok{  ) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{data)}

\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{( res, }\AttributeTok{digits=}\DecValTok{2}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{r|r|r|r|r|r}
\hline
df0 & skew & n & mean & median & sd\\
\hline
3 & 0 & 1000000 & 0 & 0.00 & 1.00\\
\hline
3 & 10 & 1000000 & 0 & -0.25 & 0.98\\
\hline
5 & 0 & 1000000 & 0 & 0.00 & 1.00\\
\hline
5 & 10 & 1000000 & 0 & -0.25 & 1.00\\
\hline
15 & 0 & 1000000 & 0 & 0.00 & 1.00\\
\hline
15 & 10 & 1000000 & 0 & -0.22 & 1.00\\
\hline
30 & 0 & 1000000 & 0 & 0.00 & 1.00\\
\hline
30 & 10 & 1000000 & 0 & -0.21 & 1.00\\
\hline
\end{tabular}

Our trimmed estimator now has a cost: trimming can cause bias.
The more extreme trimmed estimator, the median, will be systematically biased when the data-generating distribution is skewed.

The trimmed mean and median are \emph{biased} if we think of our goal as estimating the mean.
This is because these estimators tend to trim off extreme right-tail values, but there are no extreme left-tail values to trim.
However, if the trimmed estimators are much more stable than the mean estimator, we might still prefer to use them.

Before we just looked at the SE.
But we actually want to
know the standard error, bias, and overall error (RMSE).
Can we plot all these on a single plot? Yes we can!
To plot, we first gather the outcomes to make a long-form dataset of results:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{res2 }\OtherTok{\textless{}{-}} 
\NormalTok{  results }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{pivot\_longer}\NormalTok{( }
    \AttributeTok{cols =} \FunctionTok{c}\NormalTok{( RMSE, bias, SE ), }
    \AttributeTok{names\_to =} \StringTok{"Measure"}\NormalTok{,}
    \AttributeTok{values\_to =} \StringTok{"value"} 
\NormalTok{  ) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}
    \AttributeTok{Measure =} \FunctionTok{factor}\NormalTok{( Measure, }\AttributeTok{levels =} \FunctionTok{c}\NormalTok{(}\StringTok{"bias"}\NormalTok{,}\StringTok{"SE"}\NormalTok{,}\StringTok{"RMSE"}\NormalTok{))}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

And then we plot, making a facet for each outcome of interest.
We need to bundle because we have a lot of factors (one now being measure).
We first look at boxplots:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ns }\OtherTok{=} \FunctionTok{unique}\NormalTok{( res2}\SpecialCharTok{$}\NormalTok{n )}

\FunctionTok{ggplot}\NormalTok{( res2 ) }\SpecialCharTok{+}
  \FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\FunctionTok{as.factor}\NormalTok{(n), }\AttributeTok{y=}\NormalTok{value, }\AttributeTok{col=}\NormalTok{estimator ) }\SpecialCharTok{+}
  \FunctionTok{facet\_grid}\NormalTok{( skew }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Measure ) }\SpecialCharTok{+}
  \FunctionTok{geom\_hline}\NormalTok{( }\AttributeTok{yintercept=}\DecValTok{0}\NormalTok{, }\AttributeTok{col=}\StringTok{"darkgrey"}\NormalTok{ ) }\SpecialCharTok{+}
  \FunctionTok{geom\_boxplot}\NormalTok{( }\AttributeTok{position =} \StringTok{"dodge"}\NormalTok{ ) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{( }\AttributeTok{y=}\StringTok{""}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{Designing-Simulations-in-R_files/figure-latex/unnamed-chunk-268-1} \end{center}

We can also subset to look at how these trade-offs play out when \texttt{df0\ =\ 3}, where we are seeing the largest benefits from trimming in our prior analysis.
We can then use lines to show the trends, which is a bit clearer:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{( }\FunctionTok{filter}\NormalTok{( res2, df0 }\SpecialCharTok{==} \DecValTok{3}\NormalTok{ ), }
        \FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{n, }\AttributeTok{y=}\NormalTok{value, }\AttributeTok{col=}\NormalTok{estimator ) ) }\SpecialCharTok{+}
    \FunctionTok{facet\_grid}\NormalTok{( skew }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Measure ) }\SpecialCharTok{+}
    \FunctionTok{geom\_hline}\NormalTok{( }\AttributeTok{yintercept=}\DecValTok{0}\NormalTok{, }\AttributeTok{col=}\StringTok{"darkgrey"}\NormalTok{ ) }\SpecialCharTok{+}
    \FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
    \FunctionTok{scale\_x\_log10}\NormalTok{( }\AttributeTok{breaks=}\NormalTok{ns ) }\SpecialCharTok{+}
    \FunctionTok{labs}\NormalTok{( }\AttributeTok{y=}\StringTok{""}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{Designing-Simulations-in-R_files/figure-latex/unnamed-chunk-269-1} \end{center}

Or maybe we can aggregate without losing too much information:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{res2agg }\OtherTok{\textless{}{-}} \FunctionTok{group\_by}\NormalTok{( res2, n, estimator, skew, Measure ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{( }\AttributeTok{value =} \FunctionTok{mean}\NormalTok{(value), }\AttributeTok{.groups =} \StringTok{"drop"}\NormalTok{ )}

\FunctionTok{ggplot}\NormalTok{( }\FunctionTok{filter}\NormalTok{( res2, df0 }\SpecialCharTok{==} \DecValTok{3}\NormalTok{ ), }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{n, }\AttributeTok{y=}\NormalTok{value, }\AttributeTok{col=}\NormalTok{estimator ) ) }\SpecialCharTok{+}
    \FunctionTok{facet\_grid}\NormalTok{( skew }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Measure ) }\SpecialCharTok{+}
    \FunctionTok{geom\_hline}\NormalTok{( }\AttributeTok{yintercept=}\DecValTok{0}\NormalTok{, }\AttributeTok{col=}\StringTok{"darkgrey"}\NormalTok{ ) }\SpecialCharTok{+}
    \FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
    \FunctionTok{scale\_x\_log10}\NormalTok{( }\AttributeTok{breaks=}\NormalTok{ns ) }\SpecialCharTok{+}
    \FunctionTok{labs}\NormalTok{( }\AttributeTok{y=}\StringTok{""}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{Designing-Simulations-in-R_files/figure-latex/unnamed-chunk-270-1} \end{center}

In these \emph{Bias-SE-RMSE} plots, we see how different estimators have different biases and different
uncertainties.
The bias is negative for our trimmed estimators because we
are losing the big outliers above and so getting answers that are too low.

The RMSE captures the trade-off in terms of what estimator gives the lowest
overall \emph{error}. For this distribution, the mean wins as the sample size
increases because the bias basically stays the same and the SE drops. But
for smaller samples the trimming is superior. The median (essentially
trimming 50\% above and below) is overkill and has too much negative bias.

From a simulation study point of view, notice how we are looking at three
different qualities of our estimators. Some people really care about bias,
some care about RMSE. By presenting all results we are transparent about how
the different estimators operate.

Next steps would be to also examine the associated estimated standard errors
for the estimators, seeing if these estimates of estimator uncertainty are
good or poor. This leads to investigation of coverage rates and similar.

\chapter{Simulations as evidence}\label{simulations-as-evidence}

We began this book with an acknowledgement that simulation is fraught with the potential for misuse: \emph{simulations are doomed to succeed}.
We close this section by reiterating this point, and then discuss several ways researchers might design their simulations so they more arguably produce real evidence on how well things might work.

When our work is done, ideally we will have generated simulations that provide a sort of ``consumer product testing'' for the study designs and statistical methods we are exploring.
Clearly, it is important to do some consumer product tests that are grounded in how consumers will actually use the product in real life---but before you get to that point, it can be helpful to cover an array of conditions that are probably more extreme than what will be experienced in practice (like dropping things off buildings or driving over them with cars or whatever else).
For simulation, extreme levels of a factor make understanding the role they play more clear.
They can also help us understand the limits of our methods.
Extreme can go in both the good and bad directions.
Extreme contexts should also include best-case (or at least optimistic) scenarios for when our estimator of interest will excel, giving in effect an upper bound on how well things could go.

Extreme and clear simulations are also usually easier to write, manipulate, and understand.
Such simulations can help us learn about the estimators themselves.
For example, we can use simulation to uncover how different aspects of a data generating process affect the performance of an estimator.
To discover this, we would need a data generation process with clear levers controlling those aspects.
Simple simulations can be used to push theory further---we can experiment to gain an inclination of whether a model is a good idea at all, or to verify we are right about an intuition or derivation about how well an estimator can work.

But such simulations cannot be the end of our journey.
In general, a researcher should work to ensure their simulation evidence is \emph{relevant}.
A set of simulations only using unrealistic data generating processes may not be that useful.
Unless the estimators being tested have truly universally stable properties, we will not learn much from a simulation if it is not relevant to the problem at hand.
We have to circle back to providing testing of the sorts of situations an eventual user of our methods might encounter in practice.
So how can we make our simulations more relevant?

\section{Strategies for making relevant simulations}\label{strategies-for-making-relevant-simulations}

In the following subsections we go through a range of general strategies for making relevant simulations:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Break symmetries and regularities
\item
  Use extensive multi-factor simulations
\item
  Generate simulations based on prior literature.
\item
  Pick simulation factors based on real data
\item
  Resample real data directly to get authentic distributions
\item
  Design a fully calibrated simulation
\end{enumerate}

\subsection{Break symmetries and regularities}\label{break-symmetries-and-regularities}

In a series of famous causal inference papers \citep{lin2013agnostic, freedman2008regression}, researchers examined when linear regression adjustment of a randomized experiment (i.e., when controlling for baseline covariates in a randomized experiment) could cause problems.
Critically, if the treatment assignment is 50\%, then the concerns that these researchers examined do not come into play, as asymmetries between the two groups gets perfectly cancelled out.
That said, if the treatment proportion is more lopsided, then under some circumstances you can get bias, and you can get invalid standard errors, depending on other structures of the data.

Simulations can be used to explore these issues, but only if we break the symmetry of the 50\% treatment assignment.
When designing simulations, it is worth looking for places of symmetry, because in those contexts estimators will often work better than they might otherwise, and other factors may not have as much of an effect as anticipated.

Similarly, in recent work on best practices for analyzing multisite experiments \citep{miratrix2021applied}, we identified how different estimators could be targeting different estimands.
In particular, some estimators target site-average treatment effects, some target person-average treatment effects, and some target a kind of precision-weighted blend of the two.
To see this play out in practice, our simulations needed the sizes of sites to vary, and also the proportion of treated within site to vary.
If we had run simulations with equal site size and equal proportion treated, we would not see the broader behavior that separates the estimators considered.

Overall, it is important to make your data generation processes irregular.

\subsection{Make your simulation general with an extensive multi-factor experiment}\label{make-your-simulation-general-with-an-extensive-multi-factor-experiment}

``If a single simulation is not convincing, use more of them,'' is one principle a researcher might take.
By conducting extensive multifactor simulations, once can explore a large space of possible data generating scenarios.
If, across the full range of scenarios, a general story bears out, then perhaps that will be more convincing than a narrower range.

Of course, the critic will claim that some aspect that is not varying is the real culprit.
If this aspect is unrealistic, then the findings, across the board, may be less relevant.
Thus, pick the factors one varies with care.

\subsection{Use previously published simulations to beat them at their own game}\label{use-previously-published-simulations-to-beat-them-at-their-own-game}

If a relevant prior paper uses a simulation to make a case, one approach is to replicate that simulation, adding in the new estimator one wants to evaluate.
This makes it (more) clear that you are not fishing: you are using something established in the literature as a published benchmark.
By constraining oneself to published simulations, one has less wiggle room to cherry pick a data generating process that works the way you want.

\subsection{Calibrate simulation factors to real data}\label{calibrate-simulation-factors-to-real-data}

Use real data to inform choice of factor levels or other data generation features.
For example, in James's work on designing methods for meta-analysis, there is often a question of how big sample sizes should be and how many different outcomes per study there should be when simulating effect size estimates from hypothetical studies to be included in a hypothetical meta-analysis.
To make the simulations realistic, James obtained data from a set of past real meta-analyses and fit parametric models to these features, and used the resulting parameter estimates as benchmarks for how large and how varied the simulated meta analyses should be.

In this case, one would probably not use the exact estimated values, but instead use them as a point of reference and possibly explore a range of values around them.
For instance, say we find that the distribution of study sizes fits a Poisson(63) pretty well.
We might then then simulate study sizes using a Poisson with mean parameters of 40, 60, or 80 (where 40, 60, and 80 would be one factor in our multifactor experiment).

\subsection{Use real data to obtain directly}\label{use-real-data-to-obtain-directly}

You can also use real data directly to avoid a parametric model.
For example, say you need a population distribution for a slew of covariates.
Rather than using something artificial (like multivariate normal), you can pull population data on a bunch of covariates from some administrative data and then use those data as a (finite) population distribution.
You then simple sample (possibly with replacement) rows from your reference population to generate your simulationsample.

The appeal here is that your covariates will then have distributions that are much more authentic than some multivariate normal--they will include both continuous and categorical variables, they will tend to be skewed, and they will be correlated in different ways that are more interesting than anything someone could make up.

As an illustration of this approach, an old paper on heteroscedasticity-robust standard errors (Long and Irvin, 2000) does something similar.
It was important in the context that they're studying because the behavior of heteroscedasticity-robust SEs is influenced by leverage, which is a function of the covariate distribution.
Getting authentic leverage was hard to do with a parametric model, so getting examples of it from real life made the simulation more relevant.

\subsection{Fully calibrated simulations}\label{fully-calibrated-simulations}

Extending some of the prior ideas even further, one practice in increasing vogue is to generate \emph{calibrated simulations}.
These are simulations tailored to a specific applied contexts, where we design our simulation study to more narrowly inform what assumptions and structures are necessary in order to make progress in that specific context.

Often we would do this by building our simulations out of existing data.
For an example from above, one might sample, with replacement, from the covariate distribution of an actual dataset so that the distribution of covariates is authentic in how the covariates are distributed and, more importantly, how they co-relate.

But this is not far enough.
We also need to generate a realistic relationship between our covariates and outcome to truly assess how well the estimators work in practice.
It is very easy to accidentally put a very simple model in place for this final component, thus making a calibrated simulation quite naive in, perhaps, the very way that counts.

We next walk through how you might calibrate further in the context of evaluating estimators for some sort of causal inference context where we are assessing methods of estimating a treatment effect of some binary treatment.
If we just resample our covariates, but then layer a constant treatment effect on top, we may be missing critical aspects of how our estimators might fail in practice.

In the area of causal inference, the potential outcomes framework provides a natural path for generating calibrated simulations \citep{Kern2014calibrated}.
Also see \ref{potential-outcomes} for more discussion of simulations in the potential outcomes framework.
Under this framework, we would take an existing randomized experiment or observational study and then impute all the missing potential outcomes under some specific scheme.
This fully defines the sample of interest and thus any target parameters, such as a measure of heterogeneity, are then fully known.
For our simulation we then synthetically, and repeatedly, randomize and ``observe'\,' outcomes to be analyzed with the methods we are testing.
We could also resample from our dataset to generate datasets of different size, or to have a superpopulation target as our estimand.

The key feature here is the imputation step: how do we build our full set of covariates and outcomes?
One baseline method one can use is to generate a matched-pairs dataset by, for each unit, finding a close match given all the demographic and other covariate information of the sample. We then use the matched unit as the imputed potential outcome.\\
By doing this (with replacement) for all units we can generate a fully imputed dataset which we then use as our population, with all outcomes being ``real,'' as they are taken from actual data.
Such matching can preserve complex relationships in the data that are not model dependent.
In particular, if outcomes tend to be coarsely defined (e.g., on an integer scale) or have specific clumps (such as zero-inflation or rounding), this structure will be preserved.

One concern with this approach is the noise in the matching could in general dilute the structure of the treatment effect as the control- and treatment-side potential outcomes may be very unrelated, creating a lot of so-called idiosyncratic treatment variation (unit-to-unit variation in the treatment effects that is not explained by the covariates).
This is akin to measurement error diluting found relationships in linear models.
We could reduce such variation by first imputing missing outcomes using some model (e.g., a random forest) fit to the original data, and then matching on all units including the imputed potential outcome as a hidden ``covariate.''
This is not a data analysis strategy, but instead a method of generating synthetic data that both has a given structure of interest and also remains faithful to the idiosyncrasies of an actual dataset.

A second approach that allows for varying the level of a systematic effect is to specify a treatment effect model, predict treatment effects for all units and use those to impute the treatment potential outcome for all control units.
This will perfectly preserve the complex structure between the covariates and the \(Y_i(0)\)s.
Unfortunately, this would also give no idiosyncratic treatment variation .
To add in idiosyncratic variation we could then need to generate a distribution of perturbations and add these to the imputed outcomes just as an error term in a regression model.

Regardless of how we generate them, once we have a ``fully observed'' sample with the full set of treatment and control potential outcomes for all of our units, we can calculate any target estimands we like on our population, and then compare our estimators to these ground truths (even if they have no parametric analog) as desired.

Clearly, these calibration games can be fairly complex.
They do not lend themselves to a clear factor-based structure that have levers that change targeted aspects of the data generating process (although sometimes you can build in controls to make such levers possible).
In exchange, we end up with a simulation that might be more faithfully capturing aspects of a specific context, making our simulations more relevant to answering the narrower question of ``how will things work here?''

\part{Computational Considerations}\label{part-computational-considerations}

\chapter{Organizing a simulation project}\label{organizing-a-simulation-project}

As we saw earlier, a full multifactor simulation can generate a lot of output that can be hard to navigate.
They are complex, multifaceted projects by design.
This means that the project itself can be hard to manage.
It can be easy to become overwhelmed by a steady accumulation of scripts that generate data, analyze data, run simulations, and so on.

This section is designed to give you the computational skills and ideas that will make it easier to handle complex simulation projects.
We start with a discussion of file and project management, then turn to parallel processing, and finally close with some core programming habits that we have found useful for keeping on top of this type of sprawling complexity.

Simulations have two general phases: generate your results and analyze your results.
The ending of the first phase should be to save the generated results.
The beginning of the second phase should then be to load the results from a file and analyze them.
These phases can be in a separate `.R' files.
Dividing your simulations in this way allows for easily changing how one \emph{analyzes} an experiment without re-running the entire thing.

This is the simplest version of a general principle of a larger project: put code for different purposes in different files.

For example, at the minimum, for a complex multifactor simulation, you will likely have three general collections of code, not including the code to run the multifactor simulation itself:

\begin{itemize}
\tightlist
\item
  Code for generating data
\item
  Code for analyzing data
\item
  Code for running a single simulation scenario
\end{itemize}

If each of these pieces is large and complex, you might consider putting them in three different \texttt{.R} files.
To do this, we need to talk about what kinds of R scripts exist, and also talk about how you can tell one script to load another script.

\section{Well structured R scripts}\label{well-structured-r-scripts}

In R, there are two critical file types that can hold R code: \texttt{.R} files (scripts) and \texttt{.Rmd} (or \texttt{.qmd}) markdown files.
The former just holds R code, while the latter holds a mix of R code and text.
The latter is what you would render (or knit) to make a final report--it will run the code, and mix the results in with the text to give a nicely formatted report with code, figures, tables, and other printout nicely embedded.
These are what is typically used for smaller projects---all the code and discussion of code would be in a single, convienent place.

For larger projects, you will be more dependant on the first type of file, the \texttt{.R} script.
Ideally, the principle of modular programming would be applied to these files.
Each \texttt{.R} file would hold a collection of code that is related to a single task.
There are two main types of of \texttt{.R} script: those that \emph{just} have functions that can be used for other purposes (meaning that if you run them, the only thing that happens is you end up with some new functions in your current workspace), and those that are traditional scripts such that when you run them, R will do a variety of specified tasks.

\subsection{The source command}\label{about-source-command}

Inside of an R script you can ``source'' other \texttt{.R} files.
The \texttt{source()} command essentially ``cuts and pastes'' the contents of the given file into your R work session.
E.g.,

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{source}\NormalTok{( here}\SpecialCharTok{::}\FunctionTok{here}\NormalTok{( }\StringTok{"R/data\_generators.R"}\NormalTok{ ) )}
\FunctionTok{source}\NormalTok{( here}\SpecialCharTok{::}\FunctionTok{here}\NormalTok{( }\StringTok{"R/estimators.R"}\NormalTok{ ) )}
\FunctionTok{source}\NormalTok{( here}\SpecialCharTok{::}\FunctionTok{here}\NormalTok{( }\StringTok{"R/simulation\_support.R"}\NormalTok{ ) )}
\end{Highlighting}
\end{Shaded}

If the named file has code to run, it will run it.
If the named file has a list of methods, those methods will now be available for use.
The \texttt{here::here()} command is a convenience function that allows you to specify a file path relative to your R project root directory, so you can easily find your files.

You can even source files inside files that are sourced.
For example, \texttt{simulation\_support.R} could, inside it, source the other two files.
You would then only source the single simulation support file in your primary simulation script.

One reason for putting code in individual files is you can then have testing code in each of your files (in False blocks, see below), testing each of your components.
Then, when you are not focused on that component, you don't have to look at that testing code.

Another good reason for this type of modular organizing is you can then allow for a whole simulation universe, writing a variety of data generators that together form a library of options.
You can then easily create different simulations that use your different pieces, in your larger project.

For example, in one recent simulation project on estimators for an Instrumental Variable analysis, we had several different data generators for generating different types of compliance patterns (IVs are often used to handle noncompliance in randomized experiments).
Our \texttt{data\_generators.R} code file then had several methods.
When we sourced it, we end up wit the following list of methods:

\begin{verbatim}
> ls()
[1] "describe_sim_data"  "make_dat"           "make.dat_1side"     
[4] "make_dat_1side_old" "make_dat_orig"      "make_dat_simple"
[7] "make_dat_tuned"     "rand_exp"           "summarize_sim_data"
\end{verbatim}

The \texttt{describe()} and \texttt{summarize()} methods printed various statistics about a sample dataset; these are used to debug and understand how the generated data looks.
We also had a variety of different DGP methods because we had different versions that came up as we were trying to chase down errors in our estimators and understand strange behavior.

Putting the estimators in a different file also had a nice additional purpose: we also had an applied data example in our work, and we could simply source that single file and use those estimators on our actual data.
This ensured our simulation and applied analysis were perfectly aligned in terms of the estimators we were using.
Also, as we debugged our estimators and tweaked them, we immediately could re-run our applied analysis to update those results with minimal effort.

Modular programming is key.

\subsection{Putting headers in your .R file}\label{putting-headers-in-your-.r-file}

When you write a \texttt{.R} script, it is a good idea to put a header at the top of the file, giving a description of the file's purpose.
Then, you can also put dividers in your file, e.g.,

\begin{verbatim}
#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#
# Data generating functions ----
#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#
\end{verbatim}

Note the \texttt{-\/-\/-\/-} at the end of the middle line: if you add those trailing dashes, then if you click on the dropdown at the bottom of your RStudio, you will see a pop-up table of contents that allows you to quickly navigate to different parts of your source file.

\subsection{Storing testing code in your scripts}\label{about-keeping-tests-with-FALSE}

If you have an extended \texttt{.R} file with a list of functions, you might also want to store a lot of code that runs each function in turn, so you can easily remind yourself of what it does, or what the output looks like.
One way to keep this code around, but not have it run all the time when you run your script, is to put the code inside a ``FALSE block,'' that might look like so:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# My testing code {-}{-}{-}{-}}
\ControlFlowTok{if}\NormalTok{ ( }\ConstantTok{FALSE}\NormalTok{ ) \{}
\NormalTok{  res }\OtherTok{\textless{}{-}} \FunctionTok{my\_function}\NormalTok{( }\DecValTok{10}\NormalTok{, }\DecValTok{20}\NormalTok{, }\DecValTok{30}\NormalTok{ )}
\NormalTok{  res}
  \CommentTok{\# Some notes as to what I want to see.}
  
  \FunctionTok{sd}\NormalTok{( res )}
  \CommentTok{\# This should be around 20}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

You can then, when you open and look at the script, paste the code inside the block into the console when you want to run it.
If you source the script, however, it will not run at all, and thus your code will source faster and not print out any extraneous output.
This is a good way to keep your testing code with the code it is testing.
When you want to work on just the part of the project captured by your script, you can work inside the single file very easily, ignoring the other parts of your project.

You can also (or instead) write testing code, which we will talk about further below.

\section{Principled directory structures}\label{principled-directory-structures}

We \emph{strongly advocate} keeping a well-organized directory for your simulation.
We recommend using RStudio, or a similar IDE, and having a directory structure like the following:

\begin{verbatim}
my_project/
  proj.Rproj
  README.md
  R/
  data/
  results/
  scripts/
  test/
\end{verbatim}

For those who have written R pacakges, this structure will look familiar.
The \texttt{R/} directory is where you put your core R code.
We will have \texttt{.R} scripts that just hold our core methods; we can then ``load'' these scripts into our workspace as needed to gain access to those methods.
Saved methods might include the methods you have developed that you are planning on testing (unless they are in a separate package), and methods to generate data.
We \emph{do not} put the scripts that run the actual simulation here. The \texttt{R/} folder is to store the building blocks of our simulation only, not the final scripts that use those blocks to run and analyze our simulations themselves.

The \texttt{data/} directory is where you put any data files you are using in your simulation.
The \texttt{results/} directory is where you will save any generated results of your simulation.
Sometimes it might be worth having \texttt{raw\_results} and \texttt{results}, where \texttt{raw\_results} are what you save as soon as you finish running the simulation, and \texttt{results} have final results where you may have merged and summarized the raw results.

The \texttt{scripts/} directory is where you put your scripts that run the simulation and analyze simulation results.
You will likely have one script that runs the simulation, and one or more scripts that analyze the results.

Finally, the \texttt{test/} directory is where you put any testing code you have written to test your methods.
You can even use the \texttt{testthat} package to write unit tests for your methods, and put those in the \texttt{test/} directory.
This structure is not required, but it is a good idea to have a well-organized project of some type.

\section{Saving simulation results}\label{saving-files}

Always save your simulation results to a file.
Simulations are painful and time consuming to run, and you will invariably want to analyze the results of them in a variety of different ways, once you have looked at your preliminary analysis.
We advocate saving your simulation as soon as it is complete.
But there are some ways to do better than that, such as saving as you go.
This can protect you if your simulation occasionally crashes, or if you want to rerun only parts of your simulation for some reason.

\subsection{Saving simulations in general}\label{saving-simulations-in-general}

Once your simulation has completed, you can save it like so:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{dir.create}\NormalTok{(}\StringTok{"results"}\NormalTok{, }\AttributeTok{showWarnings =} \ConstantTok{FALSE}\NormalTok{ )}
\FunctionTok{write\_csv}\NormalTok{( res, }\StringTok{"results/simulation\_CRT.csv"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\texttt{write\_csv()} is a tidyverse file-writing command; see ``R for Data Science''
textbook, 11.5.

You can then load it, just before analysis, as so:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{res }\OtherTok{=} \FunctionTok{read\_csv}\NormalTok{( }\StringTok{"results/simulation\_CRT.csv"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

There are two general tools for saving. The \texttt{read/write\_csv} methods save your file in a way where you can open it with a spreadsheet program and look at it.
But your results should be in a vanilla, rectangular format (non-fancy data frame without list columns).

Alternatively, you can use the \texttt{saveRDS()} and \texttt{readRDS()} methods; these save objects to a file such that when you load them, they are as you left them.
The RDS saving keeps your R object as given.
The simpler format of a csv file means your factors, if you have them, may not preserve as factors, and so forth.

\subsection{Saving simulations as you go}\label{saving-simulations-as-you-go}

If you are not sure you have time to run your entire simulation, or you think your computer might crash half way through, or something similar, you can save each chunk you run as you go, in its own file. You then stack those files at the end to get your final results.
With clever design, you can even then selectively delete files to rerun only parts of your larger simulation---but be sure to rerun everything from scratch before you run off and publish your results, to avoid embarrassing errors.

Here, for example, is a script from a research project examining how one might use post-stratification to improve the precision of an IV estimate.
This is the script that runs the simulation.
Note the sourcing of other scripts that have all the relevant functions; these are not important here.
Due to modular programming, we can see what this script does, even without those detail.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{source}\NormalTok{( }\StringTok{"R/simulation\_functions.R"}\NormalTok{ )}

\ControlFlowTok{if}\NormalTok{ ( }\SpecialCharTok{!}\FunctionTok{file.exists}\NormalTok{(}\StringTok{"results/frags"}\NormalTok{ ) ) \{}
    \FunctionTok{dir.create}\NormalTok{(}\StringTok{"results/frags"}\NormalTok{)}
\NormalTok{\}}

\CommentTok{\# Number of simulation replicates per scenario}
\NormalTok{R }\OtherTok{=} \DecValTok{1000}

\CommentTok{\# Do simulation breaking up R into this many chunks}
\NormalTok{M\_CHUNK }\OtherTok{=} \DecValTok{10}

\DocumentationTok{\#\#\#\#\#\# Set up the multifactor simulation \#\#\#\#\#\#\#}

\CommentTok{\# chunkNo is a hack to make a bunch of smaller chunks for doing parallel more}
\CommentTok{\# efficiently.}
\NormalTok{factors }\OtherTok{=} \FunctionTok{expand\_grid}\NormalTok{( }\AttributeTok{chunkNo =} \DecValTok{1}\SpecialCharTok{:}\NormalTok{M\_CHUNK,}
                       \AttributeTok{N =} \FunctionTok{c}\NormalTok{( }\DecValTok{500}\NormalTok{, }\DecValTok{1000}\NormalTok{, }\DecValTok{2000}\NormalTok{ ),}
                       \AttributeTok{pi\_c =} \FunctionTok{c}\NormalTok{( }\FloatTok{0.05}\NormalTok{, }\FloatTok{0.075}\NormalTok{, }\FloatTok{0.10}\NormalTok{ ),}
                       \AttributeTok{nt\_shift =} \FunctionTok{c}\NormalTok{( }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{ ),}
                       \AttributeTok{pred\_comp =} \FunctionTok{c}\NormalTok{( }\StringTok{"yes"}\NormalTok{, }\StringTok{"no"}\NormalTok{ ),}
                       \AttributeTok{pred\_Y =} \FunctionTok{c}\NormalTok{( }\StringTok{"yes"}\NormalTok{, }\StringTok{"no"}\NormalTok{ ),}
                       \AttributeTok{het\_tx =} \FunctionTok{c}\NormalTok{( }\StringTok{"yes"}\NormalTok{, }\StringTok{"no"}\NormalTok{ ),}
                       \AttributeTok{sd0 =} \DecValTok{1}
\NormalTok{                       )}
\NormalTok{factors }\OtherTok{\textless{}{-}}\NormalTok{ factors }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{mutate}\NormalTok{(}
    \AttributeTok{reps =}\NormalTok{ R }\SpecialCharTok{/}\NormalTok{ M\_CHUNK,}
    \AttributeTok{seed =} \DecValTok{16200320} \SpecialCharTok{+} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{n}\NormalTok{()}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

This generates a data frame of all our factor combinations.
This is our list of ``tasks'' (each row of factors).
These tasks have repeats: the ``chunks'' means we do a portion of each scenario, as specified by our simulation factors, as a process.
This would allow for greater parallelization (e.g., if we had more cores), and also lets us save our work without finishing an entire scenario of, in this case, 1000 iterations.

To set up our simulation we make a little helper method to do one row.
With each row, once we have run it, we save it to disk.
This means if we kill our simulation half-way through, most of the work would be saved.
Our function is then going to either do the simulation (and save the result to disk immediately), or, if it can find the file with the results from a previous run, load those results from disk:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{safe\_run\_sim }\OtherTok{=} \FunctionTok{safely}\NormalTok{( run\_sim )}
\NormalTok{file\_saving\_sim }\OtherTok{=} \ControlFlowTok{function}\NormalTok{( chunkNo, seed, ... ) \{}
\NormalTok{    fname }\OtherTok{=} \FunctionTok{paste0}\NormalTok{( }\StringTok{"results/frags/fragment\_"}\NormalTok{, chunkNo, }\StringTok{"\_"}\NormalTok{, seed, }\StringTok{".rds"}\NormalTok{ )}
\NormalTok{    res }\OtherTok{=} \ConstantTok{NA}
    \ControlFlowTok{if}\NormalTok{ ( }\SpecialCharTok{!}\FunctionTok{file.exists}\NormalTok{(fname) ) \{}
\NormalTok{        res }\OtherTok{=} \FunctionTok{safe\_run\_sim}\NormalTok{( }\AttributeTok{chunkNo=}\NormalTok{chunkNo, }\AttributeTok{seed=}\NormalTok{seed, ... )}
        \FunctionTok{saveRDS}\NormalTok{(res, }\AttributeTok{file =}\NormalTok{ fname )}
\NormalTok{    \} }\ControlFlowTok{else}\NormalTok{ \{}
\NormalTok{        res }\OtherTok{=} \FunctionTok{readRDS}\NormalTok{( }\AttributeTok{file=}\NormalTok{fname )}
\NormalTok{    \}}
    \FunctionTok{return}\NormalTok{( res )}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Note how we wrap our core \texttt{run\_sim} method (that takes all our simulation factors and runs a simulation for those factors) in \texttt{safely}; \texttt{run\_sim()} was crashing very occasionally, and so to make the code more robust, we wrapped it so we could see any error messages.
Our method cleverly either loads a saved result, or generates it, for a given chunk.
This means from whatever is calling the function, it will look exactly the same whether it is loading a saved result or generating a new one.

We next run the simulation by calling \texttt{file\_saving\_sim()} for all of our simulation scenarios.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Shuffle the rows so we run in random order to load balance.}
\NormalTok{factors }\OtherTok{=} \FunctionTok{sample\_n}\NormalTok{(factors, }\FunctionTok{nrow}\NormalTok{(factors) )}

\ControlFlowTok{if}\NormalTok{ ( }\ConstantTok{TRUE}\NormalTok{ ) \{}
    \CommentTok{\# Run in parallel}
\NormalTok{    parallel}\SpecialCharTok{::}\FunctionTok{detectCores}\NormalTok{()}
    
    \FunctionTok{library}\NormalTok{(future)}
    \FunctionTok{library}\NormalTok{(furrr)}
    
    \CommentTok{\#plan(multiprocess) \# choose an appropriate plan from future package}
    \CommentTok{\#plan(multicore)}
    \FunctionTok{plan}\NormalTok{(multisession, }\AttributeTok{workers =}\NormalTok{ parallel}\SpecialCharTok{::}\FunctionTok{detectCores}\NormalTok{() }\SpecialCharTok{{-}} \DecValTok{2}\NormalTok{ )}
    
\NormalTok{    factors}\SpecialCharTok{$}\NormalTok{res }\OtherTok{\textless{}{-}} \FunctionTok{future\_pmap}\NormalTok{(factors, }\AttributeTok{.f =}\NormalTok{ file\_saving\_sim,}
                          \AttributeTok{.options =} \FunctionTok{furrr\_options}\NormalTok{(}\AttributeTok{seed =} \ConstantTok{NULL}\NormalTok{),}
                          \AttributeTok{.progress =} \ConstantTok{TRUE}\NormalTok{ )}
    
\NormalTok{\} }\ControlFlowTok{else}\NormalTok{ \{}
  \CommentTok{\# Run not in parallel, used for debugging}
\NormalTok{  factors}\SpecialCharTok{$}\NormalTok{res }\OtherTok{\textless{}{-}} \FunctionTok{pmap}\NormalTok{(factors, }\AttributeTok{.f =}\NormalTok{ file\_saving\_sim )}
\NormalTok{\}}

\NormalTok{tictoc}\SpecialCharTok{::}\FunctionTok{toc}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

Note how we shuffle the rows of our task list so that which process gets what task is randomized.
If some tasks are much longer (e.g., due to larger sample size) then this will get balanced out across our processes.
See \ref{parallel-processing} for more on parallel processing.

The \texttt{if-then} structure allows us to easily switch between parallel and nonparallel code.
This makes debugging easier: when running in parallel, stuff printed to the console does not show until the simulation is over.
Plus it would be all mixed up since multiple processes are working simultaneously.

The above overall structure allows the researcher to delete one of the ``fragment'' files from the disk, run the simulation code, and have it just do one tiny piece of the simulation.
This means the researcher can insert a \texttt{browser()} command somewhere inside the code, and debug the code, in the natural context of how the simulation is being run.

The seed setting ensures reproducibility.
Once we are done, we need to clean up our results:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sim\_results }\OtherTok{\textless{}{-}} 
\NormalTok{    factors }\SpecialCharTok{\%\textgreater{}\%} 
    \FunctionTok{unnest}\NormalTok{(}\AttributeTok{cols =}\NormalTok{ res)}

\CommentTok{\# Cut apart the results and error messages}
\NormalTok{sim\_results}\SpecialCharTok{$}\NormalTok{sr }\OtherTok{=} \FunctionTok{rep}\NormalTok{( }\FunctionTok{c}\NormalTok{(}\StringTok{"res"}\NormalTok{,}\StringTok{"err"}\NormalTok{), }\FunctionTok{nrow}\NormalTok{(sim\_results)}\SpecialCharTok{/}\DecValTok{2}\NormalTok{)}
\NormalTok{sim\_results }\OtherTok{=} \FunctionTok{pivot\_wider}\NormalTok{( sim\_results, }\AttributeTok{names\_from =}\NormalTok{ sr, }\AttributeTok{values\_from =}\NormalTok{ res )}

\FunctionTok{saveRDS}\NormalTok{( sim\_results, }\AttributeTok{file=}\StringTok{"results/simulation\_results.rds"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

Our final \texttt{simulation\_results.rds} file will have all the results from our simulation, made by stacking all of the fragments of our simulation together.

\subsection{Dynamically making directories}\label{dynamically-making-directories}

If you are generating a lot of files, then you should put them somewhere.
But where?
It can be nice to dynamically generate a directory for your files on fly.
One way to do this is to write a function that will make any needed directory, if it doesn't exist, and then put your file in that spot.
For example, you might have your own version of \texttt{write\_csv} as:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_write\_csv }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{( data, path, file ) \{}
  
  \ControlFlowTok{if}\NormalTok{ ( }\SpecialCharTok{!}\FunctionTok{dir.exists}\NormalTok{( here}\SpecialCharTok{::}\FunctionTok{here}\NormalTok{( path ) ) ) \{}
    \FunctionTok{dir.create}\NormalTok{( here}\SpecialCharTok{::}\FunctionTok{here}\NormalTok{( path ), }\AttributeTok{recursive=}\ConstantTok{TRUE}\NormalTok{ ) }
\NormalTok{  \}}
  \FunctionTok{write\_csv}\NormalTok{( data, }\FunctionTok{paste0}\NormalTok{( path, file ) )}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

This will look for a path (starting from your R Project, by taking advantage of the \texttt{here} package), and put your data file in that spot.
If the spot doesn't exist, it will make it for you.

\subsection{Loading and combining files of simulation results}\label{loading-and-combining-files-of-simulation-results}

Once your simulation files are all generated, the following code will stack them all into a giant set of results, assuming all the files are themselves data frames stored in RDS objects.
This function will try and stack all files found in a given directory; for it to work, you should ensure there are no other files stored there.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{load.all.sims }\OtherTok{=} \ControlFlowTok{function}\NormalTok{( }\AttributeTok{filehead=}\StringTok{"raw\_results/"}\NormalTok{ ) \{}
  
\NormalTok{  files }\OtherTok{=} \FunctionTok{list.files}\NormalTok{( filehead, }\AttributeTok{full.names=}\ConstantTok{TRUE}\NormalTok{)}
  
\NormalTok{  res }\OtherTok{=} \FunctionTok{map\_df}\NormalTok{( files, }\ControlFlowTok{function}\NormalTok{( fname ) \{}
    \FunctionTok{cat}\NormalTok{( }\StringTok{"Reading results from "}\NormalTok{, fname, }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{ )}
\NormalTok{    rs }\OtherTok{=} \FunctionTok{readRDS}\NormalTok{( }\AttributeTok{file =}\NormalTok{ fname )}
\NormalTok{    rs}\SpecialCharTok{$}\NormalTok{filename }\OtherTok{=}\NormalTok{ fname}
\NormalTok{    rs}
\NormalTok{  \})}
\NormalTok{  res}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

You would use as so:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{results }\OtherTok{\textless{}{-}} \FunctionTok{load.all.sims}\NormalTok{( }\AttributeTok{filehead=}\StringTok{"raw\_results/"}\NormalTok{ )}
\NormalTok{results }\OtherTok{\textless{}{-}} \FunctionTok{bind\_rows}\NormalTok{( results )}
\end{Highlighting}
\end{Shaded}

\chapter{Parallel Processing}\label{parallel-processing}

Especially if you take our advice of ``when in doubt, go more general'' and if you calculate monte carlo standard errors, you will quickly come up against the limits of your computer.
Simultions can be incredibly computationally intensive, and there are a few means for dealing with that.
The first, touched on at times throughout the book, is to optimize ones code by looking for ways to remove extraneous calculation (e.g., by writing ones own methods rather than using the safety-checking and thus sometimes slower methods in R, or by saving calculations that are shared across different estimation approaches).
The second is to use more computing power.
This latter approach is the topic of this chapter.

There are two general ways to do parallel calculation.
The first is to take advantage of the fact that most modern computers have multiple cores (i.e., computers) built in.
With this approach, we tell R to use more of the processing power of your desktop or laptop.
If your computer has eight cores, you can easily get a near eight-fold increase in the speed of your simulation.

The second is to use cloud computing, or compute on a cluster.
A computing cluster is a network of hundreds or thousands of computers, coupled with commands where you break apart a simulation into pieces and send the pieces to your army of computers.
Conceptually, this is the same as when you do baby parallel on your desktop: more cores equals more simulations per minute and thus faster simulation overall.
But the interface to a cluster can be a bit tricky, and very cluster-dependent.

But once you get it up and running, it can be a very powerful tool.
First, it takes the computing off your computer entirely, making it easier to set up a job to run for days or weeks without making your day to day life any more difficult.
Second, it gives you hundreds of cores, potentially, which means a speed-up of hundreds rather than four or eight.

Simulations are a very natural choice for parallel computation.
With a multifactor experiment it is very easy to break apart the overall into pieces.
For example, you might send each factor combination to a single machine.
Even without multi factor experiments, due to the cycle of ``generate data, then analyze,'' it is easy to have a bunch of computers doing the same thing, with a final collection step where all the individual iterations are combined into one at the end.

\section{Parallel on your computer}\label{parallel-on-your-computer}

Most modern computers have multiple cores, so you can run a parallel simulation right in the privacy of your own home!

To assess how many cores you have on your computer, you can use the \texttt{detectCores()} method in the \texttt{parallel} package:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{parallel}\SpecialCharTok{::}\FunctionTok{detectCores}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 4
\end{verbatim}

Normally, unless you tell it to do otherwise, \textbf{\emph{R only uses one core}}.
This is obviously a bit lazy on R's part.
But it is easy to take advantage of multiple cores using the \texttt{future} and \texttt{furrr} packages.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(future)}
\FunctionTok{library}\NormalTok{(furrr)}
\end{Highlighting}
\end{Shaded}

In particular, the \texttt{furrr} package replicates our \texttt{map} functions, but in parallel.
We first tell our R session what kind of parallel processing we want using the \texttt{future} package.
In general, using \texttt{plan(multisession)} is the cleanest: it will start one entire R session per core, and have each session do work for you.
The alternative, \texttt{multicore} does not seem to work well with Windows machines, nor with RStudio in general.

The call is simple:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plan}\NormalTok{(multisession, }\AttributeTok{workers =}\NormalTok{ parallel}\SpecialCharTok{::}\FunctionTok{detectCores}\NormalTok{() }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

The \texttt{workers} parameter specifies how many of your cores you want to use.
Using all but one will let your computer still operate mostly normally for checking email and so forth.
You are carving out a bit of space for your own adventures.

Once you set up your plan, you use \texttt{future\_pmap()}; it works just like \texttt{pmap()} but evaluates across all available workers specified in the plan call.
Here we are running a parallel version of the multifactor experiment discussed in Chapter @ref(exp\_design) (see chapter @ref(case\_Cronback) for the simulation itself).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tictoc}\SpecialCharTok{::}\FunctionTok{tic}\NormalTok{()}
\NormalTok{params}\SpecialCharTok{$}\NormalTok{res }\OtherTok{=} \FunctionTok{future\_pmap}\NormalTok{(params,}
                         \AttributeTok{.f =}\NormalTok{ run\_alpha\_sim,}
                         \AttributeTok{.options =} \FunctionTok{furrr\_options}\NormalTok{(}\AttributeTok{seed =} \ConstantTok{NULL}\NormalTok{))}
\NormalTok{tictoc}\SpecialCharTok{::}\FunctionTok{tic}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

Note the \texttt{.options\ =\ furrr\_options(seed\ =\ NULL)} part of the argument.
This is to silence some warnings.
Given how tasks are handed out, R will get upset if you don't do some handholding regarding how it should set seeds for pseudoranom number generation.
In particular, if you don't set the seed, the multiple sessions could end up having the same starting seed and thus run the exact same simulations (in principle).
We have seen before how to set specific seed for each simulation scenario, but \texttt{furrr} doesn't know we have done this.
This is why the extra argument about seeds: it is being explicit that we are handling seed setting on our own.

We can compare the running time to running in serial (i.e.~using only one worker):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tictoc}\SpecialCharTok{::}\FunctionTok{tic}\NormalTok{()}
\NormalTok{params}\SpecialCharTok{$}\NormalTok{res2 }\OtherTok{=}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(params, n}\SpecialCharTok{:}\NormalTok{seed) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{pmap}\NormalTok{(}\AttributeTok{.f =}\NormalTok{ run\_alpha\_sim)}
\NormalTok{tictoc}\SpecialCharTok{::}\FunctionTok{tic}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

(The \texttt{select} command is to drop the \texttt{res} column from the parallel run; it would otherwise be passed as as parameter to \texttt{run\_alpha\_sim} which would in turn cause an error due to the unrecognized parameter.)

\section{Parallel on a virtual machine}\label{parallel-on-a-virtual-machine}

Your laptop probably has around 8 cores, meaning you can have an 8 fold speed-up.
But wouldn't it be nice to have a computer with 50 cores? Or even more?
You can get one!

Cloud services, such as Amazon Web Services (AWS), can give you a \emph{virtual machine} that can have many cores (where many is usually 50 or so).
Some of these services give you what is effectively an R Studio session, and so you can run your scripts and everything just like we have discussed above, with no change.

The Data Colada blog advocates for an alternative to AWS, which is {[}Kamatera{]} (\url{https://www.kamatera.com}).
The Data Colada folks say ``it is much easier to use, way faster to set up, and flexible (e.g., you can easily update R and R Studio in it).''
See \href{https://datacolada.org/102}{Data Colada's post} for more information.

This kind of cloud computing is relatively straightforward, once you understand parallel on your own computer.
But you can go further, where you dispatch a very large number of computers on your task.
We discuss this last option next.

\section{Parallel on a cluster}\label{parallel-on-a-cluster}

In general, a ``cluster'' is a system of computers that are connected up to form a large distributed network that many different people can use to do large computational tasks (e.g., simulations!).
These clusters will have some overlaying coordinating programs that you, the user, will interact with to set up a ``job,'' or set of jobs, which is a set of tasks you want some number of the computers on the cluster to do for you in tandum.

These coordinating programs will differ, depending on what cluster you are using, but have some similarities that bear mention.
For running simulations, you only need the smallest amount of knowledge about how to engage with these systems because you don't need all the individual computers working on your project communicating with each other (which is the hard part of distributed computing, in general).

\subsection{What is a command-line interface?}\label{what-is-a-command-line-interface}

In the good ol' days, when things were simpler, yet more difficult, you would interact with your computer via a ``command-line interface.''
The easiest way to think about this is as an R console, but in a different language that the entire computer speaks.
A command line interface is designed to do things like find files with a specific name, or copy entire directories, or, importantly, start different programs.
Another place you may have used a command line inteface is when working with Git: anything fancy with Git is often done via command-line.
People will talk about a ``shell'' (a generic term for this computer interface) or ``bash'' or ``csh.''
You can get access to a shell from within RStudio by clicking on the ``Terminal'' tab.
Try it, if you've never done anything like this before, and type

\begin{verbatim}
ls
\end{verbatim}

It should list some file names.
Note this command does \emph{not} have the parenthesis after the command, like in R or most other programming languages.
The syntax of a shell is usually mystifying and brutal: it is best to just steal scripts from the internet and try not to think about it too much, unless you want to think about it a lot.

Importantly for us, from the command line interface you can start an R program, telling it to start up and run a script for you.
This way of running R is noninteractive: you say ``go do this thing,'' and R starts up, goes and does it, and then quits.
Any output R generates on the way will be saved in a file, and any files you save along the way will also be at your disposal once R has completed.

To see this in action make the following script in a file called ``dumb\_job.R'':

\begin{verbatim}
library( tidyverse )
cat( "Making numbers\n" )
Sys.sleep(30)
cat( "Now I'm ready\n" )
dat = tibble( A = rnorm( 1000 ), B = runif( 1000 ) * A )
write_csv( dat, file="sim_results.csv" )
Sys.sleep(30)
cat( "Finished\n" )
\end{verbatim}

Then open the terminal and type (the ``\textgreater{}'' is not part of what you type):

\begin{verbatim}
> ls
\end{verbatim}

Do you see your \texttt{dumb\_job.R} file? If not, your terminal session is in the wrong directory.
In your computer system, files are stored in a directory structure, and when you open a terminal, you are somewhere in that structure.

To find out where, you can type

\begin{verbatim}
> pwd
\end{verbatim}

for ``Print Working Directory''.
Save your dumb job file to wherever the above says.
You can also change directories using \texttt{cd}, e.g., \texttt{cd\ \textasciitilde{}/Desktop/temp} means ``change directory to the temp folder inside Desktop inside my home directory'' (the \texttt{\textasciitilde{}} is shorthand for home directory).
One more useful commands is \texttt{cd\ ..} (go up to the parent directory).

Once you are in the directory with your file, type:

\begin{verbatim}
> R CMD BATCH dumb_job.R R_output.txt --no-save
\end{verbatim}

The above command says ``Run R'' (the first part) in batch mode (the ``CMD BATCH'' part), meaning source the \texttt{dumb\_job.R} script as soon as R starts, saving all console output in the file \texttt{R\_output.txt} (it will be saved in the current directory where you run the program), and where you don't save the workspace when finished.

This command should take about a minute to complete, because our script sleeps a lot (the sleep represents your script doing a lot of work, like a real simulation would do).
Once the command completes (you will see your ``\textgreater{}'' prompt come back), verify that you have the \texttt{R\_output.txt} and the data file \texttt{sim\_results.csv} by typing \texttt{ls}.
If you open up your Finder or Microsoft equivilent, you can actually see the \texttt{R\_output.txt} file appear half-way through, while your job is running.
If you open it, you will see the usual header of R telling you what it loading, the ``Making numbers'' comment, and so forth.
R is saving everything as it works through your script.

Running R in this fashion is the key element to a basic way of setting up a massive job on the cluster: you will have a bunch of R programs all ``going and doing something'' on different computers in the cluster.
They will all save their results to files (they will have files of different names, or you will not be happy with the end result) and then you will gather these files together to get your final set of results.

\emph{Small Exercise:} Try putting an error in your \texttt{dumb\_job.R} script. What happens when you run it in batch mode?

\subsection{Running a job on a cluster}\label{running-a-job-on-a-cluster}

In the above, you can run a command on the command-line, and the command line interfact will pause while it runs.
As you saw, when you hit return with the above R command, the program just sat there for a minute before you got your command-line prompt back, due to the sleep.

When you properlly run a big job (program) on a cluster, it doesn't quite work that way.
You will instead set a program to run, but tell the cluster to run it somewhere else (people might say ``run in the background'').
This is good because you get your command-line prompt back, and can do other things, while the program runs in the background.

There are various methods for doing this, but they usually boil down to a request from you to some sort of managerial process that takes requests and assigns some computer, somewhere, to do them.
(Imagine a dispatcher at a taxi company. You call up, ask for a ride, and it sends you a taxi to do it. The dispatcher is just fielding requests, assinging them to taxis.)

For example, one dispatcher is the slurm (which may or may not be on the cluster you are attempting to use; this is where a lot of this information gets very cluster-specific).

You first set up a script that describes the job to be run.
It is like a work request.
This would be a plain text file, such as this example (\texttt{sbatch\_runScript.txt}):

\begin{verbatim}
#!/bin/bash
#SBATCH -n 32                                                   # Number of cores requested
#SBATCH -N 1                                                      # Ensure that all cores are on one machine
#SBATCH -t 480                                                  # Runtime in minutes
#SBATCH -p stats                                                # Partition to submit to
#SBATCH --mem-per-cpu=1000                    # Memory per cpu in MB
#SBATCH --open-mode=append                      # Append to output file, don't truncate
#SBATCH -o /output/directory/out/%j.out # Standard out goes to this file
#SBATCH -e /output/directory/out/%j.err # Standard err goes to this file
#SBATCH --mail-type=ALL                         # Type of email notification- BEGIN,END,FAIL,ALL
#SBATCH --mail-user=email@gmail.com         # Email address

# You might have some special loading of modules in the computing environment
source new-modules.sh
module load gcc/7.1.0-fasrc01
module load R
export R_LIBS_USER=$HOME/apps/R:$R_LIBS_USER

#R file to run, and txt files to produce for output and errors
R CMD BATCH estimator_performance_simulation.R logs/R_output_${INDEX_VAR}.txt --no-save --no-restore
\end{verbatim}

This file starts with a bunch of variables that describe how sbatch should handle the request.
It then has a series of commands that get the computer environment ready.
Finally, it has the \texttt{R\ CMD\ BATCH} command that does the work you want.

These scripts can be quite confusing to understand.
There are so many options!
What do these things even do?
The answer is, for researchers early on their journey to do this kind of work, ``Who knows?''
The general rule is to find an example file for the system you are working on that works, and then modify it for your own purposes.

Once you have such a file, you could run it on the command line, like this:

\begin{verbatim}
sbatch -o stdout.txt \
        --job-name=my_script \
        sbatch_runScript.txt
\end{verbatim}

You do this, and it will \emph{not} sit there and wait for the job to be done.
The \texttt{sbatch} command will instead send the job off to some computer which will do the work in parallel.

Interestingly, your R script could, at this point, do the ``one computer'' parallel type code listed above.
Note the script above has 32 cores; your single job could then have 32 cores all working away on their individual pieces of the simulation, as before (e.g., with \texttt{future\_pmap}).
You would have a 32-fold speedup, in this case.

This is the core element to having your simulation run on a cluster.
The next step is to do this \emph{a lot}, sending off a bunch of these jobs to different computers.

Some final tips

\begin{itemize}
\item
  Remember to save a workspace or RDS!! Once you tell Odyssey to run an R file, it, well, runs the R file. But, you probably want information after it's done - like an R object or even an R workspace. For any R file you want to run on Odyssey, remember at the end of the R file to put a command to save something after everything else is done. If you want to save a bunch of R objects, an R workspace might be a good way to go, but those files can be huge. A lot of times I find myself wanting only one or two R objects, and RDS files are a lot smaller.
\item
  Moving files from a cluster to your computer. You will need to first upload your files and code to the cluster, and then, once you've saved your workspace/RDS, you need those back on your computer. Using a scp client such as FileZilla is an easy way to do this file-transfer stuff. You can also use a Git repo for the code, but checking in the simulation results is not generally advisable: they are big, and not really in the spirit of a verson control system. Download your simulation results outside of Git, and keep your code in Git, is a good rule of thumb.
\end{itemize}

\subsection{Checking on a job}\label{checking-on-a-job}

Once your job is working on the cluster, it will keep at it until it finishes (or crashes, or is terminated for taking up too much memory or time).
As it chugs away, there will be different ways to check on it.
For example, you can, from the console, list the jobs you have running to see what is happening:

\begin{verbatim}
sacct -u lmiratrix
\end{verbatim}

except, of course, ``\texttt{lmiratrix}'' would be changed to whatever your username is.
This will list if your file is running, pending, timed out, etc. If it's pending, that usually means that someone else is hogging up space on the cluster and your job request is in a queue waiting to be assigned.

The \texttt{sacct} command is customizable, e.g.,

\begin{verbatim}
sacct -u lmiratrix --format=JobID,JobName%30,State
\end{verbatim}

will not truncate your job names, so you can find them more easily.

You can check on a specific job, if you know the ID:

\begin{verbatim}
squeue -j JOBID
\end{verbatim}

Something that's fun is you can check who's running files on the stats server by typing:

\begin{verbatim}
showq-slurm -p stats -o
\end{verbatim}

You can also look at the log files

\begin{verbatim}
tail my_log_file.log
\end{verbatim}

to see if it is logging information as it is working.

The email arguments, above, cause the system to email you before and after the job is complete.
The email notifications you can choose are \texttt{BEGIN}, \texttt{END}, \texttt{FAIL}, and \texttt{ALL}; \texttt{ALL} is generally good. What is a few more emails?

\subsection{Running lots of jobs on a cluster}\label{running-lots-of-jobs-on-a-cluster}

We have seen how to fire off a job (possibly a big job) that can run over days or weeks to give you your results.
There is one more piece that can allow you to use even more computing resources to do things even faster, which is to do a whole bunch of job requests like the above, all at once.
This multiple dispatching of sbatch commands is the final component for large simulations on a cluster: you are setting in motion a bunch of processes, each set to a specific task.

Asking for multiple, smaller, jobs is also nicer for the cluster than having one giant job that goes on for a long time.
By dividing a job into smaller pieces, and asking the scheduler to schedule those pieces, you can let the scheduler share and allocate resources between you and others more fairly.
It can make a list of your jobs, and farm them out as it has space.
This might go faster for you; with a really big job, the scheduler can't even allocate it until the needed number of workers is available.
With smaller jobs, you can take a lot of little spaces to get your work done.
Especially since simulation is so independent (just doing the same thing over and over) there is rarely any need for one giant process that has to do everything.

To make multiple, related, requests, we create a for-loop in the Terminal to make a whole series sbatch requests.
Then, each sbatch request will do one part of the overall simulation.
We can write this program in the shell, just like you can write R scripts in R.
A shell scripts does a bunch of shell commands for you, and can even have variables and loops and all of that fun stuff.

For example, the following \texttt{run\_full\_simulation.sh} is a script that fires off a bunch of jobs for a simulation.
Note that it makes a variable \texttt{INDEX\_VAR}, and sets up a loop so it can run 500 tasks indexed 1 through 500.

The first \texttt{export} line adds a collection of R libraries to the path stored in \texttt{R\_LIBS\_USER} (a ``path'' is a list of places where R will look for libraries).
The next line sets up a for loop: it will run the indented code once for each number from 1 to 500.
The script also specifies where to put log files and names each job with the index so you can know who is generating what file.

\begin{verbatim}
export R_LIBS_USER=$HOME/apps/R:$R_LIBS_USER

for INDEX_VAR in $(seq 1 500); do

  #print out indexes
  echo "${INDEX_VAR}"

  #give indexes to R so it can find them.
  export INDEX_VAR 

  #Run R script, and produce output files
  sbatch -o logs/sbout_p${INDEX_VAR}.stdout.txt \
        --job-name=runScr_p${INDEX_VAR} \
        sbatch_runScript.txt
  
  sleep 1 # pause to be kind to the scheduler

done
\end{verbatim}

One question is then how do the different processes know what part of the simulation they should be working on?
E.g., each worker needs to have its own seed so it don't do exactly the same simulation as a different worker!
The workers also need their own filenames so they save things in their own files.
The key is the \texttt{export\ INDEX\_VAR} line: this puts a variable in the environment that will be set to a specific number.
Inside your R script, you can get that index like so:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{index }\OtherTok{\textless{}{-}} \FunctionTok{as.numeric}\NormalTok{(}\FunctionTok{as.character}\NormalTok{(}\FunctionTok{Sys.getenv}\NormalTok{(}\StringTok{"INDEX\_VAR"}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

You can then use the index to make unique filenames when you save your results, so each process has its own filename:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{filename }\OtherTok{=} \FunctionTok{paste0}\NormalTok{( }\StringTok{"raw\_results/simulation\_results\_"}\NormalTok{, index, \_}\StringTok{".rds"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

You can also modify your seed such as with:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{factors }\OtherTok{=} \FunctionTok{mutate}\NormalTok{( factors,}
                  \AttributeTok{seed =} \FunctionTok{set.seed}\NormalTok{( }\DecValTok{1000} \SpecialCharTok{*}\NormalTok{ seed }\SpecialCharTok{+}\NormalTok{ index ) )}
\end{Highlighting}
\end{Shaded}

Now even if you have a series of seeds within the simulation script (as we have seen before), each script will have unique seeds not shared by any other script (assuming you have fewer than 1000 separate job requests).

This still doesn't exactly answer how to have each worker know what to work on.
Conider the case of our multifactor experiment, where we have a large combination of simulation trials we want to run.

There are two approaches one might use here.
One simple approach is the following: we first generate all the factors with \texttt{expand\_grid()} as usual, and then we take the row of this grid that corresponds to our index.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sim\_factors }\OtherTok{=} \FunctionTok{expand\_grid}\NormalTok{( ... )}
\NormalTok{index }\OtherTok{\textless{}{-}} \FunctionTok{as.numeric}\NormalTok{(}\FunctionTok{as.character}\NormalTok{(}\FunctionTok{Sys.getenv}\NormalTok{(}\StringTok{"INDEX\_VAR"}\NormalTok{)))}
\NormalTok{filename }\OtherTok{=} \FunctionTok{paste0}\NormalTok{( }\StringTok{"raw\_results/simulation\_results\_"}\NormalTok{, index, \_}\StringTok{".rds"}\NormalTok{ )}

\FunctionTok{stopifnot}\NormalTok{( index }\SpecialCharTok{\textgreater{}=} \DecValTok{1} \SpecialCharTok{\&\&}\NormalTok{ index }\SpecialCharTok{\textless{}=} \FunctionTok{nrow}\NormalTok{(sim\_factors ) )}
\FunctionTok{do.call}\NormalTok{( my\_sim\_function, sim\_factors[ index, ] )}
\end{Highlighting}
\end{Shaded}

The \texttt{do.call()} command runs the simulation function, passing all the arguments listed in the targeted row.
You then need to make sure you have your shell call the right number of workers to run your entire simulation.

One problem with this approach is some simulations might be a lot more work than others: consider your simulation with a huge sample size vs.~one with a small sample size.
Instead, you can have each worker run a small number of simulations of each scenario, and then stack your results later.
E.g.,

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sim\_factors }\OtherTok{=} \FunctionTok{expand\_grid}\NormalTok{( ... )}
\NormalTok{index }\OtherTok{\textless{}{-}} \FunctionTok{as.numeric}\NormalTok{(}\FunctionTok{as.character}\NormalTok{(}\FunctionTok{Sys.getenv}\NormalTok{(}\StringTok{"INDEX\_VAR"}\NormalTok{)))}
\NormalTok{sim\_factors}\SpecialCharTok{$}\NormalTok{seed }\OtherTok{=} \DecValTok{1000000} \SpecialCharTok{*}\NormalTok{ index }\SpecialCharTok{+} \DecValTok{17} \SpecialCharTok{*} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{nrow}\NormalTok{(sim\_factors)}
\end{Highlighting}
\end{Shaded}

and then do your usual \texttt{pmap} call with \texttt{R\ =\ 10} (or some other small number of replicates.)

For saving files and then loading and combining them for analysis, see Section \ref{saving-files}.

\subsection{Resources for Harvard's Odyssey}\label{resources-for-harvards-odyssey}

The above guidiance is tailored for Harvard' computing environment, primarily.
For that environment in particular, there are many additional resources such as:

\begin{itemize}
\tightlist
\item
  Odyssey Guide: \url{https://rc.fas.harvard.edu/resources/odyssey-quickstart-guide/}
\item
  R on Odyssey: \url{https://rc.fas.harvard.edu/resources/documentation/software/r/}
\end{itemize}

For installing R packages so they are seen by the scripts run by sbatch, see (\url{https://www.rc.fas.harvard.edu/resources/documentation/software-on-odyssey/r/})

Other clusters should have similar documents giving needed guidance for their specific contexts.

\subsection{Acknowledgements}\label{acknowledgements-1}

Some of the above material is based on tutorials built by Kristen Hunter and Zach Branson, past doctoral students of Harvard's statistics department.

\chapter{Debugging and Testing}\label{debugging-and-testing}

Writing code is not too hard.
Writing \emph{correct} code, however, can be quite challenging.

It is often the case that you will write code that does not do what you expect, or that does not work at all.
Trying to figure out why can be enormously frustrating and time consuming.
There are some tools, however, that can mitigate that to some extent.

\section{\texorpdfstring{Debugging with \texttt{print()}}{Debugging with print()}}\label{about-print}

When you follow modular design, you will often have methods you wrote calling other methods you wrote, which call even more methods you wrote.
When you get an error, you might not be sure where to look, or what is happening.

A simple method (a method often reviled by professional programmers, but which is still useful for more ordinary folks) for debugging is to use \texttt{print()} statements in your code.
For example, consider the following code:

\begin{Shaded}
\begin{Highlighting}[]
    \ControlFlowTok{if}\NormalTok{ ( }\FunctionTok{any}\NormalTok{( }\FunctionTok{is.na}\NormalTok{( rs}\SpecialCharTok{$}\NormalTok{estimate ) ) ) \{}
        \FunctionTok{cat}\NormalTok{( }\StringTok{"There are NAs in the estimates!}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{ )}
\NormalTok{    \}}
\end{Highlighting}
\end{Shaded}

Here we are printing something out that we suspect might be something we want to check.

There are a few methods for printing to the console: \texttt{print()}, which takes any object and prints it out. You can print a dataframe, variable, or a string:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{print}\NormalTok{( }\StringTok{"My var is "}\NormalTok{, my\_var, }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{ )}
\FunctionTok{print}\NormalTok{( my\_tibble )}
\end{Highlighting}
\end{Shaded}

You can use \texttt{cat()}, which is designed to print strings:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{( }\StringTok{"My var is "}\NormalTok{, my\_var, }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

You can use the \texttt{cli} package, which gives a bit of a nicer printout, and allows for easier formatting:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cli}\SpecialCharTok{::}\FunctionTok{cli\_alert}\NormalTok{(}\StringTok{"My var is \{my\_var\}"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The problem with printing is it is easy to have a lot of printout statements, and then when you run your code you get a wall of text.
For simulations, it is easy to print so much that it will meaningfully slow your simulation down!
You can use \texttt{print()} statements to help you figure out what is going on, but it is often better to use a more interactive debugging tool, such as the \texttt{browser()} function or \texttt{stopifnot()} statements, which we will discuss next.

\section{\texorpdfstring{Debugging with \texttt{browser()}}{Debugging with browser()}}\label{about-browser-debugging}

Consider the following code taken from a simulation:

\begin{Shaded}
\begin{Highlighting}[]
    \ControlFlowTok{if}\NormalTok{ ( }\FunctionTok{any}\NormalTok{( }\FunctionTok{is.na}\NormalTok{( rs}\SpecialCharTok{$}\NormalTok{estimate ) ) ) \{}
        \FunctionTok{browser}\NormalTok{()}
\NormalTok{    \}}
\end{Highlighting}
\end{Shaded}

The \texttt{browser()} command stops your code and puts you in an interactive console where you can look at different objects and see what is happening.
Having it triggered when something bad happens (in this case when a set of estimates has an unexpected NA) can help untangle what is driving a rare event.

The interactive console allows you to look at the current state of the code, and you can type in commands to see what is going on.
It is just like a normal R workspace, but if you look at the Environment, you will only see what the code has available at the time \texttt{browser()} was called.
If you are inside a function, for example, you will only see the things passed to the function, and the variables the function has made.

This can be very important to, for example, check what values were passed to your function--many bugs are due to the wrong thing getting passed to some code that would otherwise work.

Once in a browser, you can say \texttt{q} to quit out.
You can also type \texttt{n} to go to the next line of code.
This allows you to walk through the code step by step, seeing what happens as you move along.
Much of the time, RStudio will even jump to the part of your script where you paused, so you can see the code that will be run with each step.

\section{\texorpdfstring{Debugging with \texttt{debug()}}{Debugging with debug()}}\label{debugging-with-debug}

Another useful debugging tool is the \texttt{debug()} function.
This function allows you to set a breakpoint in your code, so that when you call the function, it will stop at the beginning of the function and put you in the same browser discussed above.
You use it like this:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{debug}\NormalTok{( gen\_dat )}
\FunctionTok{run\_simulation}\NormalTok{( some\_parameters )}
\end{Highlighting}
\end{Shaded}

Now, when \texttt{run\_simulation()} eventually calls \texttt{gen\_dat} the script will stop, and you can see exactly what was passed to \texttt{gen\_dat} and also then walk through \texttt{gen\_dat} line by line to see what is going on.

\section{\texorpdfstring{Protecting functions with \texttt{stop()}}{Protecting functions with stop()}}\label{about-stopifnot}

When writing functions, especially those that take a lot of parameters, it is often wise to include \texttt{stopifnot()} statements at the top to verify the function is getting what it expects.
These are sometimes called ``assert statements'' and are a tool for making errors show up as early as possible.
For example, look at this (fake) example of generating data with different means and variances

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{make\_groups }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{( means, sds ) \{}
\NormalTok{  Y }\OtherTok{=} \FunctionTok{rnorm}\NormalTok{( }\FunctionTok{length}\NormalTok{(means), }\AttributeTok{mean=}\NormalTok{means, }\AttributeTok{sd =}\NormalTok{ sds )}
  \FunctionTok{round}\NormalTok{( Y )}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

If we call it, but provide different lengths for our means and variances, nothing happens, because R simply recycles the standard deviation parameter:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{make\_groups}\NormalTok{( }\FunctionTok{c}\NormalTok{(}\DecValTok{100}\NormalTok{,}\DecValTok{200}\NormalTok{,}\DecValTok{300}\NormalTok{,}\DecValTok{400}\NormalTok{), }
             \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{100}\NormalTok{,}\DecValTok{10000}\NormalTok{) )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]   101   204 17426   400
\end{verbatim}

What is nasty about this possible error is nothing is telling you that something is wrong!
You could build an entire simulation on this, not realizing that your fourth group has the variance of your first, and get results that make no sense to you.
You could even publish something based on a finding that depends on this error, which would eventually be quite embarrasing.

If this function was used in our data generating code, we might eventually see some warning that something is off, but this would still not tell us where things went off the rails.
We can instead protect our function by putting in an \emph{assert statement} using \texttt{stopifnot()}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{make\_groups }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{( means, sds ) \{}
  \FunctionTok{stopifnot}\NormalTok{( }\FunctionTok{length}\NormalTok{(means) }\SpecialCharTok{==} \FunctionTok{length}\NormalTok{(sds) )}
\NormalTok{  Y }\OtherTok{=} \FunctionTok{rnorm}\NormalTok{( }\FunctionTok{length}\NormalTok{(means), }\AttributeTok{mean=}\NormalTok{means, }\AttributeTok{sd =}\NormalTok{ sds )}
  \FunctionTok{round}\NormalTok{( Y )}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Now we get this:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{make\_groups}\NormalTok{( }\FunctionTok{c}\NormalTok{(}\DecValTok{100}\NormalTok{,}\DecValTok{200}\NormalTok{,}\DecValTok{300}\NormalTok{,}\DecValTok{400}\NormalTok{), }
             \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{100}\NormalTok{,}\DecValTok{10000}\NormalTok{) )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Error in make_groups(c(100, 200, 300, 400), c(1, 100, 10000)): length(means) == length(sds) is not TRUE
\end{verbatim}

The \texttt{stopifnot()} command ensures your code is getting called as you intended.

These statements can also serve as a sort of documentation as to what you expect.
Consider, for example:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{make\_xy }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{( N, mu\_x, mu\_y, rho ) \{}
  \FunctionTok{stopifnot}\NormalTok{( }\SpecialCharTok{{-}}\DecValTok{1} \SpecialCharTok{\textless{}=}\NormalTok{ rho }\SpecialCharTok{\&\&}\NormalTok{ rho }\SpecialCharTok{\textless{}=} \DecValTok{1}\NormalTok{ )}
\NormalTok{  X }\OtherTok{=}\NormalTok{ mu\_x }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{( N )}
\NormalTok{  Y }\OtherTok{=}\NormalTok{ mu\_y }\SpecialCharTok{+}\NormalTok{ rho }\SpecialCharTok{*}\NormalTok{ X }\SpecialCharTok{+} \FunctionTok{sqrt}\NormalTok{(}\DecValTok{1}\SpecialCharTok{{-}}\NormalTok{rho}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}\SpecialCharTok{*}\FunctionTok{rnorm}\NormalTok{(N)}
  \FunctionTok{tibble}\NormalTok{(}\AttributeTok{X =}\NormalTok{ X, }\AttributeTok{Y=}\NormalTok{Y)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Here we see that rho should be between -1 and 1 quite clearly.
A good reminder of what the parameter is for.

This also protects you from inadvetently misremembering the order of your parameters when you call the function (although it is good practice to name your parameters as you pass).
Consider:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{a }\OtherTok{\textless{}{-}} \FunctionTok{make\_xy}\NormalTok{( }\DecValTok{10}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\FloatTok{0.75}\NormalTok{ )}
\NormalTok{b }\OtherTok{\textless{}{-}} \FunctionTok{make\_xy}\NormalTok{( }\DecValTok{10}\NormalTok{, }\FloatTok{0.75}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Error in make_xy(10, 0.75, 2, 3): -1 <= rho && rho <= 1 is not TRUE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{c }\OtherTok{\textless{}{-}} \FunctionTok{make\_xy}\NormalTok{( }\DecValTok{10}\NormalTok{, }\AttributeTok{rho =} \FloatTok{0.75}\NormalTok{, }\AttributeTok{mu\_x =} \DecValTok{2}\NormalTok{, }\AttributeTok{mu\_y =} \DecValTok{3}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\section{Testing code}\label{testing-code}

Testing your code is a good way to ensure that it does what you expect.
We have seen some demonstration of testing code early on, such as when we made plots of our simulated data to see if it looked like we expected.
This kind of code could be stored in the script with the functions being tested, so that you can run it again later to see if the code still works as expected, using the \texttt{FALSE} trick discussed in Section \ref{about-keeping-tests-with-FALSE}.

That sort of testing is important, but it can be hard to bring oneself to go and rerun it after making what seems like a trivial change to the core code.
It can also be hard to track down the ripple effects of changing a low-level method that is used by many other methods.

This is why people developed ``unit testing,'' an approach to testing where you write code that you can just run whenever you want, code which runs a series of tests on your code and prints out which tests work as expected, and which do not.
In R, the most common way of doing this is the \texttt{testthat} package.

There are two general aspects to \texttt{testthat} that can be useful, the \texttt{expect\_*()} methods and the \texttt{test\_that()} function.

Consider the following simple DGP to generate an X and Y variable that have a given relationship:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_DGP }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{( N, mu, beta ) \{}
  \FunctionTok{stopifnot}\NormalTok{( N }\SpecialCharTok{\textgreater{}} \DecValTok{0}\NormalTok{, beta }\SpecialCharTok{\textless{}=} \DecValTok{1}\NormalTok{ )}
\NormalTok{  dat }\OtherTok{=} \FunctionTok{tibble}\NormalTok{( }\AttributeTok{X =} \FunctionTok{rnorm}\NormalTok{( N, }\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{ ),}
                \AttributeTok{Y =}\NormalTok{ mu }\SpecialCharTok{+}\NormalTok{ beta }\SpecialCharTok{*}\NormalTok{ X }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{( N, }\AttributeTok{sd =} \DecValTok{1}\SpecialCharTok{{-}}\NormalTok{beta}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{ ) )}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

We can write test code as so:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(testthat)}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{44343}\NormalTok{)}
\FunctionTok{test\_that}\NormalTok{(}\StringTok{"my\_DGP works as expected"}\NormalTok{, \{}
\NormalTok{  dta }\OtherTok{\textless{}{-}} \FunctionTok{my\_DGP}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DecValTok{0}\NormalTok{, }\FloatTok{0.5}\NormalTok{)}
  \CommentTok{\# Check that the output is a tibble}
  \FunctionTok{expect\_s3\_class}\NormalTok{(dta, }\StringTok{"tbl\_df"}\NormalTok{)}
  
  \CommentTok{\# Check that the output has the right number of rows}
  \FunctionTok{expect\_equal}\NormalTok{(}\FunctionTok{nrow}\NormalTok{(dta), }\DecValTok{10}\NormalTok{)}
  
  \CommentTok{\# Check that the output has the right columns}
  \FunctionTok{expect\_true}\NormalTok{(}\FunctionTok{all}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{"X"}\NormalTok{, }\StringTok{"Y"}\NormalTok{) }\SpecialCharTok{\%in\%} \FunctionTok{colnames}\NormalTok{(dta)))}
  
  \CommentTok{\# Check that the mean of Y is close to mu}
\NormalTok{  dta2 }\OtherTok{=} \FunctionTok{my\_DGP}\NormalTok{(}\DecValTok{1000}\NormalTok{, }\DecValTok{2}\NormalTok{, }\FloatTok{0.5}\NormalTok{)}
  \FunctionTok{expect\_equal}\NormalTok{(}\FunctionTok{mean}\NormalTok{(dta2}\SpecialCharTok{$}\NormalTok{Y), }\DecValTok{2}\NormalTok{, }\AttributeTok{tolerance =} \FloatTok{0.1}\NormalTok{)}
  
  \CommentTok{\# Check we get an error when we should}
  \FunctionTok{expect\_error}\NormalTok{(}\FunctionTok{my\_DGP}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{10}\NormalTok{, }\DecValTok{0}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\FloatTok{0.5}\NormalTok{) )}
\NormalTok{\})}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Test passed
\end{verbatim}

This code will run the tests, and if they all pass, it will print out a happy message.

If one or more of our tests fail, we will get a set of error messages that tells us what went wrong, and where things broke:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{test\_that}\NormalTok{(}\StringTok{"my\_DGP works as expected (test 2)"}\NormalTok{, \{}
\NormalTok{  dta }\OtherTok{\textless{}{-}} \FunctionTok{my\_DGP}\NormalTok{(}\DecValTok{10000}\NormalTok{, }\DecValTok{2}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{2}\NormalTok{)}
  \FunctionTok{expect\_equal}\NormalTok{( }\FunctionTok{sd}\NormalTok{( dta}\SpecialCharTok{$}\NormalTok{X ), }\DecValTok{1}\NormalTok{, }\AttributeTok{tolerance =} \FloatTok{0.02}\NormalTok{ )}
  
\NormalTok{  dta }\OtherTok{\textless{}{-}} \FunctionTok{my\_DGP}\NormalTok{(}\DecValTok{10000}\NormalTok{, }\DecValTok{2}\NormalTok{, }\FloatTok{0.5}\NormalTok{)}
  \FunctionTok{expect\_equal}\NormalTok{( }\FunctionTok{var}\NormalTok{(dta}\SpecialCharTok{$}\NormalTok{Y), }\DecValTok{1}\NormalTok{, }\AttributeTok{tolerance =} \FloatTok{0.02}\NormalTok{ )}
  
\NormalTok{  M }\OtherTok{=} \FunctionTok{lm}\NormalTok{( Y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ X, }\AttributeTok{data=}\NormalTok{dta )}
  \FunctionTok{expect\_equal}\NormalTok{( }\FunctionTok{coef}\NormalTok{(M)[[}\DecValTok{2}\NormalTok{]], }\FloatTok{0.5}\NormalTok{, }\AttributeTok{tolerance =} \FloatTok{0.02}\NormalTok{ )}
\NormalTok{\} )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## -- Warning: my_DGP works as expected (test 2) ----
## NAs produced
## Backtrace:
##     x
##  1. +-global my_DGP(10000, 2, -2)
##  2. | \-tibble::tibble(...)
##  3. |   \-tibble:::tibble_quos(xs, .rows, .name_repair)
##  4. |     \-rlang::eval_tidy(xs[[j]], mask)
##  5. \-stats::rnorm(N, sd = 1 - beta^2)
## 
## -- Failure: my_DGP works as expected (test 2) ----
## var(dta$Y) not equal to 1.
## 1/1 mismatches
## [1] 0.792 - 1 == -0.208
\end{verbatim}

\begin{verbatim}
## Error:
## ! Test failed
\end{verbatim}

With unit testing, you write a bunch of these tests, each targeting some specific aspect of your code.
If you put all of these tests in a file, you can run them all at once:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{test\_file}\NormalTok{(here}\SpecialCharTok{::}\FunctionTok{here}\NormalTok{( }\StringTok{"code/demo\_test\_file.R"}\NormalTok{ ) )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## == Testing demo_test_file.R ======================
## 
## [ FAIL 0 | WARN 0 | SKIP 0 | PASS 0 ]
## [ FAIL 0 | WARN 0 | SKIP 0 | PASS 1 ]
## [ FAIL 0 | WARN 0 | SKIP 0 | PASS 2 ]
## [ FAIL 0 | WARN 0 | SKIP 0 | PASS 3 ]
## [ FAIL 0 | WARN 0 | SKIP 0 | PASS 4 ]
## [ FAIL 0 | WARN 0 | SKIP 0 | PASS 5 ]
## [ FAIL 0 | WARN 1 | SKIP 0 | PASS 5 ]
## [ FAIL 0 | WARN 1 | SKIP 0 | PASS 6 ]
## [ FAIL 1 | WARN 1 | SKIP 0 | PASS 6 ]
## [ FAIL 1 | WARN 1 | SKIP 0 | PASS 7 ]
## 
## -- Warning ('demo_test_file.R:35:3'): my_DGP works as expected (test 2) --
## NAs produced
## Backtrace:
##     x
##  1. +-my_DGP(10000, 2, -2) at demo_test_file.R:35:3
##  2. | \-tibble::tibble(...) at demo_test_file.R:7:3
##  3. |   \-tibble:::tibble_quos(xs, .rows, .name_repair)
##  4. |     \-rlang::eval_tidy(xs[[j]], mask)
##  5. \-stats::rnorm(N, sd = 1 - beta^2)
## 
## -- Failure ('demo_test_file.R:39:3'): my_DGP works as expected (test 2) --
## var(dta$Y) not equal to 1.
## 1/1 mismatches
## [1] 0.792 - 1 == -0.208
## 
## [ FAIL 1 | WARN 1 | SKIP 0 | PASS 7 ]
\end{verbatim}

The \texttt{test\_file()} method will then give an overall printout of all the tests made, and list which passed, which gave warnings, and which were skipped.
You can also run the test from inside RStudio: at the top-right you should see ``Run Tests''--if you click on it, it will start an entirely new work session, and source the file to test it.
This is the best way to use these testing files.

This stand-alone work session approach means it is important to make the test file stand-alone: the file should source the code you want to test, and load any needed libraries, before running the testing code.

You can finally make an entire directory of these testing files, and run them all at once with \texttt{test\_dir()}.
The usual way to store the files is in a \texttt{tests/testthat/} directory inside your project.
You can then have a \texttt{tests/testthat.R} file that runs \texttt{test\_dir()} on the \texttt{tests/testthat/} directory.
The \texttt{testthat} package is designed to allow for including unit testing in an R package, but we are repurposing it for general projects here.

Once you have your unit testing all set up, you can work on your project, and then run the unit tests to see if you broke anything.
Even more important, if you are working with a collaborator, you can both run unit tests to ensure you have not broken something that someone else was counting on!
Furthermore, you can use the test code as a reference for how the code should be used, and what the expected output is.
For any reasonably complex project, having test code can be of enormous benefit.

In principle, if you are writing code to figure out why something is not working as expected, you should put that code in your testing folder so that you can run it again later, ensuring that any bug you fixed will stay fixed moving forward.

\part{Complex Data Structures}\label{part-complex-data-structures}

\chapter{Using simulation as a power calculator}\label{sec:power}

We can use simulation as a power calculator.
In particular, to estimate power, we generate data according to our best guess as to what we might find in a planned evaluation, and then analyze these synthetic data and see if we detect the effect we built into our DGP.
We then do this repeatedly, and see how often we detect our effect.
This is power.

Now, if we are generally right about our guesses about our DGP and the associated parameters we plugged into it, in terms of some planned study, then our power will be right on.
This is all a power analysis is, using simulation or otherwise.

Simulation has benefits over using power calculators because we can take into account odd aspects of our modeling, and also do non-standard approaches to evaluation that we might not find in a normal power calculator.

We illustrate this idea with a case study.
In this example, we are planning a school-level intervention to reduce rates of discipline via a socio-emotional targeting intervention on both teachers and students, where we have strongly predictive historic data and a time-series component.
This is a planned RCT, where we will treat entire schools (so a cluster-randomized study).
We are struggling because treating each school is very expensive (we have to run a large training and coaching of the staff), so each unit is a major decision.
We want something like 4, 5, or maybe 6 treated schools.
Our diving question is: Can we get away with this?

\section{Getting design parameters from pilot data}\label{getting-design-parameters-from-pilot-data}

We had pilot data from school administrative records (in particular discipline rates for each school and year for a series of five years), and we use those to estimate parameters to plug into our simulation.
We assume our experimental sample will be on schools that have chronic issues
with discipline, so we filtered our historic data to get schools we imagined to likely be in our study.

We ended up with the following data, with log-transformed discipline rates for each year (we did this to put things on a multiplicative scale, and to make our data more normal given heavy skew in the original). Each row is a potential school in the district.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{datW }\OtherTok{=} \FunctionTok{read\_csv}\NormalTok{( }\StringTok{"data/discipline\_data.csv"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Rows: 27 Columns: 6
## -- Column specification --------------------------
## Delimiter: ","
## chr (1): Code
## dbl (5): 2015, 2016, 2017, 2018, 2019
## 
## i Use `spec()` to retrieve the full column specification for this data.
## i Specify the column types or set `show_col_types = FALSE` to quiet this message.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{datW}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 27 x 6
##    Code  `2015` `2016` `2017` `2018` `2019`
##    <chr>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>
##  1 S1     -2.87  -2.81  -2.93  -3.52  -4.90
##  2 S2     -3.60  -2.83  -2.56  -2.76  -3.32
##  3 S3     -3.00  -2.88  -2.81  -3.39  -4.91
##  4 S4     -3.90  -3.20  -2.53  -3.67  -4.34
##  5 S5     -2.46  -2.00  -3.34  -3.66  -4.71
##  6 S6     -2.86  -2.74  -2.51  -3.21  -3.80
##  7 S7     -2.47  -2.59  -2.69  -2.15  -2.43
##  8 S8     -2.13  -1.93  -1.82  -2.21  -2.95
##  9 S9     -3.36  -3.16  -3.06  -3.26  -3.10
## 10 S10    -2.89  -2.54  -2.26  -2.89  -3.25
## # i 17 more rows
\end{verbatim}

We use these to calculate a mean and covariance structure for generating data:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lpd\_mns }\OtherTok{=} \FunctionTok{apply}\NormalTok{( datW[,}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{], }\DecValTok{2}\NormalTok{, mean )}
\NormalTok{lpd\_mns}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      2015      2016      2017      2018      2019 
## -3.076298 -2.868337 -2.931562 -3.337221 -4.011440
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lpd\_cov }\OtherTok{=} \FunctionTok{cov}\NormalTok{( datW[,}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{] )}
\NormalTok{lpd\_cov}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           2015      2016      2017      2018
## 2015 0.4191843 0.2996073 0.2282627 0.3691894
## 2016 0.2996073 0.3656335 0.2014201 0.2511376
## 2017 0.2282627 0.2014201 0.3084799 0.2927782
## 2018 0.3691894 0.2511376 0.2927782 0.5767486
## 2019 0.1921622 0.1623542 0.2541191 0.2927812
##           2019
## 2015 0.1921622
## 2016 0.1623542
## 2017 0.2541191
## 2018 0.2927812
## 2019 0.5425783
\end{verbatim}

\section{The data generating process}\label{the-data-generating-process}

We next write a data generator that, given a desired number of control and treatment schools, and a treatment effect, makes a dataset by sampling vectors of discipline rates, and then imposes a ``treatment effect'' of scaling the discipline rate by the treatment coefficient for the last two years.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{make\_dat\_param }\OtherTok{=} \ControlFlowTok{function}\NormalTok{( n\_c, n\_t, }\AttributeTok{tx=}\DecValTok{1}\NormalTok{ ) \{}
\NormalTok{    n }\OtherTok{=}\NormalTok{ n\_c }\SpecialCharTok{+}\NormalTok{ n\_t}
\NormalTok{    lpdisc }\OtherTok{=}\NormalTok{ MASS}\SpecialCharTok{::}\FunctionTok{mvrnorm}\NormalTok{( n, }\AttributeTok{mu =}\NormalTok{ lpd\_mns, }\AttributeTok{Sigma =}\NormalTok{ lpd\_cov )}
\NormalTok{    lpdisc }\OtherTok{=} \FunctionTok{exp}\NormalTok{( lpdisc )}
    \FunctionTok{colnames}\NormalTok{( lpdisc ) }\OtherTok{=} \FunctionTok{paste0}\NormalTok{( }\StringTok{"pdisc\_"}\NormalTok{, }\FunctionTok{colnames}\NormalTok{( lpdisc ) )}
\NormalTok{    lpdisc }\OtherTok{=} \FunctionTok{as\_tibble}\NormalTok{( lpdisc ) }\SpecialCharTok{\%\textgreater{}\%}
        \FunctionTok{mutate}\NormalTok{( }\AttributeTok{ID =} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{n}\NormalTok{(),}
                \AttributeTok{Z =} \DecValTok{0} \SpecialCharTok{+}\NormalTok{ ( }\FunctionTok{sample}\NormalTok{( n ) }\SpecialCharTok{\textless{}=}\NormalTok{ n\_t ) )}
    
    \CommentTok{\# Add in treatment effect}
\NormalTok{    lpdisc }\OtherTok{=} \FunctionTok{mutate}\NormalTok{( lpdisc, }
                     \AttributeTok{pdisc\_2018 =}\NormalTok{ pdisc\_2018 }\SpecialCharTok{*} \FunctionTok{ifelse}\NormalTok{( Z }\SpecialCharTok{==} \DecValTok{1}\NormalTok{, tx, }\DecValTok{1}\NormalTok{ ),}
                     \AttributeTok{pdisc\_2019 =}\NormalTok{ pdisc\_2019 }\SpecialCharTok{*} \FunctionTok{ifelse}\NormalTok{( Z }\SpecialCharTok{==} \DecValTok{1}\NormalTok{, tx, }\DecValTok{1}\NormalTok{ ) )}
 
\NormalTok{    lpdisc }\SpecialCharTok{\%\textgreater{}\%} 
      \FunctionTok{relocate}\NormalTok{( ID, Z )}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Our function generates schools with discipline given by the provided mean and covariance structure; we have calibrated our data generating process to give us data that looks very similar to the data we would see in the field.
For power, realism, in terms of the aspects impacting uncertainty, is key.

For our impact model, the treatment kicks in for the final two years, multiplying discipline rate by \texttt{tx} (so \texttt{tx\ =\ 1} means no treatment effect).

We test our function:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{( }\DecValTok{59585}\NormalTok{ )}
\NormalTok{a }\OtherTok{=} \FunctionTok{make\_dat\_param}\NormalTok{( }\DecValTok{100}\NormalTok{, }\DecValTok{100}\NormalTok{, }\FloatTok{0.5}\NormalTok{ )  }
\NormalTok{a}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 200 x 7
##       ID     Z pdisc_2015 pdisc_2016 pdisc_2017
##    <int> <dbl>      <dbl>      <dbl>      <dbl>
##  1     1     1     0.0312     0.0559     0.0328
##  2     2     1     0.0421     0.0172     0.0239
##  3     3     1     0.187      0.263      0.144 
##  4     4     0     0.0439     0.0457     0.0338
##  5     5     1     0.0378     0.0577     0.0635
##  6     6     1     0.0203     0.0326     0.0377
##  7     7     0     0.0794     0.0624     0.0427
##  8     8     0     0.0257     0.0354     0.0355
##  9     9     1     0.0513     0.0692     0.104 
## 10    10     0     0.0290     0.0840     0.0711
## # i 190 more rows
## # i 2 more variables: pdisc_2018 <dbl>,
## #   pdisc_2019 <dbl>
\end{verbatim}

We can group each treatment arm and look at discipline over the years:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{aL }\OtherTok{=}\NormalTok{ a }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pivot\_longer}\NormalTok{( pdisc\_2015}\SpecialCharTok{:}\NormalTok{pdisc\_2019, }
                \AttributeTok{names\_to =} \FunctionTok{c}\NormalTok{( }\StringTok{".value"}\NormalTok{, }\StringTok{"year"}\NormalTok{ ),}
                \AttributeTok{names\_pattern =} \StringTok{"(.*)\_(.*)"}\NormalTok{ ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{( }\AttributeTok{year =} \FunctionTok{as.numeric}\NormalTok{( year ) )}

\NormalTok{aLg }\OtherTok{=}\NormalTok{ aL }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{group\_by}\NormalTok{( year, Z ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{( }\AttributeTok{pdisc =} \FunctionTok{mean}\NormalTok{( pdisc ) )}

\FunctionTok{ggplot}\NormalTok{( aLg, }\FunctionTok{aes}\NormalTok{( year, pdisc, }\AttributeTok{col=}\FunctionTok{as.factor}\NormalTok{(Z) ) ) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{( }\AttributeTok{color =} \StringTok{"Tx?"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{Designing-Simulations-in-R_files/figure-latex/unnamed-chunk-304-1} \end{center}

Our treatment group drops faster than the control. We see the nonlinear structure actually observed in our original data in terms of discipline over time has been replicated.

We next write some functions to analyze our data.
This should feel very familiar: we are just doing our simulation framework, as usual.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{eval\_dat }\OtherTok{=} \ControlFlowTok{function}\NormalTok{( sdat ) \{}
    
    \CommentTok{\# No covariate adjustment, average change model (on log outcome)}
\NormalTok{    M\_raw }\OtherTok{=} \FunctionTok{lm}\NormalTok{( }\FunctionTok{log}\NormalTok{( pdisc\_2018 ) }\SpecialCharTok{\textasciitilde{}} \DecValTok{1} \SpecialCharTok{+}\NormalTok{ Z, }\AttributeTok{data=}\NormalTok{sdat )}

    \CommentTok{\# Simple average change model using 2018 as outcome.}
\NormalTok{    M\_simple }\OtherTok{=} \FunctionTok{lm}\NormalTok{( pdisc\_2018 }\SpecialCharTok{\textasciitilde{}} \DecValTok{1} \SpecialCharTok{+}\NormalTok{ Z }\SpecialCharTok{+}\NormalTok{ pdisc\_2017 }\SpecialCharTok{+}\NormalTok{ pdisc\_2016 }\SpecialCharTok{+}\NormalTok{ pdisc\_2015,}
                   \AttributeTok{data=}\NormalTok{sdat )}

    \CommentTok{\# Simple model on logged outcome}
\NormalTok{    M\_log }\OtherTok{=} \FunctionTok{lm}\NormalTok{( }\FunctionTok{log}\NormalTok{( pdisc\_2018 ) }\SpecialCharTok{\textasciitilde{}} \DecValTok{1} \SpecialCharTok{+}\NormalTok{ Z }\SpecialCharTok{+} \FunctionTok{log}\NormalTok{( pdisc\_2017) }\SpecialCharTok{+} 
                  \FunctionTok{log}\NormalTok{( pdisc\_2016) }\SpecialCharTok{+} \FunctionTok{log}\NormalTok{( pdisc\_2015 ),}
                \AttributeTok{data=}\NormalTok{sdat )}
    
    \CommentTok{\# Ratio of average disc to average prior disc as outcome}
\NormalTok{    sdat }\OtherTok{=} \FunctionTok{mutate}\NormalTok{( sdat,}
                   \AttributeTok{avg\_disc =}\NormalTok{ (pdisc\_2018 }\SpecialCharTok{+}\NormalTok{ pdisc\_2019)}\SpecialCharTok{/}\DecValTok{2}\NormalTok{,}
                   \AttributeTok{prior\_disc =}\NormalTok{ (pdisc\_2017 }\SpecialCharTok{+}\NormalTok{ pdisc\_2016 }\SpecialCharTok{+}\NormalTok{ pdisc\_2015 )}\SpecialCharTok{/}\DecValTok{3}\NormalTok{,}
                   \AttributeTok{disc =}\NormalTok{ pdisc\_2018 }\SpecialCharTok{/}\NormalTok{ prior\_disc,}
                   \AttributeTok{disc\_two =}\NormalTok{ avg\_disc }\SpecialCharTok{/}\NormalTok{ prior\_disc )}
\NormalTok{    M\_ratio }\OtherTok{=} \FunctionTok{lm}\NormalTok{( disc }\SpecialCharTok{\textasciitilde{}} \DecValTok{1} \SpecialCharTok{+}\NormalTok{ Z, }\AttributeTok{data =}\NormalTok{ sdat )}
\NormalTok{    M\_ratio\_twopost }\OtherTok{=} \FunctionTok{lm}\NormalTok{( disc\_two }\SpecialCharTok{\textasciitilde{}} \DecValTok{1} \SpecialCharTok{+}\NormalTok{ Z, }\AttributeTok{data =}\NormalTok{ sdat )}
    
    \CommentTok{\# Use average of two post{-}tx time periods, averaged to reduce noise}
\NormalTok{    M\_twopost }\OtherTok{=} \FunctionTok{lm}\NormalTok{( }\FunctionTok{log}\NormalTok{( avg\_disc ) }\SpecialCharTok{\textasciitilde{}} \DecValTok{1} \SpecialCharTok{+}\NormalTok{ Z }\SpecialCharTok{+} \FunctionTok{log}\NormalTok{( pdisc\_2017 ) }\SpecialCharTok{+} 
                      \FunctionTok{log}\NormalTok{( pdisc\_2016 ) }\SpecialCharTok{+} \FunctionTok{log}\NormalTok{( pdisc\_2015 ), }
                    \AttributeTok{data=}\NormalTok{sdat )}

    \CommentTok{\# Time and unit fixed effects}
\NormalTok{    sdatL }\OtherTok{=} \FunctionTok{pivot\_longer}\NormalTok{( sdat, }\AttributeTok{cols =}\NormalTok{ pdisc\_2015}\SpecialCharTok{:}\NormalTok{pdisc\_2019, }
                          \AttributeTok{names\_to =} \StringTok{"year"}\NormalTok{,}
                          \AttributeTok{values\_to =} \StringTok{"pdisc"}\NormalTok{ ) }\SpecialCharTok{\%\textgreater{}\%}
        \FunctionTok{mutate}\NormalTok{( }\AttributeTok{Z =}\NormalTok{ Z }\SpecialCharTok{*}\NormalTok{ (year }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{( }\StringTok{"pdisc\_2018"}\NormalTok{, }\StringTok{"pdisc\_2019"}\NormalTok{ ) ),}
                \AttributeTok{ID =} \FunctionTok{paste0}\NormalTok{( }\StringTok{"S"}\NormalTok{, ID ) )}
    
\NormalTok{    M\_2wfe }\OtherTok{=} \FunctionTok{lm}\NormalTok{( }\FunctionTok{log}\NormalTok{( pdisc ) }\SpecialCharTok{\textasciitilde{}} \DecValTok{0} \SpecialCharTok{+}\NormalTok{ ID }\SpecialCharTok{+}\NormalTok{ year }\SpecialCharTok{+}\NormalTok{ Z,}
                 \AttributeTok{data=}\NormalTok{sdatL )}

    \CommentTok{\# Bundle all our models by getting the estimated treatment impact}
    \CommentTok{\# from each.}
\NormalTok{    models }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{( }\AttributeTok{raw=}\NormalTok{M\_raw, }\AttributeTok{simple=}\NormalTok{M\_simple,}
                       \AttributeTok{log=}\NormalTok{M\_log, }\AttributeTok{ratio =}\NormalTok{ M\_ratio, }
                       \AttributeTok{ratio\_twopost =}\NormalTok{ M\_ratio\_twopost,}
                       \AttributeTok{log\_twopost =}\NormalTok{ M\_twopost, }
                       \AttributeTok{FE =}\NormalTok{ M\_2wfe )}
\NormalTok{    rs }\OtherTok{\textless{}{-}} \FunctionTok{map\_df}\NormalTok{( models, broom}\SpecialCharTok{::}\NormalTok{tidy, }\AttributeTok{.id=}\StringTok{"model"}\NormalTok{ ) }\SpecialCharTok{\%\textgreater{}\%}
        \FunctionTok{filter}\NormalTok{( term}\SpecialCharTok{==}\StringTok{"Z"}\NormalTok{ ) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{        dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{( }\SpecialCharTok{{-}}\NormalTok{term ) }\SpecialCharTok{\%\textgreater{}\%}
      \FunctionTok{arrange}\NormalTok{( model )}
    
\NormalTok{    rs}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Our method marches through a host of models; we weren't sure what the gains would be from one model to another, so we decided to conduct power analyses on all of them.
Again, we look at what our evaluation function does:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dat }\OtherTok{=} \FunctionTok{make\_dat\_param}\NormalTok{( }\AttributeTok{n\_c =} \DecValTok{4}\NormalTok{, }\AttributeTok{n\_t =} \DecValTok{4}\NormalTok{, }\AttributeTok{tx =} \FloatTok{0.5}\NormalTok{ )}
\FunctionTok{eval\_dat}\NormalTok{( dat )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 7 x 5
##   model       estimate std.error statistic p.value
##   <chr>          <dbl>     <dbl>     <dbl>   <dbl>
## 1 FE          -0.644      0.325     -1.98   0.0576
## 2 log         -0.841      0.478     -1.76   0.176 
## 3 log_twopost -1.22       0.437     -2.80   0.0680
## 4 ratio       -0.126      0.136     -0.923  0.392 
## 5 ratio_twop~ -0.269      0.139     -1.93   0.102 
## 6 raw         -0.650      0.300     -2.17   0.0733
## 7 simple      -0.00983    0.0133    -0.742  0.512
\end{verbatim}

We have a nice set of estimates, one for each model.

\section{Running the simulation}\label{running-the-simulation-1}

Now we put it all together in our classic simulator:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sim\_run }\OtherTok{=} \ControlFlowTok{function}\NormalTok{( n\_c, n\_t, tx, R, }\AttributeTok{seed =} \ConstantTok{NULL}\NormalTok{ ) \{}
    \ControlFlowTok{if}\NormalTok{ ( }\SpecialCharTok{!}\FunctionTok{is.null}\NormalTok{( seed ) ) \{}
        \FunctionTok{set.seed}\NormalTok{(seed)}
\NormalTok{    \}}
    \FunctionTok{cat}\NormalTok{( }\StringTok{"Running n\_c, n\_t ="}\NormalTok{, n\_c, n\_t, }\StringTok{"tx ="}\NormalTok{, tx, }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{ )}
\NormalTok{    rps }\OtherTok{=} \FunctionTok{rerun}\NormalTok{( R, \{}
\NormalTok{        sdat }\OtherTok{=} \FunctionTok{make\_dat\_param}\NormalTok{(}\AttributeTok{n\_c =}\NormalTok{ n\_c, }\AttributeTok{n\_t =}\NormalTok{ n\_t, }\AttributeTok{tx =}\NormalTok{ tx)}
        \FunctionTok{eval\_dat}\NormalTok{( sdat )}
\NormalTok{    \})}
    \FunctionTok{bind\_rows}\NormalTok{( rps )}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

We then do the usual to run across a set of scenarios, running \texttt{sim\_run} on each row of the following:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{res }\OtherTok{=} \FunctionTok{expand\_grid}\NormalTok{( }\AttributeTok{tx =} \FunctionTok{c}\NormalTok{( }\DecValTok{1}\NormalTok{, }\FloatTok{0.75}\NormalTok{, }\FloatTok{0.5}\NormalTok{ ),}
                   \AttributeTok{n\_c =} \FunctionTok{c}\NormalTok{( }\DecValTok{4}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{12}\NormalTok{, }\DecValTok{20}\NormalTok{ ),}
                   \AttributeTok{n\_t =} \FunctionTok{c}\NormalTok{( }\DecValTok{4}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{6}\NormalTok{ ) )}
\NormalTok{res}\SpecialCharTok{$}\NormalTok{R }\OtherTok{=} \DecValTok{1000}
\NormalTok{res}\SpecialCharTok{$}\NormalTok{seed }\OtherTok{=} \DecValTok{1010203} \SpecialCharTok{+} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{nrow}\NormalTok{(res)}
\end{Highlighting}
\end{Shaded}

For evaluation, we load our saved results and calculate rejection rates (we use an alpha of 0.10 since we are doing one-sided testing):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{res }\OtherTok{=} \FunctionTok{readRDS}\NormalTok{( }\AttributeTok{file=}\StringTok{"data/discipline\_simulation.rds"}\NormalTok{ )}

\NormalTok{sres }\OtherTok{\textless{}{-}}\NormalTok{ res }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{group\_by}\NormalTok{( n\_c, n\_t, tx, model ) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{summarise}\NormalTok{( }\AttributeTok{E\_est =} \FunctionTok{mean}\NormalTok{( estimate ),}
               \AttributeTok{SE =} \FunctionTok{sd}\NormalTok{( estimate ),}
               \AttributeTok{E\_SE\_hat =} \FunctionTok{mean}\NormalTok{( std.error ),}
               \AttributeTok{pow =} \FunctionTok{mean}\NormalTok{( p.value }\SpecialCharTok{\textless{}=} \FloatTok{0.10}\NormalTok{ ) ) }\CommentTok{\# one{-}sided testing}
\NormalTok{sres}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 378 x 8
## # Groups:   n_c, n_t, tx [54]
##      n_c   n_t    tx model   E_est     SE E_SE_hat
##    <dbl> <dbl> <dbl> <chr>   <dbl>  <dbl>    <dbl>
##  1     4     4  0.5  FE    -0.693  0.313    0.277 
##  2     4     4  0.5  log   -0.694  0.476    0.430 
##  3     4     4  0.5  log_~ -0.692  0.431    0.383 
##  4     4     4  0.5  ratio -0.374  0.219    0.203 
##  5     4     4  0.5  rati~ -0.291  0.151    0.139 
##  6     4     4  0.5  raw   -0.719  0.535    0.515 
##  7     4     4  0.5  simp~ -0.0195 0.0194   0.0157
##  8     4     4  0.75 FE    -0.292  0.310    0.274 
##  9     4     4  0.75 log   -0.295  0.488    0.435 
## 10     4     4  0.75 log_~ -0.305  0.424    0.373 
## # i 368 more rows
## # i 1 more variable: pow <dbl>
\end{verbatim}

\section{Evaluating power}\label{evaluating-power}

Once our simulation is run, we can explore power as a function of the design characteristics.
In particular, we eventually want to calculate the chance of noticing effects of different sizes, given various sample sizes we might employ.
Our driving question is how few schools on the treated side can we get away with?
Also, we want to know how much having more schools on the control side allows us to get away with fewer schools on the treated side.

\subsection{Checking validity of our models}\label{checking-validity-of-our-models}

Before we look at power, we need to check on whether our different models are valid.
This is especiallt important as we are in a small \(n\) context, so we know asymptotics may not hold as they should.
To check our models for validity we subset our trials to where \texttt{tx\ =\ 1}, and look at the rejection rates.

We first run a regression to see if rejection is a function of sample size (are smaller samples more invalid) and treatment-control imbalance.
We center both variables so our intercepts are overall average rejection rates for each model considered:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sres }\OtherTok{=} \FunctionTok{mutate}\NormalTok{( sres,}
               \AttributeTok{n =}\NormalTok{ n\_c }\SpecialCharTok{+}\NormalTok{ n\_t,}
               \AttributeTok{imbalance =} \FunctionTok{pmax}\NormalTok{( n\_t }\SpecialCharTok{/}\NormalTok{ n\_c, n\_c }\SpecialCharTok{/}\NormalTok{ n\_t ) }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{ )}
\NormalTok{sres}\SpecialCharTok{$}\NormalTok{n }\OtherTok{=}\NormalTok{ (sres}\SpecialCharTok{$}\NormalTok{n }\SpecialCharTok{{-}} \FunctionTok{mean}\NormalTok{(sres}\SpecialCharTok{$}\NormalTok{n)) }\SpecialCharTok{/} \FunctionTok{sd}\NormalTok{(sres}\SpecialCharTok{$}\NormalTok{n)}
\NormalTok{mod }\OtherTok{=} \FunctionTok{lm}\NormalTok{( pow }\SpecialCharTok{\textasciitilde{}} \DecValTok{0} \SpecialCharTok{+}\NormalTok{ (n }\SpecialCharTok{+}\NormalTok{ imbalance) }\SpecialCharTok{*}\NormalTok{ model }\SpecialCharTok{{-}}\NormalTok{ n }\SpecialCharTok{{-}}\NormalTok{ imbalance,}
          \AttributeTok{data =} \FunctionTok{filter}\NormalTok{( sres, tx }\SpecialCharTok{==} \DecValTok{1}\NormalTok{ ) )}
\NormalTok{broom}\SpecialCharTok{::}\FunctionTok{tidy}\NormalTok{(mod) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{( }\AttributeTok{digits =} \DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{l|r|r|r|r}
\hline
term & estimate & std.error & statistic & p.value\\
\hline
modelFE & 0.143 & 0.006 & 24.593 & 0.000\\
\hline
modellog & 0.099 & 0.006 & 16.999 & 0.000\\
\hline
modellog\_twopost & 0.093 & 0.006 & 15.939 & 0.000\\
\hline
modelratio & 0.090 & 0.006 & 15.437 & 0.000\\
\hline
modelratio\_twopost & 0.093 & 0.006 & 15.967 & 0.000\\
\hline
modelraw & 0.092 & 0.006 & 15.751 & 0.000\\
\hline
modelsimple & 0.091 & 0.006 & 15.571 & 0.000\\
\hline
n:modelFE & -0.003 & 0.006 & -0.459 & 0.647\\
\hline
n:modellog & 0.001 & 0.006 & 0.141 & 0.888\\
\hline
n:modellog\_twopost & -0.005 & 0.006 & -0.919 & 0.360\\
\hline
n:modelratio & 0.000 & 0.006 & 0.071 & 0.944\\
\hline
n:modelratio\_twopost & 0.002 & 0.006 & 0.414 & 0.680\\
\hline
n:modelraw & -0.006 & 0.006 & -1.008 & 0.316\\
\hline
n:modelsimple & 0.001 & 0.006 & 0.191 & 0.849\\
\hline
imbalance:modelFE & 0.005 & 0.005 & 0.857 & 0.394\\
\hline
imbalance:modellog & -0.003 & 0.005 & -0.587 & 0.558\\
\hline
imbalance:modellog\_twopost & 0.004 & 0.005 & 0.708 & 0.480\\
\hline
imbalance:modelratio & 0.000 & 0.005 & -0.001 & 0.999\\
\hline
imbalance:modelratio\_twopost & 0.001 & 0.005 & 0.249 & 0.804\\
\hline
imbalance:modelraw & 0.006 & 0.005 & 1.154 & 0.251\\
\hline
imbalance:modelsimple & 0.001 & 0.005 & 0.180 & 0.858\\
\hline
\end{tabular}

We can also plot the nominal rejection rates under the null:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sres }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{filter}\NormalTok{( tx }\SpecialCharTok{==} \DecValTok{1}\NormalTok{ ) }\SpecialCharTok{\%\textgreater{}\%}
\FunctionTok{ggplot}\NormalTok{( }\FunctionTok{aes}\NormalTok{( n\_c, pow, }\AttributeTok{col=}\NormalTok{model ) ) }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{( }\SpecialCharTok{\textasciitilde{}}\NormalTok{ n\_t, }\AttributeTok{nrow=}\DecValTok{1}\NormalTok{ ) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_hline}\NormalTok{( }\AttributeTok{yintercept =} \FloatTok{0.10}\NormalTok{ ) }\SpecialCharTok{+}
  \FunctionTok{scale\_x\_log10}\NormalTok{(}\AttributeTok{breaks=}\FunctionTok{unique}\NormalTok{(sres}\SpecialCharTok{$}\NormalTok{n\_c) )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{Designing-Simulations-in-R_files/figure-latex/unnamed-chunk-310-1} \end{center}

We see the fixed effect models have elevated rates of rejection.
Interestingly, these rates do not seem particularly dependent on sample size or treatment-control imbalance (note lack of significant coefficeints on our regression model).
The other models all appear valid.

We can also check for bias of our methods:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sres }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{group\_by}\NormalTok{( model, tx ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{( }\AttributeTok{E\_est =} \FunctionTok{mean}\NormalTok{( E\_est ) ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{pivot\_wider}\NormalTok{( }\AttributeTok{names\_from=}\StringTok{"tx"}\NormalTok{, }\AttributeTok{values\_from=}\StringTok{"E\_est"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 7 x 4
## # Groups:   model [7]
##   model           `0.5`  `0.75`        `1`
##   <chr>           <dbl>   <dbl>      <dbl>
## 1 FE            -0.692  -0.290  -0.000703 
## 2 log           -0.692  -0.288   0.00120  
## 3 log_twopost   -0.692  -0.291   0.00241  
## 4 ratio         -0.372  -0.187  -0.000937 
## 5 ratio_twopost -0.289  -0.145  -0.00108  
## 6 raw           -0.694  -0.290   0.00327  
## 7 simple        -0.0206 -0.0104  0.0000998
\end{verbatim}

We see our models are estimating different things, none of which are the treatment effect as we parameterized it.
In particular, ``FE,'' ``log,'' ``raw,'' and ``log\_twopost'' are all estimating the impact on the log scale.
Note that \(log( 0.5 ) \approx -0.69\) and \(log( 0.75 ) \approx -0.29\).
Our ``simple'' estimator is estimating the impact on the absolute scale; reducing discipline rates by 50\% corresponds to about a 2\% reduction in actual cases.
Finally, ``ratio'' and ``ratio\_twopost'' are estimating the change in the average ratio of post-policy discipline to pre; they are akin to a gain score as compared to the log regressions.

\subsection{Assessing Precision (SE)}\label{assessing-precision-se}

Now, which methods are the most precise?
We look at the true standard errors across our methods (we drop ``simple'' and the ``ratio'' estimators since they are not on the ratio scale):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sres }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{( model, n\_c, n\_t ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{( }\AttributeTok{SE =} \FunctionTok{mean}\NormalTok{(SE ) ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{( }\SpecialCharTok{!}\NormalTok{(model }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{( }\StringTok{"simple"}\NormalTok{, }\StringTok{"ratio"}\NormalTok{, }\StringTok{"ratio\_twopost"}\NormalTok{ ) ) ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{( }\FunctionTok{aes}\NormalTok{( n\_c, SE, }\AttributeTok{col=}\NormalTok{model )) }\SpecialCharTok{+}
    \FunctionTok{facet\_grid}\NormalTok{( . }\SpecialCharTok{\textasciitilde{}}\NormalTok{ n\_t ) }\SpecialCharTok{+}
    \FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
    \FunctionTok{geom\_hline}\NormalTok{( }\AttributeTok{yintercept =} \DecValTok{0}\NormalTok{ ) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{( }\AttributeTok{colour =} \StringTok{"Model"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{Designing-Simulations-in-R_files/figure-latex/disc_precision-1} \end{center}

It looks like averaging two years for the outcome is helpful, and bumps up precision.
The two way fixed effects model seems to react to the number of control units differently than the other estimators; it is way more precise when the number of controls is few, but the other estimators catch up.
The ``raw'' estimator gives a baseline of no covariate adjustment; everything is substantially more precise than it.
The covariates matter a lot.

\subsection{Assessing power}\label{assessing-power-1}

We next look at power over our explored contexts, for the models that we find to be valid (i.e., not FE).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sres }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{( model }\SpecialCharTok{!=} \StringTok{"FE"}\NormalTok{,tx }\SpecialCharTok{!=} \DecValTok{1}\NormalTok{ ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{( }\FunctionTok{aes}\NormalTok{( n\_c, pow, }\AttributeTok{col=}\NormalTok{model )) }\SpecialCharTok{+}
    \FunctionTok{facet\_grid}\NormalTok{(  . }\SpecialCharTok{\textasciitilde{}}\NormalTok{ tx }\SpecialCharTok{+}\NormalTok{ n\_t, }\AttributeTok{labeller =}\NormalTok{ label\_both ) }\SpecialCharTok{+}
    \FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
    \FunctionTok{geom\_hline}\NormalTok{( }\AttributeTok{yintercept =} \DecValTok{0}\NormalTok{, }\AttributeTok{col=}\StringTok{"grey"}\NormalTok{ ) }\SpecialCharTok{+}
    \FunctionTok{geom\_hline}\NormalTok{( }\AttributeTok{yintercept =} \FunctionTok{c}\NormalTok{( }\FloatTok{0.10}\NormalTok{, }\FloatTok{0.80}\NormalTok{ ), }\AttributeTok{lty=}\DecValTok{2}\NormalTok{ ) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{()}\SpecialCharTok{+} \FunctionTok{theme}\NormalTok{( }\AttributeTok{legend.position=}\StringTok{"bottom"}\NormalTok{,}
                          \AttributeTok{legend.direction=}\StringTok{"horizontal"}\NormalTok{,}
                          \AttributeTok{legend.key.width=}\FunctionTok{unit}\NormalTok{(}\DecValTok{1}\NormalTok{,}\StringTok{"cm"}\NormalTok{),}
                          \AttributeTok{panel.border =} \FunctionTok{element\_blank}\NormalTok{() ) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{( }\AttributeTok{title=}\StringTok{"Power for various methods vs number of controls."}\NormalTok{,}
      \AttributeTok{y =} \StringTok{"Power"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{Designing-Simulations-in-R_files/figure-latex/disc_power-1} \end{center}

We mark 80\% power with a dashed line. For a 25\% reduction in discipline, nothing reaches desired levels of power.
For 50\% reduction, some designs do, but we need substantial numbers of control schools.
Averaging two years of outcomes post-treatment seems important: the ``twopost'' methods have a distinct power bump.
For a single year of outcome data, the log model seems our best bet.

\subsection{Assessing Minimum Detectable Effects}\label{assessing-minimum-detectable-effects}

Sometimes we want to know, given a design, what size effect we might be able to detect.
The usual measure for this is the Minimum Detectable Effect (MDE), which is usually the size of the smallest effect we could detect with power 80\%.

To calculate Minimal Detectable Effects (MDEs) for the log-scale estimators,
we first average our SEs over our different designs, grouped by sample size, and then convert the SEs to MDEs by multiplying by 2.8.
We then have to convert to our treatment scale by flipping the sign and exponentiating, to get out of the log scale.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sres2 }\OtherTok{=}\NormalTok{ sres }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{( model, n\_c, n\_t ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{( }\AttributeTok{SE =} \FunctionTok{mean}\NormalTok{( SE ),}
             \AttributeTok{E\_SE\_hat =} \FunctionTok{mean}\NormalTok{( E\_SE\_hat ) ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{( }\AttributeTok{MDE =} \FunctionTok{exp}\NormalTok{( }\SpecialCharTok{{-}}\NormalTok{ (}\FloatTok{1.64} \SpecialCharTok{+} \FloatTok{0.8}\NormalTok{) }\SpecialCharTok{*}\NormalTok{ SE ) )}

\NormalTok{sres2 }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{( }\SpecialCharTok{!}\NormalTok{(model }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{( }\StringTok{"simple"}\NormalTok{, }\StringTok{"ratio"}\NormalTok{, }\StringTok{"ratio\_twopost"}\NormalTok{ ) ) ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{( }\FunctionTok{aes}\NormalTok{( n\_c, MDE, }\AttributeTok{col=}\NormalTok{model ) ) }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{( }\SpecialCharTok{\textasciitilde{}}\NormalTok{ n\_t, }\AttributeTok{labeller =}\NormalTok{ label\_both ) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{geom\_line}\NormalTok{()  }\SpecialCharTok{+}
  \FunctionTok{geom\_hline}\NormalTok{( }\AttributeTok{yintercept =} \FloatTok{0.5}\NormalTok{ ) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{scale\_x\_log10}\NormalTok{( }\AttributeTok{breaks =} \FunctionTok{unique}\NormalTok{( sres}\SpecialCharTok{$}\NormalTok{n\_c ) ) }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{( }\AttributeTok{legend.position=}\StringTok{"bottom"}\NormalTok{,}
         \AttributeTok{legend.direction=}\StringTok{"horizontal"}\NormalTok{, }\AttributeTok{legend.key.width=}\FunctionTok{unit}\NormalTok{(}\DecValTok{1}\NormalTok{,}\StringTok{"cm"}\NormalTok{),}
         \AttributeTok{panel.border =} \FunctionTok{element\_blank}\NormalTok{() ) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{( }\AttributeTok{x =} \StringTok{"Number of control units"}\NormalTok{, }\AttributeTok{y =} \StringTok{"MDE (proportion reduction of rate)"}\NormalTok{,}
        \AttributeTok{caption =} \StringTok{"A MDE of 0.6 means a 60\% reduction (more than half) in discipline rates"}\NormalTok{,}
        \AttributeTok{title =} \StringTok{"MDE vs number of control units for various methods"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{Designing-Simulations-in-R_files/figure-latex/disc_mde-1} \end{center}

Corresponding with our findings regarding precision, above, the twopost estimator is the most sensitive, finding the smallest effects.

\section{Power for Multilevel Data}\label{power-for-multilevel-data}

Many power analyses are regarding fitting some sort of multilevel model to some type of structured data.
For example, researchers frequently want to calculate power for multisite randomized trials, where each of a series of sites has students randomized to treatment, or not.
Our earlier cluster RCT case study is another example of this.

We can use the same simulation framework to calculate power for these types of models.
We write a data generation function that generates our data given our target structure, and then repeatidly generate and analyze data to assess power, just as we have done.

As we saw earlier, however, it can be sometimes tricky to write code that properly has covariates that relate to the different levels of our model, or that divides variance appropriately across levels.
For example, in a multisite experiment, we might want a covariate that has a different mean value within each cluster, but also has variation within cluster.

Instead of immediately writing our own data generation function when faced with such a project, it might be worth looking to the literature to see what tools are available.
In this case, for example, we might come across \citet{enders2023simple}, which showcases a package, \texttt{mlmpower}, designed to generate data according to a flexible range of multilevel models.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{( mlmpower )}
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{outcome}\NormalTok{(}\StringTok{\textquotesingle{}Y\textquotesingle{}}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{within\_predictor}\NormalTok{(}\StringTok{\textquotesingle{}X\textquotesingle{}}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{between\_predictor}\NormalTok{(}\StringTok{\textquotesingle{}W\textquotesingle{}}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{effect\_size}\NormalTok{(}\AttributeTok{icc =} \FloatTok{0.1}\NormalTok{,}
              \AttributeTok{within =} \FloatTok{0.2}\NormalTok{ )}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{4232345}\NormalTok{)}

\NormalTok{dat }\OtherTok{\textless{}{-}} \FunctionTok{generate}\NormalTok{( model, }
                 \AttributeTok{n\_within =} \DecValTok{5}\NormalTok{, }
                 \AttributeTok{n\_between =} \DecValTok{3}\NormalTok{ ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{as\_tibble}\NormalTok{()}
\NormalTok{dat}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 15 x 4
##    `_id`     Y      X      W
##    <int> <dbl>  <dbl>  <dbl>
##  1     1 15.9   1.02   0.110
##  2     1  8.70  0.168  0.110
##  3     1 16.9   0.579  0.110
##  4     1 19.4   1.64   0.110
##  5     1 12.9  -1.28   0.110
##  6     2  9.33 -0.948 -0.179
##  7     2 14.1   1.66  -0.179
##  8     2 13.7   0.541 -0.179
##  9     2  7.16 -0.503 -0.179
## 10     2  5.04 -1.12  -0.179
## 11     3 18.2   0.522  1.34 
## 12     3 14.1   0.485  1.34 
## 13     3  7.54 -0.471  1.34 
## 14     3 12.9   1.07   1.34 
## 15     3 18.6   0.518  1.34
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cor}\NormalTok{( dat}\SpecialCharTok{$}\NormalTok{X, dat}\SpecialCharTok{$}\NormalTok{Y )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.6973513
\end{verbatim}

It even provides methods for calculating power, using a lmer for the estimator (it makes the estimator less visible to the user).
The method in fact runs a multifactor experiment:

\begin{verbatim}
## > ICC = 0.10, n_within = 10, n_between = 5
\end{verbatim}

\begin{verbatim}
##   Simulating >------------------------------  0...
\end{verbatim}

\begin{verbatim}
##   Simulating ==============================>  1...
\end{verbatim}

\begin{verbatim}
## 
\end{verbatim}

\begin{verbatim}
## > ICC = 0.10, n_within = 20, n_between = 5
\end{verbatim}

\begin{verbatim}
##   Simulating ==>----------------------------  8...
\end{verbatim}

\begin{verbatim}
##   Simulating ==============================>  1...
\end{verbatim}

\begin{verbatim}
## 
\end{verbatim}

\begin{verbatim}
## > ICC = 0.10, n_within = 10, n_between = 10
\end{verbatim}

\begin{verbatim}
##   Simulating ===============>---------------  4...
\end{verbatim}

\begin{verbatim}
##   Simulating ==============================>  1...
\end{verbatim}

\begin{verbatim}
## 
\end{verbatim}

\begin{verbatim}
## > ICC = 0.10, n_within = 20, n_between = 10
\end{verbatim}

\begin{verbatim}
##   Simulating ======>------------------------  1...
\end{verbatim}

\begin{verbatim}
##   Simulating ==============================>  1...
\end{verbatim}

\begin{verbatim}
## 
\end{verbatim}

\begin{verbatim}
##                   value     mc_moe
## fixed.(Intercept)  1.00 0.00000000
## fixed.cgm(X)       0.93 0.05025983
## fixed.cgm(W)       0.03 0.03360292
\end{verbatim}

We have estimated 93\% power for detecting the effect of the within-level predictor, \texttt{X}, with 10 students per site and 5 sites, given the parameters we set.
We refer the reader to the package documentation for how to use this specific package.
The broader point is it is sometimes worth digging into provided code to get material for generating data or even running simulations.
That said, each package is designed for a specific purpose, and has its own language.
Here, for example, the effect size defined as \texttt{within} is due to the package paying much attention to how covariates are centered and incorporated in the model.
Tying its parameterization to classic regression coefficients may be non-obvious.
It is thus sometimes easy to use a package to get data that looks like data, but does not actually have the structure you intend.
As always, do diagnostics and verification to ensure the tools are working as you expect.

Here, for example, we might generate a very large dataset to get estimated coefficients as a sanity check:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dat }\OtherTok{\textless{}{-}} \FunctionTok{generate}\NormalTok{( model, }
                 \AttributeTok{n\_within =} \DecValTok{100}\NormalTok{, }
                 \AttributeTok{n\_between =} \DecValTok{100}\NormalTok{ ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{as\_tibble}\NormalTok{()}
\NormalTok{M }\OtherTok{=}\NormalTok{ lme4}\SpecialCharTok{::}\FunctionTok{lmer}\NormalTok{( Y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ X }\SpecialCharTok{+}\NormalTok{ W }\SpecialCharTok{+}\NormalTok{ (}\DecValTok{1}\SpecialCharTok{|}\StringTok{\textasciigrave{}}\AttributeTok{\_id}\StringTok{\textasciigrave{}}\NormalTok{), }\AttributeTok{data=}\NormalTok{dat )}
\NormalTok{arm}\SpecialCharTok{::}\FunctionTok{display}\NormalTok{(M)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## lme4::lmer(formula = Y ~ X + W + (1 | `_id`), data = dat)
##             coef.est coef.se
## (Intercept) 10.04     0.16  
## X            2.33     0.04  
## W            0.07     0.16  
## 
## Error terms:
##  Groups   Name        Std.Dev.
##  _id      (Intercept) 1.56    
##  Residual             4.18    
## ---
## number of obs: 10000, groups: _id, 100
## AIC = 57273, DIC = 57246.8
## deviance = 57254.9
\end{verbatim}

We can check our ICC:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sigma2\_2 }\OtherTok{=} \FunctionTok{VarCorr}\NormalTok{(M)}\SpecialCharTok{$}\StringTok{\textasciigrave{}}\AttributeTok{\_id}\StringTok{\textasciigrave{}}\NormalTok{[}\DecValTok{1}\NormalTok{]}
\NormalTok{sigma2\_1 }\OtherTok{=} \FunctionTok{attr}\NormalTok{(}\FunctionTok{VarCorr}\NormalTok{(M), }\StringTok{"sc"}\NormalTok{)}\SpecialCharTok{\^{}}\DecValTok{2}
\NormalTok{sigma2\_2 }\SpecialCharTok{/}\NormalTok{ (sigma2\_2 }\SpecialCharTok{+}\NormalTok{ sigma2\_1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.1215877
\end{verbatim}

Close to the listed 0.10. More testing is needed.

\chapter{Simulation under the Potential Outcomes Framework}\label{potential-outcomes}

If we are in the business of evaluating how various methods such as matching or propensity score weighting work in practice, we would probably turn to the potential outcomes framework for our simulations.
The potential outcomes framework is a framework typically used in the causal inference literature to make very explicit statements regarding the mechanics of causality and the associated estimands one might target when estimating causal effects.
While we recommend reading, for a more thourough overview, either {[}CITE Raudenbush or Field Experiments textbook{]}, we briefly outline this framework here to set out our notation.

Take a sample of experimental units, indexed by \(i\).
For each unit, we can treat it or not.
Denote treatment as \(Z_i = 1\) for treated or \(Z_i = 0\) for not treated.
Now we imagine each unit has two potential outcomes being the outcome we would see if we treated it (\(Y_i(1)\)) or if we did not (\(Y_i(0)\)).
Finally, our observed outcome is then
\[ Y_i^{obs} = Z_i Y_i(1) + (1-Z_i)Y_i(0) .\]
For a unit, the treatment effect is \(\tau_i = Y_i(1) - Y_i(0)\); it is how much our outcome changes if we treat vs.~not treat.
Frustratingly, for each unit we can only see one of its two potential outcomes, so we can never get an estimate of these individual \(\tau_i\).
Under this view, causality is a missing data problem: if we only were able to impute the missing potential outcomes, we could have a dataset where we could calculate any estimands we wanted. E.g., the true average treatment effect \emph{for the sample} \(\mathcal{S}\) would be:

\[ ATE_{\mathcal{S}} = \frac{1}{N} \sum_{i} Y_i(1) - Y_i( 0 ) . \]
The average proportion increase, by contrast, would be

\[ API_{\mathcal{S}} = \frac{1}{N} \sum_{i} \frac{Y_i(1)}{Y_i(0)} \]

\section{Finite vs.~Superpopulation inference}\label{finite-vs.-superpopulation-inference}

Consider a sample of \(n\) units, \(\mathcal{S}\), along with their set of potential outcomes.
We can talk about the true ATE of the sample, or, if we thought of the sample as being drawn from some larger population, we could talk about the true ATE of that larger population.

This is a tension that often arises in potential outcomes based simulations: if we are focused on \(ATE_{\mathcal{S}}\) then for each sample we generate, our estimand could be (maybe only slightly) different, depending on whether our sample has more or fewer units with high \(\tau_i\).
If, on the other hand, we are focused on where the units came from (which is our data generating model), our estimand is a property of the DGP, and would be the same for each sample generated.

The catch is when we calculate our performance metrics, we now have two possible targets to pick from.
Furthermore, if we are targeting the superpopulation ATE, then our error in estimation may be due in part to the representativeness of the sample, \emph{not} the estimation or uncertainty due to the random assignment.

We will follow this theme throughout this chapter.

\section{Data generation processes for potential outcomes}\label{data-generation-processes-for-potential-outcomes}

If we want to write a simulation using the potential outcomes framework, it is clear and transparent to first generate a complete set of potential outcomes, then generate a random assignment based on some assignment mechanism, and finally generate the observed outcomes as a function of assignment and original potential outcomes.

For example, we might say that our data generation process is as follows: First generate each unit \(i = 1, \ldots, n\), as
\[
\begin{aligned}
X_i &\sim exp( 1 ) - 1 \\
Y_i(0) &= \beta_0 + \beta_1 X_i + \epsilon_i \mbox{ with } \epsilon_i \sim N( 0, \sigma^2 ) \\
\tau_i &= \tau_0 + \tau_1 X_i + \alpha u_i \mbox{ with } u_i \sim t_{df} \\
Y_i(1) &= Y_i(0) + \tau_i 
\end{aligned}
\]
with \(exp(1)\) being the standard exponential distribution and \(t_{df}\) being a \(t\) distribution with \(df\) degrees of freedom.
We subtract 1 from \(X_i\) to zero-center it (it is often convenient to have zero-centered covariates so we can then, e.g., interpret \(\tau_0\) as the true superpopulation ATE of our experiment).

The above model is saying that we first, for each unit, generate a covariate.
We then generate our two potential outcomes.
I.e., we are generating what the outcome would be for each unit if it were treated and if it were not treated.
We are driving both the level and the treatment effect with \(X_i\), assuming \(\beta_1\) and \(\tau_1\) are non-zero.

One advantage of generating all the potential outcomes is we can then calculate the finite-sample estimands such as the true average treatment effect for the generated sample: we just take the average of \(Y_i(1) - Y_i(0)\) for our sample.

Here is some code to illustrate the first part of the data generating process (we leave treatment assignment to later):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gen\_data }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{( }\AttributeTok{n =} \DecValTok{100}\NormalTok{,}
                      \AttributeTok{R2 =} \FloatTok{0.5}\NormalTok{,}
                      \AttributeTok{beta\_0 =} \DecValTok{0}\NormalTok{, }\AttributeTok{beta\_1 =} \DecValTok{1}\NormalTok{,}
                      \AttributeTok{tau\_0 =} \DecValTok{1}\NormalTok{, }\AttributeTok{tau\_1 =} \DecValTok{1}\NormalTok{, }
                      \AttributeTok{alpha =} \DecValTok{1}\NormalTok{, }\AttributeTok{df =} \DecValTok{3}\NormalTok{ ) \{}
  \FunctionTok{stopifnot}\NormalTok{( R2 }\SpecialCharTok{\textgreater{}=} \DecValTok{0} \SpecialCharTok{\&\&}\NormalTok{ R2 }\SpecialCharTok{\textless{}} \DecValTok{1}\NormalTok{ )}
\NormalTok{  X\_i }\OtherTok{=} \FunctionTok{rexp}\NormalTok{( n, }\AttributeTok{rate =} \DecValTok{1}\NormalTok{ ) }\SpecialCharTok{{-}} \DecValTok{1}
\NormalTok{  beta\_1 }\OtherTok{=} \FunctionTok{sqrt}\NormalTok{( }\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ R2 )}
\NormalTok{  sigma\_e }\OtherTok{=} \FunctionTok{sqrt}\NormalTok{( R2 )}
\NormalTok{  Y0\_i }\OtherTok{=}\NormalTok{ beta\_0 }\SpecialCharTok{+}\NormalTok{ beta\_1 }\SpecialCharTok{*}\NormalTok{ X\_i }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{( n, }\AttributeTok{sd=}\NormalTok{sigma\_e )}
\NormalTok{  tau\_i }\OtherTok{=}\NormalTok{ tau\_0 }\SpecialCharTok{+}\NormalTok{ tau\_1 }\SpecialCharTok{*}\NormalTok{ X\_i }\SpecialCharTok{+}\NormalTok{ alpha }\SpecialCharTok{*} \FunctionTok{rt}\NormalTok{( n, }\AttributeTok{df =}\NormalTok{ df )}
\NormalTok{  Y1\_i }\OtherTok{=}\NormalTok{ Y0\_i }\SpecialCharTok{+}\NormalTok{ tau\_i}
  
  \FunctionTok{tibble}\NormalTok{( }\AttributeTok{X =}\NormalTok{ X\_i, }\AttributeTok{Y0 =}\NormalTok{ Y0\_i, }\AttributeTok{Y1 =}\NormalTok{ Y1\_i )}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

And now we see our estimand can change:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{( }\DecValTok{40454}\NormalTok{ )}
\NormalTok{d1 }\OtherTok{\textless{}{-}} \FunctionTok{gen\_data}\NormalTok{( }\DecValTok{50}\NormalTok{ )}
\FunctionTok{mean}\NormalTok{( d1}\SpecialCharTok{$}\NormalTok{Y1 }\SpecialCharTok{{-}}\NormalTok{ d1}\SpecialCharTok{$}\NormalTok{Y0 )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.6374925
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{d2 }\OtherTok{\textless{}{-}} \FunctionTok{gen\_data}\NormalTok{( }\DecValTok{50}\NormalTok{ )}
\FunctionTok{mean}\NormalTok{( d2}\SpecialCharTok{$}\NormalTok{Y1 }\SpecialCharTok{{-}}\NormalTok{ d2}\SpecialCharTok{$}\NormalTok{Y0 )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.5479788
\end{verbatim}

In reviewing our code, we know our superpopulation ATE should be \texttt{tau}, or 1 exactly.
If our estimate for \texttt{d1} is 0.6 do we say that is close or far from the target?
From a finite sample performance approach, we nailed it.
From superpopulation, less so.

Also in looking at the above, there are a few details to call out:

\begin{itemize}
\tightlist
\item
  We can store the latent, intermediate quantities (both potential outcomes, in particular) so we can calculate the estimands of interest or learn about our data generating process. When we hand the data to an estimator, we would not provide this ``secret'' information.
\item
  We are using a trick to index our DGP by an R2 value rather than coefficients on X so we can have a standardized control-side outcome (the expected variation of \(Y_i(0)\) will be 1). The treatment outcomes will have more variation due to the heterogeniety of the treatment impacts.
\item
  If we were generating data with a constant treatment impact, then \(ATE_{\mathcal{S}} = ATE\) always; this is typical for many similations in the literature. That being said, treatment variation is what causes a lot of methods to fail and so having simulations that have this variation is usually important.
\end{itemize}

Once we have our \emph{schedule of potential outcomes}, we would then generate the \emph{observed outcomes} by assigning our (synthetic, randomly generated) \(n\) units to treatment or control.
For example, say we wanted to simulate an observational context where treatment was a function of our covariate.
We could model each unit as flipping a weighted coin with some probability that was a function of \(X_i\) as so:

\[
\begin{aligned}
p_i &= logit^{-1}( \xi_0 + \xi_1 X_i ) \\
Z_i &= Bern( p_i ) \\
Y_i &= Z_i Y_i(1) + (1-Z_i) Y_i(0) 
\end{aligned}
\]

Here is code for assigning our data to treatment and control:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{assign\_data }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{( dat,}
                         \AttributeTok{xi\_0 =} \SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\AttributeTok{xi\_1 =} \DecValTok{1}\NormalTok{ ) \{}
\NormalTok{  n }\OtherTok{=} \FunctionTok{nrow}\NormalTok{(dat)}
\NormalTok{  dat }\OtherTok{=} \FunctionTok{mutate}\NormalTok{( dat,}
                \AttributeTok{p =}\NormalTok{ arm}\SpecialCharTok{::}\FunctionTok{invlogit}\NormalTok{( xi\_0 }\SpecialCharTok{+}\NormalTok{ xi\_1 }\SpecialCharTok{*}\NormalTok{ X ),}
                \AttributeTok{Z =} \FunctionTok{rbinom}\NormalTok{( n, }\DecValTok{1}\NormalTok{, }\AttributeTok{prob=}\NormalTok{p ),}
                \AttributeTok{Yobs =} \FunctionTok{ifelse}\NormalTok{( Z }\SpecialCharTok{==} \DecValTok{1}\NormalTok{, Y1, Y0 ) )}
\NormalTok{  dat}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

We can then add our assignment variable to our given data as so:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{assign\_data}\NormalTok{( d2 )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 50 x 6
##           X     Y0      Y1     p     Z    Yobs
##       <dbl>  <dbl>   <dbl> <dbl> <int>   <dbl>
##  1  0.670    0.667   2.58  0.418     1   2.58 
##  2  0.371    0.314   4.57  0.348     1   4.57 
##  3  1.94     1.29    3.03  0.719     0   1.29 
##  4 -0.244    0.119 -10.0   0.224     1 -10.0  
##  5  0.00850  1.44    2.88  0.271     0   1.44 
##  6  1.41     1.14    5.02  0.600     1   5.02 
##  7 -0.864    0.461   0.802 0.134     1   0.802
##  8 -0.00533 -0.914  -1.17  0.268     0  -0.914
##  9 -0.907   -0.202   0.555 0.129     1   0.555
## 10 -0.363   -0.141   1.16  0.204     1   1.16 
## # i 40 more rows
\end{verbatim}

Note how \texttt{Yobs} is, depending on \texttt{Z}, either \texttt{Y0} or \texttt{Y1}.
Separating our our DGP and our random assignment underscores the potential outcomes framework adage of the data are what they are, and we the experimenters (or nature) is randomly assigning these whole units to various conditions and observing the consequences.

In general, we might instead put the \texttt{p\_i} part of the model in our code generating the outcomes, if we wanted to view the chance of treatment assignment as inherent to the unit (which is what we usually expect in an observational context).

\section{Finite sample performance measures}\label{finite-sample-performance-measures}

Let's generate a single dataset with our DGP from above, and run a small experiment where we actually randomize units to treatment and control:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n }\OtherTok{=} \DecValTok{100}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{442423}\NormalTok{)}
\NormalTok{dat }\OtherTok{=} \FunctionTok{gen\_data}\NormalTok{(n, }\AttributeTok{tau\_1 =} \SpecialCharTok{{-}}\DecValTok{1}\NormalTok{)}
\NormalTok{dat }\OtherTok{=} \FunctionTok{mutate}\NormalTok{( dat,}
              \AttributeTok{Z =} \DecValTok{0} \SpecialCharTok{+}\NormalTok{ (}\FunctionTok{sample}\NormalTok{( n ) }\SpecialCharTok{\textless{}=}\NormalTok{ n}\SpecialCharTok{/}\DecValTok{2}\NormalTok{),}
              \AttributeTok{Yobs =} \FunctionTok{ifelse}\NormalTok{( Z }\SpecialCharTok{==} \DecValTok{1}\NormalTok{, Y1, Y0 ) )}
\NormalTok{mod }\OtherTok{=} \FunctionTok{lm}\NormalTok{( Yobs }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Z, }\AttributeTok{data=}\NormalTok{dat )}
\FunctionTok{coef}\NormalTok{(mod)[[}\StringTok{"Z"}\NormalTok{]]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.8914992
\end{verbatim}

We can compare this to the true finite-sample ATE:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mean}\NormalTok{( dat}\SpecialCharTok{$}\NormalTok{Y1 }\SpecialCharTok{{-}}\NormalTok{ dat}\SpecialCharTok{$}\NormalTok{Y0 )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1.154018
\end{verbatim}

Our finite-population simulation would be:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rps }\OtherTok{\textless{}{-}} \FunctionTok{rerun}\NormalTok{( }\DecValTok{1000}\NormalTok{, \{}
\NormalTok{  dat }\OtherTok{=} \FunctionTok{mutate}\NormalTok{( dat,}
              \AttributeTok{Z =} \DecValTok{0} \SpecialCharTok{+}\NormalTok{ (}\FunctionTok{sample}\NormalTok{( n ) }\SpecialCharTok{\textless{}=}\NormalTok{ n}\SpecialCharTok{/}\DecValTok{2}\NormalTok{),}
              \AttributeTok{Yobs =} \FunctionTok{ifelse}\NormalTok{( Z }\SpecialCharTok{==} \DecValTok{1}\NormalTok{, Y1, Y0 ) )}
\NormalTok{  mod }\OtherTok{=} \FunctionTok{lm}\NormalTok{( Yobs }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Z, }\AttributeTok{data=}\NormalTok{dat )}
  \FunctionTok{tibble}\NormalTok{( }\AttributeTok{ATE\_hat =} \FunctionTok{coef}\NormalTok{(mod)[[}\StringTok{"Z"}\NormalTok{]],}
          \AttributeTok{SE\_hat =}\NormalTok{ arm}\SpecialCharTok{::}\FunctionTok{se.coef}\NormalTok{(mod)[[}\StringTok{"Z"}\NormalTok{]] )}
\NormalTok{  \}) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{bind\_rows}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: `rerun()` was deprecated in purrr 1.0.0.
## i Please use `map()` instead.
##   # Previously
## rerun(1000, {
## dat = mutate(dat, Z = 0 + (sample(n) <= n / 2),
## Yobs = ifelse(Z == 1, Y1, Y0))
## mod = lm(Yobs ~ Z, data = dat)
## tibble(ATE_hat = coef(mod)[["Z"]], SE_hat =
## arm::se.coef(
## mod)[["Z"]])
## })
## 
##   # Now
## map(1:1000, ~ {
## dat = mutate(dat, Z = 0 + (sample(n) <= n / 2),
## Yobs = ifelse(Z == 1, Y1, Y0))
## mod = lm(Yobs ~ Z, data = dat)
## tibble(ATE_hat = coef(mod)[["Z"]], SE_hat =
## arm::se.coef(
## mod)[["Z"]])
## })
## This warning is displayed once every 8 hours.
## Call `lifecycle::last_lifecycle_warnings()` to
## see where this warning was generated.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rps }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{summarise}\NormalTok{( }\AttributeTok{EATE\_hat =} \FunctionTok{mean}\NormalTok{( ATE\_hat ),}
                   \AttributeTok{SE =} \FunctionTok{sd}\NormalTok{( ATE\_hat ),}
                   \AttributeTok{ESE\_hat =} \FunctionTok{mean}\NormalTok{( SE\_hat ) )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1 x 3
##   EATE_hat    SE ESE_hat
##      <dbl> <dbl>   <dbl>
## 1     1.16 0.248   0.307
\end{verbatim}

We are simulating on a single dataset.
In particular, our set of potential outcomes is entirely fixed; the only source of randomness (and thus the randomness behind our SE) is the random assignment.
Now this opens up some room for critique: what if our single dataset is non-standard?

Our super-population simulation would be, by contrast:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rps\_sup }\OtherTok{\textless{}{-}} \FunctionTok{rerun}\NormalTok{( }\DecValTok{1000}\NormalTok{, \{}
\NormalTok{  dat }\OtherTok{=} \FunctionTok{gen\_data}\NormalTok{(n)}
\NormalTok{  dat }\OtherTok{=} \FunctionTok{mutate}\NormalTok{( dat,}
              \AttributeTok{Z =} \DecValTok{0} \SpecialCharTok{+}\NormalTok{ (}\FunctionTok{sample}\NormalTok{( n ) }\SpecialCharTok{\textless{}=}\NormalTok{ n}\SpecialCharTok{/}\DecValTok{2}\NormalTok{),}
              \AttributeTok{Yobs =} \FunctionTok{ifelse}\NormalTok{( Z }\SpecialCharTok{==} \DecValTok{1}\NormalTok{, Y1, Y0 ) )}
\NormalTok{  mod }\OtherTok{=} \FunctionTok{lm}\NormalTok{( Yobs }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Z, }\AttributeTok{data=}\NormalTok{dat )}
  \FunctionTok{tibble}\NormalTok{( }\AttributeTok{ATE\_hat =} \FunctionTok{coef}\NormalTok{(mod)[[}\StringTok{"Z"}\NormalTok{]],}
          \AttributeTok{SE\_hat =}\NormalTok{ arm}\SpecialCharTok{::}\FunctionTok{se.coef}\NormalTok{(mod)[[}\StringTok{"Z"}\NormalTok{]] )}
\NormalTok{  \}) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{bind\_rows}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: `rerun()` was deprecated in purrr 1.0.0.
## i Please use `map()` instead.
##   # Previously
## rerun(1000, {
## dat = gen_data(n)
## dat = mutate(dat, Z = 0 + (sample(n) <= n / 2),
## Yobs = ifelse(Z == 1, Y1, Y0))
## mod = lm(Yobs ~ Z, data = dat)
## tibble(ATE_hat = coef(mod)[["Z"]], SE_hat =
## arm::se.coef(
## mod)[["Z"]])
## })
## 
##   # Now
## map(1:1000, ~ {
## dat = gen_data(n)
## dat = mutate(dat, Z = 0 + (sample(n) <= n / 2),
## Yobs = ifelse(Z == 1, Y1, Y0))
## mod = lm(Yobs ~ Z, data = dat)
## tibble(ATE_hat = coef(mod)[["Z"]], SE_hat =
## arm::se.coef(
## mod)[["Z"]])
## })
## This warning is displayed once every 8 hours.
## Call `lifecycle::last_lifecycle_warnings()` to
## see where this warning was generated.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rps\_sup }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{summarise}\NormalTok{( }\AttributeTok{EATE\_hat =} \FunctionTok{mean}\NormalTok{( ATE\_hat ),}
                   \AttributeTok{SE =} \FunctionTok{sd}\NormalTok{( ATE\_hat ),}
                   \AttributeTok{ESE\_hat =} \FunctionTok{mean}\NormalTok{( SE\_hat ))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1 x 3
##   EATE_hat    SE ESE_hat
##      <dbl> <dbl>   <dbl>
## 1    1.000 0.381   0.378
\end{verbatim}

First, note our superpopulation simulation is not biased for the superpopulation ATE.
Also note the true SE is larger than our finite-sample simulation; this is because part of the uncertainty in our estimator is the uncertainty of whether our sample is representative of the superpopulation.

Finally, this clarifies that our linear regression estimator is estimating standard errors assuming a superpopulation model.
The true finite sample standard error is less than the expected estimated error: from a finite sample perspective, our estimator is giving overly conservative uncertainty estimates.
(This discrepancy is often called the correlation of potential outcomes problem.)

\section{Nested finite simulation procedure}\label{nested-finite-simulation-procedure}

We just saw a difference between a specific, single, finite-sample dataset and a superpopulation.
What if we wanted to know if this phenomenon was more general across a set of datasets?
This question can be levied more broadly: if we run a simulation on a single dataset, this is even more narrow than running on a single scenario: if we compare methods and find one is superior to another for our single dataset, how do we know this is not an artifact of some specific characteristic of \emph{that data} and not a general phenomonen at all?

One way forward is to run a nested simulation, where we generate a series of finite sample datasets, and then for each dataset run a small simulation.
We then calculate the expected finite sample performance across the datasets.
One could almost think of the datasets themselves as a ``factor'' in our multifactor experiment.
This is what we did in {[}CITE estimands paper{]}

Borrowing from the simulation appendix of {[}CITE estimands paper{]}, repeat \(R\) times:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Generate a dataset using a particular DGP. This data generation is the ``sampling step'' for a superpopulation (SP) framework. The DGP represents an innite superpopulation. Each dataset includes, for each observation, the potential outcome under treatment or control.
\item
  Record the true finite-sample ATE, both person and site weighted.
\item
  Then, three times, do a finite simulation as follows:
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  Randomize units to treatment and control.
\item
  Calculate the corresponding observed outcomes.
\item
  Analyze the results using the methods of interest, recording both the point estimate and estimated standard error for each.
\end{enumerate}

Having only three trials will give a poor estimate of within-dataset variability for each dataset, but the average across the
\(R\) datasets in a given scenario gives a reasonable estimate of expected variability across datasets of the type we would see given the scenario parameters.

To demonstrate we first make a mini-finite sample driver:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{one\_finite\_run }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{( }\AttributeTok{R0 =} \DecValTok{3}\NormalTok{, }\AttributeTok{n =} \DecValTok{100}\NormalTok{, ... ) \{}
\NormalTok{  dat }\OtherTok{=} \FunctionTok{gen\_data}\NormalTok{( }\AttributeTok{n =}\NormalTok{ n, ... )}
\NormalTok{  rps }\OtherTok{\textless{}{-}} \FunctionTok{rerun}\NormalTok{( R0, \{}
\NormalTok{         dat }\OtherTok{=} \FunctionTok{mutate}\NormalTok{( dat,}
                    \AttributeTok{Z =} \DecValTok{0} \SpecialCharTok{+}\NormalTok{ (}\FunctionTok{sample}\NormalTok{( n ) }\SpecialCharTok{\textless{}=}\NormalTok{ n}\SpecialCharTok{/}\DecValTok{2}\NormalTok{),}
                    \AttributeTok{Yobs =} \FunctionTok{ifelse}\NormalTok{( Z }\SpecialCharTok{==} \DecValTok{1}\NormalTok{, Y1, Y0 ) )}
\NormalTok{        mod }\OtherTok{=} \FunctionTok{lm}\NormalTok{( Yobs }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Z, }\AttributeTok{data=}\NormalTok{dat )}
        \FunctionTok{tibble}\NormalTok{( }\AttributeTok{ATE\_hat =} \FunctionTok{coef}\NormalTok{(mod)[[}\StringTok{"Z"}\NormalTok{]],}
                \AttributeTok{SE\_hat =}\NormalTok{ arm}\SpecialCharTok{::}\FunctionTok{se.coef}\NormalTok{(mod)[[}\StringTok{"Z"}\NormalTok{]] )}
\NormalTok{    \}) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{bind\_rows}\NormalTok{()}
\NormalTok{  rps}\SpecialCharTok{$}\NormalTok{ATE }\OtherTok{=} \FunctionTok{mean}\NormalTok{( dat}\SpecialCharTok{$}\NormalTok{Y1 }\SpecialCharTok{{-}}\NormalTok{ dat}\SpecialCharTok{$}\NormalTok{Y0 )}
\NormalTok{  rps}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

This driver also stores the finite sample ATE for future reference:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{one\_finite\_run}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: `rerun()` was deprecated in purrr 1.0.0.
## i Please use `map()` instead.
##   # Previously
## rerun(3, {
## dat = mutate(dat, Z = 0 + (sample(n) <= n / 2),
## Yobs = ifelse(Z == 1, Y1, Y0))
## mod = lm(Yobs ~ Z, data = dat)
## tibble(ATE_hat = coef(mod)[["Z"]], SE_hat =
## arm::se.coef(
## mod)[["Z"]])
## })
## 
##   # Now
## map(1:3, ~ {
## dat = mutate(dat, Z = 0 + (sample(n) <= n / 2),
## Yobs = ifelse(Z == 1, Y1, Y0))
## mod = lm(Yobs ~ Z, data = dat)
## tibble(ATE_hat = coef(mod)[["Z"]], SE_hat =
## arm::se.coef(
## mod)[["Z"]])
## })
## This warning is displayed once every 8 hours.
## Call `lifecycle::last_lifecycle_warnings()` to
## see where this warning was generated.
\end{verbatim}

\begin{verbatim}
## # A tibble: 3 x 3
##   ATE_hat SE_hat   ATE
##     <dbl>  <dbl> <dbl>
## 1   0.348  0.421 0.768
## 2   1.32   0.472 0.768
## 3   1.17   0.549 0.768
\end{verbatim}

We then run a bunch of finite runs.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{runs }\OtherTok{\textless{}{-}} \FunctionTok{rerun}\NormalTok{( }\DecValTok{500}\NormalTok{, }\FunctionTok{one\_finite\_run}\NormalTok{() ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{bind\_rows}\NormalTok{( }\AttributeTok{.id =} \StringTok{"runID"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: `rerun()` was deprecated in purrr 1.0.0.
## i Please use `map()` instead.
##   # Previously
##   rerun(500, one_finite_run())
## 
##   # Now
##   map(1:500, ~ one_finite_run())
## This warning is displayed once every 8 hours.
## Call `lifecycle::last_lifecycle_warnings()` to
## see where this warning was generated.
\end{verbatim}

We use \texttt{.id} because we will need to separate out each finite run and analyze separately, and then aggregate.

Each finite run is a very noisy simulation for a fixed dataset.
This means when we calculate performance measures we have to be careful to avoid bias in the calculations; in particular, we need to focus on estimating \(SE^2\) across the finite runs, not \(SE\), to avoid the bias caused by having a few observations with every estimate.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fruns }\OtherTok{\textless{}{-}}\NormalTok{ runs }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{group\_by}\NormalTok{( runID ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{( }\AttributeTok{EATE\_hat =} \FunctionTok{mean}\NormalTok{( ATE\_hat ),}
             \AttributeTok{SE2 =} \FunctionTok{var}\NormalTok{( ATE\_hat ),}
             \AttributeTok{ESE\_hat =} \FunctionTok{mean}\NormalTok{( SE\_hat ),}
             \AttributeTok{.groups =} \StringTok{"drop"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

And then we aggregate our finite sample runs:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{res }\OtherTok{\textless{}{-}}\NormalTok{ fruns }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{( }\AttributeTok{EEATE\_hat =} \FunctionTok{mean}\NormalTok{( EATE\_hat ),}
             \AttributeTok{EESE\_hat =} \FunctionTok{sqrt}\NormalTok{( }\FunctionTok{mean}\NormalTok{( ESE\_hat}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{ ) ),}
             \AttributeTok{ESE =} \FunctionTok{sqrt}\NormalTok{( }\FunctionTok{mean}\NormalTok{( SE2 ) ) ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{( }\AttributeTok{calib =} \DecValTok{100} \SpecialCharTok{*}\NormalTok{ EESE\_hat }\SpecialCharTok{/}\NormalTok{ ESE )}

\NormalTok{res}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1 x 4
##   EEATE_hat EESE_hat   ESE calib
##       <dbl>    <dbl> <dbl> <dbl>
## 1     0.996    0.380 0.331  115.
\end{verbatim}

We see our expected standard error estimate is, across the collection of finite sample scenarios all sharing a similar parent superpopulation DGP, 15\% too large for the true expected finite-sample standard error.

We need to keep the squaring. If we look at the SEs themselves, we have further apparent bias due to our \emph{estimated} \texttt{ESE\_hat} being so unstable due to too few observations:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mean}\NormalTok{( }\FunctionTok{sqrt}\NormalTok{( fruns}\SpecialCharTok{$}\NormalTok{SE2 ) )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.2944556
\end{verbatim}

We can use our collection of mini-finite-sample runs to estimate superpopulation quantities as well.
Given that the simulation datasets are i.i.d. draws, we can simply take expectations across all our simulations.
The only concern is our estimates of MCSE will be off due to the clustering in our simulation runs.

Here we calculate superpopulation performance measures (both with the squared SE and without; we prefer the squared version):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{runs }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{( }\AttributeTok{EATE\_hat =} \FunctionTok{mean}\NormalTok{( ATE\_hat ),}
             \AttributeTok{SE\_true =} \FunctionTok{sd}\NormalTok{( ATE\_hat ),}
             \AttributeTok{SE\_hat =} \FunctionTok{mean}\NormalTok{( SE\_hat ),}
             \AttributeTok{SE2\_true =} \FunctionTok{var}\NormalTok{( ATE\_hat ),}
             \AttributeTok{SE2\_hat =} \FunctionTok{mean}\NormalTok{( SE\_hat}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{ ) ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{pivot\_longer}\NormalTok{( }\AttributeTok{cols =} \FunctionTok{c}\NormalTok{(SE\_true}\SpecialCharTok{:}\NormalTok{SE2\_hat ),}
                \AttributeTok{names\_to =} \FunctionTok{c}\NormalTok{( }\StringTok{"estimand"}\NormalTok{, }\StringTok{".value"}\NormalTok{ ),}
                \AttributeTok{names\_sep =}\StringTok{"\_"}\NormalTok{ ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{( }\AttributeTok{inflate =} \DecValTok{100} \SpecialCharTok{*}\NormalTok{ hat }\SpecialCharTok{/}\NormalTok{ true )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 2 x 5
##   EATE_hat estimand  true   hat inflate
##      <dbl> <chr>    <dbl> <dbl>   <dbl>
## 1    0.996 SE       0.389 0.377    96.9
## 2    0.996 SE2      0.151 0.142    93.9
\end{verbatim}

\chapter{The Parametric bootstrap}\label{the-parametric-bootstrap}

An inference procedure very much connected to simulation studies is the parametric bootstrap.
The parametric bootstrap is a bootstrap technique designed to obtain standard error estimates for an estimated parametric model.
It can do better than the case-wise bootstrap in some circumstances, usually when there is need to avoid the discrete, chunky nature of a casewise bootstrap (which will only give values that exist in the original dataset).

For a parametric bootstrap, the core idea is to fit a given model to actual data, and then take the parameters we estimate from that model as the DGP parameters in a simulation study.
The parametric bootstrap is a simulation study for a specific scenario, and our goal is to assess how variable (and, possibly, biased) our estimator is for this specific scenario.
If the behavior of our estimator in our simulated scenario is similar to what it would be under repeated trials in the real world, then our bootstrap answers will be informative as to how well our original estimator performs in practice.
This is the bootstrap principle, or analogy with an additional assumption that the real-world is effectively well specified as the parameteric model we are fitting.

In particular we do the following:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  generate data from a model with coefficients as estimated on the original data.
\item
  repeatedly estimate our target quantity on a series of synthetic data sets, all generated from this model.
\item
  examine this collection of estimates to assess the character of the estimates themselves, i.e.~how much they vary, whether we are systematically estimating too high or too low, and so forth.
\item
  The variance and bias of our estimates in our simulation is probably like the actual variance and bias of our original estimate (this is precisely the bootstrap analogy).
\end{enumerate}

A key feature of the parametric bootstrap is it is not, generally, a multifactor simulation experiment.
We fit our model to the data, and use our best estimate of the world, as given by the fit model, to generate our data.
This means we generally want to simulate in contexts that are (mostly) \emph{pivotal}, meaning the distribution of our test statistic or point estimate is relatively stable across different scenarios.
In other words, we want the uncertainty of our estimator to not heavily depend on the exact parameter values we use in our simulation, so that if we are simulating with incorrect parameters our bootstrap analogy will still hold.

Often, to achieve a reasonable claim of being pivotal, we will focus on standardized statistics, such as the \(t\)-statistic of

\[ t = \frac{est}{\widehat{SE}} \]
It is more common for the distribution of a standardized test statistic to have a canonical distribution across scenarios than an absolute estimate.

\section{Air conditioners: a stolen case study}\label{air-conditioners-a-stolen-case-study}

Following the case study presented in {[}CITE bootstrap book{]}, consider some failure times of air conditioning units:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dat }\OtherTok{=} \FunctionTok{c}\NormalTok{( }\DecValTok{3}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{7}\NormalTok{, }\DecValTok{18}\NormalTok{, }\DecValTok{43}\NormalTok{, }\DecValTok{85}\NormalTok{, }\DecValTok{91}\NormalTok{, }\DecValTok{98}\NormalTok{, }\DecValTok{100}\NormalTok{, }\DecValTok{130}\NormalTok{, }\DecValTok{230}\NormalTok{, }\DecValTok{487}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

We are interested in the log of the average failure time:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n }\OtherTok{=} \FunctionTok{length}\NormalTok{(dat)}
\NormalTok{y.bar }\OtherTok{=} \FunctionTok{mean}\NormalTok{(dat)}
\NormalTok{theta.hat }\OtherTok{=} \FunctionTok{log}\NormalTok{( y.bar )}

\FunctionTok{c}\NormalTok{( }\AttributeTok{n =}\NormalTok{ n, }\AttributeTok{y.bar =}\NormalTok{ y.bar, }\AttributeTok{theta.hat =}\NormalTok{ theta.hat )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##          n      y.bar  theta.hat 
##  12.000000 108.083333   4.682903
\end{verbatim}

We are interested in this because we are modeling the failure time of the air conditioners with an exponential distribution.
This means we will generate new failure times with an exponential distribution:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{reps }\OtherTok{=} \FunctionTok{replicate}\NormalTok{( }\DecValTok{10000}\NormalTok{, \{}
\NormalTok{    smp }\OtherTok{=} \FunctionTok{rexp}\NormalTok{(n, }\DecValTok{1}\SpecialCharTok{/}\NormalTok{y.bar)}
    \FunctionTok{log}\NormalTok{( }\FunctionTok{mean}\NormalTok{( smp ) )}
\NormalTok{\})}

\NormalTok{res\_par }\OtherTok{=} \FunctionTok{tibble}\NormalTok{( }
  \AttributeTok{bias.hat =} \FunctionTok{mean}\NormalTok{( reps ) }\SpecialCharTok{{-}}\NormalTok{ theta.hat,}
  \AttributeTok{var.hat =} \FunctionTok{var}\NormalTok{( reps ),}
  \AttributeTok{CIlog\_low =}\NormalTok{ theta.hat }\SpecialCharTok{+}\NormalTok{ bias.hat }\SpecialCharTok{{-}} \FunctionTok{sqrt}\NormalTok{(var.hat) }\SpecialCharTok{*} \FunctionTok{qnorm}\NormalTok{(}\FloatTok{0.975}\NormalTok{),}
  \AttributeTok{CIlog\_high =}\NormalTok{ theta.hat }\SpecialCharTok{+}\NormalTok{ bias.hat }\SpecialCharTok{{-}} \FunctionTok{sqrt}\NormalTok{(var.hat) }\SpecialCharTok{*} \FunctionTok{qnorm}\NormalTok{(}\FloatTok{0.025}\NormalTok{),}
  \AttributeTok{CI\_low =} \FunctionTok{exp}\NormalTok{( CIlog\_low ),}
  \AttributeTok{CI\_high =} \FunctionTok{exp}\NormalTok{( CIlog\_high ) )}
\NormalTok{res\_par}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1 x 6
##   bias.hat var.hat CIlog_low CIlog_high CI_low
##      <dbl>   <dbl>     <dbl>      <dbl>  <dbl>
## 1  -0.0420  0.0856      4.07       5.21   58.4
## # i 1 more variable: CI_high <dbl>
\end{verbatim}

Note how we are, as usual, in our standard simulation framework of repeatidly (1) generating data and (2) analyzing the simulated data.
Nothing is changed.

The nonparametric, or case-wise, bootstrap (this is what people normally mean when they say bootstrap) would look like this:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{reps }\OtherTok{=} \FunctionTok{replicate}\NormalTok{( }\DecValTok{10000}\NormalTok{, \{}
\NormalTok{    smp }\OtherTok{=} \FunctionTok{sample}\NormalTok{( dat, }\AttributeTok{replace=}\ConstantTok{TRUE}\NormalTok{ )}
    \FunctionTok{log}\NormalTok{( }\FunctionTok{mean}\NormalTok{( smp ) )}
\NormalTok{\})}

\NormalTok{res\_np }\OtherTok{=} \FunctionTok{tibble}\NormalTok{( }
  \AttributeTok{bias.hat =} \FunctionTok{mean}\NormalTok{( reps ) }\SpecialCharTok{{-}}\NormalTok{ theta.hat,}
  \AttributeTok{var.hat =} \FunctionTok{var}\NormalTok{( reps ),}
  \AttributeTok{CIlog\_low =}\NormalTok{ theta.hat }\SpecialCharTok{+}\NormalTok{ bias.hat }\SpecialCharTok{{-}} \FunctionTok{sqrt}\NormalTok{(var.hat) }\SpecialCharTok{*} \FunctionTok{qnorm}\NormalTok{(}\FloatTok{0.975}\NormalTok{),}
  \AttributeTok{CIlog\_high =}\NormalTok{ theta.hat }\SpecialCharTok{+}\NormalTok{ bias.hat }\SpecialCharTok{{-}} \FunctionTok{sqrt}\NormalTok{(var.hat) }\SpecialCharTok{*} \FunctionTok{qnorm}\NormalTok{(}\FloatTok{0.025}\NormalTok{),}
  \AttributeTok{CI\_low =} \FunctionTok{exp}\NormalTok{( CIlog\_low ),}
  \AttributeTok{CI\_high =} \FunctionTok{exp}\NormalTok{( CIlog\_high ) )}


\FunctionTok{bind\_rows}\NormalTok{( }\AttributeTok{parametric =}\NormalTok{ res\_par, }
           \AttributeTok{casewise =}\NormalTok{ res\_np, }\AttributeTok{.id =} \StringTok{"method"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{( }\AttributeTok{length =}\NormalTok{ CI\_high }\SpecialCharTok{{-}}\NormalTok{ CI\_low )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 2 x 8
##   method     bias.hat var.hat CIlog_low CIlog_high
##   <chr>         <dbl>   <dbl>     <dbl>      <dbl>
## 1 parametric  -0.0420  0.0856      4.07       5.21
## 2 casewise    -0.0651  0.132       3.90       5.33
## # i 3 more variables: CI_low <dbl>,
## #   CI_high <dbl>, length <dbl>
\end{verbatim}

This is \emph{also} a simulation: our data generating process is a bit more vague, however, as we are just resampling the data.
This means our estimands are not as clearly specified.
For example, in our parameteric approach, our target parameter is known to be true.
In the case-wise, the connection between our DGP and the parameter \texttt{theta.hat} is less explicit.

Overall, in this case, our parametric bootstrap can model the tail behavior of an exponential better than case-wise.
Especially considering the small number of observations, it is going to be a more faithful representation of what we are doing--provided our model is well specified for the real world distribution.

\appendix


\chapter{Coding Reference}\label{coding-tidbits}

In this appendix chapter we give a bit more detail on some core programming skills that we use throughout the book.

\section{How to repeat yourself}\label{more-repeating-oneself}

At the heart of simulation is replication: we want to do the same task over and over. In this book we have showcased a variety of tools to replicate a random process. In this section we give a formal presentation of these tools.

\subsection{\texorpdfstring{Using \texttt{replicate()}}{Using replicate()}}\label{using-replicate}

The \texttt{replicate(\ n,\ expr,\ simplify\ )} method is a base-R function, which takes two arguments: a number \texttt{n} and an expression \texttt{expr} to run repeatedly. You can set \texttt{simplify\ =\ FALSE} to get the output of the function as a list, and if you set \texttt{simplify\ =\ TRUE} then R will try to simplify your results into an array.

For simple tasks where your expression gives you a single number, replicate will produce a vector of numbers:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{replicate}\NormalTok{( }\DecValTok{5}\NormalTok{, }\FunctionTok{mean}\NormalTok{( }\FunctionTok{rpois}\NormalTok{( }\DecValTok{3}\NormalTok{, }\AttributeTok{lambda =} \DecValTok{1}\NormalTok{ ) ) )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.6666667 1.0000000 0.6666667 1.0000000
## [5] 0.6666667
\end{verbatim}

If you do not simplify, you then will need to massage your results:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{one\_run }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{() \{}
\NormalTok{  dd }\OtherTok{=} \FunctionTok{rpois}\NormalTok{( }\DecValTok{3}\NormalTok{, }\AttributeTok{lambda =} \DecValTok{1}\NormalTok{ )}
  \FunctionTok{tibble}\NormalTok{( }\AttributeTok{mean =} \FunctionTok{mean}\NormalTok{( dd ), }\AttributeTok{sd =} \FunctionTok{sd}\NormalTok{( dd ) )}
\NormalTok{\}}
\NormalTok{rps }\OtherTok{\textless{}{-}} \FunctionTok{replicate}\NormalTok{( }\DecValTok{2}\NormalTok{, }\FunctionTok{one\_run}\NormalTok{(), }\AttributeTok{simplify =} \ConstantTok{FALSE}\NormalTok{ )}
\NormalTok{rps}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [[1]]
## # A tibble: 1 x 2
##    mean    sd
##   <dbl> <dbl>
## 1  1.67  1.53
## 
## [[2]]
## # A tibble: 1 x 2
##    mean    sd
##   <dbl> <dbl>
## 1  1.33  1.15
\end{verbatim}

In particular, you will probably stack all your tibbles to make one large dataset:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rps }\OtherTok{\textless{}{-}} \FunctionTok{bind\_rows}\NormalTok{( rps )}
\NormalTok{rps}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 2 x 2
##    mean    sd
##   <dbl> <dbl>
## 1  1.67  1.53
## 2  1.33  1.15
\end{verbatim}

Note that you give replicate a full piece of code that would run on its own. You can even give a whole block of code in curly braces.
This is exactly the same code as before:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rps }\OtherTok{\textless{}{-}} \FunctionTok{replicate}\NormalTok{( }\DecValTok{2}\NormalTok{, \{}
\NormalTok{  dd }\OtherTok{=} \FunctionTok{rpois}\NormalTok{( }\DecValTok{3}\NormalTok{, }\AttributeTok{lambda =} \DecValTok{1}\NormalTok{ )}
  \FunctionTok{tibble}\NormalTok{( }\AttributeTok{mean =} \FunctionTok{mean}\NormalTok{( dd ), }\AttributeTok{sd =} \FunctionTok{sd}\NormalTok{( dd ) )}
\NormalTok{\}, }\AttributeTok{simplify =} \ConstantTok{FALSE}\NormalTok{ ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{bind\_rows}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

The \texttt{replicate()} method is good for simple tasks, but for more general use, you will probably want to use \texttt{map()}.

\subsection{\texorpdfstring{Using \texttt{map()}}{Using map()}}\label{using-map}

The tidyverse way of repeating oneself is the \texttt{map()} method. The nice thing about \texttt{map()} is you map over a list of values, and thus can call a function repeatedly, but with a shifting set of inputs.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{one\_run\_v2 }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{( N ) \{}
\NormalTok{  dd }\OtherTok{=} \FunctionTok{rpois}\NormalTok{( N, }\AttributeTok{lambda =} \DecValTok{1}\NormalTok{ )}
  \FunctionTok{tibble}\NormalTok{( }\AttributeTok{mean =} \FunctionTok{mean}\NormalTok{( dd ), }\AttributeTok{sd =} \FunctionTok{sd}\NormalTok{( dd ) )}
\NormalTok{\}}
\NormalTok{n\_list }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{5}\NormalTok{)}
\NormalTok{rps }\OtherTok{\textless{}{-}} \FunctionTok{map}\NormalTok{( n\_list, one\_run\_v2 )}
\NormalTok{rps}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [[1]]
## # A tibble: 1 x 2
##    mean    sd
##   <dbl> <dbl>
## 1   0.5 0.707
## 
## [[2]]
## # A tibble: 1 x 2
##    mean    sd
##   <dbl> <dbl>
## 1   0.8 0.837
\end{verbatim}

You again would want to stack your results:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{bind\_rows}\NormalTok{(rps)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 2 x 2
##    mean    sd
##   <dbl> <dbl>
## 1   0.5 0.707
## 2   0.8 0.837
\end{verbatim}

We have a small issue here, however, which is we lost what we \emph{gave} \texttt{map()} for each call.
If we know we only get one row back from each call, we can add the column directly:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rps}\SpecialCharTok{$}\NormalTok{n }\OtherTok{=}\NormalTok{ n\_list}
\end{Highlighting}
\end{Shaded}

A better approach is to \emph{name} your list of input parameters, and then your \texttt{map} function can add those names for you as a new column when you stack:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n\_list }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{set\_names}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{map}\NormalTok{( one\_run\_v2 ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{bind\_rows}\NormalTok{( }\AttributeTok{.id =} \StringTok{"n"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 2 x 3
##   n      mean    sd
##   <chr> <dbl> <dbl>
## 1 2       2    0   
## 2 5       1.4  1.14
\end{verbatim}

An advantage here is if you are returning multiple rows (e.g., one row for each estimator tested in a more complex simulation), all the rows will get named correctly and automatically.

In older tidyverse worlds, you will see methods such as \texttt{map\_dbl()} or \texttt{map\_dfr()}. These will automatically massage your output into the target type. \texttt{map\_dfr()} will automatically bind rows, and \texttt{map\_dbl()} will try to simplify the output into a list of doubles. Modern tidyverse no longer likes this, which we find somewhat sad.

To read more about \texttt{map()}, check out out \href{https://r4ds.had.co.nz/iteration.html\#the-map-functions}{Section 21.5 of R for Data Science (1st edition)}, which provides a more thorough introduction to mapping.

\subsection{map with no inputs}\label{map-with-no-inputs}

If you do not have parameters, but still want to use \texttt{map()}, you can. E.g.,

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{map\_dfr}\NormalTok{( }\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{, \textbackslash{}(.) }\FunctionTok{one\_run}\NormalTok{() )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 3 x 2
##    mean    sd
##   <dbl> <dbl>
## 1 1     1    
## 2 0.667 0.577
## 3 0     0
\end{verbatim}

The weird ``(.)'' is a shorthand for a function that takes one argument and then calls \texttt{one\_run()} with no arguments. We are using the 1:3 notation to just make a list of the right length (3 replicates, in this case) to map over. A lot of fuss! Just use \texttt{replicate()}

To make all of this more clear, consider passing arguments that you manipulate on the fly:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{map\_dfr}\NormalTok{( n\_list, \textbackslash{}(x) }\FunctionTok{one\_run\_v2}\NormalTok{( x}\SpecialCharTok{*}\NormalTok{x ) )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 2 x 2
##    mean    sd
##   <dbl> <dbl>
## 1  1     1.41
## 2  1.16  1.11
\end{verbatim}

Anonymous functions, as these are called, can be useful to connect your pieces of simulation together.

\subsection{Other approaches for repetition}\label{other-approaches-for-repetition}

In the past, there was a tidyverse method called \texttt{rerun()}, but it is currently out of favor.
Originally, \texttt{rerun()} did exactly that: you gave it a number and a block of code, and it would rerun the block of code that many times, giving you the results as a list.
\texttt{rerun()} and \texttt{replicate()} are near equivalents.
As we saw, \texttt{replicate()} does what its name suggests---it replicates the result of an expression a specified number of times. Setting \texttt{simplify\ =\ FALSE} returns the output as a list (just like \texttt{rerun()} did).

\section{Default arguments for functions}\label{default-arguments}

To write functions that are both easy to use and configurable, set default arguments.
For example,

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_function }\OtherTok{=} \ControlFlowTok{function}\NormalTok{( }\AttributeTok{a =} \DecValTok{10}\NormalTok{, }\AttributeTok{b =} \DecValTok{20}\NormalTok{ ) \{}
     \DecValTok{100} \SpecialCharTok{*}\NormalTok{ a }\SpecialCharTok{+}\NormalTok{ b}
\NormalTok{\}}

\FunctionTok{my\_function}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1020
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{my\_function}\NormalTok{( }\DecValTok{5}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 520
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{my\_function}\NormalTok{( }\AttributeTok{b =} \DecValTok{5}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1005
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{my\_function}\NormalTok{( }\AttributeTok{b =} \DecValTok{5}\NormalTok{, }\AttributeTok{a =} \DecValTok{1}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 105
\end{verbatim}

We can still call \texttt{my\_function()} when we don't know what the arguments are, but then when we know more about the function, we can specify things of interest.
Lots of R commands work exactly this way, and for good reason.

Especially for code to generate random datasets, default arguments can be a lifesaver as you can then call the method before you know exactly what everything means.

For example, consider the \texttt{blkvar} package that has some code to generate blocked randomized datasets.
We might locate a promising method, and type it in:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{( blkvar )}
\FunctionTok{generate\_blocked\_data}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Error in generate_blocked_data(): argument "n_k" is missing, with no default
\end{verbatim}

That didn't work, but let's provide some block sizes and see what happens:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{generate\_blocked\_data}\NormalTok{( }\AttributeTok{n\_k =} \FunctionTok{c}\NormalTok{( }\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{ ) )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    B         Y0       Y1
## 1 B1  0.1651598 6.371708
## 2 B1 -0.7767558 5.613676
## 3 B1 -1.4736741 4.856552
## 4 B2 -1.0636928 4.448634
## 5 B2  0.1533518 4.334540
\end{verbatim}

Nice! We see that we have a block ID and the control and treatment potential outcomes. We also don't see a random assignment variable, so that tells us we probably need some other methods as well.
But we can play with this as it stands right away.

Next we can see that there are many things we might tune:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{args}\NormalTok{( generate\_blocked\_data )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## function (n_k, sigma_alpha = 1, sigma_beta = 0, beta = 5, sigma_0 = 1, 
##     sigma_1 = 1, corr = 0.5, exact = FALSE) 
## NULL
\end{verbatim}

The documentation will tell us more, but if we just need some sample data, we can quickly assess our method before having to do much reading and understanding.
Only once we have identified what we need do we have to turn to the documentation itself.

\section{Profiling Code}\label{profiling-code}

Simulations can be extremely time intensive.
With a large simulation it can also be hard to determine \emph{why}, exactly, the simulation is as long as it is.
Is it one of the methods?
Is it just that as sample size grows, the time grows far more rapidly than one might expect?
Knowing the answer to these questions can allow you to plan out your simulation and, sometimes, make some hard choices as to what things you want to include.

There are a variety of tools for timing code.
We are going to go through a few useful ones here.

\subsection{\texorpdfstring{Using \texttt{Sys.time()} and \texttt{system.time()}}{Using Sys.time() and system.time()}}\label{using-sys.time-and-system.time}

The simplest way to time code is to use the \texttt{system.time()} function.
This function takes an expression and returns the time it took to run that expression.
For example:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{time }\OtherTok{\textless{}{-}} \FunctionTok{system.time}\NormalTok{( }\FunctionTok{rnorm}\NormalTok{( }\DecValTok{1000000}\NormalTok{ ) )}
\NormalTok{time}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    user  system elapsed 
##   0.037   0.000   0.037
\end{verbatim}

Elapsed time is how much time actually passed.
The user time is how much time your computer spent on the task at hand, not including if it paused to do something else (like deal with a mouse click or pop-up message).
With current computers with multiple cores, you would expect user and elapsed time to be very similar.

You can also start and stop a clock by checking the system time:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{start\_time }\OtherTok{\textless{}{-}} \FunctionTok{Sys.time}\NormalTok{()}
\NormalTok{A }\OtherTok{=} \FunctionTok{rnorm}\NormalTok{( }\DecValTok{1000000}\NormalTok{ )}
\FunctionTok{mean}\NormalTok{(A)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] -0.0006836837
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sd}\NormalTok{(A)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1.000656
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{end\_time }\OtherTok{\textless{}{-}} \FunctionTok{Sys.time}\NormalTok{()}
\NormalTok{tot\_time }\OtherTok{\textless{}{-}}\NormalTok{ end\_time }\SpecialCharTok{{-}}\NormalTok{ start\_time}
\NormalTok{tot\_time}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Time difference of 0.0491066 secs
\end{verbatim}

This can be useful, but be careful, as the time is stored along with the units. If your simulation takes a long time, it will flip from a lot of seconds to a few minutes, and you can end up thinking something that took much, much longer was actually fast.

\subsection{\texorpdfstring{The \texttt{tictoc} package}{The tictoc package}}\label{the-tictoc-package}

The \texttt{tictoc} package is a very simple way to time code.
It has two functions, \texttt{tic()} and \texttt{toc()}, which you can use to mark the start and end of a block of code.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{( tictoc )}
\FunctionTok{tic}\NormalTok{(}\StringTok{"Generating data"}\NormalTok{)}
\NormalTok{A }\OtherTok{=} \FunctionTok{rnorm}\NormalTok{( }\DecValTok{1000000}\NormalTok{ )}
\FunctionTok{toc}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Generating data: 0.037 sec elapsed
\end{verbatim}

It is basically like \texttt{Sys.time()} but it has a few more features, such as being able to label the time you are measuring.

\subsection{\texorpdfstring{The \texttt{bench} package}{The bench package}}\label{the-bench-package}

The \texttt{bench} package provides some powerful tools for timing code, and is in particular good for comparing different ways of doing the same (or similar) thing.
\texttt{bench::mark()} runs each expression 10 times (by default) and tracks how long the computations take. It then summarizes the distribution of timings.
For example, we can time how long it takes to analyze some data from our cluster RCT experiment:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{( bench )}
\NormalTok{dat }\OtherTok{\textless{}{-}} \FunctionTok{gen\_cluster\_RCT}\NormalTok{(}\AttributeTok{n\_bar=}\DecValTok{100}\NormalTok{, }\AttributeTok{J =} \DecValTok{100}\NormalTok{)}
\NormalTok{timings }\OtherTok{\textless{}{-}} \FunctionTok{mark}\NormalTok{(}
      \AttributeTok{MLM =} \FunctionTok{quiet\_analysis\_MLM}\NormalTok{(dat),}
      \AttributeTok{OLS =} \FunctionTok{analysis\_OLS}\NormalTok{(dat),}
      \AttributeTok{agg =} \FunctionTok{analysis\_agg}\NormalTok{(dat),}
      \AttributeTok{check =} \ConstantTok{FALSE}
\NormalTok{)}
\NormalTok{timings}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 3 x 6
##   expression      min   median `itr/sec` mem_alloc
##   <bch:expr> <bch:tm> <bch:tm>     <dbl> <bch:byt>
## 1 MLM        105.33ms 107.14ms      9.34   25.78MB
## 2 OLS        113.55ms 113.92ms      8.68    2.51MB
## 3 agg          6.88ms   7.42ms    136.    472.97KB
## # i 1 more variable: `gc/sec` <dbl>
\end{verbatim}

You can even get a viz of how long everything took:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{( timings )}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{Designing-Simulations-in-R_files/figure-latex/unnamed-chunk-348-1} \end{center}

The ``gc'' coloring in the above indicates runs where ``garbage collection'' took place, meaning R paused to empty out some used memory.

You can also use \texttt{bench::press()} to run a variety of configurations to explore how timing works under changed parameters.
To illustrate, let's compare how long each method for the cluster RCT running example takes:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{source}\NormalTok{( here}\SpecialCharTok{::}\FunctionTok{here}\NormalTok{(}\StringTok{"case\_study\_code/clustered\_data\_simulation.R"}\NormalTok{ ) )}
\NormalTok{timings }\OtherTok{\textless{}{-}}\NormalTok{ bench}\SpecialCharTok{::}\FunctionTok{press}\NormalTok{(}
  \AttributeTok{n\_bar =} \FunctionTok{c}\NormalTok{( }\DecValTok{10}\NormalTok{, }\DecValTok{100}\NormalTok{ ),}
  \AttributeTok{J =} \FunctionTok{c}\NormalTok{( }\DecValTok{10}\NormalTok{, }\DecValTok{100}\NormalTok{ ),}
\NormalTok{  \{}
\NormalTok{    dat }\OtherTok{\textless{}{-}} \FunctionTok{gen\_cluster\_RCT}\NormalTok{(}\AttributeTok{n\_bar=}\NormalTok{n\_bar, }\AttributeTok{J =}\NormalTok{ J)}
\NormalTok{    bench}\SpecialCharTok{::}\FunctionTok{mark}\NormalTok{(}
      \AttributeTok{MLM =} \FunctionTok{quiet\_analysis\_MLM}\NormalTok{(dat),}
      \AttributeTok{OLS =} \FunctionTok{analysis\_OLS}\NormalTok{(dat),}
      \AttributeTok{agg =} \FunctionTok{analysis\_agg}\NormalTok{(dat),}
      \AttributeTok{check =} \ConstantTok{FALSE}
\NormalTok{    )}
\NormalTok{  \}}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Running with:
##   n_bar     J
\end{verbatim}

\begin{verbatim}
## 1    10    10
\end{verbatim}

\begin{verbatim}
## 2   100    10
\end{verbatim}

\begin{verbatim}
## 3    10   100
\end{verbatim}

\begin{verbatim}
## 4   100   100
\end{verbatim}

And we can make our custom plot of time:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{timings}\SpecialCharTok{$}\NormalTok{name }\OtherTok{=} \FunctionTok{attr}\NormalTok{( timings}\SpecialCharTok{$}\NormalTok{expression, }\StringTok{"description"}\NormalTok{ )}
\FunctionTok{ggplot}\NormalTok{( timings, }\FunctionTok{aes}\NormalTok{( n\_bar, median, }\AttributeTok{color =} \FunctionTok{as.factor}\NormalTok{(J) ) ) }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{( }\SpecialCharTok{\textasciitilde{}}\NormalTok{ name ) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{geom\_line}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.75\linewidth]{Designing-Simulations-in-R_files/figure-latex/unnamed-chunk-350-1} \end{center}

Note the \texttt{timings} object is not quite a classic tibble, and the expression at start captures the code run. The ``name'' line grabs the names given in the initial evaluation so we can make the plot the way we want.

\subsection{\texorpdfstring{Profiling with \texttt{profvis}}{Profiling with profvis}}\label{profiling-with-profvis}

Sometimes you don't have a head to head comparison in mind, but are instead trying to find out \emph{where} in your full simulation the time is being spent.
The \texttt{profvis} package allows for exploring this sort of question.
You ``profile'' a block of code and then you can explore the results in a browser inside of RStudio.

For example, you might have the following:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{profvis}\NormalTok{( \{}
  \FunctionTok{replicate}\NormalTok{( }\DecValTok{10}\NormalTok{, \{ }
\NormalTok{    data }\OtherTok{\textless{}{-}} \FunctionTok{gen\_cluster\_RCT}\NormalTok{( }\AttributeTok{n\_bar =} \DecValTok{100}\NormalTok{, }\AttributeTok{J =} \DecValTok{100}\NormalTok{ )}
\NormalTok{    res1 }\OtherTok{\textless{}{-}} \FunctionTok{quiet\_analysis\_MLM}\NormalTok{(data)}
\NormalTok{    res2 }\OtherTok{\textless{}{-}} \FunctionTok{analysis\_OLS}\NormalTok{(data)}
\NormalTok{    res3 }\OtherTok{\textless{}{-}} \FunctionTok{analysis\_agg}\NormalTok{(data)}
\NormalTok{  \} )}
\NormalTok{\} )}
\end{Highlighting}
\end{Shaded}

In the browser you will get the code you ran, and you can see how long each line took to run.
You will get little ``time'' bars---the bigger the bar, the greater fraction of time that line took versus the other lines.

You can also click on the ``Data'' tab and it will give you a series of cascades of function calls, so you can see how long each function took to run.
You click on a function to expand it, and it will show you how long each part inside took.

\section{Optimizing code (and why you often shouldn't)}\label{optimize-code}

Optimizing code is when you spend a bit more human effort to write code that will run faster on your computer.
In some cases, this can be a critical boost to running a simulation, where you inherently will be doing things a lot of times.
Cutting runtime down will always be tempting, as it allows you to run more replicates and get more precisely estimated performance measures for your simulation.

That being said, generally optimize code only after discovering you need to.
Optimizing as you go usually means you will spend a lot of time wrestling with code far more complicated than it needs to be.
For example, often it is the estimation method that will take a lot of computational time, so having very fast data generation code will not help shorten the overall run time of a simulation much, as you are tweaking something that is only a small part of the overall pie, in terms of time.
Keep things simple; in general your time is more important than the computer's time.

Overall, computational efficiency should usually be a secondary consideration when you are starting to design a simulation study.
It is better to produce accurate code, even if it is a bit slow, than to write code that is speedy but hard to follow (or even worse, that produces incorrect results).

That warning made, in the next sections we will look at a few optimization efforts applied to the ANOVA example from Section \ref{case-ANOVA} to illustrate some principles of optimization that come up a lot in simulation projects.

\subsection{Hand-building functions}\label{hand-building-functions}

In our initial ANOVA simulation we used the system-implemented ANOVA.
An alternative approach would be to ``hand roll'' the ANOVA F statistic and test directly.
Doing so by hand can set you up to implement modified versions of these tests later on.
Also, although hand-building a method does take more work to program, it can result in a faster piece of code (this actually is the case here) which in turn can make the overall simulation faster.

Following the formulas on p.~129 of Brown and Forsythe (1974) we write our own function as so:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ANOVA\_F }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(sim\_data) \{}

\NormalTok{  x\_bar }\OtherTok{\textless{}{-}} \FunctionTok{with}\NormalTok{(sim\_data, }\FunctionTok{tapply}\NormalTok{(x, group, mean))}
\NormalTok{  s\_sq }\OtherTok{\textless{}{-}} \FunctionTok{with}\NormalTok{(sim\_data, }\FunctionTok{tapply}\NormalTok{(x, group, var))}
\NormalTok{  n }\OtherTok{\textless{}{-}} \FunctionTok{table}\NormalTok{(sim\_data}\SpecialCharTok{$}\NormalTok{group)}
\NormalTok{  g }\OtherTok{\textless{}{-}} \FunctionTok{length}\NormalTok{(x\_bar)}

\NormalTok{  df1 }\OtherTok{\textless{}{-}}\NormalTok{ g }\SpecialCharTok{{-}} \DecValTok{1}
\NormalTok{  df2 }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(n) }\SpecialCharTok{{-}}\NormalTok{ g}

\NormalTok{  msbtw }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(n }\SpecialCharTok{*}\NormalTok{ (x\_bar }\SpecialCharTok{{-}} \FunctionTok{mean}\NormalTok{(sim\_data}\SpecialCharTok{$}\NormalTok{x))}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) }\SpecialCharTok{/}\NormalTok{ df1}
\NormalTok{  mswn }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{((n }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{) }\SpecialCharTok{*}\NormalTok{ s\_sq) }\SpecialCharTok{/}\NormalTok{ df2}
\NormalTok{  fstat }\OtherTok{\textless{}{-}}\NormalTok{ msbtw }\SpecialCharTok{/}\NormalTok{ mswn}
\NormalTok{  pval }\OtherTok{\textless{}{-}} \FunctionTok{pf}\NormalTok{(fstat, df1, df2, }\AttributeTok{lower.tail =} \ConstantTok{FALSE}\NormalTok{)}

  \FunctionTok{return}\NormalTok{(pval)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

We are using data as generated in Chapter \ref{data-generating-processes}.
To see the difference between our version and R's version, we benchmark:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{timings }\OtherTok{\textless{}{-}}\NormalTok{ bench}\SpecialCharTok{::}\FunctionTok{mark}\NormalTok{(}\AttributeTok{Rfunction =} \FunctionTok{ANOVA\_F\_aov}\NormalTok{(sim\_data),}
                          \AttributeTok{direct    =} \FunctionTok{ANOVA\_F}\NormalTok{(sim\_data))}
\FunctionTok{summary}\NormalTok{( timings )[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{] }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{( }\AttributeTok{speed\_up =} \FunctionTok{as.numeric}\NormalTok{( }\FunctionTok{max}\NormalTok{(median)}\SpecialCharTok{/}\NormalTok{median ) ) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{( }\AttributeTok{digits =} \DecValTok{2}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{l|r|r|r|r}
\hline
expression & min & median & itr/sec & speed\_up\\
\hline
Rfunction & 479us & 512us & 1912.16 & 1.00\\
\hline
direct & 217us & 229us & 4241.04 & 2.23\\
\hline
\end{tabular}

The direct function is 2.2 times faster than the built-in R function.

This result is not unusual.
Built-in R functions usually include lots of checks and error-handling, which take time to compute. These checks are crucial for messy, real-world data analysis but unnecessary with our pristine, simulated data.
Here we can skip them by doing the calculations directly.

In general, however, this is a trade-off: writing something yourself gives you a lot of chance to do something wrong, throwing off all your simulations.
It might be faster, but you may pay dearly for it in terms of extra hours coding and debugging.
Optimize only if you need to!

\subsection{Computational efficiency versus simplicity}\label{sec_comp_efficiency}

On the data generation side, an alternative approach to having a function that, for each call, generates a single set of data, would be to write a function that generates \emph{multiple} sets of simulated data all at once.

For example, for our ANOVA example we could specify that we want \texttt{R} replications of the study and have the function spit out a matrix with \texttt{R} columns, one for each simulated dataset:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{generate\_data\_matrix }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(mu, sigma\_sq, sample\_size, R) \{}

\NormalTok{  N }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(sample\_size) }
\NormalTok{  g }\OtherTok{\textless{}{-}} \FunctionTok{length}\NormalTok{(sample\_size) }
  
\NormalTok{  group }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{g, }\AttributeTok{times =}\NormalTok{ sample\_size) }
\NormalTok{  mu\_long }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(mu, }\AttributeTok{times =}\NormalTok{ sample\_size)}
\NormalTok{  sigma\_long }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(}\FunctionTok{sqrt}\NormalTok{(sigma\_sq), }\AttributeTok{times =}\NormalTok{ sample\_size) }

\NormalTok{  x\_mat }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{rnorm}\NormalTok{(N }\SpecialCharTok{*}\NormalTok{ R, }\AttributeTok{mean =}\NormalTok{ mu\_long, }\AttributeTok{sd =}\NormalTok{ sigma\_long),}
                  \AttributeTok{nrow =}\NormalTok{ N, }\AttributeTok{ncol =}\NormalTok{ R)}
\NormalTok{  sim\_data }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\AttributeTok{group =}\NormalTok{ group, }\AttributeTok{x\_mat =}\NormalTok{ x\_mat)}
    
  \FunctionTok{return}\NormalTok{(sim\_data)}
\NormalTok{\}}

\FunctionTok{generate\_data\_matrix}\NormalTok{(}\AttributeTok{mu =}\NormalTok{ mu, }\AttributeTok{sigma\_sq =}\NormalTok{ sigma\_sq,}
                     \AttributeTok{sample\_size =}\NormalTok{ sample\_size, }\AttributeTok{R =} \DecValTok{4}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $group
##  [1] 1 1 1 2 2 2 2 2 2 3 3 4 4 4 4
## 
## $x_mat
##             [,1]      [,2]        [,3]       [,4]
##  [1,] -1.3636588 0.2222331 -0.02133678  2.8644350
##  [2,]  0.9888753 1.4584378  3.07646144  2.2633001
##  [3,]  3.6053705 5.1505453 -0.13767173 -1.5196025
##  [4,]  2.5752054 2.1213946  5.00364585 -3.0626950
##  [5,]  4.1250436 3.7017485  1.73955417  1.2933326
##  [6,]  4.5874572 3.2125993  2.98976042  2.3596986
##  [7,]  2.5262919 2.6632689  0.73782032  0.2670123
##  [8,]  2.5395918 1.6464949  1.23149585 -0.2759674
##  [9,]  0.8430465 1.2816296  2.01959173  3.1534465
## [10,]  3.8326473 6.7732091  5.42478453  4.1684142
## [11,]  1.2013117 2.7802496  8.68069609  5.5589565
## [12,]  5.2417218 6.6454860  7.71799386  7.4059301
## [13,]  5.0801275 7.3738410  6.34694537  6.9914307
## [14,]  5.7676197 4.0715624  4.81740376  7.7048362
## [15,]  6.3962093 7.2171724  6.49251170  5.8166998
\end{verbatim}

This approach is a bit more computationally efficient because the setup calculations (getting \texttt{N}, \texttt{g}, \texttt{group}, \texttt{mu\_full}, and \texttt{sigma\_full}) only have to be done once instead of once per replication. It also makes clever use of vector recycling in the call to \texttt{rnorm()}. However, the structure of the resulting data is more complicated, which will make it more difficult to do the later estimation steps.
Furthermore, if the number of replicates \texttt{R} is large and each replication produces a large dataset, this ``all-at-once'' approach will entail generating and holding very large amounts of data in memory, which can create other performance issues.
On balance, we recommend the simpler approach of writing a function that generates a single simulated dataset per call (unless and until you have a principled reason to do otherwise).
It is usually the case that most time spent in the simulation is the \emph{analyzing} of the data, not the generating it, so these savings are usually not worth the bother.

\subsection{Reusing code to speed up computation}\label{reusing-code-to-speed-up-computation}

Once we have our own ANOVA method to go with our own Welch method, we see some glaring redundancies.
In particular, both \texttt{ANOVA\_F} and \texttt{Welch\_F} start by taking the simulated data and calculating summary statistics for each group, using the following code:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x\_bar }\OtherTok{\textless{}{-}} \FunctionTok{with}\NormalTok{(sim\_data, }\FunctionTok{tapply}\NormalTok{(x, group, mean))}
\NormalTok{s\_sq }\OtherTok{\textless{}{-}} \FunctionTok{with}\NormalTok{(sim\_data, }\FunctionTok{tapply}\NormalTok{(x, group, var))}
\NormalTok{n }\OtherTok{\textless{}{-}} \FunctionTok{table}\NormalTok{(sim\_data}\SpecialCharTok{$}\NormalTok{group)}
\end{Highlighting}
\end{Shaded}

In the interest of not repeating ourselves, it would better to pull this code out as a separate function and then re-write the \texttt{ANOVA\_F} and \texttt{Welch\_F} functions to take the summary statistics as input. Here is a function that takes simulated data and returns a list of summary statistics:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{summarize\_data }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(sim\_data) \{}
\NormalTok{  x\_bar }\OtherTok{\textless{}{-}} \FunctionTok{with}\NormalTok{(sim\_data, }\FunctionTok{tapply}\NormalTok{(x, group, mean))}
\NormalTok{  s\_sq }\OtherTok{\textless{}{-}} \FunctionTok{with}\NormalTok{(sim\_data, }\FunctionTok{tapply}\NormalTok{(x, group, var))}
\NormalTok{  n }\OtherTok{\textless{}{-}} \FunctionTok{table}\NormalTok{(sim\_data}\SpecialCharTok{$}\NormalTok{group)}

  \FunctionTok{list}\NormalTok{(}
    \AttributeTok{x\_bar =} \FunctionTok{as.numeric}\NormalTok{( x\_bar ),}
    \AttributeTok{s\_sq =} \FunctionTok{as.numeric}\NormalTok{( s\_sq ),}
    \AttributeTok{n =} \FunctionTok{as.numeric}\NormalTok{( n )}
\NormalTok{  )}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

We just packaged the code from above:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sim\_data }\OtherTok{=} \FunctionTok{generate\_data}\NormalTok{(}\AttributeTok{mu=}\NormalTok{mu, }\AttributeTok{sigma\_sq=}\NormalTok{sigma\_sq, }\AttributeTok{sample\_size=}\NormalTok{sample\_size)}
\FunctionTok{summarize\_data}\NormalTok{(sim\_data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $x_bar
## [1] 1.049431 2.292670 6.942982 7.194796
## 
## $s_sq
## [1] 3.301593387 2.216558131 0.000678492
## [4] 0.185864577
## 
## $n
## [1] 3 6 2 4
\end{verbatim}

Now we can re-write both our \(F\)-test functions to use the output of this function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ANOVA\_F\_agg }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x\_bar, s\_sq, n) \{}
\NormalTok{  g }\OtherTok{=} \FunctionTok{length}\NormalTok{(x\_bar)}
\NormalTok{  df1 }\OtherTok{\textless{}{-}}\NormalTok{ g }\SpecialCharTok{{-}} \DecValTok{1}
\NormalTok{  df2 }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(n) }\SpecialCharTok{{-}}\NormalTok{ g}
  
\NormalTok{  msbtw }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(n }\SpecialCharTok{*}\NormalTok{ (x\_bar }\SpecialCharTok{{-}} \FunctionTok{weighted.mean}\NormalTok{(x\_bar, }\AttributeTok{w =}\NormalTok{ n))}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) }\SpecialCharTok{/}\NormalTok{ df1}
\NormalTok{  mswn }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{((n }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{) }\SpecialCharTok{*}\NormalTok{ s\_sq) }\SpecialCharTok{/}\NormalTok{ df2}
\NormalTok{  fstat }\OtherTok{\textless{}{-}}\NormalTok{ msbtw }\SpecialCharTok{/}\NormalTok{ mswn}
\NormalTok{  pval }\OtherTok{\textless{}{-}} \FunctionTok{pf}\NormalTok{(fstat, df1, df2, }\AttributeTok{lower.tail =} \ConstantTok{FALSE}\NormalTok{)}
 
  \FunctionTok{return}\NormalTok{(pval)}
\NormalTok{\}}

\NormalTok{summary\_stats }\OtherTok{\textless{}{-}} \FunctionTok{summarize\_data}\NormalTok{(sim\_data)}
\FunctionTok{with}\NormalTok{(summary\_stats, }\FunctionTok{ANOVA\_F\_agg}\NormalTok{(}\AttributeTok{x\_bar =}\NormalTok{ x\_bar, }\AttributeTok{s\_sq =}\NormalTok{ s\_sq, }\AttributeTok{n =}\NormalTok{ n))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.00008347563
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Welch\_F\_agg }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x\_bar, s\_sq, n) \{}
\NormalTok{  g }\OtherTok{=} \FunctionTok{length}\NormalTok{(x\_bar)}
\NormalTok{  w }\OtherTok{\textless{}{-}}\NormalTok{ n }\SpecialCharTok{/}\NormalTok{ s\_sq}
\NormalTok{  u }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(w)}
\NormalTok{  x\_tilde }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(w }\SpecialCharTok{*}\NormalTok{ x\_bar) }\SpecialCharTok{/}\NormalTok{ u}
\NormalTok{  msbtw }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(w }\SpecialCharTok{*}\NormalTok{ (x\_bar }\SpecialCharTok{{-}}\NormalTok{ x\_tilde)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) }\SpecialCharTok{/}\NormalTok{ (g }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{)}

\NormalTok{  G }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{((}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ w }\SpecialCharTok{/}\NormalTok{ u)}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{/}\NormalTok{ (n }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{))}
\NormalTok{  denom }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{+}\NormalTok{  G }\SpecialCharTok{*} \DecValTok{2} \SpecialCharTok{*}\NormalTok{ (g }\SpecialCharTok{{-}} \DecValTok{2}\NormalTok{) }\SpecialCharTok{/}\NormalTok{ (g}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{{-}} \DecValTok{1}\NormalTok{)}
\NormalTok{  W }\OtherTok{\textless{}{-}}\NormalTok{ msbtw }\SpecialCharTok{/}\NormalTok{ denom}
\NormalTok{  f }\OtherTok{\textless{}{-}}\NormalTok{ (g}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{{-}} \DecValTok{1}\NormalTok{) }\SpecialCharTok{/}\NormalTok{ (}\DecValTok{3} \SpecialCharTok{*}\NormalTok{ G)}

\NormalTok{  pval }\OtherTok{\textless{}{-}} \FunctionTok{pf}\NormalTok{(W, }\AttributeTok{df1 =}\NormalTok{ g }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{, }\AttributeTok{df2 =}\NormalTok{ f, }\AttributeTok{lower.tail =} \ConstantTok{FALSE}\NormalTok{)}

  \FunctionTok{return}\NormalTok{(pval)}
\NormalTok{\}}

\FunctionTok{with}\NormalTok{(summary\_stats, }\FunctionTok{Welch\_F\_agg}\NormalTok{(}\AttributeTok{x\_bar =}\NormalTok{ x\_bar, }\AttributeTok{s\_sq =}\NormalTok{ s\_sq, }\AttributeTok{n =}\NormalTok{ n))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.002413265
\end{verbatim}

The results are the same as before.

We should always test any optimized code against something we know is stable, since optimization is an easy way to get bad bugs.
Here we check against R's implementation:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{summary\_stats }\OtherTok{\textless{}{-}} \FunctionTok{summarize\_data}\NormalTok{(sim\_data)}
\NormalTok{F\_results }\OtherTok{\textless{}{-}} \FunctionTok{with}\NormalTok{(summary\_stats,}
                  \FunctionTok{ANOVA\_F\_agg}\NormalTok{(}\AttributeTok{x\_bar =}\NormalTok{ x\_bar, }\AttributeTok{s\_sq =}\NormalTok{ s\_sq, }\AttributeTok{n =}\NormalTok{ n))}
\NormalTok{aov\_results }\OtherTok{\textless{}{-}} \FunctionTok{oneway.test}\NormalTok{(x }\SpecialCharTok{\textasciitilde{}} \FunctionTok{factor}\NormalTok{(group), }\AttributeTok{data =}\NormalTok{ sim\_data, }
                           \AttributeTok{var.equal =} \ConstantTok{TRUE}\NormalTok{)}
\FunctionTok{all.equal}\NormalTok{(aov\_results}\SpecialCharTok{$}\NormalTok{p.value, F\_results)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] TRUE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{W\_results }\OtherTok{\textless{}{-}} \FunctionTok{with}\NormalTok{(summary\_stats,}
                  \FunctionTok{Welch\_F\_agg}\NormalTok{( }\AttributeTok{x\_bar =}\NormalTok{ x\_bar,}
                               \AttributeTok{s\_sq =}\NormalTok{ s\_sq, }\AttributeTok{n =}\NormalTok{ n))}
\NormalTok{aov\_results }\OtherTok{\textless{}{-}} \FunctionTok{oneway.test}\NormalTok{(x }\SpecialCharTok{\textasciitilde{}} \FunctionTok{factor}\NormalTok{(group),}
                           \AttributeTok{data =}\NormalTok{ sim\_data, }
                           \AttributeTok{var.equal =} \ConstantTok{FALSE}\NormalTok{)}
\FunctionTok{all.equal}\NormalTok{(aov\_results}\SpecialCharTok{$}\NormalTok{p.value, W\_results)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] TRUE
\end{verbatim}

Here we are able to check against a known baseline.
Checking estimation functions can be a bit more difficult for procedures that are not already implemented in R. For example, the two other procedures examined by Brown and Forsythe, the James' test and Brown and Forsythe's \(F*\) test, are not available in base R.
They are, however, available in the user-contributed package \texttt{onewaytests}, found by searching for ``Brown-Forsythe'' at \url{http://rseek.org/}. We could benchmark our calculations against this package, but of course there is some risk that the package might not be correct. Another route is to verify your results on numerical examples reported in authoritative papers, on the assumption that there's less risk of an error there. In the original paper that proposed the test, Welch (1951) provides a worked numerical example of the procedure. He reports the following summary statistics:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{g }\OtherTok{\textless{}{-}} \DecValTok{3}
\NormalTok{x\_bar }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FloatTok{27.8}\NormalTok{, }\FloatTok{24.1}\NormalTok{, }\FloatTok{22.2}\NormalTok{)}
\NormalTok{s\_sq }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FloatTok{60.1}\NormalTok{, }\FloatTok{6.3}\NormalTok{, }\FloatTok{15.4}\NormalTok{)}
\NormalTok{n }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{20}\NormalTok{, }\DecValTok{20}\NormalTok{, }\DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

He also reports \(W = 3.35\) and \(f = 22.6\). Replicating the calculations with our \texttt{Welch\_F\_agg} function:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{Welch\_F\_agg}\NormalTok{(}\AttributeTok{x\_bar =}\NormalTok{ x\_bar, }\AttributeTok{s\_sq =}\NormalTok{ s\_sq, }\AttributeTok{n =}\NormalTok{ n)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.05479049
\end{verbatim}

We get slightly different results! But we know that our function is correct---or at least consistent with \texttt{oneway.test}---so what's going on? It turns out that there was an error in some of Welch's intermediate calculations, which can only be spotted because he reported all of his work in the paper.

We then put all these pieces in our revised \texttt{one\_run()} method as so:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{one\_run\_fast }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{( mu, sigma\_sq, sample\_size ) \{}
\NormalTok{  sim\_data }\OtherTok{\textless{}{-}} \FunctionTok{generate\_data}\NormalTok{(}\AttributeTok{mu =}\NormalTok{ mu, }\AttributeTok{sigma\_sq =}\NormalTok{ sigma\_sq,}
                            \AttributeTok{sample\_size =}\NormalTok{ sample\_size)}
\NormalTok{  summary\_stats }\OtherTok{\textless{}{-}} \FunctionTok{summarize\_data}\NormalTok{(sim\_data)}
\NormalTok{  anova\_p }\OtherTok{\textless{}{-}} \FunctionTok{with}\NormalTok{(summary\_stats, }
                  \FunctionTok{ANOVA\_F\_agg}\NormalTok{(}\AttributeTok{x\_bar =}\NormalTok{ x\_bar,}\AttributeTok{s\_sq =}\NormalTok{ s\_sq, }\AttributeTok{n =}\NormalTok{ n))}
\NormalTok{  Welch\_p }\OtherTok{\textless{}{-}} \FunctionTok{with}\NormalTok{(summary\_stats, }
                  \FunctionTok{Welch\_F\_agg}\NormalTok{(}\AttributeTok{x\_bar =}\NormalTok{ x\_bar, }\AttributeTok{s\_sq =}\NormalTok{ s\_sq, }\AttributeTok{n =}\NormalTok{ n))}
  \FunctionTok{tibble}\NormalTok{(}\AttributeTok{ANOVA =}\NormalTok{ anova\_p, }\AttributeTok{Welch =}\NormalTok{ Welch\_p)}
\NormalTok{\}}

\FunctionTok{one\_run\_fast}\NormalTok{( }\AttributeTok{mu =}\NormalTok{ mu, }\AttributeTok{sigma\_sq =}\NormalTok{ sigma\_sq,}
              \AttributeTok{sample\_size =}\NormalTok{ sample\_size )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1 x 2
##     ANOVA  Welch
##     <dbl>  <dbl>
## 1 0.00419 0.0440
\end{verbatim}

The reason this is important is we are now doing our group aggregation only once, rather than once per method.
We benchmark to see our speedup:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{timings }\OtherTok{\textless{}{-}}\NormalTok{ bench}\SpecialCharTok{::}\FunctionTok{mark}\NormalTok{(}\AttributeTok{original =} \FunctionTok{one\_run}\NormalTok{(}\AttributeTok{mu =}\NormalTok{ mu, }\AttributeTok{sigma\_sq =}\NormalTok{ sigma\_sq, }
                                          \AttributeTok{sample\_size =}\NormalTok{ sample\_size),}
                       \AttributeTok{one\_agg =} \FunctionTok{one\_run\_fast}\NormalTok{(}\AttributeTok{mu =}\NormalTok{ mu, }\AttributeTok{sigma\_sq =}\NormalTok{ sigma\_sq, }
                                              \AttributeTok{sample\_size =}\NormalTok{ sample\_size),}
                       \AttributeTok{check=}\ConstantTok{FALSE}\NormalTok{)}
\NormalTok{timings[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{] }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{( }\AttributeTok{speed\_up =} \FunctionTok{as.numeric}\NormalTok{( }\FunctionTok{max}\NormalTok{(median) }\SpecialCharTok{/}\NormalTok{ median ) ) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{( }\AttributeTok{digits =} \DecValTok{2}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{l|r|r|r|r}
\hline
expression & min & median & itr/sec & speed\_up\\
\hline
original & 1.48ms & 1.54ms & 639.86 & 1.0\\
\hline
one\_agg & 1.23ms & 1.28ms & 776.90 & 1.2\\
\hline
\end{tabular}

Our improvement is fairly modest.

To recap, there are two advantages of this kind of coding:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Code reuse is generally good because when you have the same code in multiple places it can make it harder to read and understand your code. If you see two blocks of code you might worry they are only mostly similar, not exactly similar, and waste time trying to differentiate. If you have a single, well-named function, you immediately know what a block of code is doing.
\item
  Saving the results of calculations can speed up your computation since you are saving your partial work. This can be useful to reduce calculations that are particularly time intensive.
\end{enumerate}

That said, optimizing code is dangerous (it is an easy way to introduce bugs into your simulation workflow) and time consuming for you (thinking through and writing all the fancy code is time you are not doing something else).
But it sure can be satisfying to spend an extra weekend hacking away at this sort of thing!

\chapter{Further readings and resources}\label{further-readings-and-resources}

We close with a list of things of interest we have discovered while writing this text.

\begin{itemize}
\item
  \href{https://doi.org/10.1002/sim.8086}{Morris, White, \& Crowther (2019)}. Using simulation studies to evaluate statistical methods.
\item
  High-level simulation design considerations.
\item
  Details about performance criteria calculations.
\item
  Stata-centric.
\item
  \href{https://github.com/philchalmers/SimDesign/wiki}{SimDesign} R package (Chalmers, 2019)
\item
  Tools for building generic simulation workflows.
\item
  \href{http://www.tqmp.org/RegularArticles/vol16-4/p248/}{Chalmers \& Adkin (2019)}. Writing effective and reliable Monte Carlo simulations with the SimDesign package.
\item
  \href{https://declaredesign.org/}{DeclareDesign} (Blair, Cooper, Coppock, \& Humphreys)
\item
  Specialized suite of R packages for simulating research designs.
\item
  Design philosophy is very similar to ``tidy'' simulation approach.
\item
  \href{https://meghapsimatrix.github.io/simhelpers/index.html}{SimHelpers} R package (Joshi \& Pustejovsky, 2020)
\item
  Helper functions for calculating performance criteria.
\item
  Includes Monte Carlo standard errors.
\end{itemize}

  \bibliography{book.bib,packages.bib}

\end{document}
