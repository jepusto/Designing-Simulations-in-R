# (PART) Structure and Mechanics of a Simulation Study {-}

# Structure of a simulation study {#simulation-structure}

Monte Carlo simulation is a very flexible tool that researchers use to study a vast array of different models and topics. 
Within the realm of methodological research, most simulations share a common structure, nearly always involving the same set of steps or component pieces. 
In learning to design your own simulations, it is very useful to recognize the core components that most simulation studies share.
Identifying these components will help you to organize your work and structure the coding tasks involved in writing a simulation. 

In this chapter, we outline the component structure of a methodological simulation study, highlighting the four steps involved in a simulation of a single scenario and the three additional steps involved in multifactor simulations.
We then describe a strategy for implementing simulations that mirrors the same component structure, where each step in the simulation is represented by a separate function or object.
We call this strategy __*tidy, modular simulation*__. 
Finally, we show how the tidy, modular simulation strategy informs the structure and organization of code for a simulation study, walking through basic code skeletons (cf. \@ref(function-skeletons)) for each of the steps in a single-scenario simulation.

## General structure of a simulation

The four main steps involved in a simulation study, introduced in Chapter \@ref(introduction), are summarized in the top portion of Table \@ref(tab:simulation).

Table: (\#tab:simulation) Steps in the Simulation Process 

|   | Step         | Description |
|---|--------------|-------------|
| 1 | __Generate__ | Generate a sample of artificial data based on a specific statistical model or data-generating process. |
| 2 | __Analyze__  | Apply one or more data-analysis procedures, estimators, or workflows to the artificial data.  |
| 3 | __Repeat__   | Repeat steps (1) & (2) $R$ times, recording $R$ sets of results. |
| 4 | __Summarize__| Assess the performance of the procedure across the $R$ replications. | 
| 5 | __Design__   | Specify a set of conditions to examine |
| 6 | __Execute__  | Run the simulation for each condition in the design. |
| 7 | __Synthesize__| Compare performance across conditions. |

In the simple $t$-test example presented in Chapter \@ref(t-test-simulation), we put each of these steps into action with R code:

* We used the geometric distribution as a data-generating process; 
* We used the confidence interval from a one-sample $t$-test as the data-analysis procedure; 
* We repeatedly simulated the confidence intervals with R's `replicate()` function; and
* We summarized the results by estimating the fraction of the intervals that covered the population mean. 

We also saw that it was helpful to wrap all of these steps up into a single function, so that we could run the simulation across multiple sample sizes.

These four initial steps are common and shared across nearly all simulations. 
In our first example, each of the steps was fairly simple, sometimes involving only a single line of code. 
More generally, each of the steps might be quite a bit more complex. The data-generating process might involve a more complex model with multiple variables or multilevel structure. 
The data analysis procedure might involve solving a multidimensional optimization problem to get parameter estimates, or might involve a data analysis workflow with multiple steps or contingencies.
Further, we might use more than one metric for summarizing the results across replications and describing the performance of the data analysis procedure.
Because each of the four steps involves its own set of choices, it will useful to recognize them as distinct from one another.

In methodological research projects, we usually want to examine simulation results across an array of different conditions that differ not only in terms of sample size, but also in other parameters or features of the data-generating process.
Running a simulation study across multiple conditions entails several further steps, which are summarized in the bottom portion of Table \@ref(tab:simulation). We will first need to specify the factors and specific conditions to be examined in our experimental design. We will then need to execute the simulation for each of the conditions and store all the results for further analysis. Finally, we will need to find ways to synthesize or make sense of the main patterns in the results across all of the conditions in the design. 

Just as with the first four steps, it is useful to recognize these further steps as distinct from one another, each involving its own set of choices and techniques.
The design step requires choosing which parameters and features of the data-generating process to vary, as well as which specific values to use for each factor that is varied.
Executing a simulation might require a lot of computing power, especially if the simulation design has many conditions or the data analysis procedure takes a long time to compute. 
How to effectively implement the execution step will therefore depend on the computing requirements and available resources.
Finally, many different techniques can be used to synthesizing findings from a multifactor simulation. 
Which ones are most useful will depend on your research questions and the choices you make in each of the preceeding steps.

<!--Across all seven steps, recognizing the distinct pieces will let us more effectively tackle the many decisions involved in designing and running a simulation.-->

## Tidy, modular simulations

It is apparent from Table \@ref(tab:simulation) that writing a simulation in R involves a large space of possibilities and will requiring making a bunch of decisions.
Considering the number of choices to be made, it is critical to stay organized and to approach the process systematically.
Recognizing the components of a simulation is the starting point. 
Next is to see how to translate the components into R code.

In our own methodological work, we have found it very useful to always follow the same approach to writing code for a simulation. 
We call this approach *tidy, modular simulation*. It involves two simple principles: 

1. Implement each component of a simulation as a distinct function or object.
2. Store all results in rectangular data sets.

Writing separate functions for each component step of a simulation has several benefits. 
The first is the practical benefit of turning the coding process from a monolithic (and potentially daunting) activity into a set of smaller, discrete tasks. 
This lets us focus on one task at a time and makes it easier to see progress.
Second, following this principle makes for code that is easier to read, test, and debug.
Rather than having to scan through an entire code file to understand how the data analysis procedure is implemented, we can quickly identify the function that implements it, then focus on understanding the workings of that function.
Likewise, if another researcher wanted to test out the data analysis procedure on a dataset of their own, they could do so by running the corresponding function rather than having to dissect an entire script.
Third, writing separate code for each component makes it possible to tweak the code or swap out pieces of the simulation, such as by adding additional estimation methods or trying out a data-generating process involving different distributional assumptions.
We already saw this in Chapter \@ref(t-test-simulation), where we modified our initial data-generating process to use a geometric distribution rather than a normal distribution.
In short, following the first principle makes for simpler, more robust code that is easier to navigate, easier to test, and easier to extend.

The second principle of tidy, modular simulation is to store all results in rectangular datasets, such as the base R `data.frame` object or the tidyverse `tibble` object.[^tidydata]
This principle applies to any and all output, including the simulated data from Step 1, the results of data analysis procedures from Step 2, full sets of replicated simulation results from Step 3, and summarized results from Step 4.
A primary benefit of following this principle is that it facilitates working with the output of each stage in the simulation process.
If you are comfortable using R to analyze real data, you you can use the same skills and tools to examine simulation output as long as it is in tabular form.
Many of the data processing and data analysis tools available in R work with---or even require---rectangular datasets.
Thus, using rectangular datasets makes it easier to inspect, summarize, and visualize the output.

[^tidydata]: @Wickham2014tidydata provides a broader introduction to the concept of tidy data in the context of data-analysis tasks.

## Skeleton of a simulation study

The principles of tidy simulation imply that code for a simulation study should usually follow the same broad outline and organization of Table \@ref(tab:simulation), with custom functions for each step in process.
We will describe the outlines of simulation code using function skeletons to illustrate the inputs and outputs of each component.
These skeletons skip over all the specific details, so that we can see the structure more clearly.
We will first examine the structure of the code for simulating one specific scenario, then consider how to extend the code to systematically explore a variety of scenarios, as in a multifactor simulation.

In code skeletons, the structure of the first four steps in a simulation looks like this:

```{r, echo = TRUE, eval = FALSE}

# Generate (data-generating process)

generate_data <- function( model_params ) {
  # stuff
  return(data)
}

# Analyze (data-analysis procedure)

analyze <- function( data ) {
  # stuff
  return(one_result)
}

# Repeat

one_run <- function( model_params ) {
  dat <- generate_data( model_params )
  one_result <- analyze(dat)
  return(one_result)
}

results <- replicate(R, one_run( params ))

# Summarize (calculate performance measures)

assess_performance <- function( results, model_params ) {
  # stuff
  return(performance_measures)
}

assess_performance(results, model_params)
```

The code above shows the full skeleton of a simulation. 
It involves four functions, where the outputs of one function get used as inputs in subsequent functions. 
We will now look at the inputs and outputs of each function to see how they align with the four steps in the simulation process.
Subsequent chapters examine each piece in much greater detail---putting meat on the bones of each function skeleton, to torture our metaphor---and discuss specific statistical issues and programming techniques that are useful for designing each component.

Besides illustrating the skeletal framework of a simulation, readers might find it useful to use it as a template from which to start writing their own code. 
The `simhelpers` package includes the function `create_skeleton()`, which will open a new R script that contains a template for a simulation study, with sections corresponding to each component:
```{r eval=FALSE}
simhelpers::create_skeleton()
```
The template that appears is a slightly more elaborate version of the code above, with the main difference being that it also includes some additional lines of code to wire the pieces together for a multifactor simulation.
Starting from this template, you will already be on the road to writing a tidy, modular simulation.

### Data-Generating Process

The first step in a simulation is specifying a data-generating process. This is a hypothetical model for how data might arise, involving measurements or observations of one or more variables. The bare-bones skeleton of a data-generating function looks like the following:
```{r, eval=FALSE}
generate_data <- function( model_params ) {
  # stuff
  return(data)
}
```
The function takes as input a set of model parameter values, denoted here as `model_params`. 
Based on those model parameters, the function generates a hypothetical dataset as output.
Generating our own data based on a model allows us to know what the answer is (e.g., the true population mean or the true average effect of an intervention), so that we have benchmark against which to compare the results of a data analysis procedure that generates noisy estimates of the true value.

In practice, `model_params` will usually not be just one input but rather multiple arguments.
These arguments might include inputs such as the population mean for a variable, the standard deviation of a distribution of treatment effects, or a parameter controlling the degree of skewness in the population distribution.
Many data-generating processes involving multiple variables, such as the response variable and predictor variables in a regression model. 
In such instances, the inputs of `generate_data()` might also include parameters that determine the degree of dependence or correlation between variables. 
Further, the `generate_data()` inputs will also usually include arguments relating to the sample size and structure of the hypothetical dataset.
For instance, in a simulation of a multilevel dataset where individuals are nested within clusters, the inputs might include an arguments to specify the total number of clusters and the average number of individuals per cluster.
We discuss the inputs and form of the data-generating function further in Chapter \@ref(data-generating-processes).

### Data Analysis Procedure

The second step in a simulation is specifying a data analysis procedure or set of procedures.
The bare-bones skeleton of a data-generating function looks like the following:
```{r}
analyze <- function( data ) {
  # stuff
  return(one_result)
}
```
The function should take a single dataset as input and produce a set of estimates or results (e.g., point estimates, standard errors, confidence intervals, p-values, predictions, etc.).
Because we will be using the function to analyze hypothetical datasets simulated from the data-generating process, the `analyze()` function needs to work with `data` inputs that are produced by the `generate_data()` function. 
Thus, the code in the body of `analyze()` can assume that `data` will include relevant variables with specific names.

Inside the body of the function, `analyze()` includes code to carry out a data analysis procedure.
This might involve generating a confidence interval, as in the example from Chapter \@ref(t-test-simulation).
In another context, it might involve estimating an average growth rate along with a standard error, given a dataset with longitudinal repeated measurements from multiple individuals.
In still another context, it might involve generating individual-level predictions from a machine learning algorithm.
In simulations that involve comparing multiple analysis methods, we might write an `analyze()` function for each of the methods of interest, or (generally less preferred) we might write one function that does the calculations for all of the methods together.
<!-- JEP: Why is the latter generally less preferred? -->

A well-written estimation method should, in principle, work not only on a simulated, hypothetical dataset but also on a real empirical dataset that has the same format (i.e., appropriate variable names and structure).
Because of this, the inputs of the `analyze()` function should not typically include any information about the parameters of the data-generating process.
To be realistic, the code for our simulated data-analysis procedure should not make use of anything that the analyst could not know when analyzing a real dataset.
Thus, `analyze()` has an argument for the sample dataset but not for `model_params`.
We discuss the form and content of the data analysis function further in Chapter \@ref(data-analysis-procedures).

### Repetition

The third step in a simulation is to repeatedly evaluate the data-generating process and data analysis procedure. 
In practice, this amounts to repeatedly calling `generate_data()` and then calling `analyze()` on the result.
Here is the skeleton from our simulation template:
```{r, eval=FALSE}
one_run <- function( model_params ) {
  dat <- generate_data( model_params )
  one_result <- analyze(dat)
  return(one_result)
}

results <- simhelpers::repeat_and_stack(R, one_run( params ))
```
We first create a helper function called `one_run()`, which takes `model_params` as input. 
Inside the body of the function, we call the `generate_data()` function to simulate a hypothetical dataset.
We pass this dataset to `analyze()` and return a small dataset containing the results of the data-analysis procedure.
The `one_run()` method is like a coordinator or dispatcher of the system: it generates the data, calls all the evaluation methods we want to call, combines all the results, and hands them back for recording.
Making a helper method such as `one_run()` can be useful because it facilitates debugging.

Once we have the `one_run()` helper function, we need a way to call it repeatedly. As with many things in R, there are a variety of different ways to do something over and over. 
In the above skeleton, we use the `repeat_and_stack()` function from `simhelpers`.[^replicate]
In the first argument, we specify the number of times to repeat the process. 
In the second argument, we specify an expression that evaluates `one_run()` for specified values of the model parameters stored in `params`.
The `repeat_and_stack()` function then evaluates the expression repeatedly, `R` times in all, and then stacks up all of the replications into a big dataset, with one or more rows per replication.[^map-dfr] 

[^map-dfr]: If you would prefer the output as a list rather than a stacked dataset, set `repeat_and_stack()`'s optional argument `stack = FALSE`.

[^replicate]: In the example from Chapter \@ref(t-test-simulation), we used the `replicate()` function from base R to repeat the process of generating and analyzing data. 
This function is a fine alternative to the `repeat_and_stack()` approach demonstrated in the skeleton.
The only drawback is that it requires some further work to combine the results across replications. 
Here is a different version of the skeleton, which uses `replicate()` instead of `repeat_and_stack()`:
    ```{r, eval=FALSE}
    results_list <- replicate(n = R, expr = {
      dat <- generate_data( params )
      one_result <- analyze(dat)
      return(one_result)
    }, simplify = FALSE)
    
    results <- purrr::list_rbind(results_list)
    ```
   This version of the skeleton does not create a `one_run()` helper function, but instead puts the code from the body of `one_run()` directly into the `expr` argument of `replicate()`. To learn about other ways of repeatedly evaluating the simulation process, see Appendix \@ref(repeating-oneself).
 
We go into further detail about how to approach running the simulation process in Chapter \@ref(running-the-simulation-process).
Among other things, we will illustrate how to use the `bundle_sim()` function from the `simhelpers` package to automate the process of coding this step, thereby avoiding the need to write a `one_run()` helper function.

### Performance summaries

The fourth step in a simulation is to summarize the distribution of simulation results across replications. 
Here is the skeleton from our simulation template:
```{r, eval = FALSE}
assess_performance <- function( results, model_params ) {
  # stuff
  return(performance_measures)
}

assess_performance(results, params)
```
The `assess_performance()` function takes `results` as input.
`results` should be a dataset containing all of the replications of the data-generating and analysis process. 
In contrast to the `analyze()` function, `assess_performance()` also needs to know the true parameter values of the data-generating process, so it needs to have `model_params` as its other input.
The function then uses both of these inputs to calculate performance measures and returns a summary of the performance measures in a dataset.

Performance measures are the metrics or criteria used to assess the performance of a statistical method across repeated samples from the data-generating process.
For example, we might want to know how close an estimator gets to the target parameter, on average. 
We might want to know if a confidence interval captures the true parameter the right proportion of the time, as in the simulation from Chapter \@ref(t-test-simulation).
Performance is defined in terms of the sampling distribution of estimators or analysis results, across an infinite number of replications of the data-generating process. 
In practice, we use many replications of the process, but still only a finite number. Consequently, we actually _estimate_ the performance measures and need to attend to the Monte Carlo error in the estimates. 
We discuss the specifics of different performance measures and assessment of Monte Carlo error in Chapter \@ref(performance-criteria).

### Multifactor simulations

Thus far, we have sketched out the structure of a modular, tidy simulation for a single context.
In our $t$-test case study, for example, we might ask how well the $t$-test works when we have $n=100$ units and the observations follow geometric distribution.
However, we rarely want to examine a method only in a single context.
Typically, we want to explore how well a procedure works across a range of different contexts.
If we choose conditions in a structured and thoughtful manner, we will be able to examine broad trends and potentially make more general claims about the behaviors of the data-analysis procedures under investigation.
Thus, it is helpful to think of simulations as akin to a designed experiment: in seeking to understand the properties of one or more procedures, we test them under a variety of different scenarios to see how they perform, then seek to identify more general patterns that hold beyond the specific scenarios examined.
This is the heart of simulation for methodological evaluation.

To implement a multifactor simulation, we will follows the same principles of modular, tidy simulation.
In particular, we will take the code developed for simulating a single context and bundle it into a function that can be evaluated for any and all scenarios of interest.
Simulation studies often follow a  full factorial design, in which each level of a factor (something we vary, such as sample size, true treatment effect, or residual variance) is crossed with every other level.
The experimental design then consists of sets of parameter values (including design parameters, such as sample sizes), and these too can be represented in an object, distinct from the other components of the simulation. 
We will discuss multiple-scenario simulations in Part III (starting with Chapter \@ref(exp-design)), after we more fully develop the core concepts and techniques involved in simulating a single context.

## Exercises

1. Look back at the $t$-test simulation presented in Chapter \@ref(t-test-simulation). The code presented there did not entirely follow the formal structure outlined in this chapter. Revise the code by creating separate functions for each of four components in the simulation skeleton. Using the functions, re-run the simulation and recreate one or more graphs from the exercises in the previous chapter.