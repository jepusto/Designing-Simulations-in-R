We highly recommend using RStudio, which makes using R easier.  RStudio is an Integrated Development Environment (IDE) that structures your experience, helps keep things organized, and offers multiple time-saving features to make your programming experience better. You might also consider using R Markdown. R Markdown allows for generating documents with embedded R code and R output in a clean format, which can greatly help report generation.

The easiest way to compare two estimation procedures is to generate some data from scratch, generating it in such a way that we know what the "right answer" is, and then to analyze our data using the procedures we wish to study.
For each dataset we generate, we can write down how close the each procedure got to the right answer, whether they ended up with an answer that was too high or too low, and so forth.
If our procedures include a hypothesis test against some hypothetical null, we can write down whether we rejected or did not reject this null.
 
If we do this once, we have some idea of whether the estimators worked in that specific example, but we do not know if this is due to random chance.
To assess general trends, therefore, we repeat this process over and over, keeping track of how well our estimators did each time.
We finally aggregate our results, and see if one estimator systematically outperformed the other.
But that is not quite enough: we might learn, by doing this, that estimator $A$ is better than estimator $B$ in the one context we have just simulated, but we will not know if it is _generally_ superior.


Also similar to laboratory experiments, methodological simulations also often examine conditions through a factorial experimental design, where each factor corresponds to a parameter or feature of the data-generating process.




As we saw in our initial example, we can break the the logic of simulation for a specific and specified scenario down into the four steps outlined on Table \@ref(tab:simulation).
We will then repeat this evaluation across different scenarios, so we can see how performance changes as we chance circumstance.
But first, we just focus on a single scenario.


To repeat, the tidy approach we propose has several advantages:

  * Easier to check & debug.

  * Easier to modify your code.

  * Easier to make everything run fast.

  * Facilitates creative re-use.

the more we can "black box" our methods into a single function call, the easier it will be to separate out the structure of the simulation from the complexity of the methods being evaluated.


### MCSE for power and coverage

For power and coverage, we score, for each simulation, a 0 if we do not reject (or capture the parameter) and a 1 otherwise.
We are counting successes, and take the proportion of successes as our overall estimate of performance.
Because the replications are independent, these rates follow a binomial distribution with MCSE 

$$MCSE = \sqrt{\frac{\rho_\alpha\left(1 - \rho_\alpha\right)}{R}} .$$ 