<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 9 Performance Measures | Designing Monte Carlo Simulations in R</title>
  <meta name="description" content="A text on designing, implementing, and reporting on Monte Carlo simulation studies" />
  <meta name="generator" content="bookdown 0.45 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 9 Performance Measures | Designing Monte Carlo Simulations in R" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="A text on designing, implementing, and reporting on Monte Carlo simulation studies" />
  <meta name="github-repo" content="jepusto/Designing-Simulations-in-R" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 9 Performance Measures | Designing Monte Carlo Simulations in R" />
  
  <meta name="twitter:description" content="A text on designing, implementing, and reporting on Monte Carlo simulation studies" />
  

<meta name="author" content="Luke W. Miratrix and James E. Pustejovsky (Equal authors)" />


<meta name="date" content="2026-01-06" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="running-the-simulation-process.html"/>
<link rel="next" href="simulating-multiple-scenarios.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
<span class="math inline">
\(\newcommand{\Prob}{\text{Pr}}\)
\(\newcommand{\E}{\mathbb{E}}\)
\(\newcommand{\M}{\mathbb{M}}\)
\(\newcommand{\Q}{\mathbb{Q}}\)
\(\newcommand{\Var}{\text{Var}}\)
\(\newcommand{\Bias}{\text{Bias}}\)
\(\newcommand{\RMSE}{\text{RMSE}}\)
\(\newcommand{\Cov}{\text{Cov}}\)
\(\newcommand{\cor}{\text{cor}}\)
\(\newcommand{\diag}{\text{diag}}\)
\(\newcommand{\mat}[1]{\mathbf{#1}}\)
\(\newcommand{\bs}{\boldsymbol}\)
\(\newcommand{\trace}{\text{tr}}\)
</span>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Designing Simulations in R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#license"><i class="fa fa-check"></i>License</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#about-the-authors"><i class="fa fa-check"></i>About the authors</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
<li class="part"><span><b>I An Introductory Look</b></span></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#some-of-simulations-many-uses"><i class="fa fa-check"></i><b>1.1</b> Some of simulation’s many uses</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="introduction.html"><a href="introduction.html#comparing-statistical-approaches"><i class="fa fa-check"></i><b>1.1.1</b> Comparing statistical approaches</a></li>
<li class="chapter" data-level="1.1.2" data-path="introduction.html"><a href="introduction.html#assessing-performance-of-complex-pipelines"><i class="fa fa-check"></i><b>1.1.2</b> Assessing performance of complex pipelines</a></li>
<li class="chapter" data-level="1.1.3" data-path="introduction.html"><a href="introduction.html#assessing-performance-under-misspecification"><i class="fa fa-check"></i><b>1.1.3</b> Assessing performance under misspecification</a></li>
<li class="chapter" data-level="1.1.4" data-path="introduction.html"><a href="introduction.html#assessing-the-finite-sample-performance-of-a-statistical-approach"><i class="fa fa-check"></i><b>1.1.4</b> Assessing the finite-sample performance of a statistical approach</a></li>
<li class="chapter" data-level="1.1.5" data-path="introduction.html"><a href="introduction.html#conducting-power-analyses"><i class="fa fa-check"></i><b>1.1.5</b> Conducting Power Analyses</a></li>
<li class="chapter" data-level="1.1.6" data-path="introduction.html"><a href="introduction.html#simulating-processess"><i class="fa fa-check"></i><b>1.1.6</b> Simulating processess</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#the-perils-of-simulation-as-evidence"><i class="fa fa-check"></i><b>1.2</b> The perils of simulation as evidence</a></li>
<li class="chapter" data-level="1.3" data-path="introduction.html"><a href="introduction.html#simulating-to-learn"><i class="fa fa-check"></i><b>1.3</b> Simulating to learn</a></li>
<li class="chapter" data-level="1.4" data-path="introduction.html"><a href="introduction.html#why-r"><i class="fa fa-check"></i><b>1.4</b> Why R?</a></li>
<li class="chapter" data-level="1.5" data-path="introduction.html"><a href="introduction.html#organization-of-the-text"><i class="fa fa-check"></i><b>1.5</b> Organization of the text</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="programming-preliminaries.html"><a href="programming-preliminaries.html"><i class="fa fa-check"></i><b>2</b> Programming Preliminaries</a>
<ul>
<li class="chapter" data-level="2.1" data-path="programming-preliminaries.html"><a href="programming-preliminaries.html#welcome-to-the-tidyverse"><i class="fa fa-check"></i><b>2.1</b> Welcome to the tidyverse</a></li>
<li class="chapter" data-level="2.2" data-path="programming-preliminaries.html"><a href="programming-preliminaries.html#functions"><i class="fa fa-check"></i><b>2.2</b> Functions</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="programming-preliminaries.html"><a href="programming-preliminaries.html#rolling-your-own"><i class="fa fa-check"></i><b>2.2.1</b> Rolling your own</a></li>
<li class="chapter" data-level="2.2.2" data-path="programming-preliminaries.html"><a href="programming-preliminaries.html#a-dangerous-function"><i class="fa fa-check"></i><b>2.2.2</b> A dangerous function</a></li>
<li class="chapter" data-level="2.2.3" data-path="programming-preliminaries.html"><a href="programming-preliminaries.html#using-named-arguments"><i class="fa fa-check"></i><b>2.2.3</b> Using Named Arguments</a></li>
<li class="chapter" data-level="2.2.4" data-path="programming-preliminaries.html"><a href="programming-preliminaries.html#argument-defaults"><i class="fa fa-check"></i><b>2.2.4</b> Argument Defaults</a></li>
<li class="chapter" data-level="2.2.5" data-path="programming-preliminaries.html"><a href="programming-preliminaries.html#function-skeletons"><i class="fa fa-check"></i><b>2.2.5</b> Function skeletons</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="programming-preliminaries.html"><a href="programming-preliminaries.html#pipe-dreams"><i class="fa fa-check"></i><b>2.3</b> <code>\&gt;</code> (Pipe) dreams</a></li>
<li class="chapter" data-level="2.4" data-path="programming-preliminaries.html"><a href="programming-preliminaries.html#recipes-versus-patterns"><i class="fa fa-check"></i><b>2.4</b> Recipes versus Patterns</a></li>
<li class="chapter" data-level="2.5" data-path="programming-preliminaries.html"><a href="programming-preliminaries.html#exercises"><i class="fa fa-check"></i><b>2.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="t-test-simulation.html"><a href="t-test-simulation.html"><i class="fa fa-check"></i><b>3</b> An initial simulation</a>
<ul>
<li class="chapter" data-level="3.1" data-path="t-test-simulation.html"><a href="t-test-simulation.html#simulating-a-single-scenario"><i class="fa fa-check"></i><b>3.1</b> Simulating a single scenario</a></li>
<li class="chapter" data-level="3.2" data-path="t-test-simulation.html"><a href="t-test-simulation.html#a-non-normal-population-distribution"><i class="fa fa-check"></i><b>3.2</b> A non-normal population distribution</a></li>
<li class="chapter" data-level="3.3" data-path="t-test-simulation.html"><a href="t-test-simulation.html#simulating-across-different-scenarios"><i class="fa fa-check"></i><b>3.3</b> Simulating across different scenarios</a></li>
<li class="chapter" data-level="3.4" data-path="t-test-simulation.html"><a href="t-test-simulation.html#extending-the-simulation-design"><i class="fa fa-check"></i><b>3.4</b> Extending the simulation design</a></li>
<li class="chapter" data-level="3.5" data-path="t-test-simulation.html"><a href="t-test-simulation.html#exercises-1"><i class="fa fa-check"></i><b>3.5</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>II Structure and Mechanics of a Simulation Study</b></span></li>
<li class="chapter" data-level="4" data-path="simulation-structure.html"><a href="simulation-structure.html"><i class="fa fa-check"></i><b>4</b> Structure of a simulation study</a>
<ul>
<li class="chapter" data-level="4.1" data-path="simulation-structure.html"><a href="simulation-structure.html#general-structure-of-a-simulation"><i class="fa fa-check"></i><b>4.1</b> General structure of a simulation</a></li>
<li class="chapter" data-level="4.2" data-path="simulation-structure.html"><a href="simulation-structure.html#tidy-modular-simulations"><i class="fa fa-check"></i><b>4.2</b> Tidy, modular simulations</a></li>
<li class="chapter" data-level="4.3" data-path="simulation-structure.html"><a href="simulation-structure.html#skeleton-of-a-simulation-study"><i class="fa fa-check"></i><b>4.3</b> Skeleton of a simulation study</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="simulation-structure.html"><a href="simulation-structure.html#data-generating-process"><i class="fa fa-check"></i><b>4.3.1</b> Data-Generating Process</a></li>
<li class="chapter" data-level="4.3.2" data-path="simulation-structure.html"><a href="simulation-structure.html#data-analysis-procedure"><i class="fa fa-check"></i><b>4.3.2</b> Data Analysis Procedure</a></li>
<li class="chapter" data-level="4.3.3" data-path="simulation-structure.html"><a href="simulation-structure.html#repetition"><i class="fa fa-check"></i><b>4.3.3</b> Repetition</a></li>
<li class="chapter" data-level="4.3.4" data-path="simulation-structure.html"><a href="simulation-structure.html#performance-summaries"><i class="fa fa-check"></i><b>4.3.4</b> Performance summaries</a></li>
<li class="chapter" data-level="4.3.5" data-path="simulation-structure.html"><a href="simulation-structure.html#multifactor-simulations"><i class="fa fa-check"></i><b>4.3.5</b> Multifactor simulations</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="simulation-structure.html"><a href="simulation-structure.html#exercises-2"><i class="fa fa-check"></i><b>4.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="case-ANOVA.html"><a href="case-ANOVA.html"><i class="fa fa-check"></i><b>5</b> Case Study: Heteroskedastic ANOVA and Welch</a>
<ul>
<li class="chapter" data-level="5.1" data-path="case-ANOVA.html"><a href="case-ANOVA.html#case-anova-DGP"><i class="fa fa-check"></i><b>5.1</b> The data-generating model</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="case-ANOVA.html"><a href="case-ANOVA.html#now-make-a-function"><i class="fa fa-check"></i><b>5.1.1</b> Now make a function</a></li>
<li class="chapter" data-level="5.1.2" data-path="case-ANOVA.html"><a href="case-ANOVA.html#cautious-coding"><i class="fa fa-check"></i><b>5.1.2</b> Cautious coding</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="case-ANOVA.html"><a href="case-ANOVA.html#ANOVA-hypothesis-testing-function"><i class="fa fa-check"></i><b>5.2</b> The hypothesis testing procedures</a></li>
<li class="chapter" data-level="5.3" data-path="case-ANOVA.html"><a href="case-ANOVA.html#running-the-simulation"><i class="fa fa-check"></i><b>5.3</b> Running the simulation</a></li>
<li class="chapter" data-level="5.4" data-path="case-ANOVA.html"><a href="case-ANOVA.html#summarizing-test-performance"><i class="fa fa-check"></i><b>5.4</b> Summarizing test performance</a></li>
<li class="chapter" data-level="5.5" data-path="case-ANOVA.html"><a href="case-ANOVA.html#exAnovaExercises"><i class="fa fa-check"></i><b>5.5</b> Exercises</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="case-ANOVA.html"><a href="case-ANOVA.html#BF-other-alphas"><i class="fa fa-check"></i><b>5.5.1</b> Other <span class="math inline">\(\alpha\)</span>’s</a></li>
<li class="chapter" data-level="5.5.2" data-path="case-ANOVA.html"><a href="case-ANOVA.html#BF-compare-results"><i class="fa fa-check"></i><b>5.5.2</b> Compare results</a></li>
<li class="chapter" data-level="5.5.3" data-path="case-ANOVA.html"><a href="case-ANOVA.html#BF-power"><i class="fa fa-check"></i><b>5.5.3</b> Power</a></li>
<li class="chapter" data-level="5.5.4" data-path="case-ANOVA.html"><a href="case-ANOVA.html#BF-wide-long"><i class="fa fa-check"></i><b>5.5.4</b> Wide or long?</a></li>
<li class="chapter" data-level="5.5.5" data-path="case-ANOVA.html"><a href="case-ANOVA.html#BF-other-tests"><i class="fa fa-check"></i><b>5.5.5</b> Other tests</a></li>
<li class="chapter" data-level="5.5.6" data-path="case-ANOVA.html"><a href="case-ANOVA.html#methodological-extensions"><i class="fa fa-check"></i><b>5.5.6</b> Methodological extensions</a></li>
<li class="chapter" data-level="5.5.7" data-path="case-ANOVA.html"><a href="case-ANOVA.html#power-analysis"><i class="fa fa-check"></i><b>5.5.7</b> Power analysis</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="data-generating-processes.html"><a href="data-generating-processes.html"><i class="fa fa-check"></i><b>6</b> Data-generating processes</a>
<ul>
<li class="chapter" data-level="6.1" data-path="data-generating-processes.html"><a href="data-generating-processes.html#DGP-examples"><i class="fa fa-check"></i><b>6.1</b> Examples</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="data-generating-processes.html"><a href="data-generating-processes.html#ANOVA-example"><i class="fa fa-check"></i><b>6.1.1</b> Example 1: One-way analysis of variance</a></li>
<li class="chapter" data-level="6.1.2" data-path="data-generating-processes.html"><a href="data-generating-processes.html#BVPois-example"><i class="fa fa-check"></i><b>6.1.2</b> Example 2: Bivariate Poisson model</a></li>
<li class="chapter" data-level="6.1.3" data-path="data-generating-processes.html"><a href="data-generating-processes.html#CRT-example"><i class="fa fa-check"></i><b>6.1.3</b> Example 3: Hierarchical linear model for a cluster-randomized trial</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="data-generating-processes.html"><a href="data-generating-processes.html#components-of-a-dgp"><i class="fa fa-check"></i><b>6.2</b> Components of a DGP</a></li>
<li class="chapter" data-level="6.3" data-path="data-generating-processes.html"><a href="data-generating-processes.html#DGP-functions"><i class="fa fa-check"></i><b>6.3</b> A statistical model is a recipe for data generation</a></li>
<li class="chapter" data-level="6.4" data-path="data-generating-processes.html"><a href="data-generating-processes.html#DGP-plotting"><i class="fa fa-check"></i><b>6.4</b> Plot the artificial data</a></li>
<li class="chapter" data-level="6.5" data-path="data-generating-processes.html"><a href="data-generating-processes.html#check-the-data-generating-function"><i class="fa fa-check"></i><b>6.5</b> Check the data-generating function</a></li>
<li class="chapter" data-level="6.6" data-path="data-generating-processes.html"><a href="data-generating-processes.html#case-cluster"><i class="fa fa-check"></i><b>6.6</b> Example: Simulating clustered data</a>
<ul>
<li class="chapter" data-level="6.6.1" data-path="data-generating-processes.html"><a href="data-generating-processes.html#a-design-decision-what-do-we-want-to-manipulate"><i class="fa fa-check"></i><b>6.6.1</b> A design decision: What do we want to manipulate?</a></li>
<li class="chapter" data-level="6.6.2" data-path="data-generating-processes.html"><a href="data-generating-processes.html#a-model-for-a-cluster-rct"><i class="fa fa-check"></i><b>6.6.2</b> A model for a cluster RCT</a></li>
<li class="chapter" data-level="6.6.3" data-path="data-generating-processes.html"><a href="data-generating-processes.html#from-equations-to-code"><i class="fa fa-check"></i><b>6.6.3</b> From equations to code</a></li>
<li class="chapter" data-level="6.6.4" data-path="data-generating-processes.html"><a href="data-generating-processes.html#DGP-standardization"><i class="fa fa-check"></i><b>6.6.4</b> Standardization in the DGP</a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="data-generating-processes.html"><a href="data-generating-processes.html#three-parameter-IRT"><i class="fa fa-check"></i><b>6.7</b> Sometimes a DGP is all you need</a></li>
<li class="chapter" data-level="6.8" data-path="data-generating-processes.html"><a href="data-generating-processes.html#more-to-explore"><i class="fa fa-check"></i><b>6.8</b> More to explore</a></li>
<li class="chapter" data-level="6.9" data-path="data-generating-processes.html"><a href="data-generating-processes.html#exercises-3"><i class="fa fa-check"></i><b>6.9</b> Exercises</a>
<ul>
<li class="chapter" data-level="6.9.1" data-path="data-generating-processes.html"><a href="data-generating-processes.html#Welch-t-dgp"><i class="fa fa-check"></i><b>6.9.1</b> The Welch test on a shifted-and-scaled <span class="math inline">\(t\)</span> distribution</a></li>
<li class="chapter" data-level="6.9.2" data-path="data-generating-processes.html"><a href="data-generating-processes.html#plot-the-bivariate-poisson"><i class="fa fa-check"></i><b>6.9.2</b> Plot the bivariate Poisson</a></li>
<li class="chapter" data-level="6.9.3" data-path="data-generating-processes.html"><a href="data-generating-processes.html#BVP-check"><i class="fa fa-check"></i><b>6.9.3</b> Check the bivariate Poisson function</a></li>
<li class="chapter" data-level="6.9.4" data-path="data-generating-processes.html"><a href="data-generating-processes.html#BVP-error"><i class="fa fa-check"></i><b>6.9.4</b> Add error-catching to the bivariate Poisson function</a></li>
<li class="chapter" data-level="6.9.5" data-path="data-generating-processes.html"><a href="data-generating-processes.html#BVNB1"><i class="fa fa-check"></i><b>6.9.5</b> A bivariate negative binomial distribution</a></li>
<li class="chapter" data-level="6.9.6" data-path="data-generating-processes.html"><a href="data-generating-processes.html#BVNB2"><i class="fa fa-check"></i><b>6.9.6</b> Another bivariate negative binomial distribution</a></li>
<li class="chapter" data-level="6.9.7" data-path="data-generating-processes.html"><a href="data-generating-processes.html#cluster-RCT-plot"><i class="fa fa-check"></i><b>6.9.7</b> Plot the data from a cluster-randomized trial</a></li>
<li class="chapter" data-level="6.9.8" data-path="data-generating-processes.html"><a href="data-generating-processes.html#cluster-RCT-checks"><i class="fa fa-check"></i><b>6.9.8</b> Checking the Cluster RCT DGP</a></li>
<li class="chapter" data-level="6.9.9" data-path="data-generating-processes.html"><a href="data-generating-processes.html#cluster-RCT-heterogeneity"><i class="fa fa-check"></i><b>6.9.9</b> More school-level variation</a></li>
<li class="chapter" data-level="6.9.10" data-path="data-generating-processes.html"><a href="data-generating-processes.html#cluster-RCT-baseline"><i class="fa fa-check"></i><b>6.9.10</b> Cluster-randomized trial with baseline predictors</a></li>
<li class="chapter" data-level="6.9.11" data-path="data-generating-processes.html"><a href="data-generating-processes.html#IRT-DGP-parameters"><i class="fa fa-check"></i><b>6.9.11</b> 3-parameter IRT datasets</a></li>
<li class="chapter" data-level="6.9.12" data-path="data-generating-processes.html"><a href="data-generating-processes.html#IRT-DGP-checking"><i class="fa fa-check"></i><b>6.9.12</b> Check the 3-parameter IRT DGP</a></li>
<li class="chapter" data-level="6.9.13" data-path="data-generating-processes.html"><a href="data-generating-processes.html#IRT-DGP-breaking"><i class="fa fa-check"></i><b>6.9.13</b> Explore the 3-parameter IRT model</a></li>
<li class="chapter" data-level="6.9.14" data-path="data-generating-processes.html"><a href="data-generating-processes.html#meta-regression-DGP"><i class="fa fa-check"></i><b>6.9.14</b> Random effects meta-regression</a></li>
<li class="chapter" data-level="6.9.15" data-path="data-generating-processes.html"><a href="data-generating-processes.html#Vevea-Hedges-DGP"><i class="fa fa-check"></i><b>6.9.15</b> Meta-regression with selective reporting</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="data-analysis-procedures.html"><a href="data-analysis-procedures.html"><i class="fa fa-check"></i><b>7</b> Data analysis procedures</a>
<ul>
<li class="chapter" data-level="7.1" data-path="data-analysis-procedures.html"><a href="data-analysis-procedures.html#estimation-functions"><i class="fa fa-check"></i><b>7.1</b> Writing estimation functions</a></li>
<li class="chapter" data-level="7.2" data-path="data-analysis-procedures.html"><a href="data-analysis-procedures.html#multiple-estimation-procedures"><i class="fa fa-check"></i><b>7.2</b> Including Multiple Data Analysis Procedures</a></li>
<li class="chapter" data-level="7.3" data-path="data-analysis-procedures.html"><a href="data-analysis-procedures.html#validating-an-estimation-function"><i class="fa fa-check"></i><b>7.3</b> Validating an Estimation Function</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="data-analysis-procedures.html"><a href="data-analysis-procedures.html#checking-against-existing-implementations"><i class="fa fa-check"></i><b>7.3.1</b> Checking against existing implementations</a></li>
<li class="chapter" data-level="7.3.2" data-path="data-analysis-procedures.html"><a href="data-analysis-procedures.html#checking-novel-procedures"><i class="fa fa-check"></i><b>7.3.2</b> Checking novel procedures</a></li>
<li class="chapter" data-level="7.3.3" data-path="data-analysis-procedures.html"><a href="data-analysis-procedures.html#checking-with-simulations"><i class="fa fa-check"></i><b>7.3.3</b> Checking with simulations</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="data-analysis-procedures.html"><a href="data-analysis-procedures.html#handling-errors-warnings-and-other-hiccups"><i class="fa fa-check"></i><b>7.4</b> Handling errors, warnings, and other hiccups</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="data-analysis-procedures.html"><a href="data-analysis-procedures.html#capturing-errors-and-warnings"><i class="fa fa-check"></i><b>7.4.1</b> Capturing errors and warnings</a></li>
<li class="chapter" data-level="7.4.2" data-path="data-analysis-procedures.html"><a href="data-analysis-procedures.html#adapting-for-errors"><i class="fa fa-check"></i><b>7.4.2</b> Adapting estimation procedures for errors and warnings</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="data-analysis-procedures.html"><a href="data-analysis-procedures.html#exercises-4"><i class="fa fa-check"></i><b>7.5</b> Exercises</a>
<ul>
<li class="chapter" data-level="7.5.1" data-path="data-analysis-procedures.html"><a href="data-analysis-procedures.html#BFFs-forever"><i class="fa fa-check"></i><b>7.5.1</b> More Heteroskedastic ANOVA</a></li>
<li class="chapter" data-level="7.5.2" data-path="data-analysis-procedures.html"><a href="data-analysis-procedures.html#contingent-testing"><i class="fa fa-check"></i><b>7.5.2</b> Contingent testing</a></li>
<li class="chapter" data-level="7.5.3" data-path="data-analysis-procedures.html"><a href="data-analysis-procedures.html#cross-check-CRT-estimators"><i class="fa fa-check"></i><b>7.5.3</b> Check the cluster-RCT functions</a></li>
<li class="chapter" data-level="7.5.4" data-path="data-analysis-procedures.html"><a href="data-analysis-procedures.html#CRT-ANCOVA-estimators"><i class="fa fa-check"></i><b>7.5.4</b> Extending the cluster-RCT functions</a></li>
<li class="chapter" data-level="7.5.5" data-path="data-analysis-procedures.html"><a href="data-analysis-procedures.html#contingent-estimator-processing"><i class="fa fa-check"></i><b>7.5.5</b> Contingent estimator processing</a></li>
<li class="chapter" data-level="7.5.6" data-path="data-analysis-procedures.html"><a href="data-analysis-procedures.html#IRT-3PL-estimation"><i class="fa fa-check"></i><b>7.5.6</b> Estimating 3-parameter item response theory models</a></li>
<li class="chapter" data-level="7.5.7" data-path="data-analysis-procedures.html"><a href="data-analysis-procedures.html#Vevea-Hedges-estimation"><i class="fa fa-check"></i><b>7.5.7</b> Meta-regression with selective reporting</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="running-the-simulation-process.html"><a href="running-the-simulation-process.html"><i class="fa fa-check"></i><b>8</b> Running the Simulation Process</a>
<ul>
<li class="chapter" data-level="8.1" data-path="running-the-simulation-process.html"><a href="running-the-simulation-process.html#repeating-oneself"><i class="fa fa-check"></i><b>8.1</b> Repeating oneself</a></li>
<li class="chapter" data-level="8.2" data-path="running-the-simulation-process.html"><a href="running-the-simulation-process.html#one-run-at-a-time"><i class="fa fa-check"></i><b>8.2</b> One run at a time</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="running-the-simulation-process.html"><a href="running-the-simulation-process.html#one-run-reparameterization"><i class="fa fa-check"></i><b>8.2.1</b> Reparameterizing</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="running-the-simulation-process.html"><a href="running-the-simulation-process.html#bundle-sim-demo"><i class="fa fa-check"></i><b>8.3</b> Bundling simulations with <code>simhelpers</code></a></li>
<li class="chapter" data-level="8.4" data-path="running-the-simulation-process.html"><a href="running-the-simulation-process.html#seeds-and-pseudo-RNGs"><i class="fa fa-check"></i><b>8.4</b> Seeds and pseudo-random number generators</a></li>
<li class="chapter" data-level="8.5" data-path="running-the-simulation-process.html"><a href="running-the-simulation-process.html#exercises-5"><i class="fa fa-check"></i><b>8.5</b> Exercises</a>
<ul>
<li class="chapter" data-level="8.5.1" data-path="running-the-simulation-process.html"><a href="running-the-simulation-process.html#Welch-simulation"><i class="fa fa-check"></i><b>8.5.1</b> Welch simulations</a></li>
<li class="chapter" data-level="8.5.2" data-path="running-the-simulation-process.html"><a href="running-the-simulation-process.html#Pearson-sampling-distributions"><i class="fa fa-check"></i><b>8.5.2</b> Compare sampling distributions of Pearson’s correlation coefficients</a></li>
<li class="chapter" data-level="8.5.3" data-path="running-the-simulation-process.html"><a href="running-the-simulation-process.html#reparameterization-redux"><i class="fa fa-check"></i><b>8.5.3</b> Reparameterization, redux</a></li>
<li class="chapter" data-level="8.5.4" data-path="running-the-simulation-process.html"><a href="running-the-simulation-process.html#fancy-cluster-RCT-sims"><i class="fa fa-check"></i><b>8.5.4</b> Fancy clustered RCT simulations</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="performance-measures.html"><a href="performance-measures.html"><i class="fa fa-check"></i><b>9</b> Performance Measures</a>
<ul>
<li class="chapter" data-level="9.1" data-path="performance-measures.html"><a href="performance-measures.html#assessing-point-estimators"><i class="fa fa-check"></i><b>9.1</b> Measures for Point Estimators</a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="performance-measures.html"><a href="performance-measures.html#clusterRCTperformance"><i class="fa fa-check"></i><b>9.1.1</b> Comparing the Performance of the Cluster RCT Estimation Procedures</a></li>
<li class="chapter" data-level="9.1.2" data-path="performance-measures.html"><a href="performance-measures.html#less-conventional-measures"><i class="fa fa-check"></i><b>9.1.2</b> Less Conventional Performance Measures</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="performance-measures.html"><a href="performance-measures.html#measures-for-variance-estimators"><i class="fa fa-check"></i><b>9.2</b> Measures for Variance Estimators</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="performance-measures.html"><a href="performance-measures.html#satterthwaite-degrees-of-freedom"><i class="fa fa-check"></i><b>9.2.1</b> Satterthwaite degrees of freedom</a></li>
<li class="chapter" data-level="9.2.2" data-path="performance-measures.html"><a href="performance-measures.html#assessing-ses-for-the-cluster-rct-simulation"><i class="fa fa-check"></i><b>9.2.2</b> Assessing SEs for the Cluster RCT Simulation</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="performance-measures.html"><a href="performance-measures.html#measures-for-confidence-intervals"><i class="fa fa-check"></i><b>9.3</b> Measures for Confidence Intervals</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="performance-measures.html"><a href="performance-measures.html#cluster-RCT-CI-coverage"><i class="fa fa-check"></i><b>9.3.1</b> Confidence Intervals in the Cluster RCT Simulation</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="performance-measures.html"><a href="performance-measures.html#assessing-inferential-procedures"><i class="fa fa-check"></i><b>9.4</b> Measures for Inferential Procedures (Hypothesis Tests)</a>
<ul>
<li class="chapter" data-level="9.4.1" data-path="performance-measures.html"><a href="performance-measures.html#validity"><i class="fa fa-check"></i><b>9.4.1</b> Validity</a></li>
<li class="chapter" data-level="9.4.2" data-path="performance-measures.html"><a href="performance-measures.html#power"><i class="fa fa-check"></i><b>9.4.2</b> Power</a></li>
<li class="chapter" data-level="9.4.3" data-path="performance-measures.html"><a href="performance-measures.html#rejection-rates"><i class="fa fa-check"></i><b>9.4.3</b> Rejection Rates</a></li>
<li class="chapter" data-level="9.4.4" data-path="performance-measures.html"><a href="performance-measures.html#inference-in-the-cluster-rct-simulation"><i class="fa fa-check"></i><b>9.4.4</b> Inference in the Cluster RCT Simulation</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="performance-measures.html"><a href="performance-measures.html#sec-relative-performance"><i class="fa fa-check"></i><b>9.5</b> Relative or Absolute Measures?</a>
<ul>
<li class="chapter" data-level="9.5.1" data-path="performance-measures.html"><a href="performance-measures.html#performance-relative-to-a-benchmark-estimator"><i class="fa fa-check"></i><b>9.5.1</b> Performance relative to a benchmark estimator</a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="performance-measures.html"><a href="performance-measures.html#implicit-estimands"><i class="fa fa-check"></i><b>9.6</b> Estimands Not Represented By a Parameter</a></li>
<li class="chapter" data-level="9.7" data-path="performance-measures.html"><a href="performance-measures.html#MCSE"><i class="fa fa-check"></i><b>9.7</b> Uncertainty in Performance Estimates (the Monte Carlo Standard Error)</a>
<ul>
<li class="chapter" data-level="9.7.1" data-path="performance-measures.html"><a href="performance-measures.html#conventional-measures-for-point-estimators"><i class="fa fa-check"></i><b>9.7.1</b> Conventional measures for point estimators</a></li>
<li class="chapter" data-level="9.7.2" data-path="performance-measures.html"><a href="performance-measures.html#less-conventional-measures-for-point-estimators"><i class="fa fa-check"></i><b>9.7.2</b> Less conventional measures for point estimators</a></li>
<li class="chapter" data-level="9.7.3" data-path="performance-measures.html"><a href="performance-measures.html#MCSE-for-relative-variance"><i class="fa fa-check"></i><b>9.7.3</b> MCSE for Relative Variance Estimators</a></li>
<li class="chapter" data-level="9.7.4" data-path="performance-measures.html"><a href="performance-measures.html#mcse-for-confidence-intervals-and-hypothesis-tests"><i class="fa fa-check"></i><b>9.7.4</b> MCSE for Confidence Intervals and Hypothesis Tests</a></li>
<li class="chapter" data-level="9.7.5" data-path="performance-measures.html"><a href="performance-measures.html#calculating-mcses-with-the-simhelpers-package"><i class="fa fa-check"></i><b>9.7.5</b> Calculating MCSEs With the <code>simhelpers</code> Package</a></li>
<li class="chapter" data-level="9.7.6" data-path="performance-measures.html"><a href="performance-measures.html#mcse-calculation-in-our-cluster-rct-example"><i class="fa fa-check"></i><b>9.7.6</b> MCSE Calculation in our Cluster RCT Example</a></li>
</ul></li>
<li class="chapter" data-level="9.8" data-path="performance-measures.html"><a href="performance-measures.html#summary-of-peformance-measures"><i class="fa fa-check"></i><b>9.8</b> Summary of Peformance Measures</a></li>
<li class="chapter" data-level="9.9" data-path="performance-measures.html"><a href="performance-measures.html#concluding-thoughts"><i class="fa fa-check"></i><b>9.9</b> Concluding thoughts</a></li>
<li class="chapter" data-level="9.10" data-path="performance-measures.html"><a href="performance-measures.html#exercises-6"><i class="fa fa-check"></i><b>9.10</b> Exercises</a>
<ul>
<li class="chapter" data-level="9.10.1" data-path="performance-measures.html"><a href="performance-measures.html#Brown-Forsythe-performance"><i class="fa fa-check"></i><b>9.10.1</b> Brown and Forsythe (1974) results</a></li>
<li class="chapter" data-level="9.10.2" data-path="performance-measures.html"><a href="performance-measures.html#size-adjusted-power"><i class="fa fa-check"></i><b>9.10.2</b> Size-adjusted power</a></li>
<li class="chapter" data-level="9.10.3" data-path="performance-measures.html"><a href="performance-measures.html#three-correlation-estimators"><i class="fa fa-check"></i><b>9.10.3</b> Three correlation estimators</a></li>
<li class="chapter" data-level="9.10.4" data-path="performance-measures.html"><a href="performance-measures.html#cluster-RCT-t-confidence-intervals"><i class="fa fa-check"></i><b>9.10.4</b> Confidence interval comparison</a></li>
<li class="chapter" data-level="9.10.5" data-path="performance-measures.html"><a href="performance-measures.html#jackknife-MCSE"><i class="fa fa-check"></i><b>9.10.5</b> Jackknife calculation of MCSEs for RMSE</a></li>
<li class="chapter" data-level="9.10.6" data-path="performance-measures.html"><a href="performance-measures.html#jackknife-MCSE-ratio"><i class="fa fa-check"></i><b>9.10.6</b> Jackknife calculation of MCSEs for RMSE ratios</a></li>
<li class="chapter" data-level="9.10.7" data-path="performance-measures.html"><a href="performance-measures.html#cluster-RCT-SPATE"><i class="fa fa-check"></i><b>9.10.7</b> Distribution theory for person-level average treatment effects</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>III Systematic Simulations</b></span></li>
<li class="chapter" data-level="10" data-path="simulating-multiple-scenarios.html"><a href="simulating-multiple-scenarios.html"><i class="fa fa-check"></i><b>10</b> Simulating across multiple scenarios</a>
<ul>
<li class="chapter" data-level="10.1" data-path="simulating-multiple-scenarios.html"><a href="simulating-multiple-scenarios.html#simulating-across-levels-of-a-single-factor"><i class="fa fa-check"></i><b>10.1</b> Simulating across levels of a single factor</a>
<ul>
<li class="chapter" data-level="10.1.1" data-path="simulating-multiple-scenarios.html"><a href="simulating-multiple-scenarios.html#a-performance-summary-function"><i class="fa fa-check"></i><b>10.1.1</b> A performance summary function</a></li>
<li class="chapter" data-level="10.1.2" data-path="simulating-multiple-scenarios.html"><a href="simulating-multiple-scenarios.html#adding-performance-calculations-to-the-simulation-driver"><i class="fa fa-check"></i><b>10.1.2</b> Adding performance calculations to the simulation driver</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="simulating-multiple-scenarios.html"><a href="simulating-multiple-scenarios.html#simulating-across-multiple-factors"><i class="fa fa-check"></i><b>10.2</b> Simulating across multiple factors</a></li>
<li class="chapter" data-level="10.3" data-path="simulating-multiple-scenarios.html"><a href="simulating-multiple-scenarios.html#using-pmap-to-run-multifactor-simulations"><i class="fa fa-check"></i><b>10.3</b> Using pmap to run multifactor simulations</a></li>
<li class="chapter" data-level="10.4" data-path="simulating-multiple-scenarios.html"><a href="simulating-multiple-scenarios.html#when-to-calculate-performance-metrics"><i class="fa fa-check"></i><b>10.4</b> When to calculate performance metrics</a>
<ul>
<li class="chapter" data-level="10.4.1" data-path="simulating-multiple-scenarios.html"><a href="simulating-multiple-scenarios.html#aggregate-as-you-simulate-inside"><i class="fa fa-check"></i><b>10.4.1</b> Aggregate as you simulate (inside)</a></li>
<li class="chapter" data-level="10.4.2" data-path="simulating-multiple-scenarios.html"><a href="simulating-multiple-scenarios.html#keep-all-simulation-runs-outside"><i class="fa fa-check"></i><b>10.4.2</b> Keep all simulation runs (outside)</a></li>
<li class="chapter" data-level="10.4.3" data-path="simulating-multiple-scenarios.html"><a href="simulating-multiple-scenarios.html#getting-raw-results-ready-for-analysis"><i class="fa fa-check"></i><b>10.4.3</b> Getting raw results ready for analysis</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="simulating-multiple-scenarios.html"><a href="simulating-multiple-scenarios.html#summary"><i class="fa fa-check"></i><b>10.5</b> Summary</a></li>
<li class="chapter" data-level="10.6" data-path="simulating-multiple-scenarios.html"><a href="simulating-multiple-scenarios.html#exercises-7"><i class="fa fa-check"></i><b>10.6</b> Exercises</a>
<ul>
<li class="chapter" data-level="10.6.1" data-path="simulating-multiple-scenarios.html"><a href="simulating-multiple-scenarios.html#extending-Brown-Forsythe-power"><i class="fa fa-check"></i><b>10.6.1</b> Extending Brown and Forsythe</a></li>
<li class="chapter" data-level="10.6.2" data-path="simulating-multiple-scenarios.html"><a href="simulating-multiple-scenarios.html#exercise:trimmed-mean"><i class="fa fa-check"></i><b>10.6.2</b> Comparing the trimmed mean, median and mean</a></li>
<li class="chapter" data-level="10.6.3" data-path="simulating-multiple-scenarios.html"><a href="simulating-multiple-scenarios.html#exercise:multifactor-latent-correlation"><i class="fa fa-check"></i><b>10.6.3</b> Estimating latent correlations</a></li>
<li class="chapter" data-level="10.6.4" data-path="simulating-multiple-scenarios.html"><a href="simulating-multiple-scenarios.html#exercise:multifactor-meta-regression"><i class="fa fa-check"></i><b>10.6.4</b> Meta-regression</a></li>
<li class="chapter" data-level="10.6.5" data-path="simulating-multiple-scenarios.html"><a href="simulating-multiple-scenarios.html#exercise:examine-a-multifactor-simulation-design"><i class="fa fa-check"></i><b>10.6.5</b> Examine a multifactor simulation design</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="designing-multifactor-simulations.html"><a href="designing-multifactor-simulations.html"><i class="fa fa-check"></i><b>11</b> Designing multifactor simulations</a>
<ul>
<li class="chapter" data-level="11.1" data-path="designing-multifactor-simulations.html"><a href="designing-multifactor-simulations.html#choosing-parameter-combinations"><i class="fa fa-check"></i><b>11.1</b> Choosing parameter combinations</a></li>
<li class="chapter" data-level="11.2" data-path="designing-multifactor-simulations.html"><a href="designing-multifactor-simulations.html#case-study-a-multifactor-evaluation-of-cluster-rct-estimators"><i class="fa fa-check"></i><b>11.2</b> Case Study: A multifactor evaluation of cluster RCT estimators</a>
<ul>
<li class="chapter" data-level="11.2.1" data-path="designing-multifactor-simulations.html"><a href="designing-multifactor-simulations.html#choosing-parameters-for-the-clustered-rct"><i class="fa fa-check"></i><b>11.2.1</b> Choosing parameters for the Clustered RCT</a></li>
<li class="chapter" data-level="11.2.2" data-path="designing-multifactor-simulations.html"><a href="designing-multifactor-simulations.html#redundant-factor-combinations"><i class="fa fa-check"></i><b>11.2.2</b> Redundant factor combinations</a></li>
<li class="chapter" data-level="11.2.3" data-path="designing-multifactor-simulations.html"><a href="designing-multifactor-simulations.html#running-the-simulations"><i class="fa fa-check"></i><b>11.2.3</b> Running the simulations</a></li>
<li class="chapter" data-level="11.2.4" data-path="designing-multifactor-simulations.html"><a href="designing-multifactor-simulations.html#calculating-performance-metrics"><i class="fa fa-check"></i><b>11.2.4</b> Calculating performance metrics</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="presentation-of-results.html"><a href="presentation-of-results.html"><i class="fa fa-check"></i><b>12</b> Exploring and presenting simulation results</a>
<ul>
<li class="chapter" data-level="12.1" data-path="presentation-of-results.html"><a href="presentation-of-results.html#tabulation"><i class="fa fa-check"></i><b>12.1</b> Tabulation</a>
<ul>
<li class="chapter" data-level="12.1.1" data-path="presentation-of-results.html"><a href="presentation-of-results.html#example-estimators-of-treatment-variation"><i class="fa fa-check"></i><b>12.1.1</b> Example: estimators of treatment variation</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="presentation-of-results.html"><a href="presentation-of-results.html#visualization"><i class="fa fa-check"></i><b>12.2</b> Visualization</a>
<ul>
<li class="chapter" data-level="12.2.1" data-path="presentation-of-results.html"><a href="presentation-of-results.html#example-0-rmse-in-cluster-rcts"><i class="fa fa-check"></i><b>12.2.1</b> Example 0: RMSE in Cluster RCTs</a></li>
<li class="chapter" data-level="12.2.2" data-path="presentation-of-results.html"><a href="presentation-of-results.html#example-1-biserial-correlation-estimation"><i class="fa fa-check"></i><b>12.2.2</b> Example 1: Biserial correlation estimation</a></li>
<li class="chapter" data-level="12.2.3" data-path="presentation-of-results.html"><a href="presentation-of-results.html#example-2-variance-estimation-and-meta-regression"><i class="fa fa-check"></i><b>12.2.3</b> Example 2: Variance estimation and Meta-regression</a></li>
<li class="chapter" data-level="12.2.4" data-path="presentation-of-results.html"><a href="presentation-of-results.html#example-3-heat-maps-of-coverage"><i class="fa fa-check"></i><b>12.2.4</b> Example 3: Heat maps of coverage</a></li>
<li class="chapter" data-level="12.2.5" data-path="presentation-of-results.html"><a href="presentation-of-results.html#example-4-relative-performance-of-treatment-effect-estimators"><i class="fa fa-check"></i><b>12.2.5</b> Example 4: Relative performance of treatment effect estimators</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="presentation-of-results.html"><a href="presentation-of-results.html#modeling"><i class="fa fa-check"></i><b>12.3</b> Modeling</a>
<ul>
<li class="chapter" data-level="12.3.1" data-path="presentation-of-results.html"><a href="presentation-of-results.html#example-1-biserial-revisited"><i class="fa fa-check"></i><b>12.3.1</b> Example 1: Biserial, revisited</a></li>
<li class="chapter" data-level="12.3.2" data-path="presentation-of-results.html"><a href="presentation-of-results.html#example-2-comparing-methods-for-cross-classified-data"><i class="fa fa-check"></i><b>12.3.2</b> Example 2: Comparing methods for cross-classified data</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="presentation-of-results.html"><a href="presentation-of-results.html#reporting"><i class="fa fa-check"></i><b>12.4</b> Reporting</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="building-good-visualization.html"><a href="building-good-visualization.html"><i class="fa fa-check"></i><b>13</b> Building good visualizations</a>
<ul>
<li class="chapter" data-level="13.1" data-path="building-good-visualization.html"><a href="building-good-visualization.html#subsetting-and-many-small-multiples"><i class="fa fa-check"></i><b>13.1</b> Subsetting and Many Small Multiples</a></li>
<li class="chapter" data-level="13.2" data-path="building-good-visualization.html"><a href="building-good-visualization.html#bundling"><i class="fa fa-check"></i><b>13.2</b> Bundling</a></li>
<li class="chapter" data-level="13.3" data-path="building-good-visualization.html"><a href="building-good-visualization.html#aggregation"><i class="fa fa-check"></i><b>13.3</b> Aggregation</a></li>
<li class="chapter" data-level="13.4" data-path="building-good-visualization.html"><a href="building-good-visualization.html#comparing-true-ses-with-standardization"><i class="fa fa-check"></i><b>13.4</b> Comparing true SEs with standardization</a></li>
<li class="chapter" data-level="13.5" data-path="building-good-visualization.html"><a href="building-good-visualization.html#the-bias-se-rmse-plot"><i class="fa fa-check"></i><b>13.5</b> The Bias-SE-RMSE plot</a></li>
<li class="chapter" data-level="13.6" data-path="building-good-visualization.html"><a href="building-good-visualization.html#assessing-the-quality-of-the-estimated-ses"><i class="fa fa-check"></i><b>13.6</b> Assessing the quality of the estimated SEs</a>
<ul>
<li class="chapter" data-level="13.6.1" data-path="building-good-visualization.html"><a href="building-good-visualization.html#stability-of-estimated-ses"><i class="fa fa-check"></i><b>13.6.1</b> Stability of estimated SEs</a></li>
</ul></li>
<li class="chapter" data-level="13.7" data-path="building-good-visualization.html"><a href="building-good-visualization.html#assessing-confidence-intervals"><i class="fa fa-check"></i><b>13.7</b> Assessing confidence intervals</a></li>
<li class="chapter" data-level="13.8" data-path="building-good-visualization.html"><a href="building-good-visualization.html#exercises-8"><i class="fa fa-check"></i><b>13.8</b> Exercises</a>
<ul>
<li class="chapter" data-level="13.8.1" data-path="building-good-visualization.html"><a href="building-good-visualization.html#assessing-uncertainty"><i class="fa fa-check"></i><b>13.8.1</b> Assessing uncertainty</a></li>
<li class="chapter" data-level="13.8.2" data-path="building-good-visualization.html"><a href="building-good-visualization.html#assessing-power"><i class="fa fa-check"></i><b>13.8.2</b> Assessing power</a></li>
<li class="chapter" data-level="13.8.3" data-path="building-good-visualization.html"><a href="building-good-visualization.html#going-deeper-with-coverage"><i class="fa fa-check"></i><b>13.8.3</b> Going deeper with coverage</a></li>
<li class="chapter" data-level="13.8.4" data-path="building-good-visualization.html"><a href="building-good-visualization.html#pearson-correlations-with-a-bivariate-poisson-distribution"><i class="fa fa-check"></i><b>13.8.4</b> Pearson correlations with a bivariate Poisson distribution</a></li>
<li class="chapter" data-level="13.8.5" data-path="building-good-visualization.html"><a href="building-good-visualization.html#making-another-plot-for-assessing-ses"><i class="fa fa-check"></i><b>13.8.5</b> Making another plot for assessing SEs</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="14" data-path="special-topics-on-reporting-simulation-results.html"><a href="special-topics-on-reporting-simulation-results.html"><i class="fa fa-check"></i><b>14</b> Special Topics on Reporting Simulation Results</a>
<ul>
<li class="chapter" data-level="14.1" data-path="special-topics-on-reporting-simulation-results.html"><a href="special-topics-on-reporting-simulation-results.html#using-regression-to-analyze-simulation-results"><i class="fa fa-check"></i><b>14.1</b> Using regression to analyze simulation results</a>
<ul>
<li class="chapter" data-level="14.1.1" data-path="special-topics-on-reporting-simulation-results.html"><a href="special-topics-on-reporting-simulation-results.html#example-1-biserial-revisited-1"><i class="fa fa-check"></i><b>14.1.1</b> Example 1: Biserial, revisited</a></li>
<li class="chapter" data-level="14.1.2" data-path="special-topics-on-reporting-simulation-results.html"><a href="special-topics-on-reporting-simulation-results.html#example-2-cluster-rct-example-revisited"><i class="fa fa-check"></i><b>14.1.2</b> Example 2: Cluster RCT example, revisited</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="special-topics-on-reporting-simulation-results.html"><a href="special-topics-on-reporting-simulation-results.html#using-regression-trees-to-find-important-factors"><i class="fa fa-check"></i><b>14.2</b> Using regression trees to find important factors</a></li>
<li class="chapter" data-level="14.3" data-path="special-topics-on-reporting-simulation-results.html"><a href="special-topics-on-reporting-simulation-results.html#analyzing-results-with-few-iterations-per-scenario"><i class="fa fa-check"></i><b>14.3</b> Analyzing results with few iterations per scenario</a>
<ul>
<li class="chapter" data-level="14.3.1" data-path="special-topics-on-reporting-simulation-results.html"><a href="special-topics-on-reporting-simulation-results.html#example-clusterrct-with-only-100-replicates-per-scenario"><i class="fa fa-check"></i><b>14.3.1</b> Example: ClusterRCT with only 100 replicates per scenario</a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="special-topics-on-reporting-simulation-results.html"><a href="special-topics-on-reporting-simulation-results.html#what-to-do-with-warnings-in-simulations"><i class="fa fa-check"></i><b>14.4</b> What to do with warnings in simulations</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="case-study-comparing-different-estimators.html"><a href="case-study-comparing-different-estimators.html"><i class="fa fa-check"></i><b>15</b> Case study: Comparing different estimators</a>
<ul>
<li class="chapter" data-level="15.1" data-path="case-study-comparing-different-estimators.html"><a href="case-study-comparing-different-estimators.html#bias-variance-tradeoffs"><i class="fa fa-check"></i><b>15.1</b> Bias-variance tradeoffs</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="simulations-as-evidence.html"><a href="simulations-as-evidence.html"><i class="fa fa-check"></i><b>16</b> Simulations as evidence</a>
<ul>
<li class="chapter" data-level="16.1" data-path="simulations-as-evidence.html"><a href="simulations-as-evidence.html#strategies-for-making-relevant-simulations"><i class="fa fa-check"></i><b>16.1</b> Strategies for making relevant simulations</a>
<ul>
<li class="chapter" data-level="16.1.1" data-path="simulations-as-evidence.html"><a href="simulations-as-evidence.html#break-symmetries-and-regularities"><i class="fa fa-check"></i><b>16.1.1</b> Break symmetries and regularities</a></li>
<li class="chapter" data-level="16.1.2" data-path="simulations-as-evidence.html"><a href="simulations-as-evidence.html#make-your-simulation-general-with-an-extensive-multi-factor-experiment"><i class="fa fa-check"></i><b>16.1.2</b> Make your simulation general with an extensive multi-factor experiment</a></li>
<li class="chapter" data-level="16.1.3" data-path="simulations-as-evidence.html"><a href="simulations-as-evidence.html#use-previously-published-simulations-to-beat-them-at-their-own-game"><i class="fa fa-check"></i><b>16.1.3</b> Use previously published simulations to beat them at their own game</a></li>
<li class="chapter" data-level="16.1.4" data-path="simulations-as-evidence.html"><a href="simulations-as-evidence.html#calibrate-simulation-factors-to-real-data"><i class="fa fa-check"></i><b>16.1.4</b> Calibrate simulation factors to real data</a></li>
<li class="chapter" data-level="16.1.5" data-path="simulations-as-evidence.html"><a href="simulations-as-evidence.html#use-real-data-to-obtain-directly"><i class="fa fa-check"></i><b>16.1.5</b> Use real data to obtain directly</a></li>
<li class="chapter" data-level="16.1.6" data-path="simulations-as-evidence.html"><a href="simulations-as-evidence.html#fully-calibrated-simulations"><i class="fa fa-check"></i><b>16.1.6</b> Fully calibrated simulations</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>IV Computational Considerations</b></span></li>
<li class="chapter" data-level="17" data-path="organizing-a-simulation-project.html"><a href="organizing-a-simulation-project.html"><i class="fa fa-check"></i><b>17</b> Organizing a simulation project</a>
<ul>
<li class="chapter" data-level="17.1" data-path="organizing-a-simulation-project.html"><a href="organizing-a-simulation-project.html#well-structured-r-scripts"><i class="fa fa-check"></i><b>17.1</b> Well structured R scripts</a>
<ul>
<li class="chapter" data-level="17.1.1" data-path="organizing-a-simulation-project.html"><a href="organizing-a-simulation-project.html#about-source-command"><i class="fa fa-check"></i><b>17.1.1</b> The source command</a></li>
<li class="chapter" data-level="17.1.2" data-path="organizing-a-simulation-project.html"><a href="organizing-a-simulation-project.html#putting-headers-in-your-.r-file"><i class="fa fa-check"></i><b>17.1.2</b> Putting headers in your .R file</a></li>
<li class="chapter" data-level="17.1.3" data-path="organizing-a-simulation-project.html"><a href="organizing-a-simulation-project.html#about-keeping-tests-with-FALSE"><i class="fa fa-check"></i><b>17.1.3</b> Storing testing code in your scripts</a></li>
</ul></li>
<li class="chapter" data-level="17.2" data-path="organizing-a-simulation-project.html"><a href="organizing-a-simulation-project.html#principled-directory-structures"><i class="fa fa-check"></i><b>17.2</b> Principled directory structures</a></li>
<li class="chapter" data-level="17.3" data-path="organizing-a-simulation-project.html"><a href="organizing-a-simulation-project.html#saving-files"><i class="fa fa-check"></i><b>17.3</b> Saving simulation results</a>
<ul>
<li class="chapter" data-level="17.3.1" data-path="organizing-a-simulation-project.html"><a href="organizing-a-simulation-project.html#saving-simulations-in-general"><i class="fa fa-check"></i><b>17.3.1</b> Saving simulations in general</a></li>
<li class="chapter" data-level="17.3.2" data-path="organizing-a-simulation-project.html"><a href="organizing-a-simulation-project.html#saving-simulations-as-you-go"><i class="fa fa-check"></i><b>17.3.2</b> Saving simulations as you go</a></li>
<li class="chapter" data-level="17.3.3" data-path="organizing-a-simulation-project.html"><a href="organizing-a-simulation-project.html#dynamically-making-directories"><i class="fa fa-check"></i><b>17.3.3</b> Dynamically making directories</a></li>
<li class="chapter" data-level="17.3.4" data-path="organizing-a-simulation-project.html"><a href="organizing-a-simulation-project.html#loading-and-combining-files-of-simulation-results"><i class="fa fa-check"></i><b>17.3.4</b> Loading and combining files of simulation results</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="18" data-path="parallel-processing.html"><a href="parallel-processing.html"><i class="fa fa-check"></i><b>18</b> Parallel Processing</a>
<ul>
<li class="chapter" data-level="18.1" data-path="parallel-processing.html"><a href="parallel-processing.html#parallel-on-your-computer"><i class="fa fa-check"></i><b>18.1</b> Parallel on your computer</a></li>
<li class="chapter" data-level="18.2" data-path="parallel-processing.html"><a href="parallel-processing.html#parallel-on-a-virtual-machine"><i class="fa fa-check"></i><b>18.2</b> Parallel on a virtual machine</a></li>
<li class="chapter" data-level="18.3" data-path="parallel-processing.html"><a href="parallel-processing.html#parallel-on-a-cluster"><i class="fa fa-check"></i><b>18.3</b> Parallel on a cluster</a>
<ul>
<li class="chapter" data-level="18.3.1" data-path="parallel-processing.html"><a href="parallel-processing.html#what-is-a-command-line-interface"><i class="fa fa-check"></i><b>18.3.1</b> What is a command-line interface?</a></li>
<li class="chapter" data-level="18.3.2" data-path="parallel-processing.html"><a href="parallel-processing.html#running-a-job-on-a-cluster"><i class="fa fa-check"></i><b>18.3.2</b> Running a job on a cluster</a></li>
<li class="chapter" data-level="18.3.3" data-path="parallel-processing.html"><a href="parallel-processing.html#checking-on-a-job"><i class="fa fa-check"></i><b>18.3.3</b> Checking on a job</a></li>
<li class="chapter" data-level="18.3.4" data-path="parallel-processing.html"><a href="parallel-processing.html#running-lots-of-jobs-on-a-cluster"><i class="fa fa-check"></i><b>18.3.4</b> Running lots of jobs on a cluster</a></li>
<li class="chapter" data-level="18.3.5" data-path="parallel-processing.html"><a href="parallel-processing.html#resources-for-harvards-odyssey"><i class="fa fa-check"></i><b>18.3.5</b> Resources for Harvard’s Odyssey</a></li>
<li class="chapter" data-level="18.3.6" data-path="parallel-processing.html"><a href="parallel-processing.html#acknowledgements-1"><i class="fa fa-check"></i><b>18.3.6</b> Acknowledgements</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="19" data-path="debugging-and-testing.html"><a href="debugging-and-testing.html"><i class="fa fa-check"></i><b>19</b> Debugging and Testing</a>
<ul>
<li class="chapter" data-level="19.1" data-path="debugging-and-testing.html"><a href="debugging-and-testing.html#about-print"><i class="fa fa-check"></i><b>19.1</b> Debugging with <code>print()</code></a></li>
<li class="chapter" data-level="19.2" data-path="debugging-and-testing.html"><a href="debugging-and-testing.html#about-browser-debugging"><i class="fa fa-check"></i><b>19.2</b> Debugging with <code>browser()</code></a></li>
<li class="chapter" data-level="19.3" data-path="debugging-and-testing.html"><a href="debugging-and-testing.html#debugging-with-debug"><i class="fa fa-check"></i><b>19.3</b> Debugging with <code>debug()</code></a></li>
<li class="chapter" data-level="19.4" data-path="debugging-and-testing.html"><a href="debugging-and-testing.html#about-stopifnot"><i class="fa fa-check"></i><b>19.4</b> Protecting functions with <code>stop()</code></a></li>
<li class="chapter" data-level="19.5" data-path="debugging-and-testing.html"><a href="debugging-and-testing.html#testing-code"><i class="fa fa-check"></i><b>19.5</b> Testing code</a></li>
</ul></li>
<li class="part"><span><b>V Complex Data Structures</b></span></li>
<li class="chapter" data-level="20" data-path="sec:power.html"><a href="sec:power.html"><i class="fa fa-check"></i><b>20</b> Using simulation as a power calculator</a>
<ul>
<li class="chapter" data-level="20.1" data-path="sec:power.html"><a href="sec:power.html#getting-design-parameters-from-pilot-data"><i class="fa fa-check"></i><b>20.1</b> Getting design parameters from pilot data</a></li>
<li class="chapter" data-level="20.2" data-path="sec:power.html"><a href="sec:power.html#the-data-generating-process"><i class="fa fa-check"></i><b>20.2</b> The data generating process</a></li>
<li class="chapter" data-level="20.3" data-path="sec:power.html"><a href="sec:power.html#running-the-simulation-1"><i class="fa fa-check"></i><b>20.3</b> Running the simulation</a></li>
<li class="chapter" data-level="20.4" data-path="sec:power.html"><a href="sec:power.html#evaluating-power"><i class="fa fa-check"></i><b>20.4</b> Evaluating power</a>
<ul>
<li class="chapter" data-level="20.4.1" data-path="sec:power.html"><a href="sec:power.html#checking-validity-of-our-models"><i class="fa fa-check"></i><b>20.4.1</b> Checking validity of our models</a></li>
<li class="chapter" data-level="20.4.2" data-path="sec:power.html"><a href="sec:power.html#assessing-precision-se"><i class="fa fa-check"></i><b>20.4.2</b> Assessing Precision (SE)</a></li>
<li class="chapter" data-level="20.4.3" data-path="sec:power.html"><a href="sec:power.html#assessing-power-1"><i class="fa fa-check"></i><b>20.4.3</b> Assessing power</a></li>
<li class="chapter" data-level="20.4.4" data-path="sec:power.html"><a href="sec:power.html#assessing-minimum-detectable-effects"><i class="fa fa-check"></i><b>20.4.4</b> Assessing Minimum Detectable Effects</a></li>
</ul></li>
<li class="chapter" data-level="20.5" data-path="sec:power.html"><a href="sec:power.html#power-for-multilevel-data"><i class="fa fa-check"></i><b>20.5</b> Power for Multilevel Data</a></li>
</ul></li>
<li class="chapter" data-level="21" data-path="potential-outcomes.html"><a href="potential-outcomes.html"><i class="fa fa-check"></i><b>21</b> Simulation under the Potential Outcomes Framework</a>
<ul>
<li class="chapter" data-level="21.1" data-path="potential-outcomes.html"><a href="potential-outcomes.html#finite-vs.-superpopulation-inference"><i class="fa fa-check"></i><b>21.1</b> Finite vs. Superpopulation inference</a></li>
<li class="chapter" data-level="21.2" data-path="potential-outcomes.html"><a href="potential-outcomes.html#data-generation-processes-for-potential-outcomes"><i class="fa fa-check"></i><b>21.2</b> Data generation processes for potential outcomes</a></li>
<li class="chapter" data-level="21.3" data-path="potential-outcomes.html"><a href="potential-outcomes.html#finite-sample-performance-measures"><i class="fa fa-check"></i><b>21.3</b> Finite sample performance measures</a></li>
<li class="chapter" data-level="21.4" data-path="potential-outcomes.html"><a href="potential-outcomes.html#nested-finite-simulation-procedure"><i class="fa fa-check"></i><b>21.4</b> Nested finite simulation procedure</a></li>
</ul></li>
<li class="chapter" data-level="22" data-path="the-parametric-bootstrap.html"><a href="the-parametric-bootstrap.html"><i class="fa fa-check"></i><b>22</b> The Parametric bootstrap</a>
<ul>
<li class="chapter" data-level="22.1" data-path="the-parametric-bootstrap.html"><a href="the-parametric-bootstrap.html#air-conditioners-a-stolen-case-study"><i class="fa fa-check"></i><b>22.1</b> Air conditioners: a stolen case study</a></li>
</ul></li>
<li class="appendix"><span><b>Appendices</b></span></li>
<li class="chapter" data-level="A" data-path="coding-tidbits.html"><a href="coding-tidbits.html"><i class="fa fa-check"></i><b>A</b> Coding Reference</a>
<ul>
<li class="chapter" data-level="A.1" data-path="coding-tidbits.html"><a href="coding-tidbits.html#more-repeating-oneself"><i class="fa fa-check"></i><b>A.1</b> How to repeat yourself</a>
<ul>
<li class="chapter" data-level="A.1.1" data-path="coding-tidbits.html"><a href="coding-tidbits.html#using-replicate"><i class="fa fa-check"></i><b>A.1.1</b> Using <code>replicate()</code></a></li>
<li class="chapter" data-level="A.1.2" data-path="coding-tidbits.html"><a href="coding-tidbits.html#using-map"><i class="fa fa-check"></i><b>A.1.2</b> Using <code>map()</code></a></li>
<li class="chapter" data-level="A.1.3" data-path="coding-tidbits.html"><a href="coding-tidbits.html#map-with-no-inputs"><i class="fa fa-check"></i><b>A.1.3</b> map with no inputs</a></li>
<li class="chapter" data-level="A.1.4" data-path="coding-tidbits.html"><a href="coding-tidbits.html#other-approaches-for-repetition"><i class="fa fa-check"></i><b>A.1.4</b> Other approaches for repetition</a></li>
</ul></li>
<li class="chapter" data-level="A.2" data-path="coding-tidbits.html"><a href="coding-tidbits.html#default-arguments"><i class="fa fa-check"></i><b>A.2</b> Default arguments for functions</a></li>
<li class="chapter" data-level="A.3" data-path="coding-tidbits.html"><a href="coding-tidbits.html#profiling-code"><i class="fa fa-check"></i><b>A.3</b> Profiling Code</a>
<ul>
<li class="chapter" data-level="A.3.1" data-path="coding-tidbits.html"><a href="coding-tidbits.html#using-sys.time-and-system.time"><i class="fa fa-check"></i><b>A.3.1</b> Using <code>Sys.time()</code> and <code>system.time()</code></a></li>
<li class="chapter" data-level="A.3.2" data-path="coding-tidbits.html"><a href="coding-tidbits.html#the-tictoc-package"><i class="fa fa-check"></i><b>A.3.2</b> The <code>tictoc</code> package</a></li>
<li class="chapter" data-level="A.3.3" data-path="coding-tidbits.html"><a href="coding-tidbits.html#the-bench-package"><i class="fa fa-check"></i><b>A.3.3</b> The <code>bench</code> package</a></li>
<li class="chapter" data-level="A.3.4" data-path="coding-tidbits.html"><a href="coding-tidbits.html#profiling-with-profvis"><i class="fa fa-check"></i><b>A.3.4</b> Profiling with <code>profvis</code></a></li>
</ul></li>
<li class="chapter" data-level="A.4" data-path="coding-tidbits.html"><a href="coding-tidbits.html#optimize-code"><i class="fa fa-check"></i><b>A.4</b> Optimizing code (and why you often shouldn’t)</a>
<ul>
<li class="chapter" data-level="A.4.1" data-path="coding-tidbits.html"><a href="coding-tidbits.html#hand-building-functions"><i class="fa fa-check"></i><b>A.4.1</b> Hand-building functions</a></li>
<li class="chapter" data-level="A.4.2" data-path="coding-tidbits.html"><a href="coding-tidbits.html#sec_comp_efficiency"><i class="fa fa-check"></i><b>A.4.2</b> Computational efficiency versus simplicity</a></li>
<li class="chapter" data-level="A.4.3" data-path="coding-tidbits.html"><a href="coding-tidbits.html#reusing-code-to-speed-up-computation"><i class="fa fa-check"></i><b>A.4.3</b> Reusing code to speed up computation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="B" data-path="further-readings-and-resources.html"><a href="further-readings-and-resources.html"><i class="fa fa-check"></i><b>B</b> Further readings and resources</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Designing Monte Carlo Simulations in R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="performance-measures" class="section level1 hasAnchor" number="9">
<h1><span class="header-section-number">Chapter 9</span> Performance Measures<a href="performance-measures.html#performance-measures" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Once we run a simulation, we end up with a pile of results to sort through.
For example, Figure <a href="performance-measures.html#fig:CRT-ATE-hist">9.1</a> depicts the distribution of average treatment effect estimates from the cluster-randomized experiment simulation, which we generated in Chapter <a href="running-the-simulation-process.html#running-the-simulation-process">8</a>.
There are three different estimators, each with 1000 replications.
Each histogram is an approximation of the <em>sampling distribution</em> of the estimator, meaning its distribution across repetitions of the data-generating process.
With results such as these, the question before us is now how to evaluate how well each procedure works. If we are comparing several different estimators, we also need to determine which ones work better or worse than others. In this chapter, we look at a variety of <strong>performance measures</strong> that can answer these questions.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:CRT-ATE-hist"></span>
<img src="Designing-Simulations-in-R_files/figure-html/CRT-ATE-hist-1.png" alt="Sampling distribution of average treatment effect estimates from a cluster-randomized trial with a true average treatment effect of 0.3." width="75%" />
<p class="caption">
Figure 9.1: Sampling distribution of average treatment effect estimates from a cluster-randomized trial with a true average treatment effect of 0.3.
</p>
</div>
<p>Performance measures are summaries of a sampling distribution that describe how an estimator or data analysis procedure behaves on average if we could repeat the data-generating process an infinite number of times.
For example, the bias of an estimator is the difference between the average value of the estimator and the corresponding target parameter.
Bias measures the central tendency of the sampling distribution, capturing how far off, on average, the estimator would be from the true parameter value if we repeated the data-generating process an infinite number of times.
In Figure <a href="performance-measures.html#fig:CRT-ATE-hist">9.1</a>, black dashed lines mark the true average treatment effect of 0.3 and the colored vertical lines with circles at the end mark the means of the estimators.
The distance between the colored lines and the black dashed lines corresponds to the bias of the estimator.
This distance is nearly zero for the aggregation estimator and the multilevel model estimator, but larger for the linear regression estimator.</p>
<p>Different types of data-analysis results produce different types of information, and so the relevant set of performance measures depends on the type of data analysis result we are studying.
For procedures that produce point estimates or point predictions, conventional performance measures include bias, variance, and root mean squared error.
If the point estimates come with corresponding standard errors, then we may also want to evaluate how accurately the standard errors represent the true uncertainty of the point estimators; conventional performance measures for capturing this include the relative bias and relative root mean squared error of the variance estimator.
For procedures that produce confidence intervals or other types of interval estimates, conventional performance measures include the coverage rate and average interval width.
Finally, for inferential procedures that involve hypothesis tests (or more generally, classification tasks), conventional performance measures include Type I error rates and power.
We describe each of these measures in Sections <a href="performance-measures.html#assessing-point-estimators">9.1</a> through <a href="performance-measures.html#assessing-inferential-procedures">9.4</a>.</p>
<p>Performance measures are defined with respect to sampling distributions, or the results of applying a data analysis procedure to data generated according to a particular process across an infinite number of replications.
In defining specific measures, we will follow statistical conventions to denote the mean, variance, and other moments of the sampling distribution.
For a random variable <span class="math inline">\(T\)</span>, we will use the expectation operator <span class="math inline">\(\E(T)\)</span> to denote the mean of the sampling distribution of <span class="math inline">\(T\)</span>, <span class="math inline">\(\M(T)\)</span> to denote the median of its sampling distribution, <span class="math inline">\(\Var(T)\)</span> to denote the variance of its sampling distribution, and <span class="math inline">\(\Prob()\)</span> to denote probabilities of specific outcomes with respect to its sampling distribution.
We will use <span class="math inline">\(\Q_p(T)\)</span> to denote the <span class="math inline">\(p^{th}\)</span> quantile of a distribution, which is the value <span class="math inline">\(x\)</span> such that <span class="math inline">\(\Prob(T \leq x) = p\)</span>. With this notation, the median of a continuous distribution is equivalent to the 0.5 quantile: <span class="math inline">\(\M(T) = \Q_{0.5}(T)\)</span>.</p>
<p>For some simple combinations of data-generating processes and data analysis procedures, it may be possible to derive exact mathematical formulas for calculating some performance measures (such as exact mathematical expressions for the bias and variance of the linear regression estimator).
But for many problems, the math is difficult or intractable—that’s why we do simulations in the first place.
Simulations do not produce the <em>exact</em> sampling distribution or give us <em>exact</em> values of performance measures.
Instead, simulations generate <em>samples</em> (usually large samples) from the the sampling distribution, and we can use these to compute <em>estimates</em> of the performance measures of interest.
In Figure <a href="performance-measures.html#fig:CRT-ATE-hist">9.1</a>, we calculated the bias of each estimator by taking the mean of 1000 observations from its sampling distribution. If we were to repeat the whole set of calculations (with a different seed), then our bias results would shift slightly because they are imperfect estimates of the actual bias.</p>
<p>In working with simulation results, it is important to keep track of the degree of uncertainty in performance measure estimates.
We call such uncertainty <em>Monte Carlo error</em> because it is the error arising from using a finite number of replications of the Monte Carlo simulation process.
One way to quantify it is with the <em>Monte Carlo standard error (MCSE)</em>, or the standard error of a performance estimate based on a finite number of replications.
Just as when we analyze real data, we can apply statistical techniques to estimate the MCSE and even to generate confidence intervals for performance measures.</p>
<p>The magnitude of MCSE is driven by how many replications we use: if we only use a few, we will have noisy estimates of performance with large MCSEs; if we use millions of replications, the MCSE will usually be tiny.
It is important to keep in mind that the MCSE is not measuring anything about how a data analysis procedure performs in general.
It only describes how precisely we have approximated a performance criterion, an artifact of how we conducted the simulation.
Moreover, MCSEs are under our control.
Given a desired MCSE, we can determine how many replications we would need to ensure our performance estimates have the specified level of precision.
Section <a href="performance-measures.html#MCSE">9.7</a> provides details about how to compute MCSEs for conventional performance measures, along with some discussion of general techniques for computing MCSE for less conventional measures.</p>
<div id="assessing-point-estimators" class="section level2 hasAnchor" number="9.1">
<h2><span class="header-section-number">9.1</span> Measures for Point Estimators<a href="performance-measures.html#assessing-point-estimators" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The most common performance measures used to assess a point estimator are bias, variance, and root mean squared error.
Bias compares the mean of the sampling distribution to the target parameter.
Positive bias implies that the estimator tends to systematically over-state the quantity of interest, while negative bias implies that it systematically under-shoots the quantity of interest.
If bias is zero (or nearly zero), we say that the estimator is unbiased (or approximately unbiased).
Variance (or its square root, the true standard error) describes the spread of the sampling distribution, or the extent to which it varies around its central tendency.
All else equal, we would like estimators to have low variance (or to be more precise).
Root mean squared error (RMSE) is a conventional measure of the overall accuracy of an estimator, or its average degree of error with respect to the target parameter.
For absolute assessments of performance, an estimator with low bias, low variance, and thus low RMSE is desired.
In making comparisons of several different estimators, one with lower RMSE is usually preferable to one with higher RMSE.
If two estimators have comparable RMSE, then the estimator with lower bias would usually be preferable.</p>
<p>To define these quantities more precisely, let us consider a generic estimator <span class="math inline">\(T\)</span> that is targeting a parameter <span class="math inline">\(\theta\)</span>.
We call the target parameter the <em>estimand</em>.
In most cases, in running our simulation we set the estimand <span class="math inline">\(\theta\)</span> and then generate a (typically large) series of <span class="math inline">\(R\)</span> datasets, for each of which <span class="math inline">\(\theta\)</span> is the true target parameter.
We then analyze each dataset, obtaining a sample of estimates <span class="math inline">\(T_1,...,T_R\)</span>.
Formally, the bias, variance, and RMSE of <span class="math inline">\(T\)</span> are defined as
<span class="math display" id="eq:bias-variance-RMSE">\[
\begin{aligned}
\Bias(T) &amp;= \E(T) - \theta, \\
\Var(T) &amp;= \E\left[\left(T - \E (T)\right)^2 \right], \\
\RMSE(T) &amp;= \sqrt{\E\left[\left(T - \theta\right)^2 \right]}.
\end{aligned}
\tag{9.1}
\]</span>
These three measures are inter-connected.
In particular, RMSE is the combination of (squared) bias and variance, as in
<span class="math display" id="eq:RMSE-decomposition">\[
\left[\RMSE(T)\right]^2 = \left[\Bias(T)\right]^2 + \Var(T).
\tag{9.2}
\]</span></p>
<p>When conducting a simulation, we do not compute these performance measures directly but rather must estimate them using the replicates <span class="math inline">\(T_1,...,T_R\)</span> generated from the sampling distribution.
There is nothing very surprising about how we construct estimates of the performance measures.
It is just a matter of substituting sample quantities in place of the expectations and variances.
Specifically, we estimate bias by taking
<span class="math display" id="eq:bias-estimator">\[
\widehat{\Bias}(T) = \bar{T} - \theta,
\tag{9.3}
\]</span>
where <span class="math inline">\(\bar{T}\)</span> is the arithmetic mean of the replicates, <span class="math inline">\(\bar{T} = \frac{1}{R}\sum_{r=1}^R T_r\)</span>.
We estimate variance by taking the sample variance of the replicates, as
<span class="math display" id="eq:var-estimator">\[
S_T^2 = \frac{1}{R - 1}\sum_{r=1}^R \left(T_r - \bar{T}\right)^2.
\tag{9.4}
\]</span>
<span class="math inline">\(S_T\)</span> (the square root of <span class="math inline">\(S^2_T\)</span>) is an estimate of the empirical standard error of <span class="math inline">\(T\)</span>, or the standard deviation of the estimator across an infinite set of replications of the data-generating process.<a href="#fn11" class="footnote-ref" id="fnref11"><sup>11</sup></a>
We usually prefer to work with the empirical SE <span class="math inline">\(S_T\)</span> rather than the sampling variance <span class="math inline">\(S_T^2\)</span> because the former quantity has the same units as the target parameter.
Finally, the RMSE estimate can be calculated as
<span class="math display" id="eq:rmse-estimator">\[
\widehat{\RMSE}(T) = \sqrt{\frac{1}{R} \sum_{r = 1}^R \left( T_r - \theta\right)^2 }.  
\tag{9.5}
\]</span>
Often, people talk about the MSE (Mean Squared Error), which is just the square of RMSE.
Just as the true SE is usually easier to interpret than the sampling variance, units of RMSE are easier to interpret than the units of MSE.</p>
<p>It is important to recognize that the above performance measures depend on the scale of the parameter.
For example, if our estimators are measuring a treatment impact in dollars, then the bias, SE, and RMSE of the estimators are all in dollars.
(The variance and MSE would be in dollars squared, which is why we take their square roots to put them back on the more intepretable scale of dollars.)</p>
<p>In many simulations, the scale of the outcome is an arbitrary feature of the data-generating process, making the absolute magnitude of performance measures less meaningful.
To ease interpretation of performance measures, it is useful to consider their magnitude relative to the baseline level of variation in the outcome.
One way to achieve this is to generate data so the outcome has unit variance (i.e., we generate outcomes in <em>standardized units</em>).
Doing so puts the bias, empirical standard error, and root mean squared error on the scale of standard deviation units, which can facilitate interpretation about what constitutes a meaningfully large bias or a meaningful difference in RMSE.</p>
<p>In addition to understanding the scale of these performance measures, it is also important to recognize that their magnitude depends on the metric of the parameter.
A non-linear transformation of a parameter will generally lead to changes in the magnitude of the performance measures.
For instance, suppose that <span class="math inline">\(\theta\)</span> measures the proportion of time that something occurs.
One natural way to transform this parameter would be to put it on the log-odds (logit) scale.
However, because the log-odds transformation is non-linear,
<span class="math display">\[
\text{Bias}\left[\text{logit}(T)\right] \neq \text{logit}\left(\text{Bias}[T]\right), \qquad \text{RMSE}\left[\text{logit}(T)\right] \neq \text{logit}\left(\text{RMSE}[T]\right),
\]</span>
and so on.
This is a consequence of how these performance measures are defined.
One might see this property as a limitation on the utility of using bias and RMSE to measure the performance of an estimator, because these measures can be quite sensitive to the metric of the parameter.</p>
<div id="clusterRCTperformance" class="section level3 hasAnchor" number="9.1.1">
<h3><span class="header-section-number">9.1.1</span> Comparing the Performance of the Cluster RCT Estimation Procedures<a href="performance-measures.html#clusterRCTperformance" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We now demonstrate the calculation of performance measures for the point estimators of average treatment effects in the cluster-RCT example.
In Chapter <a href="running-the-simulation-process.html#running-the-simulation-process">8</a>, we generated a large set of replications of several different treatment effect estimators.
Using these results, we can assess the bias, standard error, and RMSE of three different estimators of the ATE.
These performance measures address the following questions:</p>
<ul>
<li>Is the estimator systematically off? (bias)</li>
<li>Is it precise? (standard error)</li>
<li>Does it predict well? (RMSE)</li>
</ul>
<p>Let us see how the three estimators compare on these measures.</p>
<div id="are-the-estimators-biased" class="section level4 unnumbered hasAnchor">
<h4>Are the estimators biased?<a href="performance-measures.html#are-the-estimators-biased" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Bias is defined with respect to a target estimand.
Here we assess whether our estimates are systematically different from the <span class="math inline">\(\gamma_1\)</span> parameter, which we defined in standardized units by setting the standard deviation of the student-level distribution of the outcome equal to one.
For these data, we generated data based on a school-level ATE parameter of 0.30 SDs.</p>
<div class="sourceCode" id="cb294"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb294-1"><a href="performance-measures.html#cb294-1" tabindex="-1"></a>ATE <span class="ot">&lt;-</span> <span class="fl">0.30</span></span>
<span id="cb294-2"><a href="performance-measures.html#cb294-2" tabindex="-1"></a></span>
<span id="cb294-3"><a href="performance-measures.html#cb294-3" tabindex="-1"></a>runs <span class="sc">%&gt;%</span> </span>
<span id="cb294-4"><a href="performance-measures.html#cb294-4" tabindex="-1"></a>  <span class="fu">group_by</span>( method ) <span class="sc">%&gt;%</span></span>
<span id="cb294-5"><a href="performance-measures.html#cb294-5" tabindex="-1"></a>  <span class="fu">summarise</span>( </span>
<span id="cb294-6"><a href="performance-measures.html#cb294-6" tabindex="-1"></a>    <span class="at">mean_ATE_hat =</span> <span class="fu">mean</span>( ATE_hat ),</span>
<span id="cb294-7"><a href="performance-measures.html#cb294-7" tabindex="-1"></a>    <span class="at">bias =</span> <span class="fu">mean</span>( ATE_hat ) <span class="sc">-</span> ATE</span>
<span id="cb294-8"><a href="performance-measures.html#cb294-8" tabindex="-1"></a>  )</span></code></pre></div>
<pre><code>## # A tibble: 3 × 3
##   method mean_ATE_hat      bias
##   &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt;
## 1 Agg           0.300 -0.000166
## 2 LR            0.382  0.0824  
## 3 MLM           0.312  0.0122</code></pre>
<p>There is no indication of major bias for aggregation or multi-level modeling.
Linear regression, with a bias of about 0.09 SDs, appears about ten times as biased as the other estimators.
This is because the linear regression is targeting the person-level average average treatment effect.
The data-generating process of this simulation makes larger sites have larger effects, so the person-level average effect is going to be higher because those larger sites will count more.
In contrast, our estimand is the school-level average treatment effect, or the simple average of each school’s true impact, which we have set to 0.30.
The aggregation and multi-level modeling methods target this school-level average effect.
If we had instead decided that the target estimand should be the person-level average effect, then we would find that linear regression is unbiased whereas aggregation and multi-level modeling are biased.
This example illustrates how crucial it is to think carefully about the appropriate target parameter and to assess performance with respect to a well-justified and clearly articulated target.</p>
</div>
<div id="which-method-has-the-smallest-standard-error" class="section level4 unnumbered hasAnchor">
<h4>Which method has the smallest standard error?<a href="performance-measures.html#which-method-has-the-smallest-standard-error" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The empirical standard error measures the degree of variability in a point estimator.
It reflects how stable our estimates are across replications of the data-generating process.
We calculate the standard error by taking the standard deviation of the replications of each estimator.
For purposes of interpretation, it is useful to compare the empirical standard errors to the variation in a benchmark estimator.
Here, we treat the linear regression estimator as the benchmark and compute the magnitude of the empirical SEs of each method <em>relative</em> to the SE of the linear regression estimator:</p>
<div class="sourceCode" id="cb296"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb296-1"><a href="performance-measures.html#cb296-1" tabindex="-1"></a>true_SE <span class="ot">&lt;-</span> </span>
<span id="cb296-2"><a href="performance-measures.html#cb296-2" tabindex="-1"></a>  runs <span class="sc">%&gt;%</span> </span>
<span id="cb296-3"><a href="performance-measures.html#cb296-3" tabindex="-1"></a>  <span class="fu">group_by</span>( method ) <span class="sc">%&gt;%</span></span>
<span id="cb296-4"><a href="performance-measures.html#cb296-4" tabindex="-1"></a>  <span class="fu">summarise</span>( <span class="at">SE =</span> <span class="fu">sd</span>( ATE_hat ) ) <span class="sc">%&gt;%</span></span>
<span id="cb296-5"><a href="performance-measures.html#cb296-5" tabindex="-1"></a>  <span class="fu">mutate</span>( <span class="at">per_SE =</span> SE <span class="sc">/</span> SE[method<span class="sc">==</span><span class="st">&quot;LR&quot;</span>] )</span>
<span id="cb296-6"><a href="performance-measures.html#cb296-6" tabindex="-1"></a></span>
<span id="cb296-7"><a href="performance-measures.html#cb296-7" tabindex="-1"></a>true_SE</span></code></pre></div>
<pre><code>## # A tibble: 3 × 3
##   method    SE per_SE
##   &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;
## 1 Agg    0.214  0.956
## 2 LR     0.224  1    
## 3 MLM    0.213  0.953</code></pre>
<p>In a real data analysis, these standard errors are what we would be trying to approximate with a standard error estimator.
Aggregation and multi-level modeling have SEs about 8% smaller than linear regression.
For these data-generating conditions, aggregation and multi-level modeling are preferable to linear regression because they are more precise.</p>
</div>
<div id="which-method-has-the-smallest-root-mean-squared-error" class="section level4 unnumbered hasAnchor">
<h4>Which method has the smallest Root Mean Squared Error?<a href="performance-measures.html#which-method-has-the-smallest-root-mean-squared-error" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>So far linear regression is not doing well: it has more bias and a larger standard error than the other two estimators.
We can assess overall accuracy by combining these two quantities with the RMSE:</p>
<div class="sourceCode" id="cb298"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb298-1"><a href="performance-measures.html#cb298-1" tabindex="-1"></a>runs <span class="sc">%&gt;%</span> </span>
<span id="cb298-2"><a href="performance-measures.html#cb298-2" tabindex="-1"></a>  <span class="fu">group_by</span>( method ) <span class="sc">%&gt;%</span></span>
<span id="cb298-3"><a href="performance-measures.html#cb298-3" tabindex="-1"></a>  <span class="fu">summarise</span>( </span>
<span id="cb298-4"><a href="performance-measures.html#cb298-4" tabindex="-1"></a>    <span class="at">bias =</span> <span class="fu">mean</span>( ATE_hat <span class="sc">-</span> ATE ),</span>
<span id="cb298-5"><a href="performance-measures.html#cb298-5" tabindex="-1"></a>    <span class="at">SE =</span> <span class="fu">sd</span>( ATE_hat ),</span>
<span id="cb298-6"><a href="performance-measures.html#cb298-6" tabindex="-1"></a>    <span class="at">RMSE =</span> <span class="fu">sqrt</span>( <span class="fu">mean</span>( (ATE_hat <span class="sc">-</span> ATE)<span class="sc">^</span><span class="dv">2</span> ) )</span>
<span id="cb298-7"><a href="performance-measures.html#cb298-7" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb298-8"><a href="performance-measures.html#cb298-8" tabindex="-1"></a>  <span class="fu">mutate</span>( </span>
<span id="cb298-9"><a href="performance-measures.html#cb298-9" tabindex="-1"></a>    <span class="at">per_RMSE =</span> RMSE <span class="sc">/</span> RMSE[method<span class="sc">==</span><span class="st">&quot;LR&quot;</span>]</span>
<span id="cb298-10"><a href="performance-measures.html#cb298-10" tabindex="-1"></a>  )</span></code></pre></div>
<pre><code>## # A tibble: 3 × 5
##   method      bias    SE  RMSE per_RMSE
##   &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;
## 1 Agg    -0.000166 0.214 0.214    0.897
## 2 LR      0.0824   0.224 0.238    1    
## 3 MLM     0.0122   0.213 0.213    0.896</code></pre>
<p>We also include SE and bias for ease of reference.</p>
<p>RMSE takes into account both bias and variance.
For aggregation and multi-level modeling, the RMSE is the same as the standard error, which makes sense because these estimators are not biased.
For linear regression, the combination of bias plus increased variability yields a higher RMSE, with the standard error dominating the bias term (note how RMSE and SE are more similar than RMSE and bias).
The difference between the estimators are pronounced because RMSE is the square root of the <em>squared</em> bias and <em>squared</em> standard error.
Overall, aggregation and multi-level modeling have RMSEs around 17% smaller than linear regression—a consequential difference in accuracy.</p>
</div>
</div>
<div id="less-conventional-measures" class="section level3 hasAnchor" number="9.1.2">
<h3><span class="header-section-number">9.1.2</span> Less Conventional Performance Measures<a href="performance-measures.html#less-conventional-measures" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Depending on the model and estimation procedures being examined, a range of different measures might be used to assess estimator performance.
For point estimation, we have introduced bias, variance and RMSE as three core measures of performance.
However, all of these measures are sensitive to outliers in the sampling distribution.
Consider an estimator that generally does well, except for an occasional large mistake. Because conventional measures are based on arithmetic averages, they will indicate that the estimator performs very poorly overall.
Other measures such as the median bias and the median absolute deviation of <span class="math inline">\(T\)</span> are less sensitive to outliers in the sampling distribution compared to the conventional measures.
Estimating these measures will involve calculating sample quantiles of <span class="math inline">\(T_1,...,T_R\)</span>, which are functions of the sample ordered from smallest to largest.
We will denote the <span class="math inline">\(r^{th}\)</span> order statistic as <span class="math inline">\(T_{(r)}\)</span> for <span class="math inline">\(r = 1,...,R\)</span>.</p>
<p>Median bias is an alternative measure of the central tendency of a sampling distribution.
Positive median bias implies that more than 50% of the sampling distribution exceeds the quantity of interest, while negative median bias implies that more than 50% of the sampling distribution fall below the quantity of interest.
Formally,
<span class="math display" id="eq:median-bias">\[
\text{Median-Bias}(T) = \M(T) - \theta
\tag{9.6}.
\]</span>
An estimator of median bias is computed using the sample median, as
<span class="math display" id="eq:sample-median-bias">\[
\widehat{\text{Median-Bias}}(T) = M_T - \theta
\tag{9.7}
\]</span>
where <span class="math inline">\(M_T = T_{((R+1)/2)}\)</span> if <span class="math inline">\(R\)</span> is odd or <span class="math inline">\(M_T = \frac{1}{2}\left(T_{(R/2)} + T_{(R/2+1)}\right)\)</span> if <span class="math inline">\(R\)</span> is even.</p>
<p>Another robust measure of central tendency uses the <span class="math inline">\(p \times 100\%\)</span>-trimmed mean, which ignores the estimates in the lowest and highest <span class="math inline">\(p\)</span>-quantiles of the sampling distribution.
Formally, the trimmed-mean bias is
<span class="math display" id="eq:trimmed-bias">\[
\text{Trimmed-Bias}(T; p) = \E\left[ T \left| \Q_{p}(T) &lt; T &lt; \Q_{(1 - p)}(T) \right.\right] - \theta.
\tag{9.8}
\]</span>
Median bias is thus a special case of trimmed mean bias, with <span class="math inline">\(p = 0.5\)</span>.
To estimate the trimmed bias, we take the mean of the middle <span class="math inline">\(1 - 2p\)</span> fraction of the distribution
<span class="math display" id="eq:sample-trimmed-bias">\[
\widehat{\text{Trimmed-Bias}}(T; p) = \tilde{T}_{\{p\}} - \theta. \tag{9.9}
\]</span>
where
<span class="math display">\[
\tilde{T}_{\{p\}} = \frac{1}{(1 - 2p)R} \sum_{r=pR + 1}^{(1-p)R} T_{(r)}
\]</span>
For a symmetric sampling distribution, trimmed-mean bias will be the same as the conventional (mean) bias, but its estimator <span class="math inline">\(\tilde{T}_{\{p\}}\)</span> will be less affected by outlying values (i.e., values of <span class="math inline">\(T\)</span> very far from the center of the distribution) compared to <span class="math inline">\(\bar{T}\)</span>.
However, if a sampling distribution is not symmetric, trimmed-mean bias become distinct performance measures, which put less emphasis on large errors compared to the conventional bias measure.</p>
<p>A further robust measure of central tendency is based on winsorizing the sampling distribution, or truncating all errors larger than a certain maximum size.
Using a winsorized distribution amounts to arguing that you don’t care about errors beyond a certain size, so anything beyond a certain threshold will be treated the same as if it were exactly on the threshold.
The threshold for truncation is usually defined relative to the first and third quartiles of the sampling distribution, along with a given span of the inter-quartile range.
The thresholds for truncation are taken as
<span class="math display">\[
\begin{aligned}
L_w &amp;= \Q_{0.25}(T) - w \times (\Q_{0.75}(T) - \Q_{0.25}(T)) \\
U_w &amp;= \Q_{0.75}(T) + w \times (\Q_{0.75}(T) - \Q_{0.25}(T)),
\end{aligned}
\]</span>
where <span class="math inline">\(\Q_{0.25}(T)\)</span> and <span class="math inline">\(\Q_{0.75}(T)\)</span> are the first and third quartiles of the distribution of <span class="math inline">\(T\)</span>, respectively, and <span class="math inline">\(w\)</span> is the number of inter-quartile ranges below which an observation will be treated as an outlier.<a href="#fn12" class="footnote-ref" id="fnref12"><sup>12</sup></a>
Let <span class="math inline">\(X = \min\{\max\{T, L_w\}, U_w\}\)</span>.
The winsorized bias, variance, and RMSE are then defined using winsorized values in place of the raw values of <span class="math inline">\(T\)</span>, as
<span class="math display" id="eq:winsorized-variance-RMSE">\[
\begin{aligned}
\text{Bias}(X) &amp;= \E\left(X\right) - \theta \\
\Var\left(X\right) &amp;= \E\left[\left(X - \E (T^{(w)})\right)^2 \right], \\
\RMSE\left(X\right) &amp;= \sqrt{\E\left[\left(X - \theta\right)^2 \right]}.
\end{aligned}
\tag{9.10}
\]</span>
To compute estimates of the winsorized performance criteria, we substitute sample quantiles <span class="math inline">\(T_{(R/4)}\)</span> and <span class="math inline">\(T_{(3R/4)}\)</span> in place of <span class="math inline">\(\Q_{0.25}(T)\)</span> and <span class="math inline">\(\Q_{0.25}(T)\)</span>, respectively, to get estimated thresholds, <span class="math inline">\(\hat{L}_w\)</span> and <span class="math inline">\(\hat{U}_w\)</span>, find <span class="math inline">\(\hat{X}_r = \min\{\max\{T_r, \hat{L}_w\}, \hat{U}_w\}\)</span>, and compute the sample performance measures using Equations <a href="performance-measures.html#eq:bias-estimator">(9.3)</a>, <a href="performance-measures.html#eq:var-estimator">(9.4)</a>, and <a href="performance-measures.html#eq:rmse-estimator">(9.5)</a>, but with <span class="math inline">\(\hat{X}\)</span> in place of <span class="math inline">\(T_r\)</span>.</p>
<p>Alternative measures of the overall accuracy of an estimator can also be defined using quantiles.
For instance, an alternative to RMSE is to use the median absolute error (MAE), defined as
<span class="math display" id="eq:MAE">\[
\text{MAE} = \M\left(\left|T - \theta\right|\right).
\tag{9.11}
\]</span>
Letting <span class="math inline">\(E_r = |T_r - \theta|\)</span>, the MAE can be estimated by taking the sample median of <span class="math inline">\(E_1,...,E_R\)</span>.
Many other robust measures of the spread of the sampling distribution are also available, including the Rosseeuw-Croux scale estimator <span class="math inline">\(Q_n\)</span> <span class="citation">[@Rousseeuw1993alternatives]</span> and the biweight midvariance <span class="citation">[@Wilcox2022introduction]</span>.
<span class="citation">@Maronna2006robust</span> provide a useful introduction to these measures and robust statistics more broadly.
The <code>robustbase</code> package <span class="citation">[@robustbase]</span> provides functions for calculating many of these robust statistics.</p>
</div>
</div>
<div id="measures-for-variance-estimators" class="section level2 hasAnchor" number="9.2">
<h2><span class="header-section-number">9.2</span> Measures for Variance Estimators<a href="performance-measures.html#measures-for-variance-estimators" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Statistics is concerned not only with how to estimate things, but also with understanding the extent of uncertainty in estimates of target parameters.
These concerns apply in Monte Carlo simulation studies as well.
In a simulation, we can simply compute an estimator’s actual properties.
When we use an estimator with real data, we need to <em>estimate</em> its associated standard error and generate confidence intervals and other assessments of uncertainty.
To understand if these uncertainty assessments work in practice, we need to evaluate not only the behavior of the estimator itself, but also the behavior of these associated quantities.</p>
<p>Commonly used measures for quantifying the performance of estimated standard errors include relative bias, relative standard error, and relative root mean squared error.
These measures are defined in relative terms (rather than absolute ones) by comparing their magnitude to the <em>true</em> degree of uncertainty.
Typically, performance measures are computed for <em>variance</em> estimators rather than standard error estimators.
There are a few reasons for working with variance rather than standard error.
First, in practice, so-called unbiased standard errors usually are not actually unbiased.<a href="#fn13" class="footnote-ref" id="fnref13"><sup>13</sup></a>
For linear regression, for example, the classic standard error estimator is an unbiased <em>variance</em> estimator, but the standard error estimator is not exactly unbiased because
<span class="math display">\[
\E[ \sqrt{ V } ] \neq \sqrt{ \E[ V ] }.
\]</span>
Variance is also the measure that gives us the bias-variance decomposition of Equation <a href="performance-measures.html#eq:RMSE-decomposition">(9.2)</a>. Thus, if we are trying to determine whether MSE is due to instability or systematic bias, operating in this squared space may be preferable.</p>
<p>To make this concrete, let us consider a generic standard error estimator <span class="math inline">\(\widehat{SE}\)</span> to go along with our generic estimator <span class="math inline">\(T\)</span> of target parameter <span class="math inline">\(\theta\)</span>, and let <span class="math inline">\(V = \widehat{SE}^2\)</span>.
We can simulate to obtain a large sample of standard errors, <span class="math inline">\(\widehat{SE}_1,...,\widehat{SE}_R\)</span> and variance estimators <span class="math inline">\(V_r = \widehat{SE}_r^2\)</span> for <span class="math inline">\(r = 1,...,R\)</span>.
Formally, the relative bias, standard error, and RMSE of <span class="math inline">\(V\)</span> are defined as
<span class="math display" id="eq:relative-bias-SE-RMSE">\[
\begin{aligned}
\text{Relative Bias}(V) &amp;= \frac{\E(V)}{\Var(T)} \\
\text{Relative SE}(V) &amp;= \frac{\Var(V)}{\Var(T)} \\
\text{Relative RMSE}(V) &amp;= \frac{\sqrt{\E\left[\left(V - \Var(T)\right)^2 \right]}}{\Var(T)}.
\end{aligned}
\tag{9.12}
\]</span>
In contrast to performance measures for <span class="math inline">\(T\)</span>, we define these measures in relative terms because the raw magnitude of <span class="math inline">\(V\)</span> is not a stable or interpretable parameter.
Rather, the sampling distribution of <span class="math inline">\(V\)</span> will generally depend on many of the parameters of the data-generating process, including the sample size and any other design parameters.
Defining bias in relative terms makes for a more interpretable metric: a value of 1 corresponds to exact unbiasedness of the variance estimator.
Relative bias measures <em>proportionate</em> under- or over-estimation.
For example, a relative bias of 1.12 would mean the standard error was, on average, 12% too large.
We discuss relative performance measures further in Section <a href="performance-measures.html#sec-relative-performance">9.5</a>.</p>
<p>To estimate these relative performance measures, we proceed by substituting sample quantities in place of the expectations and variances.
In contrast to the performance measures for <span class="math inline">\(T\)</span>, we will not generally be able to compute the true degree of uncertainty exactly.
Instead, we must estimate the target quantity <span class="math inline">\(\Var(T)\)</span> using <span class="math inline">\(S_T^2\)</span>, the sample variance of <span class="math inline">\(T\)</span> across replications.
Denoting the arithmetic mean of the variance estimates as
<span class="math display">\[
\bar{V} = \frac{1}{R} \sum_{r=1}^R V_r
\]</span>
and the sample variance as
<span class="math display">\[
S_V^2 = \frac{1}{R - 1}\sum_{r=1}^R \left(V_r - \bar{V}\right)^2,
\]</span>
we estimate the relative bias, standard error, and RMSE of <span class="math inline">\(V\)</span> using
<span class="math display" id="eq:relative-bias-SE-RMSE-estimators">\[
\begin{aligned}
\widehat{\text{Relative Bias}}(V) &amp;= \frac{\bar{V}}{S_T^2} \\
\widehat{\text{Relative SE}}(V) &amp;= \frac{S_V}{S_T^2} \\
\widehat{\text{Relative RMSE}}(V) &amp;= \frac{\sqrt{\frac{1}{R}\sum_{r=1}^R\left(V_r - S_T^2\right)^2}}{S_T^2}.
\end{aligned}
\tag{9.13}
\]</span>
These performance measures are informative about the properties of the uncertainty estimator <span class="math inline">\(V\)</span> (or standard error <span class="math inline">\(\widehat{SE}\)</span>), which have implications for the performance of other uncertainty assessments such as hypothesis tests and confidence intervals.
Relative bias describes whether the central tendency of <span class="math inline">\(V\)</span> aligns with the actual degree of uncertainty in the point estimator <span class="math inline">\(T\)</span>.
Relative bias of less than 1 implies that <span class="math inline">\(V\)</span> tends to under-state the amount of uncertainty, which will lead to confidence intervals that are overly narrow and do not cover the true parameter value at the desired rate.
Relative bias greater than 1 implies that <span class="math inline">\(V\)</span> tends to over-state the amount of uncertainty in the point estimator, making it seem like <span class="math inline">\(T\)</span> is less precise than it truly is.
Relative standard errors describe the variability of <span class="math inline">\(V\)</span> in comparison to the true degree of uncertainty in <span class="math inline">\(T\)</span>—the lower the better.
A relative standard error of 0.5 would mean that the variance estimator has average error of 50% of the true uncertainty, implying that <span class="math inline">\(V\)</span> will often be off by a factor of 2 compared to the true sampling variance of <span class="math inline">\(T\)</span>.
Ideally, a variance estimator will have small relative bias, small relative standard errors, and thus small relative RMSE.</p>
<div id="satterthwaite-degrees-of-freedom" class="section level3 hasAnchor" number="9.2.1">
<h3><span class="header-section-number">9.2.1</span> Satterthwaite degrees of freedom<a href="performance-measures.html#satterthwaite-degrees-of-freedom" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Another more abstract measure of the stability of a variance estimator is its Satterthwaite degrees of freedom.
For some simple statistical models such as classical analysis of variance and linear regression with homoskedastic errors, the variance estimator is computed by taking a sum of squares of normally distributed errors.
In such cases, the sampling distribution of the variance estimator is a multiple of a <span class="math inline">\(\chi^2\)</span> distribution, with degrees of freedom corresponding to the number of independent observations used to compute the sum of squares.
In the context of analysis of variance problems, <span class="citation">@Satterthwaite1946approximate</span> described a method of approximating the variability of more complex statistics, involving linear combinations of sums of squares, by using a chi-squared distribution with a certain degrees of freedom.
When applied to an arbitrary variance estimator <span class="math inline">\(V\)</span>, these degrees of freedom can be interpreted as the number of independent, normally distributed errors going into a sum of squares that would lead to a variance estimator that is equally precise as <span class="math inline">\(V\)</span>.
More succinctly, these degrees of freedom correspond to the amount of independent observations used to estimate <span class="math inline">\(V\)</span>.</p>
<p>Following <span class="citation">@Satterthwaite1946approximate</span>, we define the degrees of freedom of <span class="math inline">\(V\)</span> as
<span class="math display" id="eq:Satterthwaite-df">\[
df = \frac{2 \left[\E(V)\right]^2}{\Var(V)}.
\tag{9.14}
\]</span>
We can estimate the degrees of freedom by taking
<span class="math display" id="eq:Satterthwaite-df-estimator">\[
\widehat{df} = \frac{2 \bar{V}^2}{S_V^2}.
\tag{9.15}
\]</span>
For simple statistical methods in which <span class="math inline">\(V\)</span> is based on a sum-of-squares of normally distributed errors, then the Satterthwaite degrees of freedom will be constant and correspond exactly to the number of independent observations in the sum of squares.
Even with more complex methods, the degrees of freedom are interpretable: higher degrees of freedom imply that <span class="math inline">\(V\)</span> is based on more observations, and thus will be a more precise estimate of the actual degree of uncertainty in <span class="math inline">\(T\)</span>.</p>
</div>
<div id="assessing-ses-for-the-cluster-rct-simulation" class="section level3 hasAnchor" number="9.2.2">
<h3><span class="header-section-number">9.2.2</span> Assessing SEs for the Cluster RCT Simulation<a href="performance-measures.html#assessing-ses-for-the-cluster-rct-simulation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Returning to the cluster RCT example, we will assess whether our estimated SEs are about right by comparing the average <em>estimated</em> (squared) standard error versus the empirical sampling variance.
Our standard errors are <em>inflated</em> if they are systematically larger than they should be, across the simulation runs.
We will also look at how stable our variance estimates are by comparing their standard deviation to the empirical sampling variance and by computing the Satterthwaite degrees of freedom.</p>
<div class="sourceCode" id="cb300"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb300-1"><a href="performance-measures.html#cb300-1" tabindex="-1"></a>SE_performance <span class="ot">&lt;-</span> </span>
<span id="cb300-2"><a href="performance-measures.html#cb300-2" tabindex="-1"></a>  runs <span class="sc">%&gt;%</span>  </span>
<span id="cb300-3"><a href="performance-measures.html#cb300-3" tabindex="-1"></a>  <span class="fu">mutate</span>( <span class="at">V =</span> SE_hat<span class="sc">^</span><span class="dv">2</span> ) <span class="sc">%&gt;%</span></span>
<span id="cb300-4"><a href="performance-measures.html#cb300-4" tabindex="-1"></a>  <span class="fu">group_by</span>( method ) <span class="sc">%&gt;%</span></span>
<span id="cb300-5"><a href="performance-measures.html#cb300-5" tabindex="-1"></a>  <span class="fu">summarise</span>( </span>
<span id="cb300-6"><a href="performance-measures.html#cb300-6" tabindex="-1"></a>    <span class="at">SE_sq =</span> <span class="fu">var</span>( ATE_hat ),</span>
<span id="cb300-7"><a href="performance-measures.html#cb300-7" tabindex="-1"></a>    <span class="at">V_bar =</span> <span class="fu">mean</span>( V ),</span>
<span id="cb300-8"><a href="performance-measures.html#cb300-8" tabindex="-1"></a>    <span class="at">rel_bias =</span> V_bar <span class="sc">/</span> SE_sq,</span>
<span id="cb300-9"><a href="performance-measures.html#cb300-9" tabindex="-1"></a>    <span class="at">S_V =</span> <span class="fu">sd</span>( V ),</span>
<span id="cb300-10"><a href="performance-measures.html#cb300-10" tabindex="-1"></a>    <span class="at">rel_SE_V =</span> S_V <span class="sc">/</span> SE_sq,</span>
<span id="cb300-11"><a href="performance-measures.html#cb300-11" tabindex="-1"></a>    <span class="at">df =</span> <span class="dv">2</span> <span class="sc">*</span> <span class="fu">mean</span>( V )<span class="sc">^</span><span class="dv">2</span> <span class="sc">/</span> <span class="fu">var</span>( V )</span>
<span id="cb300-12"><a href="performance-measures.html#cb300-12" tabindex="-1"></a>  )</span>
<span id="cb300-13"><a href="performance-measures.html#cb300-13" tabindex="-1"></a></span>
<span id="cb300-14"><a href="performance-measures.html#cb300-14" tabindex="-1"></a>SE_performance</span></code></pre></div>
<pre><code>## # A tibble: 3 × 7
##   method  SE_sq  V_bar rel_bias    S_V rel_SE_V
##   &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;
## 1 Agg    0.0457 0.0515     1.13 0.0175    0.384
## 2 LR     0.0500 0.0553     1.11 0.0231    0.461
## 3 MLM    0.0454 0.0510     1.12 0.0173    0.382
## # ℹ 1 more variable: df &lt;dbl&gt;</code></pre>
<p>The variance estimators for the aggregation estimator and multilevel model estimator appear to be a bit conservative on average, with relative bias of around 1.13, or about 13% higher than the true sampling variance.
The column labelled <code>rel_SE_V</code> reports how variable the variance estimators are relative to the true sampling variances of the estimators.
The column labelled <code>df</code> reports the Satterthwaite degrees of freedom of each variance estimator.
Both of these measures indicate that the linear regression variance estimator is less stable than the other methods, with around 6 fewer degrees of freedom.
The linear regression method uses a cluster-robust variance estimator, which is known to be a bit unstable <span class="citation">[@cameronPractitionerGuideClusterRobust2015]</span>.
Overall, it is a bad day for linear regression.</p>
</div>
</div>
<div id="measures-for-confidence-intervals" class="section level2 hasAnchor" number="9.3">
<h2><span class="header-section-number">9.3</span> Measures for Confidence Intervals<a href="performance-measures.html#measures-for-confidence-intervals" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Some estimation procedures provide confidence intervals (or confidence sets) which are ranges of values, or interval estimators, that should include the true parameter value with a specified confidence level.
For a 95% confidence level, the interval should include the true parameter in 95% replications of the data-generating process.
However, with the exception of some simple methods and models, methods for constructing confidence intervals usually involve approximations and simplifying assumptions, so their actual coverage rate might deviate from the intended confidence level.</p>
<p>We typically measure confidence interval performance along two dimensions: <strong>coverage rate</strong> and <strong>expected width</strong>.
Suppose that the confidence interval is for the target parameter <span class="math inline">\(\theta\)</span> and has intended coverage level <span class="math inline">\(\beta\)</span> for <span class="math inline">\(0 &lt; \beta &lt; 1\)</span>.
Denote the lower and upper end-points of the <span class="math inline">\(\beta\)</span>-level confidence interval as <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>.
<span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are random quantities—they will differ each time we compute the interval on a different replication of the data-generating process.
The coverage rate of a <span class="math inline">\(\beta\)</span>-level interval estimator is the probability that it covers the true parameter, formally defined as
<span class="math display" id="eq:coverage">\[
\text{Coverage}(A,B) = \Prob(A \leq \theta \leq B).
\tag{9.16}
\]</span>
For a well-performing interval estimator, <span class="math inline">\(\text{Coverage}\)</span> will at least <span class="math inline">\(\beta\)</span> and, ideally will not exceed <span class="math inline">\(\beta\)</span> by too much.
The expected width of a <span class="math inline">\(\beta\)</span>-level interval estimator is the average difference between the upper and lower endpoints, formally defined as
<span class="math display" id="eq:expected-width">\[
\text{Width}(A,B) = \E(B - A).
\tag{9.17}
\]</span>
Smaller expected width means that the interval tends to be narrower, on average, and thus more informative about the value of the target parameter.</p>
<p>In practice, we approximate the coverage and width of a confidence interval by summarizing across replications of the data-generating process.
Let <span class="math inline">\(A_r\)</span> and <span class="math inline">\(B_r\)</span> denote the lower and upper end-points of the confidence interval from simulation replication <span class="math inline">\(r\)</span>, and let <span class="math inline">\(W_r = B_r - A_r\)</span>, all for <span class="math inline">\(r = 1,...,R\)</span>.
The coverage rate and expected length measures can be estimated as
<span class="math display" id="eq:coverage-width">\[
\begin{aligned}
\widehat{\text{Coverage}}(A,B) &amp;= \frac{1}{R}\sum_{r=1}^R I(A_r \leq \theta \leq B_r) \\
\widehat{\text{Width}}(A,B) &amp;= \frac{1}{R} \sum_{r=1}^R W_r = \frac{1}{R} \sum_{r=1}^R \left(B_r - A_r\right).
\end{aligned}
\tag{9.18}
\]</span>
Following a strict statistical interpretation, a confidence interval performs acceptably if it has actual coverage rate greater than or equal to <span class="math inline">\(\beta\)</span>.
If multiple methods satisfy this criterion, then the method with the lowest expected width would be preferable. Some analysts prefer to look at lower and upper coverage separately, where lower coverage is <span class="math inline">\(\Prob(A \leq \theta)\)</span> and upper coverage is <span class="math inline">\(\Prob(\theta \leq B)\)</span>.</p>
<p>In many instances, confidence intervals are constructed using point estimators and
uncertainty estimators.
For example, a conventional Wald-type confidence interval is centered on a point estimator, with end-points taken to be a multiple of an estimated standard error below and above the point estimator:
<span class="math display">\[
A = T - c \times \widehat{SE}, \quad B = T + c \times \widehat{SE}
\]</span>
for some critical value <span class="math inline">\(c\)</span> (e.g.,for a normal critical value with a <span class="math inline">\(\beta = 0.95\)</span> confidence level, <span class="math inline">\(c = 1.96\)</span>).
Because of these connections, confidence interval coverage will often be closely related to the performance of the point estimator and variance estimator.
Biased point estimators will tend to have confidence intervals with coverage below the desired level because they are not centered in the right place.
Likewise, variance estimators that have relative bias below 1 will tend to produce confidence intervals that are too short, leading to coverage below the desired level.
Thus, confidence interval coverage captures multiple aspects of the performance of an estimation procedure.</p>
<div id="cluster-RCT-CI-coverage" class="section level3 hasAnchor" number="9.3.1">
<h3><span class="header-section-number">9.3.1</span> Confidence Intervals in the Cluster RCT Simulation<a href="performance-measures.html#cluster-RCT-CI-coverage" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Returning to the CRT simulation, we will examine the coverage and expected width of normal Wald-type confidence intervals for each of the estimators under consideration.
To do this, we first have to calculate the confidence intervals because we did not do so in the estimation function.
We compute a normal critical value for a <span class="math inline">\(\beta = 0.95\)</span> confidence level using <code>qnorm(0.975)</code>, then compute the lower and upper end-points using the point estimators and estimated standard errors:</p>
<div class="sourceCode" id="cb302"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb302-1"><a href="performance-measures.html#cb302-1" tabindex="-1"></a>runs_CIs <span class="ot">&lt;-</span> </span>
<span id="cb302-2"><a href="performance-measures.html#cb302-2" tabindex="-1"></a>  runs <span class="sc">%&gt;%</span> </span>
<span id="cb302-3"><a href="performance-measures.html#cb302-3" tabindex="-1"></a>  <span class="fu">mutate</span>( </span>
<span id="cb302-4"><a href="performance-measures.html#cb302-4" tabindex="-1"></a>    <span class="at">A =</span> ATE_hat <span class="sc">-</span> <span class="fu">qnorm</span>(<span class="fl">0.975</span>) <span class="sc">*</span> SE_hat,</span>
<span id="cb302-5"><a href="performance-measures.html#cb302-5" tabindex="-1"></a>    <span class="at">B =</span> ATE_hat <span class="sc">+</span> <span class="fu">qnorm</span>(<span class="fl">0.975</span>) <span class="sc">*</span> SE_hat</span>
<span id="cb302-6"><a href="performance-measures.html#cb302-6" tabindex="-1"></a>  )</span></code></pre></div>
<p>Now we can estimate the coverage rate and expected width of these confidence intervals:</p>
<div class="sourceCode" id="cb303"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb303-1"><a href="performance-measures.html#cb303-1" tabindex="-1"></a>runs_CIs <span class="sc">%&gt;%</span></span>
<span id="cb303-2"><a href="performance-measures.html#cb303-2" tabindex="-1"></a>  <span class="fu">group_by</span>( method ) <span class="sc">%&gt;%</span></span>
<span id="cb303-3"><a href="performance-measures.html#cb303-3" tabindex="-1"></a>  <span class="fu">summarise</span>( </span>
<span id="cb303-4"><a href="performance-measures.html#cb303-4" tabindex="-1"></a>    <span class="at">coverage =</span> <span class="fu">mean</span>( A <span class="sc">&lt;=</span> ATE <span class="sc">&amp;</span> ATE <span class="sc">&lt;=</span> B ),</span>
<span id="cb303-5"><a href="performance-measures.html#cb303-5" tabindex="-1"></a>    <span class="at">width =</span> <span class="fu">mean</span>( B <span class="sc">-</span> A )</span>
<span id="cb303-6"><a href="performance-measures.html#cb303-6" tabindex="-1"></a>  )</span></code></pre></div>
<pre><code>## # A tibble: 3 × 3
##   method coverage width
##   &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;
## 1 Agg       0.948 0.877
## 2 LR        0.92  0.903
## 3 MLM       0.945 0.872</code></pre>
<p>The coverage rate is close to the desired level of 0.95 for the multilevel model and aggregation estimators, but it is around 5 percentage points too low for linear regression.
The lower-than-nominal coverage level occurs because of the bias of the linear regression point estimator.
The linear regression confidence intervals are also a bit wider than the other methods due to the larger sampling variance of its point estimator and higher variability (lower degrees of freedom) of its standard error estimator.</p>
<p>The normal Wald-type confidence intervals we have examined here are based on fairly rough approximations.
In practice, we might want to examine more carefully constructed intervals such as ones that use critical values based on <span class="math inline">\(t\)</span> distributions or ones constructed by profile likelihood.
Especially in scenarios with a small or moderate number of clusters, such methods might provide better intervals, with coverage closer to the desired confidence level.
See Exercise <a href="performance-measures.html#cluster-RCT-t-confidence-intervals">9.10.4</a>.</p>
</div>
</div>
<div id="assessing-inferential-procedures" class="section level2 hasAnchor" number="9.4">
<h2><span class="header-section-number">9.4</span> Measures for Inferential Procedures (Hypothesis Tests)<a href="performance-measures.html#assessing-inferential-procedures" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Hypothesis testing entails first specifying a null hypothesis, such as that there is no difference in average outcomes between two experimental groups. One then collects data and evaluates whether the observed data is compatible with the null hypothesis.
Hypothesis test results are often describes in terms of a <span class="math inline">\(p\)</span>-value, which measures how extreme or surprising a feature of the observed data (a test statistic) is relative to what one would expect if the null hypothesis is true.
A small <span class="math inline">\(p\)</span>-value (such as <span class="math inline">\(p &lt; .05\)</span> or <span class="math inline">\(p &lt; .01\)</span>) indicates that the observed data would be unlikely to occur if the null is true, leading the researcher to reject the null hypothesis.
Alternately, testing procedures might be formulated by comparing a test statistic to a specified critical value; a test statistic exceeding the critical value would lead the researcher to reject the null.</p>
<p>Hypothesis testing procedures aim to control the level of false positives, corresponding to the probability that the null hypothesis is rejected when it holds in truth.
The level of a testing procedure is often denoted as <span class="math inline">\(\alpha\)</span>, and it has become conventional in many fields to conduct tests with a level of <span class="math inline">\(\alpha = .05\)</span>.<a href="#fn14" class="footnote-ref" id="fnref14"><sup>14</sup></a>
Just as in the case of confidence intervals, hypothesis testing procedures can sometimes be developed that will have false positive rates exactly equal to the intended level <span class="math inline">\(\alpha\)</span>.
However, in many other problems, hypothesis testing procedures involve approximations or assumption violations, so that the actual rate of false positives might deviate from the intended <span class="math inline">\(\alpha\)</span>-level.
When we evaluate a hypothesis testing procedure, we are concerned with two primary measures of performance: <em>validity</em> and <em>power</em>.</p>
<div id="validity" class="section level3 hasAnchor" number="9.4.1">
<h3><span class="header-section-number">9.4.1</span> Validity<a href="performance-measures.html#validity" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Validity pertains to whether we erroneously reject a true null more than we should.
An <span class="math inline">\(\alpha\)</span>-level testing procedure is valid if it has no more than an <span class="math inline">\(\alpha\)</span> chance of rejecting the null, when the null is true.
If we were using the conventional <span class="math inline">\(\alpha = .05\)</span> level, then a valid testing procedure will reject the null in only 50 of 1000 replications of a data-generating process where the null hypothesis actually holds true.</p>
<p>To assess validity, we will need to specify a data generating process where the null hypothesis holds (e.g., where there is no difference in average outcomes between experimental groups).
We then generate a large series of data sets with a true null, conduct the testing procedure on each dataset and record the <span class="math inline">\(p\)</span>-value or critical value, then score whether we reject the null hypothesis.
In practice, we may be interested in evaluating a testing procedure by exploring data generation processes where the null is true but other aspects of the data (such as outliers, skewed outcome distributions, or small sample size) make estimation difficult, or where auxiliary assumptions of the testing procedure are violated.
Examining such data-generating processes allows us to understand if our methods are robust to patterns that might be encountered in real data analysis.
The key to evaluating the validity of a procedure is that, for whatever data-generating process we examine, the null hypothesis must be true.</p>
</div>
<div id="power" class="section level3 hasAnchor" number="9.4.2">
<h3><span class="header-section-number">9.4.2</span> Power<a href="performance-measures.html#power" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Power is concerned with the chance that we notice when an effect or a difference exists—that is, the probability of rejecting the null hypothesis when it does not actually hold.
Compared to validity, power is a more nuanced concept because larger effects will clearly be easier to notice than smaller ones, and more blatant violations of a null hypothesis will be easier to identify than subtle ones.
Furthermore, the rate at which we can detect violations of a null will depend on the <span class="math inline">\(\alpha\)</span> level of the testing procedure. A lower <span class="math inline">\(\alpha\)</span> level will make for a less sensitive test, requiring stronger evidence to rule out a null hypothesis.
Conversely, a higher <span class="math inline">\(\alpha\)</span> level will reject more readily, leading to higher power but at a cost of increased false positives.
<!-- When assessing validity, we want the rejection rate to be at or below the specified $\alpha$ level, and when assessing power we want the rejection rate to be as high as possible. --></p>
<p>In order to evaluate the power of a testing procedure by simulation, we will need to generate data where there is something to detect.
In other words, we will need to ensure that the null hypothesis is violated (and that some specific alternative hypothesis of interest holds).
The process of evaluating the power of a testing procedure is otherwise identical to that for evaluating its validity: generate many datasets, carry out the testing procedure, and track the rate at which the null hypothesis is rejected.
The only difference is the <em>conditions</em> under which the data are generated.</p>
<p>We find it useful to think of power as a <em>function</em> rather than as a single quantity because its absolute magnitude will generally depend on the sample size of a dataset and the magnitude of the effect of interest.
Because of this, power evaluations will typically involve examining a <em>sequence</em> of data-generating scenarios with varying sample size or varying effect size.
Further, if our goal is to evaluate several different testing procedures, the absolute power of a procedure will be of less concern than the <em>relative</em> performance of one procedure compared to another.
<!-- JEP: Should we formally define relative power or infinitesimal power? -->
<!-- JEP: We should say something about convention of seeking power >= 0.8. --></p>
</div>
<div id="rejection-rates" class="section level3 hasAnchor" number="9.4.3">
<h3><span class="header-section-number">9.4.3</span> Rejection Rates<a href="performance-measures.html#rejection-rates" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>When evaluating either validity or power, the main performance measure is the <strong>rejection rate</strong> of the hypothesis test. Letting <span class="math inline">\(P\)</span> be the p-value from a procedure for testing the null hypothesis that a parameter <span class="math inline">\(\theta = 0\)</span>, generated under a data-generating process with parameter <span class="math inline">\(\theta\)</span> (which could in truth be zero or non-zero). The rejection rate is then
<span class="math display" id="eq:rejection-rate">\[
\rho_\alpha(\theta) = \Prob(P &lt; \alpha)
\tag{9.19}
\]</span>
When data are simulated from a process in which the null hypothesis is true, then the rejection rate is equivalent to the Type-I error rate of the test, which should ideally be near the desired <span class="math inline">\(\alpha\)</span> level.
When the data are simulated from a process in which the null hypothesis is violated, then the rejection rate is equivalent to the power of the test (for the given alternate hypothesis specified in the data-generating process).
Ideally, a testing procedure should have actual Type-I error equal to the nominal level <span class="math inline">\(\alpha\)</span> (this is the definition of validity), but such exact tests are rare.</p>
<p>To estimate the rejection rate of a test, we calculate the proportion of replications where the test rejects the null hypothesis.
Letting <span class="math inline">\(P_1,...,P_R\)</span> be the p-values simulated from <span class="math inline">\(R\)</span> replications of a data-generating process with true parameter <span class="math inline">\(\theta\)</span>, we estimate the rejection rate by calculating
<span class="math display" id="eq:rejection-rate-estimate">\[
r_\alpha(\theta) = \frac{1}{R} \sum_{r=1}^R I(P_r &lt; \alpha).
\tag{9.20}
\]</span>
It may be of interest to evaluate the performance of the test at several different <span class="math inline">\(\alpha\)</span> levels.
For instance, <span class="citation">@brown1974SmallSampleBehavior</span> evaluated the Type-I error rates and power of their tests using <span class="math inline">\(\alpha = .01\)</span>, <span class="math inline">\(.05\)</span>, and <span class="math inline">\(.10\)</span>.
Simulating the <span class="math inline">\(p\)</span>-value of the test makes it easy to estimate rejection rates for multiple <span class="math inline">\(\alpha\)</span> levels, since we simply need to apply Equation <a href="performance-measures.html#eq:rejection-rate-estimate">(9.20)</a> for several values of <span class="math inline">\(\alpha\)</span>.
When simulating from a data-generating process where the null hypothesis holds, one can also plot the empirical cumulative distribution function of the <span class="math inline">\(p\)</span>-values; for an exactly valid test, the <span class="math inline">\(p\)</span>-values should follow a standard uniform distribution with a cumulative distribution falling along the <span class="math inline">\(45^\circ\)</span> line.</p>
<p>Methodologists hold a variety of perspectives on how close the actual Type-I error rate should be in order to qualify as suitable for use in practice. Following a strict statistical definition, a hypothesis testing procedure is said to be <strong>level-<span class="math inline">\(\alpha\)</span></strong> if its actual Type-I error rate is <em>always</em> less than or equal to <span class="math inline">\(\alpha\)</span>, for any specific conditions of a data-generating process.
Among a collection of level-<span class="math inline">\(\alpha\)</span> testing procedures, we would prefer the one with highest power.
If looking only at null rejection rates, then the test with Type-I error closest to <span class="math inline">\(\alpha\)</span> would usually be preferred.
However, some scholars prefer to use a less stringent criterion, where the Type-I error rate of a testing procedure would be considered acceptable if it is within 50% of the desired <span class="math inline">\(\alpha\)</span> level.
For instance, a testing procedure with <span class="math inline">\(\alpha = .05\)</span> would be considered acceptable if its Type-I error is no more than 7.5%; with <span class="math inline">\(\alpha = .01\)</span>, it would be considered acceptable if its Type-I error is no more than 1.5%.
<!-- JEP: Is there a standard/canonical a reference for this perspective? --></p>
</div>
<div id="inference-in-the-cluster-rct-simulation" class="section level3 hasAnchor" number="9.4.4">
<h3><span class="header-section-number">9.4.4</span> Inference in the Cluster RCT Simulation<a href="performance-measures.html#inference-in-the-cluster-rct-simulation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Returning to the cluster RCT simulation, we will evaluate the validity and power of hypothesis tests for the average treatment effect based on each of the three estimation methods.
The data used in previous sections of the chapter was simulated under a process with a non-null treatment effect parameter (equal to 0.3 SDs), so the null hypothesis of zero average treatment effect does not hold.
Thus, the rejection rates for this scenario correspond to estimates of power.
We compute the rejection rate for tests with an <span class="math inline">\(\alpha\)</span> level of <span class="math inline">\(.05\)</span>:</p>
<div class="sourceCode" id="cb305"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb305-1"><a href="performance-measures.html#cb305-1" tabindex="-1"></a>runs <span class="sc">%&gt;%</span> </span>
<span id="cb305-2"><a href="performance-measures.html#cb305-2" tabindex="-1"></a>  <span class="fu">group_by</span>( method ) <span class="sc">%&gt;%</span></span>
<span id="cb305-3"><a href="performance-measures.html#cb305-3" tabindex="-1"></a>  <span class="fu">summarise</span>( <span class="at">power =</span> <span class="fu">mean</span>( p_value <span class="sc">&lt;=</span> <span class="fl">0.05</span> ) )</span></code></pre></div>
<pre><code>## # A tibble: 3 × 2
##   method power
##   &lt;chr&gt;  &lt;dbl&gt;
## 1 Agg    0.241
## 2 LR     0.32 
## 3 MLM    0.263</code></pre>
<p>For this particular scenario, none of the tests have especially high power, and the linear regression estimator apparently has higher power than the aggregation method and the multi-level model.</p>
<p>To make sense of this power pattern, we need to also consider the validity of the testing procedures.
We can do so by re-running the simulation using code we constructed in Chapter <a href="running-the-simulation-process.html#running-the-simulation-process">8</a> using the <code>simhelpers</code> package.
To evaluate the Type-I error rate of the tests, we will set the average treatment effect parameter to zero by specifying <code>ATE = 0</code>:</p>
<div class="sourceCode" id="cb307"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb307-1"><a href="performance-measures.html#cb307-1" tabindex="-1"></a><span class="fu">set.seed</span>( <span class="dv">404044</span> )</span>
<span id="cb307-2"><a href="performance-measures.html#cb307-2" tabindex="-1"></a>runs_val <span class="ot">&lt;-</span> <span class="fu">sim_cluster_RCT</span>( </span>
<span id="cb307-3"><a href="performance-measures.html#cb307-3" tabindex="-1"></a>  <span class="at">reps =</span> <span class="dv">1000</span>, </span>
<span id="cb307-4"><a href="performance-measures.html#cb307-4" tabindex="-1"></a>  <span class="at">J =</span> <span class="dv">20</span>, <span class="at">n_bar =</span> <span class="dv">30</span>, <span class="at">alpha =</span> <span class="fl">0.75</span>,</span>
<span id="cb307-5"><a href="performance-measures.html#cb307-5" tabindex="-1"></a>  <span class="at">gamma_1 =</span> <span class="dv">0</span>, <span class="at">gamma_2 =</span> <span class="fl">0.5</span>,</span>
<span id="cb307-6"><a href="performance-measures.html#cb307-6" tabindex="-1"></a>  <span class="at">sigma2_u =</span> <span class="fl">0.2</span>, <span class="at">sigma2_e =</span> <span class="fl">0.8</span></span>
<span id="cb307-7"><a href="performance-measures.html#cb307-7" tabindex="-1"></a>)</span></code></pre></div>
<p>Assessing validity involves repeating the exact same rejection rate calculations as we did for power:</p>
<div class="sourceCode" id="cb308"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb308-1"><a href="performance-measures.html#cb308-1" tabindex="-1"></a>runs_val <span class="sc">%&gt;%</span> </span>
<span id="cb308-2"><a href="performance-measures.html#cb308-2" tabindex="-1"></a>  <span class="fu">group_by</span>( estimator ) <span class="sc">%&gt;%</span></span>
<span id="cb308-3"><a href="performance-measures.html#cb308-3" tabindex="-1"></a>  <span class="fu">summarise</span>( <span class="at">power =</span> <span class="fu">mean</span>( p_value <span class="sc">&lt;=</span> <span class="fl">0.05</span> ) )</span></code></pre></div>
<pre><code>## # A tibble: 3 × 2
##   estimator power
##   &lt;chr&gt;     &lt;dbl&gt;
## 1 MLM       0.03 
## 2 OLS       0.054
## 3 agg       0.029</code></pre>
<p>The Type-I error rates of the tests for the aggregation and multi-level modeling approaches are around 5%, as desired.
The test for the linear regression estimator has Type-I error above the specified <span class="math inline">\(\alpha\)</span>-level due to the upward bias of the point estimator used in constructing the test.
The elevated rejection rate might be part of the reason that the linear regression test has higher power than the other procedures.
It is not entirely fair to compare the power of these testing procedures, because one of them has Type-I error in excess of the desired level.
<!-- JEP: These results are off for some reason?!?! --></p>
<p>As discussed above, linear regression targets the person-level average treatment effect.
In the scenario we simulated for evaluating validity, the person-level average effect is not zero because we have specified a non-zero impact heterogeneity parameter (<span class="math inline">\(\gamma_2=0.2\)</span>), meaning that the school-specific treatment effects vary around 0.
To see if this is why the linear regression test has an inflated Type-I error rate, we could re-run the simulation using settings where both the school-level and person-level average effects are truly zero.</p>
</div>
</div>
<div id="sec-relative-performance" class="section level2 hasAnchor" number="9.5">
<h2><span class="header-section-number">9.5</span> Relative or Absolute Measures?<a href="performance-measures.html#sec-relative-performance" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In considering performance measures for point estimators, we have defined the measures in terms of differences (bias, median bias) and average deviations (variance and RMSE), all of which are on the scale of the target parameter.
In contrast, for evaluating estimated standard errors we have defined measures in relative terms, calculated as <em>ratios</em> of the target quantity rather than as differences.
In the latter case, relative measures are justified because the target quantity (the true degree of uncertainty) is always positive and is usually strongly affected by design parameters of the data-generating process.
Is it ever reasonable to use relative measures for point estimators? If so, how should we decide whether to use relative or absolute measures?</p>
<p>Many published simulation studies have used relative performance measures for evaluating point estimators. For instance, studies might use relative bias or relative RMSE, defined as
<span class="math display" id="eq:relative-bias-RMSE">\[
\begin{aligned}
\text{Relative }\Bias(T) &amp;= \frac{\E(T)}{\theta}, \\
\text{Relative }\RMSE(T) &amp;= \frac{\sqrt{\E\left[\left(T - \theta\right)^2 \right]}}{\theta}.
\end{aligned}
\tag{9.21}
\]</span>
and estimated as
<span class="math display" id="eq:relative-bias-RMSE-estimators">\[
\begin{aligned}
\widehat{\text{Relative }\Bias(T)} &amp;= \frac{\bar{T}}{\theta}, \\
\widehat{\text{Relative }\RMSE(T)} &amp;= \frac{\widehat{RMSE}(T)}{\theta}.
\end{aligned}
\tag{9.22}
\]</span>
As justification for evaluating bias in relative terms, authors often appeal to <span class="citation">@hoogland1998RobustnessStudiesCovariance</span>, who suggested that relative bias of under 5% (i.e., relative bias falling between 0.95 and 1.05) could be considered acceptable for an estimation procedure.
However, <span class="citation">@hoogland1998RobustnessStudiesCovariance</span> were writing about a very specific context—robustness studies of structural equation modeling techniques—that have parameters of a particular form.
In our view, their proposed rule-of-thumb is often generalized far beyond the circumstances where it might be defensible, including to problems where it is clearly arbitrary and inappropriate.</p>
<p>A more principled approach to choosing between absolute and relative measures is to consider how the magnitude of the measure changes across different values of the target parameter <span class="math inline">\(\theta\)</span>.
If the estimand of interest is a location parameter, then shifting <span class="math inline">\(\theta\)</span> by 0.1 or by 10.1 would not usually lead to changes in the magnitude of bias, variance, or RMSE.
The relationship between bias and the target parameter might be similar to Scenario A in Figure <a href="performance-measures.html#fig:absolute-relative">9.2</a>, where bias is roughly constant across a range of different values of <span class="math inline">\(\theta\)</span>.
Focusing on relative measures in this scenario would lead to a much more complicated story because different values of <span class="math inline">\(\theta\)</span> will produce drastically different values, ranging from nearly unbiased to nearly infinite bias (for <span class="math inline">\(\theta\)</span> very close to zero).</p>
<p>Another possibility is that shifting <span class="math inline">\(\theta\)</span> by 0.1 or 10.1 will lead to proportionate changes in the magnitude of bias, variance, or RMSE.
The relationship between bias and the target parameter might be similar to Scenario B in <a href="performance-measures.html#fig:absolute-relative">9.2</a>, where bias is is roughly a constant multiple of the target parameter <span class="math inline">\(\theta\)</span>.
Focusing on relative measures in this scenario is useful because it leads to a simple story: relative bias is always around 1.12 across all values of <span class="math inline">\(\theta\)</span>, even though the raw bias varies considerably.
We would usually expect this type of pattern to occur for scale parameters.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:absolute-relative"></span>
<img src="Designing-Simulations-in-R_files/figure-html/absolute-relative-1.png" alt="Hypothetical relationships between bias and a target parameter $\theta$. In Scenario A, bias is unrelated to $\theta$ and absolute bias is a more appropriate measure. In Scenario B, bias is proportional to $\theta$ and relative bias is a more appropriate measure." width="75%" />
<p class="caption">
Figure 9.2: Hypothetical relationships between bias and a target parameter <span class="math inline">\(\theta\)</span>. In Scenario A, bias is unrelated to <span class="math inline">\(\theta\)</span> and absolute bias is a more appropriate measure. In Scenario B, bias is proportional to <span class="math inline">\(\theta\)</span> and relative bias is a more appropriate measure.
</p>
</div>
<p>How do we know which of these scenarios is a better match for a particular problem?
For some estimators and data-generating processes, it may be possible to analyze a problem with statistical theory and examine the how bias or variance would be expected to change as a function of <span class="math inline">\(\theta\)</span>.
However, many problems are too complex to be tractable.
Another, much more feasible route is to evaluate performance for <em>multiple</em> values of the target parameter.
As done in many simulation studies, we can simulate sampling distributions and calculate performance measures (in raw terms) for several different values of a parameter, selected so that we can distinguish between constant and multiplicative relationships.
Then, in analyzing the simulation results, we can generate graphs such as those in Figure <a href="performance-measures.html#fig:absolute-relative">9.2</a> to understand how performance changes as a function of the target parameter.
If the absolute bias is roughly the same for all values of <span class="math inline">\(\theta\)</span> (as in Scenario A), then it makes sense to report absolute bias as the summary performance criterion.
On the other hand, if the bias grows roughly in proportion to <span class="math inline">\(\theta\)</span> (as in Scenario B), then relative bias might be a better summary criterion.</p>
<div id="performance-relative-to-a-benchmark-estimator" class="section level3 hasAnchor" number="9.5.1">
<h3><span class="header-section-number">9.5.1</span> Performance relative to a benchmark estimator<a href="performance-measures.html#performance-relative-to-a-benchmark-estimator" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Another way to define performance measures in relative terms to by taking the ratio of the performance measure for one estimator over the performance measure for a benchmark estimator.
We have already demonstrated this approach in calculating performance measures for the cluster RCT example (Section <a href="performance-measures.html#clusterRCTperformance">9.1.1</a>), where we used the linear regression estimator as the benchmark against which to compare the other estimators.
This approach is natural in simulations that involve comparing the performance of multiple estimators and where one of the estimators could be considered the current standard or conventional method.</p>
<p>Comparing the performance of one estimator relative to another can be especially useful when examining measures whose magnitude varies drastically across design parameters.
For most statistical methods, we would usually expect precision and accuracy to improve (variance and RMSE to decrease) as sample size increases.
Comparing estimators in terms of <em>relative</em> precision or <em>relative</em> accuracy may make it easier to identify consistent patterns in the simulation results.
For instance, this approach might allow us to summarize findings by saying that “the aggregation estimator has standard errors that are consistently 6-10% smaller than the standard errors of the linear regression estimator.”
This is much easier to interpret than saying that “aggregation has standard errors that are around 0.01 smaller than linear regression, on average.”
In the latter case, it is very difficult to determine whether a difference of 0.01 is large or small, and focusing on an average difference conceals relevant variation across scenarios involving different sample sizes.</p>
<p>Comparing performance relative to a benchmark method can be an effective tool, but it also has potential drawbacks.
Because these relative performance measures are inherently comparative, higher or lower ratios could either be due to the behavior of the method of interest (the numerator) or due to the behavior of the benchmark method (the denominator).
Ratio comparisons are also less effective for performance measures that are on a constrained scale, such as power.
If we have a power of 0.05, and we improve it to 0.10, we have doubled our power, but if it is 0.10 and we increase to 0.15, we have only increased by 50%.
Ratios can also be very deceiving when the denominator quantity is near zero or when it can take on either negative or positive values; this can be a problem when examining bias relative to a benchmark estimator.
Because of these drawbacks, it is prudent to compute and examine performance measures in absolute terms in addition to examining relative comparisons between methods.</p>
</div>
</div>
<div id="implicit-estimands" class="section level2 hasAnchor" number="9.6">
<h2><span class="header-section-number">9.6</span> Estimands Not Represented By a Parameter<a href="performance-measures.html#implicit-estimands" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In our Cluster RCT example, we focused on the estimand of the school-level ATE, represented by the model parameter <span class="math inline">\(\gamma_1\)</span>.
What if we were instead interested in the person-level average effect?
This estimand does not correspond to any input parameter in our data generating process.
Instead, it is defined <em>implicitly</em> by a combination of other parameters.
In order to compute performance characteristics such as bias and RMSE, we would need to calculate the parameter based on the inputs of the data-generating processes.
There are at least three possible ways to accomplish this.</p>
<p>One way is to use mathematical distribution theory to compute an implied parameter.
Our target parameter will be some function of the parameters and random variables in the data-generating process, and it may be possible to evaluate that function algebraically or numerically (i.e., using numerical integration functions such as <code>integrate()</code>).
This can be a very worthwhile exercise if it provides insights into the relationship between the target parameter and the inputs of the data-generating process.
However, this approach requires knowledge of distribution theory, and it can get quite complicated and technical.<a href="#fn15" class="footnote-ref" id="fnref15"><sup>15</sup></a>
Other approaches are often feasible and more closely aligned with the tools and techniques of Monte Carlo simulation.</p>
<p>An alternative approach is to simply generate a massive dataset—so large that it can stand in for the entire data-generating model—and then simply calculate the target parameter of interest in this massive dataset. In the cluster-RCT example, we can apply this strategy by generating data from a very large number of clusters and then simply calculating the true person-average effect across all generated clusters.
If the dataset is big enough, then the uncertainty in this estimate will be negligible compared to the uncertainty in our simulation.</p>
<p>We implement this approach as follows, generating a dataset with 100,000 clusters:</p>
<div class="sourceCode" id="cb310"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb310-1"><a href="performance-measures.html#cb310-1" tabindex="-1"></a>dat <span class="ot">&lt;-</span> <span class="fu">gen_cluster_RCT</span>( </span>
<span id="cb310-2"><a href="performance-measures.html#cb310-2" tabindex="-1"></a>  <span class="at">n_bar =</span> <span class="dv">30</span>, <span class="at">J =</span> <span class="dv">100000</span>, </span>
<span id="cb310-3"><a href="performance-measures.html#cb310-3" tabindex="-1"></a>  <span class="at">gamma_1 =</span> <span class="fl">0.3</span>, <span class="at">gamma_2 =</span> <span class="fl">0.5</span>,</span>
<span id="cb310-4"><a href="performance-measures.html#cb310-4" tabindex="-1"></a>  <span class="at">sigma2_u =</span> <span class="fl">0.20</span>, <span class="at">sigma2_e =</span> <span class="fl">0.80</span>,</span>
<span id="cb310-5"><a href="performance-measures.html#cb310-5" tabindex="-1"></a>  <span class="at">alpha =</span> <span class="fl">0.75</span>  </span>
<span id="cb310-6"><a href="performance-measures.html#cb310-6" tabindex="-1"></a>)</span>
<span id="cb310-7"><a href="performance-measures.html#cb310-7" tabindex="-1"></a>ATE_person <span class="ot">&lt;-</span> <span class="fu">mean</span>( dat<span class="sc">$</span>Yobs[dat<span class="sc">$</span>Z<span class="sc">==</span><span class="dv">1</span>] ) <span class="sc">-</span> <span class="fu">mean</span>( dat<span class="sc">$</span>Yobs[dat<span class="sc">$</span>Z<span class="sc">==</span><span class="dv">0</span>] )</span>
<span id="cb310-8"><a href="performance-measures.html#cb310-8" tabindex="-1"></a>ATE_person</span></code></pre></div>
<pre><code>## [1] 0.3919907</code></pre>
<p>The extremely precise estimate of the person-average effect is 0.39, which is consistent with what we would expect given the bias we saw earlier for the linear model.</p>
<p>If we recalculate performance measures for all of our estimators with respect to the <code>ATE_person</code> estimand, the bias and RMSE of our estimators will shift but the standard errors will stay the same as in previous performance calculations using the school-level average effect:</p>
<div class="sourceCode" id="cb312"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb312-1"><a href="performance-measures.html#cb312-1" tabindex="-1"></a>performance_person_ATE <span class="ot">&lt;-</span></span>
<span id="cb312-2"><a href="performance-measures.html#cb312-2" tabindex="-1"></a>  runs <span class="sc">%&gt;%</span> </span>
<span id="cb312-3"><a href="performance-measures.html#cb312-3" tabindex="-1"></a>  <span class="fu">group_by</span>( method ) <span class="sc">%&gt;%</span></span>
<span id="cb312-4"><a href="performance-measures.html#cb312-4" tabindex="-1"></a>  <span class="fu">summarise</span>( </span>
<span id="cb312-5"><a href="performance-measures.html#cb312-5" tabindex="-1"></a>    <span class="at">bias =</span> <span class="fu">mean</span>( ATE_hat ) <span class="sc">-</span> ATE_person,</span>
<span id="cb312-6"><a href="performance-measures.html#cb312-6" tabindex="-1"></a>    <span class="at">SE =</span> <span class="fu">sd</span>( ATE_hat ),</span>
<span id="cb312-7"><a href="performance-measures.html#cb312-7" tabindex="-1"></a>    <span class="at">RMSE =</span> <span class="fu">sqrt</span>( <span class="fu">mean</span>( (ATE_hat <span class="sc">-</span> ATE_person)<span class="sc">^</span><span class="dv">2</span> ) )</span>
<span id="cb312-8"><a href="performance-measures.html#cb312-8" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb312-9"><a href="performance-measures.html#cb312-9" tabindex="-1"></a>  <span class="fu">mutate</span>( <span class="at">per_RMSE =</span> RMSE <span class="sc">/</span> RMSE[method<span class="sc">==</span><span class="st">&quot;LR&quot;</span>] )</span>
<span id="cb312-10"><a href="performance-measures.html#cb312-10" tabindex="-1"></a></span>
<span id="cb312-11"><a href="performance-measures.html#cb312-11" tabindex="-1"></a>performance_person_ATE</span></code></pre></div>
<pre><code>## # A tibble: 3 × 5
##   method     bias    SE  RMSE per_RMSE
##   &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;
## 1 Agg    -0.0922  0.214 0.233     1.04
## 2 LR     -0.00963 0.224 0.224     1   
## 3 MLM    -0.0798  0.213 0.227     1.02</code></pre>
<p>For the person-weighted estimand, the aggregation estimator and multilevel model are biased but the linear regression estimator is unbiased.
However, the aggregation estimator and multilevel model estimator still have smaller standard errors than the linear regression estimator.
RMSE now captures the trade-off between bias and reduced variance.
Overall, aggregation and multilevel modeling have RMSE that is around 3% larger than linear regression.</p>
<p>A further approach for calculating <code>ATE_person</code> would be to record the true person average effect of the dataset with each simulation iteration, and then average the sample-specific parameters at the end.
The overall average of the dataset-specific <code>ATE_person</code> parameters corresponds to the population person-level ATE.
This approach is equivalent to generating a single massive dataset—we just generate it piece by piece.</p>
<p>To implement this approach, we would need to modify the data-generating function <code>gen_cluster_RCT()</code> to track the additional information.
For instance, we might calculate</p>
<div class="sourceCode" id="cb314"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb314-1"><a href="performance-measures.html#cb314-1" tabindex="-1"></a>tx_effect <span class="ot">&lt;-</span> gamma_1 <span class="sc">+</span> gamma_2 <span class="sc">*</span> ( nj <span class="sc">-</span> n_bar ) <span class="sc">/</span> n_bar</span>
<span id="cb314-2"><a href="performance-measures.html#cb314-2" tabindex="-1"></a>beta_0j <span class="ot">&lt;-</span> gamma_0 <span class="sc">+</span> Zj <span class="sc">*</span> tx_effect <span class="sc">+</span> u0j</span></code></pre></div>
<p>and then include <code>tx_effect</code> along with <code>Yobs</code> and <code>Z</code> as a column in our dataset.
This approach is quite similar to directly calculating <em>potential outcomes</em>, as discussed in Chapter <a href="potential-outcomes.html#potential-outcomes">21</a>.</p>
<p>After modifying the data-generating function, we will also need to modify the analysis function(s) to record the sample-specific treatment effect parameter.
We might have, for example:</p>
<div class="sourceCode" id="cb315"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb315-1"><a href="performance-measures.html#cb315-1" tabindex="-1"></a>analyze_data <span class="ot">=</span> <span class="cf">function</span>( dat ) {</span>
<span id="cb315-2"><a href="performance-measures.html#cb315-2" tabindex="-1"></a>  MLM <span class="ot">&lt;-</span> <span class="fu">analysis_MLM</span>( dat )</span>
<span id="cb315-3"><a href="performance-measures.html#cb315-3" tabindex="-1"></a>  LR <span class="ot">&lt;-</span> <span class="fu">analysis_OLS</span>( dat )</span>
<span id="cb315-4"><a href="performance-measures.html#cb315-4" tabindex="-1"></a>  Agg <span class="ot">&lt;-</span> <span class="fu">analysis_agg</span>( dat )</span>
<span id="cb315-5"><a href="performance-measures.html#cb315-5" tabindex="-1"></a>  res <span class="ot">&lt;-</span> <span class="fu">bind_rows</span>( </span>
<span id="cb315-6"><a href="performance-measures.html#cb315-6" tabindex="-1"></a>    <span class="at">MLM =</span> MLM, <span class="at">LR =</span> LR, <span class="at">Agg =</span> Agg,</span>
<span id="cb315-7"><a href="performance-measures.html#cb315-7" tabindex="-1"></a>    <span class="at">.id =</span> <span class="st">&quot;method&quot;</span> </span>
<span id="cb315-8"><a href="performance-measures.html#cb315-8" tabindex="-1"></a>  )</span>
<span id="cb315-9"><a href="performance-measures.html#cb315-9" tabindex="-1"></a>  res<span class="sc">$</span>ATE_person <span class="ot">&lt;-</span> <span class="fu">mean</span>( dat<span class="sc">$</span>tx_effect )</span>
<span id="cb315-10"><a href="performance-measures.html#cb315-10" tabindex="-1"></a>  <span class="fu">return</span>( res )</span>
<span id="cb315-11"><a href="performance-measures.html#cb315-11" tabindex="-1"></a>}</span></code></pre></div>
<p>Now when we run our simulation, we will have a column corresponding to the true person-level average treatment effect for each dataset.
We could then take the average of these value across replications to estimate the true person average treatment effect in the population, and then use this as the target parameter for performance calculations.</p>
<p>An estimand not represented by any single input parameter is more difficult to work with than one that corresponds directly to an input parameter.
Still, it is feasible to examine such estimands with a bit of forethought and careful programming.
The key is to be clear about what you are trying to estimate because the performance of an estimator depends critically on the estimand against which it is compared.</p>
</div>
<div id="MCSE" class="section level2 hasAnchor" number="9.7">
<h2><span class="header-section-number">9.7</span> Uncertainty in Performance Estimates (the Monte Carlo Standard Error)<a href="performance-measures.html#MCSE" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The performance measures we have described are all defined with respect to the sampling distribution of an estimator, or its distribution across an infinite number of replications of the data-generating process.
Of course, simulations will only involve a finite set of replications, based on which we calculate <em>estimates</em> of the performance measures.
These estimates involve some Monte Carlo error because they are based on a limited number of replications.
It is important to understand the extent of Monte Carlo error when interpreting simulation results, so we need methods for asssessing this source of uncertainty.</p>
<p>To account for Monte Carlo error, we can think of our simulation results as a sample from a population.
Each replication is an independent and identically distributed draw from the population of the sampling distribution.
Once we frame the problem in these terms, standard statistical techniques for independent and identically distributed random variables can be applied to calculate standard errors.
We call these standard errors Monte Carlo Simulation Errors, or MCSEs.
For most of the performance measures, closed-form expressions are available for calculating MCSEs.
For a few of the measures, we can apply techniques such as the jackknife to calculate reasonable approximations for MCSEs.</p>
<div id="conventional-measures-for-point-estimators" class="section level3 hasAnchor" number="9.7.1">
<h3><span class="header-section-number">9.7.1</span> Conventional measures for point estimators<a href="performance-measures.html#conventional-measures-for-point-estimators" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>For the measures that we have described for evaluating point estimators, Monte Carlo standard errors can be calculated using conventional formulas.<a href="#fn16" class="footnote-ref" id="fnref16"><sup>16</sup></a>
Recall that we have a point estimator <span class="math inline">\(T\)</span> of a target parameter <span class="math inline">\(\theta\)</span>, and we calculate the mean of the estimator <span class="math inline">\(\bar{T}\)</span> and its sample standard deviation <span class="math inline">\(S_T\)</span> across <span class="math inline">\(R\)</span> replications of the simulation process.
In addition, we will need to calculate the standardized skewness and kurtosis of <span class="math inline">\(T\)</span> as</p>
<p><span class="math display" id="eq:skewness-kurtosis">\[
\begin{aligned}
\text{Skewness (standardized):} &amp;  &amp;g_T &amp;= \frac{1}{R S_T^3}\sum_{r=1}^R \left(T_r - \bar{T}\right)^3 \\
\text{Kurtosis (standardized):} &amp;  &amp;k_T &amp;= \frac{1}{R S_T^4} \sum_{r=1}^R \left(T_r - \bar{T}\right)^4.
\end{aligned}
\tag{9.23}
\]</span></p>
<p>The bias of <span class="math inline">\(T\)</span> is estimated as <span class="math inline">\(\bar{T} - \theta\)</span>, so the MCSE for bias is equal the MCSE of <span class="math inline">\(\bar{T}\)</span>. It can be estimated as
<span class="math display" id="eq:MCSE-bias">\[
MCSE\left(\widehat{\Bias}(T)\right) = \sqrt{\frac{S_T^2}{R}}.
\tag{9.24}
\]</span>
The sampling variance of <span class="math inline">\(T\)</span> is estimated as <span class="math inline">\(S_T^2\)</span>, with MCSE of
<span class="math display" id="eq:MCSE-var">\[
MCSE\left(\widehat{\Var}(T)\right) = S_T^2 \sqrt{\frac{k_T - 1}{R}}.
\tag{9.25}
\]</span>
The empirical standard error (the square root of the sampling variance) is estimated as <span class="math inline">\(S_T\)</span>. Using a delta method approximation<a href="#fn17" class="footnote-ref" id="fnref17"><sup>17</sup></a>, the MCSE of <span class="math inline">\(S_T\)</span> is
<span class="math display" id="eq:MCSE-SE">\[
MCSE\left(S_T\right) = \frac{S_T}{2}\sqrt{\frac{k_T - 1}{R}}.
\tag{9.26}
\]</span></p>
<p>We estimate RMSE using Equation <a href="performance-measures.html#eq:rmse-estimator">(9.5)</a>, which can also be written as
<span class="math display">\[
\widehat{\RMSE}(T) = \sqrt{(\bar{T} - \theta)^2 + \frac{R - 1}{R} S_T^2}.
\]</span>
An MCSE for the estimated mean squared error (the square of RMSE) is
<span class="math display" id="eq:MCSE-MSE">\[
MCSE( \widehat{MSE} ) = \sqrt{\frac{1}{R}\left[S_T^4 (k_T - 1) + 4 S_T^3 g_T\left(\bar{T} - \theta\right) + 4 S_T^2 \left(\bar{T} - \theta\right)^2\right]}.
\tag{9.27}
\]</span>
Again following a delta method approximation, a MCSE for the RMSE is
<span class="math display" id="eq:MCSE-RMSE">\[
MCSE( \widehat{RMSE} ) = \frac{\sqrt{\frac{1}{R}\left[S_T^4 (k_T - 1) + 4 S_T^3 g_T\left(\bar{T} - \theta\right) + 4 S_T^2 \left(\bar{T} - \theta\right)^2\right]}}{2 \times \widehat{RMSE}}.
\tag{9.28}
\]</span></p>
<p>Section <a href="performance-measures.html#sec-relative-performance">9.5</a> discussed circumstances where we might prefer to calculate performance measures in relative rather than absolute terms.
For measures that are calculated by dividing a raw measure by the target parameter, the MCSE for the relative measure is simply the MCSE for the raw measure divided by the target parameter.
For instance, the MCSE of relative bias <span class="math inline">\(\bar{T} / \theta\)</span> is
<span class="math display" id="eq:MCSE-relative-bias">\[
MCSE\left( \frac{\bar{T}}{\theta} \right) = \frac{1}{\theta} MCSE(\bar{T}) = \frac{S_T}{\theta \sqrt{R}}.
\tag{9.29}
\]</span>
MCSEs for relative variance and relative RMSE follow similarly.</p>
</div>
<div id="less-conventional-measures-for-point-estimators" class="section level3 hasAnchor" number="9.7.2">
<h3><span class="header-section-number">9.7.2</span> Less conventional measures for point estimators<a href="performance-measures.html#less-conventional-measures-for-point-estimators" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In Section <a href="performance-measures.html#less-conventional-measures">9.1.2</a> we described several alternative performance measures for evaluating point estimators, which are less commonly used but are more robust to outliers compared to measures such as bias and variance.
MCSEs for these less conventional measures can be obtained using results from the theory of robust statistics <span class="citation">[@Hettmansperger2010robust; @Maronna2006robust]</span>.</p>
<p><span class="citation">@McKean1984comparison</span> proposed a standard error estimator for the sample median from a continuous but not necessarily normal distribution, derived from a non-parametric confidence interval for the sample median.
We use their approach to compute a MCSE for <span class="math inline">\(M_T\)</span>, the sample median of <span class="math inline">\(T\)</span>.
Let <span class="math inline">\(c = \left\lceil(R + 1) / 2 - 1.96 \times \sqrt{R/4}\right\rfloor\)</span>, where the inner expression is rounded to the nearest integer.
Then
<span class="math display" id="eq:MCSE-median">\[
MCSE\left(M_T\right) = \frac{T_{(R + 1 - c)} - T_{(c)}}{2 \times 1.96}.
\tag{9.30}
\]</span>
A Monte Carlo standard error for the median absolute deviation can be computed following the same approach, but substituting the order statistics of <span class="math inline">\(E_r = | T_r - \theta|\)</span> in place of those for <span class="math inline">\(T_r\)</span>.</p>
<p>Trimmed mean bias with trimming proportion <span class="math inline">\(p\)</span> is calculated by taking the mean of the the middle <span class="math inline">\((1 - 2p) \times R\)</span> observations, which we have denoted as <span class="math inline">\(\tilde{T}_{\{p\}}\)</span>. A MCSE for the trimmed mean (and for the trimmed mean bias) is
<span class="math display" id="eq:MCSE-trimmed-mean">\[
MCSE\left(\tilde{T}_{\{p\}}\right) = \sqrt{\frac{U_p}{R}},
\tag{9.31}
\]</span>
where
<span class="math display">\[
U_p = \frac{1}{(1 - 2p)R}\left( pR\left(T_{(pR)}  - \tilde{T}_{\{p\}}\right)^2 + pR\left(T_{((1-p)R + 1)}  - \tilde{T}_{\{p\}}\right)^2 + \sum_{r=pR + 1}^{(1 - p)R} \left(T_{(r)} - \tilde{T}_{\{p\}}\right)^2 \right)
\]</span>
<span class="citation">[@Maronna2006robust, Eq. 2.85]</span>.</p>
<p>Performance measures based on winsorization include winsorized bias, winsorized standard error, and winsorized RMSE.
MCSEs for these measures can be computed using the same formuals as for the conventional measures of bias, empirical standard error, and RMSE, but using sample moments of <span class="math inline">\(\hat{X}_r\)</span> in place of the sample moments of <span class="math inline">\(T_r\)</span>.</p>
</div>
<div id="MCSE-for-relative-variance" class="section level3 hasAnchor" number="9.7.3">
<h3><span class="header-section-number">9.7.3</span> MCSE for Relative Variance Estimators<a href="performance-measures.html#MCSE-for-relative-variance" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Estimating the MCSE of relative performance measures for variance estimators is complicated by the appearance of an estimated quantity in the denominator of the ratio.
For instance, the relative bias of <span class="math inline">\(V\)</span> is estimates as the ratio <span class="math inline">\(\bar{V} / S_T^2\)</span>, and both the numerator and denominator are estimated quantities that will include some Monte Carlo error.
To properly account for the Monte Carlo uncertainty of the ratio, one possibility is to use formulas for the standard errors of ratio estimators.
Alternately, we can use general uncertainty approximation techniques such as the jackknife or bootstrap <span class="citation">[@boos2015Assessing]</span>.
The jackknife involves calculating a statistic of interest repeatedly, each time excluding one observation from the calculation.
The variance of this set of one-left-out statistics then serves as a reasonable approximation to the actual sampling variance of the statistic calculated from the full sample.</p>
<p>To apply the jackknife to assess MCSEs of relative bias or relative RMSE of a variance estimator, we will need to compute several statistics repeatedly.
Let <span class="math inline">\(\bar{V}_{(j)}\)</span> and <span class="math inline">\(S_{T(j)}^2\)</span> be the average variance estimate and the empirical variance estimate calculated from the set of replicates <strong><em>that excludes replicate <span class="math inline">\(j\)</span></em></strong>, for <span class="math inline">\(j = 1,...,R\)</span>.
The relative bias estimate, excluding replicate <span class="math inline">\(j\)</span> would then be <span class="math inline">\(\bar{V}_{(j)} / S_{T(j)}^2\)</span>.
Calculating all <span class="math inline">\(R\)</span> versions of this relative bias estimate and taking the variance of these <span class="math inline">\(R\)</span> versions yields a jackknife MCSE:
<span class="math display" id="eq:MCSE-relative-bias-V">\[
MCSE\left( \frac{ \bar{V}}{S_T^2} \right) = \sqrt{\frac{1}{R} \sum_{j=1}^R \left(\frac{\bar{V}_{(j)}}{S_{T(j)}^2} - \frac{\bar{V}}{S_T^2}\right)^2}.
\tag{9.32}
\]</span>
Similarly, a MCSE for the relative standard error of <span class="math inline">\(V\)</span> is
<span class="math display" id="eq:MCSE-relative-var-V">\[
MCSE\left( \frac{ S_V}{S_T^2} \right) = \sqrt{\frac{1}{R} \sum_{j=1}^R \left(\frac{S_{V(j)}}{S_{T(j)}^2} - \frac{S_V}{S_T^2}\right)^2},
\tag{9.33}
\]</span>
where <span class="math inline">\(S_{V(j)}\)</span> is the sample variance of <span class="math inline">\(V_1,...,V_R\)</span>, omitting replicate <span class="math inline">\(j\)</span>.
To compute a MCSE for the relative RMSE of <span class="math inline">\(V\)</span>, we will need to compute the performance measure after omitting each observation in turn.
Letting
<span class="math display">\[
RRMSE_{V} = \frac{1}{S_{T}^2}\sqrt{(\bar{V} - S_{T}^2)^2 + \frac{R - 1}{R} S_{V}^2}
\]</span>
and
<span class="math display">\[
RRMSE_{V(j)} = \frac{1}{S_{T(j)}^2}\sqrt{(\bar{V}_{(j)} - S_{T(j)}^2)^2 + \frac{R - 1}{R} S_{V(j)}^2},
\]</span>
a jackknife MCSE for the estimated relative RMSE of <span class="math inline">\(V\)</span> is
<span class="math display" id="eq:MCSE-relative-rmse-V">\[
MCSE\left( RRMSE_{V} \right) = \sqrt{\frac{1}{R} \sum_{j=1}^R \left(RRMSE_{V(j)} - RRMSE_{V}\right)^2}.
\tag{9.34}
\]</span></p>
<p>Jackknife calculation would be cumbersome if we did it by brute force. However, a few algebra tricks provide a much quicker way. The tricks come from observing that
<span class="math display">\[
\begin{aligned}
\bar{V}_{(j)} &amp;= \frac{1}{R - 1}\left(R \bar{V} - V_j\right) \\
S_{V(j)}^2 &amp;= \frac{1}{R - 2} \left[(R - 1) S_V^2 - \frac{R}{R - 1}\left(V_j - \bar{V}\right)^2\right] \\
S_{T(j)}^2 &amp;= \frac{1}{R - 2} \left[(R - 1) S_T^2 - \frac{R}{R - 1}\left(T_j - \bar{T}\right)^2\right]
\end{aligned}
\]</span>
These formulas can be used to avoid re-computing the mean and sample variance from every subsample.
Instead, all we need to do is calculate the overall mean and overall variance, and then do a small adjustment with each jackknife iteration.</p>
<p>Jackknife methods are useful for approximating MCSEs of other performance measures beyond just those for variance estimators.
For instance, the jackknife is a convenient alternative for computing the MCSE of the empirical standard error or (raw) RMSE of a point estimator, which avoids the need to compute skewness or kurtosis.
However, <span class="citation">@boos2015Assessing</span> notes that the jackknife does not work for performance measures involving medians, although bootstrapping remains valid.</p>
</div>
<div id="mcse-for-confidence-intervals-and-hypothesis-tests" class="section level3 hasAnchor" number="9.7.4">
<h3><span class="header-section-number">9.7.4</span> MCSE for Confidence Intervals and Hypothesis Tests<a href="performance-measures.html#mcse-for-confidence-intervals-and-hypothesis-tests" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Performance measures for confidence intervals and hypothesis tests are simple compared to those we have described for point and variance estimators.
For evaluating hypothesis tests, the main measure is the rejection rate of the test, which is a proportion estimated as <span class="math inline">\(r_\alpha\)</span> (Equation <a href="performance-measures.html#eq:rejection-rate-estimate">(9.20)</a>).
A MCSE for the estimated rejection rate is
<span class="math display" id="eq:MCSE-rejection-rate">\[
MCSE(r_\alpha) = \sqrt{\frac{r_\alpha ( 1 - r_\alpha)}{R}}.
\tag{9.35}
\]</span>
This MCSE uses the estimated rejection rate to approximate its Monte Carlo error.
When evaluating the validity of a test, we may expect the rejection rate to be fairly close to the nominal <span class="math inline">\(\alpha\)</span> level, in which case we could compute a MCSE using <span class="math inline">\(\alpha\)</span> in place of <span class="math inline">\(r_\alpha\)</span>, taking <span class="math inline">\(\sqrt{\alpha(1 - \alpha) / R}\)</span>.
When evaluating power, we will not usually know the neighborhood of the rejection rate in advance of the simulation.
However, a conservative upper bound on the MCSE can be derived by observing that MCSE is maximized when <span class="math inline">\(\rho_\alpha = \frac{1}{2}\)</span>, and so
<span class="math display">\[
MCSE(r_\alpha) \leq \sqrt{\frac{1}{4 R}}.
\]</span></p>
<p>When evaluating confidence interval performance, we focus on coverage rates and expected widths.
MCSEs for the estimated coverage rate work similarly to those for rejection rates.
If the coverage rate is expected to be in the neighborhood of the intended coverage level <span class="math inline">\(\beta\)</span>, then we can approximate the MCSE as
<span class="math display" id="eq:MCSE-coverage">\[
MCSE(\widehat{\text{Coverage}}(A,B)) = \sqrt{\frac{\beta(1 - \beta)}{R}}.
\tag{9.36}
\]</span>
Alternately, Equation <a href="performance-measures.html#eq:MCSE-coverage">(9.36)</a> could be computed using the estimated coverage rate <span class="math inline">\(\widehat{\text{Coverage}}(A,B)\)</span> in place of <span class="math inline">\(\beta\)</span>.</p>
<p>Finally, the expected confidence interval width can be estimated as <span class="math inline">\(\bar{W}\)</span>, with MCSE
<span class="math display" id="eq:MCSE-width">\[
MCSE(\bar{W}) = \sqrt{\frac{S_W^2}{R}},
\tag{9.37}
\]</span>
where <span class="math inline">\(S_W^2\)</span> is the sample variance of <span class="math inline">\(W_1,...,W_R\)</span>, the widths of the confidence interval from each replication.</p>
</div>
<div id="calculating-mcses-with-the-simhelpers-package" class="section level3 hasAnchor" number="9.7.5">
<h3><span class="header-section-number">9.7.5</span> Calculating MCSEs With the <code>simhelpers</code> Package<a href="performance-measures.html#calculating-mcses-with-the-simhelpers-package" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The <code>simhelpers</code> package provides several functions for calculating most of the performance measures that we have reviewed, along with MCSEs for each performance measures.
The functions are easy to use.
Consider this set of simulation runs on the Welch dataset:</p>
<div class="sourceCode" id="cb316"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb316-1"><a href="performance-measures.html#cb316-1" tabindex="-1"></a><span class="fu">library</span>( simhelpers )</span>
<span id="cb316-2"><a href="performance-measures.html#cb316-2" tabindex="-1"></a><span class="fu">data</span>( welch_res )</span>
<span id="cb316-3"><a href="performance-measures.html#cb316-3" tabindex="-1"></a></span>
<span id="cb316-4"><a href="performance-measures.html#cb316-4" tabindex="-1"></a>welch <span class="ot">&lt;-</span> </span>
<span id="cb316-5"><a href="performance-measures.html#cb316-5" tabindex="-1"></a>  welch_res <span class="sc">%&gt;%</span></span>
<span id="cb316-6"><a href="performance-measures.html#cb316-6" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">select</span>(<span class="sc">-</span>seed, <span class="sc">-</span>iterations ) <span class="sc">%&gt;%</span></span>
<span id="cb316-7"><a href="performance-measures.html#cb316-7" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">method =</span> <span class="fu">case_match</span>(method, <span class="st">&quot;Welch t-test&quot;</span> <span class="sc">~</span> <span class="st">&quot;Welch&quot;</span>, <span class="at">.default =</span> method))</span>
<span id="cb316-8"><a href="performance-measures.html#cb316-8" tabindex="-1"></a></span>
<span id="cb316-9"><a href="performance-measures.html#cb316-9" tabindex="-1"></a><span class="fu">head</span>(welch)</span></code></pre></div>
<pre><code>## # A tibble: 6 × 9
##      n1    n2 mean_diff method      est    var
##   &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt;
## 1    50    50         0 t-test  0.0258  0.0954
## 2    50    50         0 Welch   0.0258  0.0954
## 3    50    50         0 t-test  0.00516 0.0848
## 4    50    50         0 Welch   0.00516 0.0848
## 5    50    50         0 t-test -0.0798  0.0818
## 6    50    50         0 Welch  -0.0798  0.0818
## # ℹ 3 more variables: p_val &lt;dbl&gt;,
## #   lower_bound &lt;dbl&gt;, upper_bound &lt;dbl&gt;</code></pre>
<p>We can calculate performance measures across all the range of scenarios.
Here is the rejection rate for the traditional <span class="math inline">\(t\)</span>-test based on the subset of simulation results with sample sizes of <span class="math inline">\(n_1 = n_2 = 50\)</span> and a mean difference of 0, using <span class="math inline">\(\alpha\)</span> levels of .01 and .05:</p>
<div class="sourceCode" id="cb318"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb318-1"><a href="performance-measures.html#cb318-1" tabindex="-1"></a>welch_sub <span class="ot">&lt;-</span> <span class="fu">filter</span>(welch, method <span class="sc">==</span> <span class="st">&quot;t-test&quot;</span>, n1 <span class="sc">==</span> <span class="dv">50</span>, n2 <span class="sc">==</span> <span class="dv">50</span>, mean_diff <span class="sc">==</span> <span class="dv">0</span> )</span>
<span id="cb318-2"><a href="performance-measures.html#cb318-2" tabindex="-1"></a></span>
<span id="cb318-3"><a href="performance-measures.html#cb318-3" tabindex="-1"></a><span class="fu">calc_rejection</span>(welch_sub, <span class="at">p_values =</span> p_val, <span class="at">alpha =</span> <span class="fu">c</span>(.<span class="dv">01</span>, .<span class="dv">05</span>))</span></code></pre></div>
<pre><code>##   K_rejection rej_rate_01 rej_rate_05
## 1        1000       0.009       0.048
##   rej_rate_mcse_01 rej_rate_mcse_05
## 1      0.002986469      0.006759882</code></pre>
<p>The column labeled <code>K_rejection</code> reports the number of replications used to calculate the performance measures.</p>
<p>Here is the coverage rate calculated for the same condition:</p>
<div class="sourceCode" id="cb320"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb320-1"><a href="performance-measures.html#cb320-1" tabindex="-1"></a><span class="fu">calc_coverage</span>(</span>
<span id="cb320-2"><a href="performance-measures.html#cb320-2" tabindex="-1"></a>  welch_sub, </span>
<span id="cb320-3"><a href="performance-measures.html#cb320-3" tabindex="-1"></a>  <span class="at">lower_bound =</span> lower_bound, <span class="at">upper_bound =</span> upper_bound, </span>
<span id="cb320-4"><a href="performance-measures.html#cb320-4" tabindex="-1"></a>  <span class="at">true_param =</span> mean_diff</span>
<span id="cb320-5"><a href="performance-measures.html#cb320-5" tabindex="-1"></a>)</span></code></pre></div>
<pre><code>## # A tibble: 1 × 5
##   K_coverage coverage coverage_mcse width
##        &lt;int&gt;    &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;
## 1       1000    0.952       0.00676  1.25
## # ℹ 1 more variable: width_mcse &lt;dbl&gt;</code></pre>
<p>The performance functions are designed to be used within a <code>tidyverse</code>-style workflow, including on grouped datasets. For instance, we can calculate rejection rates for every distinct scenario examined in the simulation:</p>
<div class="sourceCode" id="cb322"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb322-1"><a href="performance-measures.html#cb322-1" tabindex="-1"></a>all_rejection_rates <span class="ot">&lt;-</span> </span>
<span id="cb322-2"><a href="performance-measures.html#cb322-2" tabindex="-1"></a>  welch <span class="sc">%&gt;%</span> </span>
<span id="cb322-3"><a href="performance-measures.html#cb322-3" tabindex="-1"></a>  <span class="fu">group_by</span>( n1, n2, mean_diff, method ) <span class="sc">%&gt;%</span></span>
<span id="cb322-4"><a href="performance-measures.html#cb322-4" tabindex="-1"></a>  <span class="fu">summarise</span>( </span>
<span id="cb322-5"><a href="performance-measures.html#cb322-5" tabindex="-1"></a>    <span class="fu">calc_rejection</span>( <span class="at">p_values =</span> p_val, <span class="at">alpha =</span> <span class="fu">c</span>(.<span class="dv">01</span>, .<span class="dv">05</span>) ) </span>
<span id="cb322-6"><a href="performance-measures.html#cb322-6" tabindex="-1"></a>  )</span></code></pre></div>
<p>The resulting summaries are reported in table <a href="performance-measures.html#tab:Welch-rejection">9.1</a>.</p>
<table class="table table-striped table-hover" style="width: auto !important; margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:Welch-rejection">Table 9.1: </span>Rejection rates of conventional and Welch t-test for varying sample sizes and population mean differences.
</caption>
<thead>
<tr>
<th style="text-align:right;">
n1
</th>
<th style="text-align:right;">
n2
</th>
<th style="text-align:right;">
mean_diff
</th>
<th style="text-align:left;">
method
</th>
<th style="text-align:right;">
K_rejection
</th>
<th style="text-align:right;">
rej_rate_01
</th>
<th style="text-align:right;">
rej_rate_05
</th>
<th style="text-align:right;">
rej_rate_mcse_01
</th>
<th style="text-align:right;">
rej_rate_mcse_05
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
50
</td>
<td style="text-align:right;">
50
</td>
<td style="text-align:right;">
0.0
</td>
<td style="text-align:left;">
Welch
</td>
<td style="text-align:right;">
1000
</td>
<td style="text-align:right;">
0.009
</td>
<td style="text-align:right;">
0.047
</td>
<td style="text-align:right;">
0.003
</td>
<td style="text-align:right;">
0.007
</td>
</tr>
<tr>
<td style="text-align:right;">
50
</td>
<td style="text-align:right;">
50
</td>
<td style="text-align:right;">
0.0
</td>
<td style="text-align:left;">
t-test
</td>
<td style="text-align:right;">
1000
</td>
<td style="text-align:right;">
0.009
</td>
<td style="text-align:right;">
0.048
</td>
<td style="text-align:right;">
0.003
</td>
<td style="text-align:right;">
0.007
</td>
</tr>
<tr>
<td style="text-align:right;">
50
</td>
<td style="text-align:right;">
50
</td>
<td style="text-align:right;">
0.5
</td>
<td style="text-align:left;">
Welch
</td>
<td style="text-align:right;">
1000
</td>
<td style="text-align:right;">
0.157
</td>
<td style="text-align:right;">
0.335
</td>
<td style="text-align:right;">
0.012
</td>
<td style="text-align:right;">
0.015
</td>
</tr>
<tr>
<td style="text-align:right;">
50
</td>
<td style="text-align:right;">
50
</td>
<td style="text-align:right;">
0.5
</td>
<td style="text-align:left;">
t-test
</td>
<td style="text-align:right;">
1000
</td>
<td style="text-align:right;">
0.162
</td>
<td style="text-align:right;">
0.340
</td>
<td style="text-align:right;">
0.012
</td>
<td style="text-align:right;">
0.015
</td>
</tr>
<tr>
<td style="text-align:right;">
50
</td>
<td style="text-align:right;">
50
</td>
<td style="text-align:right;">
1.0
</td>
<td style="text-align:left;">
Welch
</td>
<td style="text-align:right;">
1000
</td>
<td style="text-align:right;">
0.677
</td>
<td style="text-align:right;">
0.871
</td>
<td style="text-align:right;">
0.015
</td>
<td style="text-align:right;">
0.011
</td>
</tr>
<tr>
<td style="text-align:right;">
50
</td>
<td style="text-align:right;">
50
</td>
<td style="text-align:right;">
1.0
</td>
<td style="text-align:left;">
t-test
</td>
<td style="text-align:right;">
1000
</td>
<td style="text-align:right;">
0.686
</td>
<td style="text-align:right;">
0.876
</td>
<td style="text-align:right;">
0.015
</td>
<td style="text-align:right;">
0.010
</td>
</tr>
<tr>
<td style="text-align:right;">
50
</td>
<td style="text-align:right;">
50
</td>
<td style="text-align:right;">
2.0
</td>
<td style="text-align:left;">
Welch
</td>
<td style="text-align:right;">
1000
</td>
<td style="text-align:right;">
1.000
</td>
<td style="text-align:right;">
1.000
</td>
<td style="text-align:right;">
0.000
</td>
<td style="text-align:right;">
0.000
</td>
</tr>
<tr>
<td style="text-align:right;">
50
</td>
<td style="text-align:right;">
50
</td>
<td style="text-align:right;">
2.0
</td>
<td style="text-align:left;">
t-test
</td>
<td style="text-align:right;">
1000
</td>
<td style="text-align:right;">
1.000
</td>
<td style="text-align:right;">
1.000
</td>
<td style="text-align:right;">
0.000
</td>
<td style="text-align:right;">
0.000
</td>
</tr>
<tr>
<td style="text-align:right;">
50
</td>
<td style="text-align:right;">
70
</td>
<td style="text-align:right;">
0.0
</td>
<td style="text-align:left;">
Welch
</td>
<td style="text-align:right;">
1000
</td>
<td style="text-align:right;">
0.008
</td>
<td style="text-align:right;">
0.039
</td>
<td style="text-align:right;">
0.003
</td>
<td style="text-align:right;">
0.006
</td>
</tr>
<tr>
<td style="text-align:right;">
50
</td>
<td style="text-align:right;">
70
</td>
<td style="text-align:right;">
0.0
</td>
<td style="text-align:left;">
t-test
</td>
<td style="text-align:right;">
1000
</td>
<td style="text-align:right;">
0.004
</td>
<td style="text-align:right;">
0.027
</td>
<td style="text-align:right;">
0.002
</td>
<td style="text-align:right;">
0.005
</td>
</tr>
<tr>
<td style="text-align:right;">
50
</td>
<td style="text-align:right;">
70
</td>
<td style="text-align:right;">
0.5
</td>
<td style="text-align:left;">
Welch
</td>
<td style="text-align:right;">
1000
</td>
<td style="text-align:right;">
0.202
</td>
<td style="text-align:right;">
0.426
</td>
<td style="text-align:right;">
0.013
</td>
<td style="text-align:right;">
0.016
</td>
</tr>
<tr>
<td style="text-align:right;">
50
</td>
<td style="text-align:right;">
70
</td>
<td style="text-align:right;">
0.5
</td>
<td style="text-align:left;">
t-test
</td>
<td style="text-align:right;">
1000
</td>
<td style="text-align:right;">
0.139
</td>
<td style="text-align:right;">
0.341
</td>
<td style="text-align:right;">
0.011
</td>
<td style="text-align:right;">
0.015
</td>
</tr>
<tr>
<td style="text-align:right;">
50
</td>
<td style="text-align:right;">
70
</td>
<td style="text-align:right;">
1.0
</td>
<td style="text-align:left;">
Welch
</td>
<td style="text-align:right;">
1000
</td>
<td style="text-align:right;">
0.820
</td>
<td style="text-align:right;">
0.937
</td>
<td style="text-align:right;">
0.012
</td>
<td style="text-align:right;">
0.008
</td>
</tr>
<tr>
<td style="text-align:right;">
50
</td>
<td style="text-align:right;">
70
</td>
<td style="text-align:right;">
1.0
</td>
<td style="text-align:left;">
t-test
</td>
<td style="text-align:right;">
1000
</td>
<td style="text-align:right;">
0.743
</td>
<td style="text-align:right;">
0.904
</td>
<td style="text-align:right;">
0.014
</td>
<td style="text-align:right;">
0.009
</td>
</tr>
<tr>
<td style="text-align:right;">
50
</td>
<td style="text-align:right;">
70
</td>
<td style="text-align:right;">
2.0
</td>
<td style="text-align:left;">
Welch
</td>
<td style="text-align:right;">
1000
</td>
<td style="text-align:right;">
1.000
</td>
<td style="text-align:right;">
1.000
</td>
<td style="text-align:right;">
0.000
</td>
<td style="text-align:right;">
0.000
</td>
</tr>
<tr>
<td style="text-align:right;">
50
</td>
<td style="text-align:right;">
70
</td>
<td style="text-align:right;">
2.0
</td>
<td style="text-align:left;">
t-test
</td>
<td style="text-align:right;">
1000
</td>
<td style="text-align:right;">
1.000
</td>
<td style="text-align:right;">
1.000
</td>
<td style="text-align:right;">
0.000
</td>
<td style="text-align:right;">
0.000
</td>
</tr>
</tbody>
</table>
</div>
<div id="mcse-calculation-in-our-cluster-rct-example" class="section level3 hasAnchor" number="9.7.6">
<h3><span class="header-section-number">9.7.6</span> MCSE Calculation in our Cluster RCT Example<a href="performance-measures.html#mcse-calculation-in-our-cluster-rct-example" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In Section <a href="performance-measures.html#clusterRCTperformance">9.1.1</a>, we computed performance measures for three point estimators of the school-level average treatment effect in a cluster RCT.
We can carry out the same calculations using the <code>calc_absolute()</code> function from <code>simhelpers</code>, which also provides MCSEs for each measure.
Examining the MCSEs is useful to ensure that 1000 replications of the simulation is suffiicent to provide reasonably precise estimates of the performance measures.
In particular, we have:</p>
<div class="sourceCode" id="cb323"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb323-1"><a href="performance-measures.html#cb323-1" tabindex="-1"></a><span class="fu">library</span>( simhelpers )</span>
<span id="cb323-2"><a href="performance-measures.html#cb323-2" tabindex="-1"></a></span>
<span id="cb323-3"><a href="performance-measures.html#cb323-3" tabindex="-1"></a>runs <span class="sc">%&gt;%</span> </span>
<span id="cb323-4"><a href="performance-measures.html#cb323-4" tabindex="-1"></a>  <span class="fu">group_by</span>(method) <span class="sc">%&gt;%</span></span>
<span id="cb323-5"><a href="performance-measures.html#cb323-5" tabindex="-1"></a>  <span class="fu">summarise</span>( </span>
<span id="cb323-6"><a href="performance-measures.html#cb323-6" tabindex="-1"></a>    <span class="fu">calc_absolute</span>( </span>
<span id="cb323-7"><a href="performance-measures.html#cb323-7" tabindex="-1"></a>      <span class="at">estimates =</span> ATE_hat, <span class="at">true_param =</span> ATE,</span>
<span id="cb323-8"><a href="performance-measures.html#cb323-8" tabindex="-1"></a>      <span class="at">criteria =</span> <span class="fu">c</span>(<span class="st">&quot;bias&quot;</span>,<span class="st">&quot;stddev&quot;</span>, <span class="st">&quot;rmse&quot;</span>)</span>
<span id="cb323-9"><a href="performance-measures.html#cb323-9" tabindex="-1"></a>    ) </span>
<span id="cb323-10"><a href="performance-measures.html#cb323-10" tabindex="-1"></a>  )</span></code></pre></div>
<pre><code>## # A tibble: 3 × 8
##   method K_absolute      bias bias_mcse stddev
##   &lt;chr&gt;       &lt;int&gt;     &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;
## 1 Agg          1000 -0.000166   0.00676  0.214
## 2 LR           1000  0.0824     0.00707  0.224
## 3 MLM          1000  0.0122     0.00674  0.213
## # ℹ 3 more variables: stddev_mcse &lt;dbl&gt;,
## #   rmse &lt;dbl&gt;, rmse_mcse &lt;dbl&gt;</code></pre>
<p>We see the MCSEs are quite small relative to the linear regression bias term and all the SEs (<code>stddev</code>) and RMSEs. Results based on 1000 replications seems adequate to support our conclusions about the gross trends identified.
We have <em>not</em> simulated enough to rule out the possibility that the aggregation estimator and multilevel modeling estimator could be slightly biased. Given our MCSEs, they could have true bias of as much as 0.01 (two MCSEs).</p>
</div>
</div>
<div id="summary-of-peformance-measures" class="section level2 hasAnchor" number="9.8">
<h2><span class="header-section-number">9.8</span> Summary of Peformance Measures<a href="performance-measures.html#summary-of-peformance-measures" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<!-- JEP: Do we need this summary table? Should we move it to the front of the chapter? Expand to include all of the performance measures, not just those for point estimators? -->
<p>We list most of the performance criteria we saw in this chapter in the table below, for reference:</p>
<table>
<colgroup>
<col width="18%" />
<col width="43%" />
<col width="16%" />
<col width="21%" />
</colgroup>
<thead>
<tr class="header">
<th>Criterion</th>
<th>Definition</th>
<th>Estimator</th>
<th>Monte Carlo Standard Error</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Measures for point estimators</strong></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>Bias</td>
<td><span class="math inline">\(\E(T) - \theta\)</span></td>
<td><span class="math inline">\(\bar{T} - \theta\)</span></td>
<td><a href="performance-measures.html#eq:MCSE-bias">(9.24)</a></td>
</tr>
<tr class="odd">
<td>Median bias</td>
<td><span class="math inline">\(\M(T) - \theta\)</span></td>
<td><span class="math inline">\(m_T - \theta\)</span></td>
<td><a href="performance-measures.html#eq:MCSE-median">(9.30)</a></td>
</tr>
<tr class="even">
<td>Trimmed bias</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>Variance</td>
<td><span class="math inline">\(\E\left[\left(T - \text{E}(T)\right)^2\right]\)</span></td>
<td><span class="math inline">\(S_T^2\)</span></td>
<td><a href="performance-measures.html#eq:MCSE-var">(9.25)</a></td>
</tr>
<tr class="even">
<td>Standard error</td>
<td><span class="math inline">\(\sqrt{\E\left[\left(T - \text{E}(T)\right)^2\right]}\)</span></td>
<td><span class="math inline">\(S_T\)</span></td>
<td><a href="performance-measures.html#eq:MCSE-SE">(9.26)</a></td>
</tr>
<tr class="odd">
<td>Mean squared error</td>
<td><span class="math inline">\(\E\left[\left(T - \theta\right)^2\right]\)</span></td>
<td><span class="math inline">\(\left(\bar{T} - \theta\right)^2 + \frac{R - 1}{R}S_T^2\)</span></td>
<td><a href="performance-measures.html#eq:MCSE-MSE">(9.27)</a></td>
</tr>
<tr class="even">
<td>Root mean squared error</td>
<td><span class="math inline">\(\sqrt{\E\left[\left(T - \theta\right)^2\right]}\)</span></td>
<td><span class="math inline">\(\sqrt{\left(\bar{T} - \theta\right)^2 + \frac{R - 1}{R} S_T^2}\)</span></td>
<td><a href="performance-measures.html#eq:MCSE-RMSE">(9.28)</a></td>
</tr>
<tr class="odd">
<td>Median absolute error</td>
<td><span class="math inline">\(\M\left[\left|T - \theta\right|\right]\)</span></td>
<td><span class="math inline">\(\left[\left|T - \theta\right|\right]_{R/2}\)</span></td>
<td><a href="performance-measures.html#eq:MCSE-median">(9.30)</a></td>
</tr>
<tr class="even">
<td>Relative bias</td>
<td><span class="math inline">\(\E(T) / \theta\)</span></td>
<td><span class="math inline">\(\bar{T} / \theta\)</span></td>
<td><a href="performance-measures.html#eq:MCSE-relative-bias">(9.29)</a></td>
</tr>
<tr class="odd">
<td>Relative median bias</td>
<td><span class="math inline">\(\M(T) / \theta\)</span></td>
<td><span class="math inline">\(m_T / \theta\)</span></td>
<td><a href="performance-measures.html#eq:MCSE-relative-bias">(9.29)</a></td>
</tr>
<tr class="even">
<td>Relative RMSE</td>
<td><span class="math inline">\(\sqrt{\E\left[\left(T - \theta\right)^2\right]} / \theta\)</span></td>
<td><span class="math inline">\(\frac{\sqrt{\left(\bar{T} - \theta\right)^2 + \frac{R - 1}{R}S_T^2}}{\theta}\)</span></td>
<td><a href="performance-measures.html#eq:MCSE-relative-bias">(9.29)</a></td>
</tr>
</tbody>
</table>
<ul>
<li>Bias and median bias are measures of whether the estimator is systematically higher or lower than the target parameter.</li>
<li>Variance is a measure of the <strong>precision</strong> of the estimator—that is, how far it deviates <em>from its average</em>. We might look at the square root of this, to assess the precision in the units of the original measure. This is the true SE of the estimator.</li>
<li>Mean-squared error is a measure of <strong>overall accuracy</strong>, i.e. is a measure how far we typically are from the truth. We more frequently use the root mean squared error, or RMSE, which is just the square root of the MSE.</li>
<li>The median absolute deviation (MAD) is another measure of overall accuracy that is less sensitive to outlier estimates. The RMSE can be driven up by a single bad egg. The MAD is less sensitive to this.</li>
</ul>
</div>
<div id="concluding-thoughts" class="section level2 hasAnchor" number="9.9">
<h2><span class="header-section-number">9.9</span> Concluding thoughts<a href="performance-measures.html#concluding-thoughts" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In practice, many data analysis procedures produce multiple pieces of information—not just point estimates, but also standard errors and confidence intervals and p-values from null hypothesis tests—and those pieces are inter-related.
For instance, a confidence interval is usually computed from a point estimate and its standard error.
Consequently, the performance of that confidence interval will be strongly affected by whether the point estimator is biased and whether the standard error tends to understates or over-states the true uncertainty.
Likewise, the performance of a hypothesis testing procedure will often strongly depend on the properties of the point estimator and standard error used to compute the test.<br />
Thus, most simulations will involve evaluating a data analysis procedure on several measures to arrive at a holistic understanding of its performance.</p>
<p>Moreover, the main aim of many simulations is to compare the performance of several different estimators or to determine which of several data analysis procedures is preferable.
For such aims, we will need to use the performance measures to understand whether a set of procedures work differently, when and how one is superior to the other, and what factors influence differences in performance.
To fully understand the advantages and trade-offs among a set of estimators, we will generally need to compare them using several performance measures.</p>
</div>
<div id="exercises-6" class="section level2 hasAnchor" number="9.10">
<h2><span class="header-section-number">9.10</span> Exercises<a href="performance-measures.html#exercises-6" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="Brown-Forsythe-performance" class="section level3 hasAnchor" number="9.10.1">
<h3><span class="header-section-number">9.10.1</span> Brown and Forsythe (1974) results<a href="performance-measures.html#Brown-Forsythe-performance" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ol style="list-style-type: decimal">
<li><p>Use the <code>generate_ANOVA_data</code> data-generating function for one-way heteroskedastic ANOVA (Section <a href="case-ANOVA.html#case-anova-DGP">5.1</a>) and the data-analysis function you wrote for Exercise <a href="data-analysis-procedures.html#BFFs-forever">7.5.1</a> to create a simulation driver function for the Brown and Forsythe simulations.</p></li>
<li><p>Use your simulation driver to evaluate the Type-I error rate of the ANOVA <span class="math inline">\(F\)</span>-test, Welch’s test, and the BFF* test for a scenario with four groups, sample sizes <span class="math inline">\(n_1 = 11\)</span>, <span class="math inline">\(n_2 = 16\)</span>, <span class="math inline">\(n_3 = 16\)</span>, <span class="math inline">\(n_4 = 21\)</span>, equal group means <span class="math inline">\(\mu_1 = \mu_2 = \mu_3 = \mu_4 = 0\)</span>, and group standard deviations <span class="math inline">\(\sigma_1 = 3\)</span>, <span class="math inline">\(\sigma_2 = 2\)</span>, <span class="math inline">\(\sigma_3 = 2\)</span>, <span class="math inline">\(\sigma_4 = 1\)</span>. Are all of the tests level-<span class="math inline">\(\alpha\)</span>?</p></li>
<li><p>Use your simulation driver to evaluate the power of each test for a scenario with group means of <span class="math inline">\(\mu_1 = 0\)</span>, <span class="math inline">\(\mu_2 = 0.2\)</span>, <span class="math inline">\(\mu_3 = 0.4\)</span>, <span class="math inline">\(\mu_4 = 0.6\)</span>, with sample sizes and group standard deviations as listed above. Which test has the highest power?</p></li>
</ol>
</div>
<div id="size-adjusted-power" class="section level3 hasAnchor" number="9.10.2">
<h3><span class="header-section-number">9.10.2</span> Size-adjusted power<a href="performance-measures.html#size-adjusted-power" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>When different hypothesis testing procedures have different rejection rates under the null hypothesis, it becomes difficult to interpret differences in the non-null power of the tests.
One approach for conducting a fair comparison of testing procedures in this situation is to compute the <em>size-adjusted</em> power of the tests.
Size-adjusted power involves computing the rejection rate of a test using a different threshold <span class="math inline">\(\alpha&#39;\)</span>, selected so that the Type-I error rate of the test is equal to the desired <span class="math inline">\(\alpha\)</span> level.
Specifically, size adjusted power is
<span class="math display">\[
\rho^{adjusted}_\alpha(\theta) = \Prob(P &lt; \rho_\alpha(0)).
\]</span>
To estimate size-adjusted power using simulation, we first need to estimate the Type-I error rate, <span class="math inline">\(r_\alpha(0)\)</span>. We can then evaluate the rejection rate of the testing procedure under scenarios with other values of <span class="math inline">\(\theta\)</span> by computing
<span class="math display">\[
r^{adjusted}_\alpha(\theta) = \frac{1}{R} \sum_{r=1}^R I(P_r &lt; r_{\alpha}(0)).
\]</span></p>
<p>Compute the size-adjusted power of the ANOVA <span class="math inline">\(F\)</span>-test, Welch’s test, and the BFF* test for the scenario in part (3) of Exercise <a href="performance-measures.html#Brown-Forsythe-performance">9.10.1</a>.</p>
</div>
<div id="three-correlation-estimators" class="section level3 hasAnchor" number="9.10.3">
<h3><span class="header-section-number">9.10.3</span> Three correlation estimators<a href="performance-measures.html#three-correlation-estimators" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Consider the bivariate negative binomial distribution model described in Exercise <a href="data-generating-processes.html#BVNB2">6.9.6</a>.
Suppose that we want to estimate the correlation parameter <span class="math inline">\(\rho\)</span> of the latent bivariate normal distribution.
Without studying the statistical theory for this problem, we might think to use simulation to evaluate whether any common correlation measures work well for estimating this parameter.
Potential candidate estimators include the usual sample Pearson’s correlation, Spearman’s rank correlation, or Kendall’s <span class="math inline">\(\tau\)</span> coefficient.
The latter two estimators might seem promising because they are based on the ranked data, so could be more appropriate that Pearson’s correlation for frequency count variates.</p>
<p>The following estimation function computes all three correlations, along with corresponding <span class="math inline">\(p\)</span>-values for the null hypothesis of no association and confidence intervals computed using Fisher’s <span class="math inline">\(z\)</span> transformation. The confidence interval calculations are developed for Pearson’s correlation under bivariate normality, so they might not be appropriate for this data-generating process or for Spearman’s or Kendall’s correlations.
Still, we can use simulation to see how well or how poorly they perform.</p>
<div class="sourceCode" id="cb325"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb325-1"><a href="performance-measures.html#cb325-1" tabindex="-1"></a>three_corrs <span class="ot">&lt;-</span> <span class="cf">function</span>(</span>
<span id="cb325-2"><a href="performance-measures.html#cb325-2" tabindex="-1"></a>  data, </span>
<span id="cb325-3"><a href="performance-measures.html#cb325-3" tabindex="-1"></a>  <span class="at">method =</span> <span class="fu">c</span>(<span class="st">&quot;pearson&quot;</span>,<span class="st">&quot;kendall&quot;</span>,<span class="st">&quot;spearman&quot;</span>), </span>
<span id="cb325-4"><a href="performance-measures.html#cb325-4" tabindex="-1"></a>  <span class="at">level =</span> <span class="fl">0.95</span></span>
<span id="cb325-5"><a href="performance-measures.html#cb325-5" tabindex="-1"></a>) {</span>
<span id="cb325-6"><a href="performance-measures.html#cb325-6" tabindex="-1"></a>  </span>
<span id="cb325-7"><a href="performance-measures.html#cb325-7" tabindex="-1"></a>  r_est <span class="ot">&lt;-</span> <span class="fu">lapply</span>(method, \(m) <span class="fu">cor.test</span>(data<span class="sc">$</span>C1, data<span class="sc">$</span>C2, <span class="at">method =</span> m, <span class="at">exact =</span> <span class="cn">FALSE</span>))</span>
<span id="cb325-8"><a href="performance-measures.html#cb325-8" tabindex="-1"></a>  est <span class="ot">&lt;-</span> <span class="fu">sapply</span>(r_est, \(x) <span class="fu">as.numeric</span>(x<span class="sc">$</span>estimate))</span>
<span id="cb325-9"><a href="performance-measures.html#cb325-9" tabindex="-1"></a>  pval <span class="ot">&lt;-</span> <span class="fu">sapply</span>(r_est, \(x) x<span class="sc">$</span>p.value)</span>
<span id="cb325-10"><a href="performance-measures.html#cb325-10" tabindex="-1"></a>  z_est <span class="ot">&lt;-</span> <span class="fu">atanh</span>(est)</span>
<span id="cb325-11"><a href="performance-measures.html#cb325-11" tabindex="-1"></a>  se_z <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">/</span> <span class="fu">sqrt</span>(<span class="fu">nrow</span>(data) <span class="sc">-</span> <span class="dv">3</span>)</span>
<span id="cb325-12"><a href="performance-measures.html#cb325-12" tabindex="-1"></a>  crit <span class="ot">&lt;-</span> <span class="fu">qnorm</span>(<span class="dv">1</span> <span class="sc">-</span> (<span class="dv">1</span> <span class="sc">-</span> level) <span class="sc">/</span> <span class="dv">2</span>)</span>
<span id="cb325-13"><a href="performance-measures.html#cb325-13" tabindex="-1"></a>  ci_lo <span class="ot">&lt;-</span> <span class="fu">tanh</span>(z_est <span class="sc">-</span> crit <span class="sc">*</span> se_z)</span>
<span id="cb325-14"><a href="performance-measures.html#cb325-14" tabindex="-1"></a>  ci_hi <span class="ot">&lt;-</span> <span class="fu">tanh</span>(z_est <span class="sc">+</span> crit <span class="sc">*</span> se_z)</span>
<span id="cb325-15"><a href="performance-measures.html#cb325-15" tabindex="-1"></a>  </span>
<span id="cb325-16"><a href="performance-measures.html#cb325-16" tabindex="-1"></a>  <span class="fu">data.frame</span>( </span>
<span id="cb325-17"><a href="performance-measures.html#cb325-17" tabindex="-1"></a>    <span class="at">stat =</span> method,</span>
<span id="cb325-18"><a href="performance-measures.html#cb325-18" tabindex="-1"></a>    <span class="at">r =</span> est,</span>
<span id="cb325-19"><a href="performance-measures.html#cb325-19" tabindex="-1"></a>    <span class="at">z =</span> z_est,</span>
<span id="cb325-20"><a href="performance-measures.html#cb325-20" tabindex="-1"></a>    <span class="at">se_z =</span> se_z,</span>
<span id="cb325-21"><a href="performance-measures.html#cb325-21" tabindex="-1"></a>    <span class="at">pval =</span> pval,</span>
<span id="cb325-22"><a href="performance-measures.html#cb325-22" tabindex="-1"></a>    <span class="at">ci_lo =</span> ci_lo,</span>
<span id="cb325-23"><a href="performance-measures.html#cb325-23" tabindex="-1"></a>    <span class="at">ci_hi =</span> ci_hi</span>
<span id="cb325-24"><a href="performance-measures.html#cb325-24" tabindex="-1"></a>  )</span>
<span id="cb325-25"><a href="performance-measures.html#cb325-25" tabindex="-1"></a>  </span>
<span id="cb325-26"><a href="performance-measures.html#cb325-26" tabindex="-1"></a>}</span></code></pre></div>
<ol style="list-style-type: decimal">
<li><p>Combine your data-generating function and <code>three_corrs()</code> into a simulation driver.</p></li>
<li><p>Use your simulation driver to generate 500 replications of the simulation for a scenario with <span class="math inline">\(N = 20\)</span>, <span class="math inline">\(\mu_1 = \mu_2 = 5\)</span>, <span class="math inline">\(p_1 = p_2 = 0.5\)</span>, and <span class="math inline">\(\rho = 0.7\)</span>.</p></li>
<li><p>Compute the bias, empirical standard error, and RMSE of each correlation estimator, along with corresponding MCSEs. Which correlation estimator is most accurate?</p></li>
<li><p>Compute the coverage rate and expected width of the confidence intervals based on each correlation estimator. Do any of the estimators have reasonable coverage? If so, which has the best expected width?</p></li>
<li><p>Use your simulation driver to estimate the Type-I error rate of the hypothesis tests for each correlation coefficient for a scenario with <span class="math inline">\(N = 20\)</span>, <span class="math inline">\(\mu_1 = \mu_2 = 5\)</span>, <span class="math inline">\(p_1 = p_2 = 0.5\)</span>. Are any of the tests level-<span class="math inline">\(\alpha\)</span>?</p></li>
</ol>
</div>
<div id="cluster-RCT-t-confidence-intervals" class="section level3 hasAnchor" number="9.10.4">
<h3><span class="header-section-number">9.10.4</span> Confidence interval comparison<a href="performance-measures.html#cluster-RCT-t-confidence-intervals" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Consider the estimation functions for the cluster RCT example, as given in Section <a href="data-analysis-procedures.html#multiple-estimation-procedures">7.2</a>.
Modify the functions to return <strong>both</strong> normal Wald-type 95% confidence intervals (as computed in Section <a href="performance-measures.html#cluster-RCT-CI-coverage">9.3.1</a>) and cluster-robust confidence intervals based on <span class="math inline">\(t\)</span> distributions with Satterthwaite degrees of freedom.
For the latter, use <code>conf_int()</code> from the <code>clubSandwich</code> package, as in the following example code:</p>
<div class="sourceCode" id="cb326"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb326-1"><a href="performance-measures.html#cb326-1" tabindex="-1"></a><span class="fu">library</span>(clubSandwich)</span>
<span id="cb326-2"><a href="performance-measures.html#cb326-2" tabindex="-1"></a></span>
<span id="cb326-3"><a href="performance-measures.html#cb326-3" tabindex="-1"></a>M1 <span class="ot">&lt;-</span> lme4<span class="sc">::</span><span class="fu">lmer</span>( </span>
<span id="cb326-4"><a href="performance-measures.html#cb326-4" tabindex="-1"></a>  Yobs <span class="sc">~</span> <span class="dv">1</span> <span class="sc">+</span> Z <span class="sc">+</span> (<span class="dv">1</span> <span class="sc">|</span> sid), </span>
<span id="cb326-5"><a href="performance-measures.html#cb326-5" tabindex="-1"></a>  <span class="at">data =</span> dat </span>
<span id="cb326-6"><a href="performance-measures.html#cb326-6" tabindex="-1"></a>)</span>
<span id="cb326-7"><a href="performance-measures.html#cb326-7" tabindex="-1"></a><span class="fu">conf_int</span>(M1, <span class="at">vcov =</span> <span class="st">&quot;CR2&quot;</span>)</span>
<span id="cb326-8"><a href="performance-measures.html#cb326-8" tabindex="-1"></a></span>
<span id="cb326-9"><a href="performance-measures.html#cb326-9" tabindex="-1"></a>M2 <span class="ot">&lt;-</span> estimatr<span class="sc">::</span><span class="fu">lm_robust</span>( </span>
<span id="cb326-10"><a href="performance-measures.html#cb326-10" tabindex="-1"></a>  Yobs <span class="sc">~</span> <span class="dv">1</span> <span class="sc">+</span> Z, <span class="at">data =</span> dat, </span>
<span id="cb326-11"><a href="performance-measures.html#cb326-11" tabindex="-1"></a>  <span class="at">clusters =</span> sid,  <span class="at">se_type =</span> se_type</span>
<span id="cb326-12"><a href="performance-measures.html#cb326-12" tabindex="-1"></a>)</span>
<span id="cb326-13"><a href="performance-measures.html#cb326-13" tabindex="-1"></a><span class="fu">conf_int</span>(M2, <span class="at">cluster =</span> dat<span class="sc">$</span>sid, <span class="at">vcov =</span> <span class="st">&quot;CR2&quot;</span>)</span>
<span id="cb326-14"><a href="performance-measures.html#cb326-14" tabindex="-1"></a></span>
<span id="cb326-15"><a href="performance-measures.html#cb326-15" tabindex="-1"></a>M3 <span class="ot">&lt;-</span> estimatr<span class="sc">::</span><span class="fu">lm_robust</span>( </span>
<span id="cb326-16"><a href="performance-measures.html#cb326-16" tabindex="-1"></a>  Ybar <span class="sc">~</span> <span class="dv">1</span> <span class="sc">+</span> Z, <span class="at">data =</span> datagg, </span>
<span id="cb326-17"><a href="performance-measures.html#cb326-17" tabindex="-1"></a>  <span class="at">se_type =</span> se_type </span>
<span id="cb326-18"><a href="performance-measures.html#cb326-18" tabindex="-1"></a>)</span>
<span id="cb326-19"><a href="performance-measures.html#cb326-19" tabindex="-1"></a><span class="fu">conf_int</span>(M3, <span class="at">cluster =</span> dat<span class="sc">$</span>sid, <span class="at">vcov =</span> <span class="st">&quot;CR2&quot;</span>)</span></code></pre></div>
<p>Pick some simulation parameters and estimate the coverage and interval width of both types of confidence intervals.
How do the normal Wald-type intervals compare to the cluster-robust intervals?</p>
</div>
<div id="jackknife-MCSE" class="section level3 hasAnchor" number="9.10.5">
<h3><span class="header-section-number">9.10.5</span> Jackknife calculation of MCSEs for RMSE<a href="performance-measures.html#jackknife-MCSE" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The following code generates 100 replications of a simulation of three average treatment effect estimators in a cluster RCT, using a simulation driver function we developed in Section <a href="running-the-simulation-process.html#bundle-sim-demo">8.3</a> using components described in Sections <a href="data-generating-processes.html#case-cluster">6.6</a> and <a href="data-analysis-procedures.html#multiple-estimation-procedures">7.2</a>.</p>
<div class="sourceCode" id="cb327"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb327-1"><a href="performance-measures.html#cb327-1" tabindex="-1"></a><span class="fu">set.seed</span>( <span class="dv">20251029</span> )</span>
<span id="cb327-2"><a href="performance-measures.html#cb327-2" tabindex="-1"></a>runs_val <span class="ot">&lt;-</span> <span class="fu">sim_cluster_RCT</span>( </span>
<span id="cb327-3"><a href="performance-measures.html#cb327-3" tabindex="-1"></a>  <span class="at">reps =</span> <span class="dv">100</span>, </span>
<span id="cb327-4"><a href="performance-measures.html#cb327-4" tabindex="-1"></a>  <span class="at">J =</span> <span class="dv">16</span>, <span class="at">n_bar =</span> <span class="dv">20</span>, <span class="at">alpha =</span> <span class="fl">0.5</span>,</span>
<span id="cb327-5"><a href="performance-measures.html#cb327-5" tabindex="-1"></a>  <span class="at">gamma_1 =</span> <span class="fl">0.3</span>, <span class="at">gamma_2 =</span> <span class="fl">0.8</span>,</span>
<span id="cb327-6"><a href="performance-measures.html#cb327-6" tabindex="-1"></a>  <span class="at">sigma2_u =</span> <span class="fl">0.25</span>, <span class="at">sigma2_e =</span> <span class="fl">0.75</span></span>
<span id="cb327-7"><a href="performance-measures.html#cb327-7" tabindex="-1"></a>)</span></code></pre></div>
<p>Compute the RMSE of each estimator, and use the jackknife technique described in Section <a href="performance-measures.html#MCSE-for-relative-variance">9.7.3</a> to compute a MCSE for the RMSE.
Check your results against the results from <code>calc_absolute()</code> in the <code>simhelpers</code> package.</p>
</div>
<div id="jackknife-MCSE-ratio" class="section level3 hasAnchor" number="9.10.6">
<h3><span class="header-section-number">9.10.6</span> Jackknife calculation of MCSEs for RMSE ratios<a href="performance-measures.html#jackknife-MCSE-ratio" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Continuing from Exercise <a href="performance-measures.html#jackknife-MCSE">9.10.5</a>, compute the ratio of the RMSE of each estimator to the RMSE of the linear regression estimator. Use the jackknife technique to compute a MCSE for these RMSE ratios.</p>
</div>
<div id="cluster-RCT-SPATE" class="section level3 hasAnchor" number="9.10.7">
<h3><span class="header-section-number">9.10.7</span> Distribution theory for person-level average treatment effects<a href="performance-measures.html#cluster-RCT-SPATE" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Section <a href="data-generating-processes.html#case-cluster">6.6</a> described a data-generating process for a cluster-randomized experiment in which the school-specific treatment effects varied according to the size of the school.
The auxiliary model for the size of school <span class="math inline">\(j\)</span> was
<span class="math display">\[
n_j \sim \text{Unif}\left[ (1-\alpha)\bar{n}, (1+\alpha)\bar{n} \right],
\]</span>
where <span class="math inline">\(\bar{n}\)</span> was the average school size and <span class="math inline">\(0 \leq \alpha &lt; 1\)</span> determines the degree of variation in school sizes.
The data-generating process for the outcome data was
<span class="math display">\[
Y_{ij} = \gamma_{0} + \gamma_{1} Z_j + \gamma_{2} Z_j S_j  + u_j + \epsilon_{ij},
\]</span>
where <span class="math inline">\(Y_{ij}\)</span> is the outcome for student <span class="math inline">\(i\)</span> in school <span class="math inline">\(j\)</span>, <span class="math inline">\(Z_j\)</span> is an indicator for whether school <span class="math inline">\(j\)</span> is assigned to treatment <span class="math inline">\((Z_j = 1)\)</span> or control <span class="math inline">\((Z_j = 0)\)</span>, and <span class="math inline">\(S_j = \frac{n_j - \bar{n}}{\bar{n}}\)</span>.
The error terms <span class="math inline">\(u_j\)</span> and <span class="math inline">\(e_{ij}\)</span> are both assumed to be normally distributed with zero means.</p>
<p>Under this model, the average treatment effect for school <span class="math inline">\(j\)</span> is <span class="math inline">\(\tau_j = \gamma_1 + \gamma_2 S_j\)</span>. Because <span class="math inline">\(\E(S_j) = 0\)</span> by construction, the average of the school-specific treatment effects is <span class="math inline">\(\gamma_1\)</span>. This is the school-level population average treatment effect estimate. But what is the student-level population average treatment effect estimate? Use the properties of the uniform distribution to find the student-level population average treatment effect <span class="math inline">\(\E\left( \frac{n_j}{\bar{n}} \times \tau_j \right)\)</span>. Check your derivation by simulating a large sample of school sizes and school-specific treatment effects.</p>

</div>
</div>
</div>



<div class="footnotes">
<hr />
<ol start="11">
<li id="fn11"><p>Generally, when people say “Standard Error” they actually mean <em>estimated</em> Standard Error, (<span class="math inline">\(\widehat{SE}\)</span>), as we would calculate in a real data analysis (where we have only a single realization of the data-generating process). It is easy to forget that this standard error is itself an estimate of a parameter–the true or empirical SE—and thus has its own uncertainty.<a href="performance-measures.html#fnref11" class="footnote-back">↩︎</a></p></li>
<li id="fn12"><p>For a normally distributed sampling distribution, the interquartile range is 1.35 SD; with <span class="math inline">\(w = 2\)</span>, the lower and upper thresholds would then fall at <span class="math inline">\(\pm 3.37\)</span> SD, or the <span class="math inline">\(0.04^{th}\)</span> and <span class="math inline">\(99.96^{th}\)</span> percentiles.
Still assuming a normal sampling distribution, taking <span class="math inline">\(w = 2.5\)</span> will mean that the thresholds fall at the <span class="math inline">\(0.003^{th}\)</span> and <span class="math inline">\(99.997^{th}\)</span> percentiles.<a href="performance-measures.html#fnref12" class="footnote-back">↩︎</a></p></li>
<li id="fn13"><p>See the delightfully titled section 11.5, “The Joke Is on Us: The Standard Deviation Estimator is Biased after All,” in <span class="citation">@westfall2013understanding</span> for further discussion.<a href="performance-measures.html#fnref13" class="footnote-back">↩︎</a></p></li>
<li id="fn14"><p>The convention of using <span class="math inline">\(\alpha = .05\)</span> does not have a strong theoretical rationale. Many scholars have criticized the rote application of this convention and argued for using other <span class="math inline">\(\alpha\)</span> levels. See <span class="citation">@Benjamin2017redefine</span> and <span class="citation">@Lakens2018justify</span> for spirited arguments about choosing <span class="math inline">\(\alpha\)</span> levels for hypothesis testing.<a href="performance-measures.html#fnref14" class="footnote-back">↩︎</a></p></li>
<li id="fn15"><p>In the cluster-RCT example, the distribution theory is tractable. See Exercise <a href="performance-measures.html#cluster-RCT-SPATE">9.10.7</a><a href="performance-measures.html#fnref15" class="footnote-back">↩︎</a></p></li>
<li id="fn16"><p>To be precise, the formulas that we give are <em>estimators</em> for the Monte Carlo standard errors of the performance measure estimators. Our presentation does not emphasize this point because the performance measures will usually be estimated using a large number of replications from an independent and identically distributed process, so the distinction between empirical and estimated standard errors will not be consequential.<a href="performance-measures.html#fnref16" class="footnote-back">↩︎</a></p></li>
<li id="fn17"><p>The delta method approximation says (with some conditions), that if we assume <span class="math inline">\(X \sim N\left( \phi, \sigma_X^2 \right)\)</span>, then we can approximate the distribution of <span class="math inline">\(g(X)\)</span> for some continuous function <span class="math inline">\(g(\cdot)\)</span> as
<span class="math display">\[ g(X) \sim N\left( g(\phi), \;\; g&#39;(\phi)^2 \times \sigma_X^2 \right),\]</span>
where <span class="math inline">\(g&#39;(\phi)\)</span> is the derivative of <span class="math inline">\(g(\cdot)\)</span> evaluated at <span class="math inline">\(\phi\)</span>.
Following this approximation,
<span class="math display">\[ SE( g(X) ) \approx \left| g&#39;(\theta) \right|  \times SE(X) .\]</span>
For estimation, we plug in <span class="math inline">\(\hat{\theta}\)</span> and our estimate of <span class="math inline">\(SE(X)\)</span> into the above.
To find the MCSE for <span class="math inline">\(S_T\)</span>, we can apply the delta method approximation to <span class="math inline">\(X = S_T^2\)</span> with <span class="math inline">\(g(x) = \sqrt(x)\)</span> and <span class="math inline">\(g&#39;(x) =\frac{1}{2\sqrt{x}}\)</span>.<a href="performance-measures.html#fnref17" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="running-the-simulation-process.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="simulating-multiple-scenarios.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": false,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": "https://github.com/jepusto/Designing-Simulations-in-R/edit/master/040-Performance-criteria.Rmd",
    "text": "Edit"
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": ["Designing-Simulations-in-R.pdf", "Designing-Simulations-in-R.epub"],
  "search": {
    "engine": "lunr",
    "options": null
  },
  "toc": {
    "collapse": "section",
    "scroll_highlight": true
  },
  "toolbar": {
    "position": "fixed"
  },
  "info": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
