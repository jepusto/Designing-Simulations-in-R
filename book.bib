%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Luke Miratrix at 2025-08-13 09:51:12 -0700 


%% Saved with string encoding Unicode (UTF-8) 

@article{Benjamin2017redefine,
  title = {Redefine Statistical Significance},
  author = {Benjamin, Daniel J. and Berger, James O. and Johannesson, Magnus and Nosek, Brian A. and Wagenmakers, E.-J. and Berk, Richard and Bollen, Kenneth A. and Brembs, Björn and Brown, Lawrence and Camerer, Colin and Cesarini, David and Chambers, Christopher D. and Clyde, Merlise and Cook, Thomas D. and De Boeck, Paul and Dienes, Zoltan and Dreber, Anna and Easwaran, Kenny and Efferson, Charles and Fehr, Ernst and Fidler, Fiona and Field, Andy P. and Forster, Malcolm and George, Edward I. and Gonzalez, Richard and Goodman, Steven and Green, Edwin and Green, Donald P. and Greenwald, Anthony G. and Hadfield, Jarrod D. and Hedges, Larry V. and Held, Leonhard and Hua Ho, Teck and Hoijtink, Herbert and Hruschka, Daniel J. and Imai, Kosuke and Imbens, Guido and Ioannidis, John P. A. and Jeon, Minjeong and Jones, James Holland and Kirchler, Michael and Laibson, David and List, John and Little, Roderick and Lupia, Arthur and Machery, Edouard and Maxwell, Scott E. and McCarthy, Michael and Moore, Don A. and Morgan, Stephen L. and Munafó, Marcus and Nakagawa, Shinichi and Nyhan, Brendan and Parker, Timothy H. and Pericchi, Luis and Perugini, Marco and Rouder, Jeff and Rousseau, Judith and Savalei, Victoria and Schönbrodt, Felix D. and Sellke, Thomas and Sinclair, Betsy and Tingley, Dustin and Van Zandt, Trisha and Vazire, Simine and Watts, Duncan J. and Winship, Christopher and Wolpert, Robert L. and Xie, Yu and Young, Cristobal and Zinman, Jonathan and Johnson, Valen E.},
  journaltitle = {Nature Human Behaviour},
  shortjournal = {Nat Hum Behav},
  volume = {2},
  number = {1},
  pages = {6--10},
  issn = {2397-3374},
  doi = {10.1038/s41562-017-0189-z},
  url = {https://www.nature.com/articles/s41562-017-0189-z},
  urldate = {2025-09-19},
  langid = {english},
  year = {2017},
}

@article{Lakens2018justify,
  title = {Justify Your Alpha},
  author = {Lakens, Daniel and Adolfi, Federico G. and Albers, Casper J. and Anvari, Farid and Apps, Matthew A. J. and Argamon, Shlomo E. and Baguley, Thom and Becker, Raymond B. and Benning, Stephen D. and Bradford, Daniel E. and Buchanan, Erin M. and Caldwell, Aaron R. and Van Calster, Ben and Carlsson, Rickard and Chen, Sau-Chin and Chung, Bryan and Colling, Lincoln J. and Collins, Gary S. and Crook, Zander and Cross, Emily S. and Daniels, Sameera and Danielsson, Henrik and DeBruine, Lisa and Dunleavy, Daniel J. and Earp, Brian D. and Feist, Michele I. and Ferrell, Jason D. and Field, James G. and Fox, Nicholas W. and Friesen, Amanda and Gomes, Caio and Gonzalez-Marquez, Monica and Grange, James A. and Grieve, Andrew P. and Guggenberger, Robert and Grist, James and Van Harmelen, Anne-Laura and Hasselman, Fred and Hochard, Kevin D. and Hoffarth, Mark R. and Holmes, Nicholas P. and Ingre, Michael and Isager, Peder M. and Isotalus, Hanna K. and Johansson, Christer and Juszczyk, Konrad and Kenny, David A. and Khalil, Ahmed A. and Konat, Barbara and Lao, Junpeng and Larsen, Erik Gahner and Lodder, Gerine M. A. and Lukavský, Jiří and Madan, Christopher R. and Manheim, David and Martin, Stephen R. and Martin, Andrea E. and Mayo, Deborah G. and McCarthy, Randy J. and McConway, Kevin and McFarland, Colin and Nio, Amanda Q. X. and Nilsonne, Gustav and De Oliveira, Cilene Lino and De Xivry, Jean-Jacques Orban and Parsons, Sam and Pfuhl, Gerit and Quinn, Kimberly A. and Sakon, John J. and Saribay, S. Adil and Schneider, Iris K. and Selvaraju, Manojkumar and Sjoerds, Zsuzsika and Smith, Samuel G. and Smits, Tim and Spies, Jeffrey R. and Sreekumar, Vishnu and Steltenpohl, Crystal N. and Stenhouse, Neil and Świątkowski, Wojciech and Vadillo, Miguel A. and Van Assen, Marcel A. L. M. and Williams, Matt N. and Williams, Samantha E. and Williams, Donald R. and Yarkoni, Tal and Ziano, Ignazio and Zwaan, Rolf A.},
  year = {2018},
  journaltitle = {Nature Human Behaviour},
  shortjournal = {Nat Hum Behav},
  volume = {2},
  number = {3},
  pages = {168--171},
  issn = {2397-3374},
  doi = {10.1038/s41562-018-0311-x},
  langid = {english}
}

@article{cameronPractitionerGuideClusterRobust2015,
  title = {A Practitioner's Guide to Cluster-Robust Inference},
  author = {Cameron, A Colin and Miller, Douglas L},
  year = {2015},
  journal = {Journal of Human Resources},
  volume = {50},
  number = {2},
  pages = {317--372},
  doi = {10.3368/jhr.50.2.317}
}

@article{Satterthwaite1946approximate,
  title = {An Approximate Distribution of Estimates of Variance Components},
  author = {Satterthwaite, F. E.},
  year = {1946},
  month = dec,
  journal = {Biometrics Bulletin},
  volume = {2},
  number = {6},
  eprint = {10.2307/3002019},
  eprinttype = {jstor},
  pages = {110},
  issn = {00994987},
  doi = {10.2307/3002019},
  urldate = {2025-09-19}
}

@Manual{robustbase,
    title = {robustbase: Basic Robust Statistics},
    author = {Martin Maechler and Peter Rousseeuw and Christophe Croux
      and Valentin Todorov and Andreas Ruckstuhl and Matias
      Salibian-Barrera and Tobias Verbeke and Manuel Koller and Eduardo
      L. T. Conceicao and Maria {Anna di Palma}},
    year = {2024},
    note = {R package version 0.99-4-1},
    url = {http://robustbase.r-forge.r-project.org/},
    url = {http://robustbase.r-forge.r-project.org/},
  }
  
@book{Maronna2006robust,
  title = {Robust Statistics: Theory and Methods},
  shorttitle = {Robust Statistics},
  author = {Maronna, Ricardo A. and Martin, R. Douglas and Yohai, V{\'i}ctor J.},
  year = {2006},
  series = {Wiley Series in Probability and Statistics},
  publisher = {J. Wiley},
  address = {Chichester (GB)},
  isbn = {978-0-470-01092-1},
  langid = {english},
  lccn = {519.5}
}

@book{Wilcox2022introduction,
  title = {Introduction to Robust Estimation and Hypothesis Testing},
  author = {Wilcox, Rand R.},
  year = {2022},
  edition = {Fifth edition},
  publisher = {Academic Press, an imprint of Elsevier},
  address = {London, United Kingdom San Diego, United States Cambridge, MA Oxford, United Kingdom},
  isbn = {978-0-12-820099-5 978-0-12-820098-8},
  langid = {english}
}


@article{Rousseeuw1993alternatives,
  title = {Alternatives to the {{Median Absolute Deviation}}},
  author = {Rousseeuw, Peter J. and Croux, Christophe},
  year = {1993},
  month = dec,
  journal = {Journal of the American Statistical Association},
  volume = {88},
  number = {424},
  pages = {1273--1283},
  issn = {0162-1459, 1537-274X},
  doi = {10.1080/01621459.1993.10476408},
  urldate = {2025-09-02},
  langid = {english}
}


@article{antonakis2021ignoring,
	author = {Antonakis, John and Bastardoz, Nicolas and R{\"o}nkk{\"o}, Mikko},
	date-added = {2025-08-13 09:51:08 -0700},
	date-modified = {2025-08-13 09:51:08 -0700},
	journal = {Organizational Research Methods},
	number = {2},
	pages = {443--483},
	publisher = {Sage Publications Sage CA: Los Angeles, CA},
	title = {On ignoring the random effects assumption in multilevel models: Review, critique, and recommendations},
	volume = {24},
	year = {2021}}

@article{little2013praise,
	author = {Little, Roderick J},
	date-added = {2025-08-13 08:54:18 -0700},
	date-modified = {2025-08-13 08:54:18 -0700},
	journal = {Journal of the American Statistical Association},
	number = {502},
	pages = {359--369},
	publisher = {Taylor \& Francis},
	title = {In praise of simplicity not mathematistry! Ten simple powerful ideas for the statistical scientist},
	volume = {108},
	year = {2013}}

@article{enders2023simple,
	author = {Enders, Craig K and Keller, Brian T and Woller, Michael P},
	date-added = {2025-07-31 11:41:41 -0700},
	date-modified = {2025-07-31 11:41:41 -0700},
	journal = {Psychological Methods},
	publisher = {American Psychological Association},
	title = {A simple Monte Carlo method for estimating power in multilevel designs.},
	year = {2023}}

@article{freedman2008regression,
	author = {Freedman, David A},
	date-added = {2025-07-31 08:16:18 -0700},
	date-modified = {2025-07-31 08:16:18 -0700},
	journal = {Advances in Applied Mathematics},
	number = {2},
	pages = {180--193},
	publisher = {Elsevier},
	title = {On regression adjustments to experimental data},
	volume = {40},
	year = {2008}}

@article{lin2013agnostic,
	author = {Lin, Winston},
	date-added = {2025-07-31 08:15:29 -0700},
	date-modified = {2025-07-31 08:15:29 -0700},
	journal = {The Annals of Applied Statistics},
	pages = {295--318},
	publisher = {JSTOR},
	title = {Agnostic notes on regression adjustments to experimental data: Reexamining Freedman's critique},
	year = {2013}}

@article{lee2023comparing,
	author = {Lee, Young Ri and Pustejovsky, James E},
	date-added = {2025-06-19 09:21:53 -0700},
	date-modified = {2025-06-19 09:21:53 -0700},
	journal = {Psychological Methods},
	publisher = {American Psychological Association},
	title = {Comparing random effects models, ordinary least squares, or fixed effects with cluster robust standard errors for cross-classified data.},
	year = {2023}}

@article{pustejovsky2015four,
	author = {Pustejovsky, James E and Swan, Daniel M},
	date-added = {2025-05-22 14:44:03 -0400},
	date-modified = {2025-05-22 14:44:03 -0400},
	journal = {Multivariate Behavioral Research},
	number = {3},
	pages = {365--380},
	publisher = {Taylor \& Francis},
	title = {Four methods for analyzing partial interval recording data, with application to single-case research},
	volume = {50},
	year = {2015}}

@article{tipton2015small,
	author = {Tipton, Elizabeth and Pustejovsky, James E},
	date-added = {2025-05-22 14:42:36 -0400},
	date-modified = {2025-05-22 14:42:36 -0400},
	journal = {Journal of Educational and Behavioral Statistics},
	number = {6},
	pages = {604--634},
	publisher = {Sage Publications Sage CA: Los Angeles, CA},
	title = {Small-sample adjustments for tests of moderators and model fit using robust variance estimation in meta-regression},
	volume = {40},
	year = {2015}}

@article{pustejovsky2014converting,
	author = {Pustejovsky, James E},
	date-added = {2025-05-22 14:33:07 -0400},
	date-modified = {2025-05-22 14:33:07 -0400},
	journal = {Psychological Methods},
	number = {1},
	pages = {92},
	publisher = {American Psychological Association},
	title = {Converting from d to r to z when the design uses extreme groups, dichotomization, or experimental control.},
	volume = {19},
	year = {2014}}

@book{tufte1983visual,
	author = {Tufte, Edward R and Graves-Morris, Peter R},
	date-added = {2025-05-22 10:16:50 -0400},
	date-modified = {2025-05-22 10:16:50 -0400},
	number = {9},
	publisher = {Graphics press Cheshire, CT},
	title = {The visual display of quantitative information},
	volume = {2},
	year = {1983}}

@article{pashley2024improving,
	author = {Pashley, Nicole E and Keele, Luke and Miratrix, Luke W},
	date-added = {2025-06-19 13:54:19 -0700},
	date-modified = {2025-06-19 13:54:19 -0700},
	journal = {Journal of the Royal Statistical Society Series A: Statistics in Society},
	pages = {qnae073},
	publisher = {Oxford University Press UK},
	title = {Improving instrumental variable estimators with poststratification},
	year = {2024}}

@article{boos2015Assessing,
	author = {Boos, Dennis D. and Osborne, Jason A.},
	doi = {10.1111/insr.12087},
	journaltitle = {International Statistical Review},
	number = {2},
	pages = {228--238},
	title = {Assessing Variability of Complex Descriptive Statistics in {{Monte Carlo}} Studies Using Resampling Methods},
	volume = {83},
	year = {2015},
}

@article{boulesteix2020Replication,
	author = {Boulesteix, Anne-Laure and Hoffmann, Sabine and Charlton, Alethea and Seibold, Heidi},
	doi = {10.1111/1740-9713.01444},
	journaltitle = {Significance},
	number = {5},
	pages = {18--21},
	title = {A Replication Crisis in Methodological Research?},
	volume = {17},
	year = {2020},
}

@article{boulesteix2013Plea,
	author = {Boulesteix, Anne-Laure and Lauer, Sabine and Eugster, Manuel J. A.},
	doi = {10.1371/journal.pone.0061562},
	journaltitle = {PLOS ONE},
	number = {4},
	pages = {e61562},
	title = {A Plea for Neutral Comparison Studies in Computational Sciences},
	volume = {8},
	year = {2013},
}

@article{boulesteix2017evidencebased,
	abstract = {The goal of medical research is to develop interventions that are in some sense superior, with respect to patient outcome, to interventions currently in use. Similarly, the goal of research in methodological computational statistics is to develop data analysis tools that are themselves superior to the existing tools. The methodology of the evaluation of medical interventions continues to be discussed extensively in the literature and it is now well accepted that medicine should be at least partly ``evidence-based''. Although we statisticians are convinced of the importance of unbiased, well-thought-out study designs and evidence-based approaches in the context of clinical research, we tend to ignore these principles when designing our own studies for evaluating statistical methods in the context of our methodological research. In this paper, we draw an analogy between clinical trials and real-data-based benchmarking experiments in methodological statistical science, with datasets playing the role of patients and methods playing the role of medical interventions. Through this analogy, we suggest directions for improvement in the design and interpretation of studies which use real data to evaluate statistical methods, in particular with respect to dataset inclusion criteria and the reduction of various forms of bias. More generally, we discuss the concept of ``evidence-based'' statistical research, its limitations and its impact on the design and interpretation of real-data-based benchmark experiments. We suggest that benchmark studies---a method of assessment of statistical methods using real-world datasets---might benefit from adopting (some) concepts from evidence-based medicine towards the goal of more evidence-based statistical research.},
	author = {Boulesteix, Anne-Laure and Wilson, Rory and Hapfelmeier, Alexander},
	doi = {10.1186/s12874-017-0417-2},
	issue = {1},
	journaltitle = {BMC Medical Research Methodology},
	number = {1},
	pages = {1--12},
	title = {Towards Evidence-Based Computational Statistics: Lessons from Clinical Research on the Role and Design of Real-Data Benchmark Studies},
	volume = {17},
	year = {2017},
}

@book{borenstein2021introduction,
	address = {Chichester, UK},
	author = {Borenstein, Michael and Hedges, Larry V. and Higgins, Julian P.T. and Rothstein, Hannah R.},
	edition = {3},
	isbn = {978-1-119-55437-7},
	publisher = {John Wiley \& Sons},
	title = {Introduction to Meta-Analysis},
	year = {2021}}

@article{Cho2023bivariate,
	abstract = {The zero-inflated negative binomial distribution has been widely used for count data analyses in various biomedical settings due to its capacity of modeling excess zeros and overdispersion. When there are correlated count variables, a bivariate model is essential for understanding their full distributional features. Examples include measuring correlation of two genes in sparse single-cell RNA sequencing data and modeling dental caries count indices on two different tooth surface types. For these purposes, we develop a richly parametrized bivariate zero-inflated negative binomial model that has a simple latent variable framework and eight free parameters with intuitive interpretations. In the scRNA-seq data example, the correlation is estimated after adjusting for the effects of dropout events represented by excess zeros. In the dental caries data, we analyze how the treatment with Xylitol lozenges affects the marginal mean and other patterns of response manifested in the two dental caries traits. An R package ``bzinb'' is available on Comprehensive R Archive Network.},
	author = {Cho, Hunyong and Liu, Chuwen and Preisser, John S and Wu, Di},
	doi = {10.1177/09622802231172028},
	journaltitle = {Statistical Methods in Medical Research},
	number = {7},
	pages = {1300--1317},
	title = {A Bivariate Zero-Inflated Negative Binomial Model and Its Applications to Biomedical Settings},
	volume = {32},
	year = {2023},
}

@article{gilbert2024multilevel,
	author = {Gilbert, Joshua and Miratrix, Luke},
	date-added = {2025-03-18 10:52:45 -0400},
	date-modified = {2025-03-18 10:52:45 -0400},
	journal = {arXiv preprint arXiv:2401.07294},
	title = {Multilevel Metamodels: A Novel Approach to Enhance Efficiency and Generalizability in Monte Carlo Simulation Studies},
	year = {2024}}

@article{friedman1988metamodel,
	author = {Friedman, Linda Weiser and Pressman, Israel},
	date-added = {2025-03-18 10:52:11 -0400},
	date-modified = {2025-03-18 10:52:11 -0400},
	journal = {Journal of the Operational Research Society},
	number = {10},
	pages = {939--948},
	publisher = {Taylor \& Francis},
	title = {The metamodel in simulation analysis: Can it be trusted?},
	volume = {39},
	year = {1988}}

@article{kleijnen1981regression,
	author = {Kleijnen, Jack PC},
	date-added = {2025-03-18 10:50:05 -0400},
	date-modified = {2025-03-18 10:50:05 -0400},
	journal = {Journal of the Operational Research Society},
	number = {1},
	pages = {35--43},
	publisher = {Taylor \& Francis},
	title = {Regression analysis for simulation practitioners},
	volume = {32},
	year = {1981}}

@article{gelman2002let,
	author = {Gelman, Andrew and Pasarica, Cristian and Dodhia, Rahul},
	date-added = {2025-02-20 11:37:52 -0500},
	date-modified = {2025-02-20 11:37:52 -0500},
	journal = {The American Statistician},
	number = {2},
	pages = {121--130},
	publisher = {Taylor \& Francis},
	title = {Let's practice what we preach: turning tables into graphs},
	volume = {56},
	year = {2002}}

@article{hunter2023PowerMultiplicityProject,
	abstract = {&amp;lt;p&amp;gt;For randomized controlled trials (RCTs) with a single intervention's impact being measured on multiple outcomes, researchers often apply a multiple testing procedure (such as Bonferroni or Benjamini-Hochberg) to adjust p values. Such an adjustment reduces the likelihood of spurious findings, but also changes the statistical power, sometimes substantially. A reduction in power means a reduction in the probability of detecting effects when they do exist. This consideration is frequently ignored in typical power analyses, as existing tools do not easily accommodate the use of multiple testing procedures. We introduce the PUMP (Power Under Multiplicity Project) R package as a tool for analysts to estimate statistical power, minimum detectable effect size, and sample size requirements for multi-level RCTs with multiple outcomes. PUMP uses a simulation-based approach to flexibly estimate power for a wide variety of experimental designs, number of outcomes, multiple testing procedures, and other user choices. By assuming linear mixed effects models, we can draw directly from the joint distribution of test statistics across outcomes and thus estimate power via simulation. One of PUMP's main innovations is accommodating multiple outcomes, which are accounted for in two ways. First, power estimates from PUMP properly account for the adjustment in p values from applying a multiple testing procedure. Second, when considering multiple outcomes rather than a single outcome, different definitions of statistical power emerge. PUMP allows researchers to consider a variety of definitions of power in order to choose the most appropriate types of power for the goals of their study. The package supports a variety of commonly used frequentist multi-level RCT designs and linear mixed effects models. In addition to the main functionality of estimating power, minimum detectable effect size, and sample size requirements, the package allows the user to easily explore sensitivity of these quantities to changes in underlying assumptions.&amp;lt;/p&amp;gt;},
	author = {Hunter, Kristen B. and Miratrix, Luke and Porter, Kristin},
	date-added = {2024-07-15 10:33:03 -0700},
	date-modified = {2024-07-15 10:33:19 -0700},
	doi = {10.18637/jss.v108.i06},
	journal = {Journal of Statistical Software},
	number = {6},
	pages = {1--43},
	title = {PUMP: Estimating Power, Minimum Detectable Effect Size, and Sample Size When Adjusting for Multiple Outcomes in Multi-Level Experiments},
	url = {https://www.jstatsoft.org/index.php/jss/article/view/v108i06},
	volume = {108},
	year = {2024},
	bdsk-url-1 = {https://www.jstatsoft.org/index.php/jss/article/view/v108i06},
	bdsk-url-2 = {https://doi.org/10.18637/jss.v108.i06}}

@article{lehmann1975statistical,
	author = {Lehmann, Erich Leo and others},
	date-added = {2024-07-03 09:15:58 -0400},
	date-modified = {2024-07-03 09:15:58 -0400},
	journal = {Nonparametrics. San Francisco, CA, Holden-Day},
	publisher = {Springer},
	title = {Statistical methods based on ranks},
	volume = {2},
	year = {1975}}

@book{good2013permutation,
	author = {Good, Phillip},
	date-added = {2024-07-03 09:15:30 -0400},
	date-modified = {2024-07-03 09:15:30 -0400},
	publisher = {Springer Science \& Business Media},
	title = {Permutation tests: a practical guide to resampling methods for testing hypotheses},
	year = {2013}}

@article{efron2000BootstrapModernStatistics,
	author = {Efron, Bradley},
	doi = {10.2307/2669773},
	eprint = {2669773},
	eprinttype = {jstor},
	file = {C:\Users\jamespustejovsky\Zotero\storage\94U6CHMR\Efron - 2000 - The Bootstrap and Modern Statistics.pdf},
	issn = {0162-1459},
	journal = {Journal of the American Statistical Association},
	number = {452},
	pages = {1293--1296},
	publisher = {[American Statistical Association, Taylor \& Francis, Ltd.]},
	title = {The Bootstrap and Modern Statistics},
	urldate = {2024-07-04},
	volume = {95},
	year = {2000},
	bdsk-url-1 = {https://doi.org/10.2307/2669773}}

@article{Wickham2014tidydata,
	abstract = {A huge amount of effort is spent cleaning data to get it ready for analysis, but there has been little research on how to make data cleaning as easy and effective as possible. This paper tackles a small, but important, component of data cleaning: data tidying. Tidy datasets are easy to manipulate, model and visualize, and have a specific structure: each variable is a column, each observation is a row, and each type of observational unit is a table. This framework makes it easy to tidy messy datasets because only a small set of tools are needed to deal with a wide range of un-tidy datasets. This structure also makes it easier to develop tidy tools for data analysis, tools that both input and output tidy datasets. The advantages of a consistent data structure and matching tools are demonstrated with a case study free from mundane data manipulation chores.},
	author = {Wickham, Hadley},
	doi = {10.18637/jss.v059.i10},
	journal = {Journal of Statistical Software},
	number = {10},
	pages = {1--23},
	title = {Tidy Data},
	url = {https://www.jstatsoft.org/index.php/jss/article/view/v059i10},
	volume = {59},
	year = {2014},
	bdsk-url-1 = {https://www.jstatsoft.org/index.php/jss/article/view/v059i10},
	bdsk-url-2 = {https://doi.org/10.18637/jss.v059.i10}}

@book{westfall2013understanding,
	author = {Westfall, Peter H and Henning, Kevin SS},
	date-added = {2024-06-12 14:12:32 -0400},
	date-modified = {2024-06-12 14:12:32 -0400},
	publisher = {CRC Press Boca Raton, FL},
	title = {Understanding advanced statistical methods},
	volume = {543},
	year = {2013}}

@article{Bloom2016using,
	abstract = {{The present article considers a fundamental question in evaluation research: ``By how much do program effects vary across sites?'' The article first presents a theoretical model of cross-site impact variation and a related estimation model with a random treatment coefficient and fixed site-specific intercepts. This approach eliminates several biases that can arise from unbalanced sample designs for multisite randomized trials. The article then describes how the approach operates, explores its assumptions, and applies the approach to data from three large welfare-to-work trials. The article also illustrates how to report cross-site impact findings and presents diagnostics for assessing these findings. To keep the article manageable, it focuses on experimental estimates of effects of program assignment (effects of intent to treat), although the ideas presented can be extended to analyses of multisite quasi-experiments and experimental estimates of effects of program participation (complier average causal effects).}},
	author = {Bloom, Howard S. and Raudenbush, Stephen W. and Weiss, Michael J. and Porter, Kristin},
	doi = {10.1080/19345747.2016.1264518},
	journal = {Journal of Research on Educational Effectiveness},
	local-url = {file://localhost/Users/lmiratrix/Documents/Papers%20Library/Bloom/Journal%20of%20Research%20on%20Educational%20Effectiveness_Porter_1.pdf},
	month = {11},
	number = {4},
	pages = {0--0},
	title = {{Using Multisite Experiments to Study Cross-Site Variation in Treatment Effects: A Hybrid Approach With Fixed Intercepts and a Random Treatment Coefficient}},
	volume = {10},
	year = {2016},
}

@article{brown1974SmallSampleBehavior,
	author = {Brown, Morton B. and Forsythe, Alan B.},
	doi = {10.1080/00401706.1974.10489158},
	journal = {Technometrics},
	langid = {english},
	month = feb,
	number = {1},
	pages = {129--132},
	title = {The {{Small Sample Behavior}} of {{Some Statistics Which Test}} the {{Equality}} of {{Several Means}}},
	volume = {16},
	year = {1974},
}

@article{james1951ComparisonSeveralGroups,
	author = {James, G. S.},
	doi = {10.2307/2332578},
	journal = {Biometrika},
	month = dec,
	number = {3/4},
	pages = {324},
	title = {The Comparison of Several Groups of Observations When the Ratios of the Population Variances Are Unknown},
	volume = {38},
	year = {1951},
	bdsk-url-1 = {https://doi.org/10.2307/2332578}}

@article{welch1951ComparisonSeveralMean,
	author = {Welch, B. L.},
	doi = {10.2307/2332579},
	journal = {Biometrika},
	month = dec,
	number = {3/4},
	pages = {330},
	title = {On the Comparison of Several Mean Values: {{An}} Alternative Approach},
	volume = {38},
	year = {1951},
}

@article{mehrotra1997ImprovingBrownforsytheSolution,
	abstract = {Over two decades ago, Brown and Forsythe (B-F) (1974) proposed an innovative solution to the problem of comparing independent normal means under heteroscedasticity. Since then, their testing procedure has gained in popularity and authors have published various articles in which the B-F test has formed the basis of their research. The purpose of this paper is to point out, and correct, a flaw in the B-F testing procedure. Specifically, it is shown that the approximation proposed by B-F for the null distribution of their test statistic is inadequate. An improved approximation is provided and the small sample null properties of the modified B-F test are studied via simulation. The empirical findings support the theoretical result that the modified B-F test does a better job of preserving the test size compared to the original B-F test.},
	author = {Mehrotra, Devan V.},
	doi = {10.1080/03610919708813431},
	journal = {Communications in Statistics - Simulation and Computation},
	keywords = {ANOVA,heteroscedasticity,Satterthwaite approximation},
	month = jan,
	number = {3},
	pages = {1139--1145},
	publisher = {Taylor \& Francis},
	title = {Improving the Brown-Forsythe Solution to the Generalized Behrens-Fisher Problem},
	volume = {26},
	year = {1997},
}

@article{Kern2014calibrated,
	abstract = {{Randomized experiments are considered the gold standard for causal inference because they can provide unbiased estimates of treatment effects for the experimental participants. However, researchers and policymakers are often interested in using a specific experiment to inform decisions about other target populations. In education research, increasing attention is being paid to the potential lack of generalizability of randomized experiments because the experimental participants may be unrepresentative of the target population of interest. This article examines whether generalization may be assisted by statistical methods that adjust for observed differences between the experimental participants and members of a target population. The methods examined include approaches that reweight the experimental data so that participants more closely resemble the target population and methods that utilize models of the outcome. Two simulation studies and one empirical analysis investigate and compare the methods' performance. One simulation uses purely simulated data while the other utilizes data from an evaluation of a school-based dropout prevention program. Our simulations suggest that machine learning methods outperform regression-based methods when the required structural (ignorability) assumptions are satisfied. When these assumptions are violated, all of the methods examined perform poorly. Our empirical analysis uses data from a multisite experiment to assess how well results from a given site predict impacts in other sites. Using a variety of extrapolation methods, predicted effects for each site are compared to actual benchmarks. Flexible modeling approaches perform best, although linear regression is not far behind. Taken together, these results suggest that flexible modeling techniques can aid generalization while underscoring the fact that even state-of-the-art statistical techniques still rely on strong assumptions.}},
	author = {Kern, Holger L. and Stuart, Elizabeth A. and Hill, Jennifer and Green, Donald P.},
	doi = {10.1080/19345747.2015.1060282},
	journal = {Journal of Research on Educational Effectiveness},
	month = {03},
	number = {1},
	pages = {103--127},
	title = {{Assessing Methods for Generalizing Experimental Impact Estimates to Target Populations}},
	volume = {9},
	year = {2014},
}

@article{White1980heteroskedasticity,
	author = {White, Halbert},
	journal = {Econometrica},
	number = {4},
	pages = {817--838},
	title = {A Heteroskedasticity-Consistent Covariance Matrix Estimator and a Direct Test for Heteroskedasticity},
	volume = {48},
	year = {1980}}

@article{dong2013PowerUpToolCalculating,
	author = {Dong, Nianbo and Maynard, Rebecca},
	doi = {10.1080/19345747.2012.673143},
	journal = {Journal of Research on Educational Effectiveness},
	langid = {english},
	month = jan,
	number = {1},
	pages = {24--67},
	title = {{{{\emph{PowerUp}}}}{\emph{!}} : {{A Tool}} for {{Calculating Minimum Detectable Effect Sizes}} and {{Minimum Required Sample Sizes}} for {{Experimental}} and {{Quasi-Experimental Design Studies}}},
	volume = {6},
	year = {2013},
}

@article{tipton2014stratified,
	abstract = { Background:An important question in the design of experiments is how to ensure that the findings from the experiment are generalizable to a larger population. This concern with generalizability is particularly important when treatment effects are heterogeneous and when selecting units into the experiment using random sampling is not possible---two conditions commonly met in large-scale educational experiments.Method:This article introduces a model-based balanced-sampling framework for improving generalizations, with a focus on developing methods that are robust to model misspecification. Additionally, the article provides a new method for sample selection within this framework: First units in an inference population are divided into relatively homogenous strata using cluster analysis, and then the sample is selected using distance rankings.Result:In order to demonstrate and evaluate the method, a reanalysis of a completed experiment is conducted. This example compares samples selected using the new method with the actual sample used in the experiment. Results indicate that even under high nonresponse, balance is better on most covariates and that fewer coverage errors result.Conclusion:The article concludes with a discussion of additional benefits and limitations of the method. },
	author = {Elizabeth Tipton},
	doi = {10.1177/0193841X13516324},
	journal = {Evaluation Review},
	number = {2},
	pages = {109-139},
	title = {Stratified Sampling Using Cluster Analysis: A Sample Selection Strategy for Improved Generalizations From Experiments},
	volume = {37},
	year = {2013},
}

@article{faul2009StatisticalPowerAnalyses,
	author = {Faul, Franz and Erdfelder, Edgar and Buchner, Axel and Lang, Albert-Georg},
	doi = {10.3758/BRM.41.4.1149},
	journal = {Behavior Research Methods},
	langid = {english},
	month = nov,
	number = {4},
	pages = {1149--1160},
	title = {Statistical Power Analyses Using {{G}}*{{Power}} 3.1: {{Tests}} for Correlation and Regression Analyses},
	volume = {41},
	year = {2009},
}

@article{longUsingHeteroscedasticityConsistent2000,
	abstract = {In the presence of heteroscedasticity, ordinary least squares (OLS) estimates are unbiased, but the usual tests of significance are generally inappropriate and their use can lead to incorrect inferences. Tests based on a heteroscedasticity consistent covariance matrix (HCCM), however, are consistent even in the presence of heteroscedasticity of an unknown form. Most applications that use a HCCM appear to rely on the asymptotic version known as HC0. Our Monte Carlo simulations show that HC0 often results in incorrect inferences when N {$\leq$} 250, while three relatively unknown, small sample versions of the HCCM, and especially a version known as HC3, work well even for N's as small as 25. We recommend that: (1) data analysts should correct for heteroscedasticity using a HCCM whenever there is reason to suspect heteroscedasticity; (2) the decision to use HCCM-based tests should not be determined by a screening test for heteroscedasticity; and (3) when N {$\leq$} 250, the HCCM known as HC3 should be used. Since HC3 is simple to compute, we encourage authors of statistical software to add this estimator to their programs.},
	author = {Long, J. Scott and Ervin, Laurie H.},
	doi = {10.1080/00031305.2000.10474549},
	journal = {The American Statistician},
	number = {3},
	pages = {217--224},
	publisher = {Taylor \& Francis},
	title = {Using Heteroscedasticity Consistent Standard Errors in the Linear Regression Model},
	volume = {54},
	year = {2000},
}

@book{GerberGreen,
	author = {Gerber, Alan S and Green, Donald P},
	publisher = {W. W. Norton \& Company},
	series = {W. W. Norton \& Company},
	title = {{Field Experiments: Design, Analysis, and Interpretation}},
	year = {2012}}

@article{sundberg2003conditional,
	author = {Sundberg, Rolf},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	month = {00},
	number = {1},
	pages = {299 -- 315},
	title = {{Conditional statistical inference and quantification of relevance}},
	volume = {65},
	year = {2003}}

@article{staiger2010searching,
	author = {Staiger, Douglas O and Rockoff, Jonah E},
	journal = {Journal of Economic perspectives},
	number = {3},
	pages = {97--118},
	title = {Searching for effective teachers with imperfect information},
	volume = {24},
	year = {2010}}

@article{abdulkadirouglu2017research,
	author = {Abdulkadiro{\u{g}}lu, Atila and Angrist, Joshua D and Narita, Yusuke and Pathak, Parag A},
	journal = {Econometrica},
	number = {5},
	pages = {1373--1432},
	title = {Research design meets market design: Using centralized assignment for impact evaluation},
	volume = {85},
	year = {2017}}

@misc{fryda2014H2oInterfaceH2O,
	abstract = {R interface for 'H2O', the scalable open source machine learning platform that offers parallelized implementations of many supervised and unsupervised machine learning algorithms such as Generalized Linear Models (GLM), Gradient Boosting Machines (including XGBoost), Random Forests, Deep Neural Networks (Deep Learning), Stacked Ensembles, Naive Bayes, Generalized Additive Models (GAM), ANOVA GLM, Cox Proportional Hazards, K-Means, PCA, ModelSelection, Word2Vec, as well as a fully automatic machine learning algorithm (H2O AutoML).},
	author = {Fryda, Tomas and LeDell, Erin and Gill, Navdeep and Aiello, Spencer and Fu, Anqi and Candel, Arno and Click, Cliff and Kraljevic, Tom and Nykodym, Tomas and Aboyoun, Patrick and Kurka, Michal and Malohlava, Michal and Poirier, Sebastien and Wong, Wendy},
	doi = {10.32614/CRAN.package.h2o},
	month = jun,
	pages = {3.44.0.3},
	publisher = {Comprehensive R Archive Network},
	shorttitle = {H2o},
	title = {H2o: {{R Interface}} for the '{{H2O}}' {{Scalable Machine Learning Platform}}},
	urldate = {2024-06-28},
	year = {2014},
	bdsk-url-1 = {https://doi.org/10.32614/CRAN.package.h2o}}

@book{xie2015,
	address = {Boca Raton, Florida},
	author = {Yihui Xie},
	edition = {2nd},
	note = {ISBN 978-1498716963},
	publisher = {Chapman and Hall/CRC},
	title = {Dynamic Documents with {R} and knitr},
	url = {http://yihui.name/knitr/},
	year = {2015},
}

@article{alfons2010ObjectOrientedFrameworkStatistical,
	author = {Alfons, Andreas and Templ, Matthias and Filzmoser, Peter},
	doi = {10.18637/jss.v037.i03},
	journal = {Journal of Statistical Software},
	keywords = {cited},
	number = {3},
	pages = {1--36},
	title = {An {{Object-Oriented Framework}} for {{Statistical Simulation}}: {{The R Package simFrame}}},
	volume = {37},
	year = {2010},
	bdsk-url-1 = {https://doi.org/10.18637/jss.v037.i03}}

@article{bien2016SimulatorEngineStreamline,
	archiveprefix = {arXiv},
	author = {Bien, Jacob},
	eprint = {1607.00021},
	journal = {arXiv:1607.00021},
	keywords = {cited},
	title = {The Simulator: {{An Engine}} to {{Streamline Simulations}}},
	year = {2016}}

@article{blair2019DeclaringDiagnosingResearch,
	abstract = {Researchers need to select high-quality research designs and communicate those designs clearly to readers. Both tasks are difficult. We provide a framework for formally ``declaring'' the analytically relevant features of a research design in a demonstrably complete manner, with applications to qualitative, quantitative, and mixed methods research. The approach to design declaration we describe requires defining a model of the world (M), an inquiry (I), a data strategy (D), and an answer strategy (A). Declaration of these features in code provides sufficient information for researchers and readers to use Monte Carlo techniques to diagnose properties such as power, bias, accuracy of qualitative causal inferences, and other ``diagnosands.'' Ex ante declarations can be used to improve designs and facilitate preregistration, analysis, and reconciliation of intended and actual analyses. Ex post declarations are useful for describing, sharing, reanalyzing, and critiquing existing designs. We provide open-source software, DeclareDesign, to implement the proposed approach.},
	author = {Blair, Graeme and Cooper, Jasper and Coppock, Alexander and Humphreys, Macartan},
	doi = {10.1017/S0003055419000194},
	journal = {American Political Science Review},
	keywords = {cited},
	langid = {english},
	month = aug,
	number = {3},
	pages = {838--859},
	publisher = {Cambridge University Press},
	title = {Declaring and {{Diagnosing Research Designs}}},
	urldate = {2024-01-01},
	volume = {113},
	year = {2019},
}

@book{blair2023ResearchDesignSocial,
	address = {Princeton},
	author = {Blair, Graeme and Coppock, Alexander and Humphreys, Macartan},
	keywords = {cited},
	publisher = {Princeton University Press},
	title = {Research {{Design}} in the {{Social Sciences}}: {{Declaration}}, {{Diagnosis}}, and {{Redesign}}},
	year = {2023}}

@article{boos2015AssessingVariabilityComplex,
	abstract = {SummaryGood statistical practice dictates that summaries in Monte Carlo studies should always be accompanied by standard errors. Those standard errors are easy to provide for summaries that are sample means over the replications of the Monte Carlo output: for example, bias estimates, power estimates for tests and mean squared error estimates. But often more complex summaries are of interest: medians (often displayed in boxplots), sample variances, ratios of sample variances and non-normality measures such as skewness and kurtosis. In principle, standard errors for most of these latter summaries may be derived from the Delta Method, but that extra step is often a barrier for standard errors to be provided. Here, we highlight the simplicity of using the jackknife and bootstrap to compute these standard errors, even when the summaries are somewhat complicated. {\copyright} 2014 The Authors. International Statistical Review {\copyright} 2014 International Statistical Institute},
	author = {Boos, Dennis D. and Osborne, Jason A.},
	doi = {10.1111/insr.12087},
	journal = {International Statistical Review},
	keywords = {Bootstrap,cited,coefficient of variation,delta method,influence curve,jackknife,standard errors,variability of ratios},
	langid = {english},
	number = {2},
	pages = {228--238},
	title = {Assessing {{Variability}} of {{Complex Descriptive Statistics}} in {{Monte Carlo Studies Using Resampling Methods}}},
	volume = {83},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.1111/insr.12087}}

@article{boulesteix2020IntroductionStatisticalSimulations,
	abstract = {In health research, statistical methods are frequently used to address a wide variety of research questions. For almost every analytical challenge, different methods are available. But how do we choose between different methods and how do we judge whether the chosen method is appropriate for our specific study? Like in any science, in statistics, experiments can be run to find out which methods should be used under which circumstances. The main objective of this paper is to demonstrate that simulation studies, that is, experiments investigating synthetic data with known properties, are an invaluable tool for addressing these questions. We aim to provide a first introduction to simulation studies for data analysts or, more generally, for researchers involved at different levels in the analyses of health data, who (1) may rely on simulation studies published in statistical literature to choose their statistical methods and who, thus, need to understand the criteria of assessing the validity and relevance of simulation results and their interpretation; and/or (2) need to understand the basic principles of designing statistical simulations in order to efficiently collaborate with more experienced colleagues or start learning to conduct their own simulations. We illustrate the implementation of a simulation study and the interpretation of its results through a simple example inspired by recent literature, which is completely reproducible using the R-script available from online supplemental file 1.},
	author = {Boulesteix, Anne-Laure and Groenwold, Rolf HH and Abrahamowicz, Michal and Binder, Harald and Briel, Matthias and Hornung, Roman and Morris, Tim P. and Rahnenf{\"u}hrer, J{\"o}rg and Sauerbrei, Willi},
	chapter = {Epidemiology},
	doi = {10.1136/bmjopen-2020-039921},
	journal = {BMJ Open},
	keywords = {cited,epidemiology,protocols & guidelines,statistics & research methods},
	langid = {english},
	month = dec,
	number = {12},
	pages = {e039921},
	pmid = {33318113},
	publisher = {British Medical Journal Publishing Group},
	title = {Introduction to Statistical Simulations in Health Research},
	volume = {10},
	year = {2020},
}

@misc{brown2023SimprFlexibleTidyverse,
	author = {Brown, Ethan},
	keywords = {cited},
	title = {Simpr: {{Flexible}} '{{Tidyverse}}'-{{Friendly Simulations}}},
	year = {2023}}

@book{carsey2013MonteCarloSimulation,
	author = {Carsey, Thomas M and Harden, Jeffrey J},
	keywords = {cited},
	publisher = {Sage Publications},
	title = {Monte {{Carlo Simulation}} and {{Resampling Methods}} for {{Social Science}}},
	year = {2013}}

@article{chalmers2020WritingEffectiveReliable,
	author = {Chalmers, R. Philip and Adkins, Mark C.},
	doi = {10.20982/tqmp.16.4.p248},
	journal = {The Quantitative Methods for Psychology},
	keywords = {cited},
	month = may,
	number = {4},
	pages = {248--280},
	title = {Writing {{Effective}} and {{Reliable Monte Carlo Simulations}} with the {{SimDesign Package}}},
	volume = {16},
	year = {2020},
}

@book{chang2010MonteCarloSimulation,
	abstract = {Helping you become a creative, logical thinker and skillful "simulator," Monte Carlo Simulation for the Pharmaceutical Industry: Concepts, Algorithms, and Case Studies provides broad coverage of the entire drug development process, from drug discovery to preclinical and clinical trial aspects to commercialization. It presents the theories and metho},
	author = {Chang, Mark},
	keywords = {cited,Mathematics / Probability & Statistics / General,Medical / Pharmacology},
	langid = {english},
	month = sep,
	publisher = {CRC Press},
	shorttitle = {Monte {{Carlo Simulation}} for the {{Pharmaceutical Industry}}},
	title = {Monte {{Carlo Simulation}} for the {{Pharmaceutical Industry}}: {{Concepts}}, {{Algorithms}}, and {{Case Studies}}},
	year = {2010}}

@article{claesen2021ComparingDreamReality,
	abstract = {Preregistration is a method to increase research transparency by documenting research decisions on a public, third-party repository prior to any influence by data. It is becoming increasingly popular in all subfields of psychology and beyond. Adherence to the preregistration plan may not always be feasible and even is not necessarily desirable, but without disclosure of deviations, readers who do not carefully consult the preregistration plan might get the incorrect impression that the study was exactly conducted and reported as planned. In this paper, we have investigated adherence and disclosure of deviations for all articles published with the Preregistered badge in Psychological Science between February 2015 and November 2017 and shared our findings with the corresponding authors for feedback. Two out of 27 preregistered studies contained no deviations from the preregistration plan. In one study, all deviations were disclosed. Nine studies disclosed none of the deviations. We mainly observed (un)disclosed deviations from the plan regarding the reported sample size, exclusion criteria and statistical analysis. This closer look at preregistrations of the first generation reveals possible hurdles for reporting preregistered studies and provides input for future reporting guidelines. We discuss the results and possible explanations, and provide recommendations for preregistered research.},
	author = {Claesen, Aline and Gomes, Sara and Tuerlinckx, Francis and Vanpaemel, Wolf},
	doi = {10.1098/rsos.211037},
	file = {C:\Users\jamespustejovsky\Zotero\storage\WX4BM5R7\Claesen et al. - 2021 - Comparing dream to reality an assessment of adher.pdf},
	journal = {Royal Society Open Science},
	keywords = {open science,preregistration,psychological science,researcher degrees of freedom,transparency},
	month = oct,
	number = {10},
	pages = {211037},
	publisher = {Royal Society},
	shorttitle = {Comparing Dream to Reality},
	title = {Comparing Dream to Reality: An Assessment of Adherence of the First Generation of Preregistered Studies},
	volume = {8},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1098/rsos.211037}}

@article{cruwell2023WhatBadgeComputational,
	abstract = {In April 2019, Psychological Science published its first issue in which all Research Articles received the Open Data badge. We used that issue to investigate the effectiveness of this badge, focusing on the adherence to its aim at Psychological Science: sharing both data and code to ensure reproducibility of results. Twelve researchers of varying experience levels attempted to reproduce the results of the empirical articles in the target issue (at least three researchers per article). We found that all 14 articles provided at least some data and six provided analysis code, but only one article was rated to be exactly reproducible, and three were rated as essentially reproducible with minor deviations. We suggest that researchers should be encouraged to adhere to the higher standard in force at Psychological Science. Moreover, a check of reproducibility during peer review may be preferable to the disclosure method of awarding badges.},
	author = {Cr{\"u}well, Sophia and Apthorp, Deborah and Baker, Bradley J. and Colling, Lincoln and Elson, Malte and Geiger, Sandra J. and Lobentanzer, Sebastian and Mon{\'e}ger, Jean and Patterson, Alex and Schwarzkopf, D. Samuel and Zaneva, Mirela and Brown, Nicholas J. L.},
	doi = {10.1177/09567976221140828},
	file = {C:\Users\jamespustejovsky\Zotero\storage\IQU35GF2\Cr{\"u}well et al. - 2023 - What's in a Badge A Computational Reproducibility.pdf},
	issn = {0956-7976},
	journal = {Psychological Science},
	langid = {english},
	month = apr,
	number = {4},
	pages = {512--522},
	publisher = {SAGE Publications Inc},
	title = {What's in a {{Badge}}? {{A Computational Reproducibility Investigation}} of the {{Open Data Badge Policy}} in {{One Issue}} of {{Psychological Science}}},
	volume = {34},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1177/09567976221140828}}

@book{davison1997BootstrapMethodsTheir,
	address = {Cambridge},
	author = {Davison, A. C. and Hinkley, D. V.},
	publisher = {Cambridge University Press},
	title = {Bootstrap Methods and Their Applications},
	year = {1997}}

@article{feiveson2002PowerSimulation,
	abstract = {This paper describes how to write Stata programs to estimate the power of virtually any statistical test that Stata can perform. Examples given include the t test, Poisson regression, Cox regression, and the nonparametric rank-sum test.},
	author = {Feiveson, A. H.},
	doi = {10.1177/1536867X0200200201},
	journal = {The Stata Journal},
	langid = {english},
	month = jun,
	number = {2},
	pages = {107--124},
	publisher = {SAGE Publications},
	title = {Power by {{Simulation}}},
	volume = {2},
	year = {2002},
}

@book{gamma1995DesignPatternsElements,
	abstract = {A book review of Design Patterns: Elements of Reusable Object-Oriented Software by Erich Gamma, Richard Helm, Ralph Johnson, and John Vlissides is presented.},
	address = {Reading, MA},
	author = {Gamma, Erich and Helm, Richard and Johnson, Ralph and Vlissides, John},
	chapter = {Books},
	isbn = {0-201-63361-2},
	keywords = {Design pattern,Design Patterns,Object-oriented programming},
	publisher = {Addison-Wesley Publishing Co.},
	title = {Design {{Patterns}}: {{Elements}} of {{Reusable Object-Oriented Software}}},
	urldate = {2024-01-05},
	year = {1995}}

@article{gasparini2018RsimsumSummariseResults,
	author = {Gasparini, Alessandro},
	doi = {10.21105/joss.00739},
	journal = {Journal of Open Source Software},
	keywords = {cited},
	number = {26},
	pages = {739},
	title = {Rsimsum: {{Summarise}} Results from {{Monte Carlo}} Simulation Studies},
	volume = {3},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.21105/joss.00739}}

@book{gelman2013BayesianDataAnalysis,
	author = {Gelman, Andrew and Carlin, John B. and Stern, Hal S. and Dunson, David B. and Vehtari, Aki and Rubin, Donald B.},
	doi = {10.1201/b16018},
	isbn = {978-0-429-11307-9},
	langid = {english},
	month = nov,
	publisher = {{Chapman and Hall/CRC}},
	title = {Bayesian {{Data Analysis}}},
	year = {2013},
}

@article{gelman2014StatisticalCrisisScience,
	abstract = {{$<$}em{$>$}Gale{$<$}/em{$>$} Academic OneFile includes The statistical crisis in science: data-dependent analy by Andrew Gelman and Eric Loken. Click to explore.},
	author = {Gelman, Andrew and Loken, Eric},
	journal = {American Scientist},
	langid = {english},
	month = nov,
	number = {6},
	pages = {460--466},
	publisher = {Sigma Xi, The Scientific Research Society},
	title = {The Statistical Crisis in Science: Data-Dependent Analysis--a \&quot;Garden of Forking Paths\&quot;--Explains Why Many Statistically Significant Comparisons Don't Hold Up},
	volume = {102},
	year = {2014}}

@article{goldfeld2020SimstudyIlluminatingResearch,
	author = {Goldfeld, Keith and {Wujciak-Jens}, Jacob},
	doi = {10.21105/joss.02763},
	journal = {Journal of Open Source Software},
	keywords = {cited},
	number = {54},
	pages = {2763},
	publisher = {The Open Journal},
	title = {Simstudy: {{Illuminating}} Research Methods through Data Generation},
	volume = {5},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.21105/joss.02763}}

@article{green2016SIMRPackagePower,
	abstract = {The r package simr allows users to calculate power for generalized linear mixed models from the lme4 package. The power calculations are based on Monte Carlo simulations. It includes tools for (i) running a power analysis for a given model and design; and (ii) calculating power curves to assess trade-offs between power and sample size. This paper presents a tutorial using a simple example of count data with mixed effects (with structure representative of environmental monitoring data) to guide the user along a gentle learning curve, adding only a few commands or options at a time.},
	author = {Green, Peter and MacLeod, Catriona J.},
	doi = {10.1111/2041-210X.12504},
	journal = {Methods in Ecology and Evolution},
	keywords = {cited,experimental design,glmm,Monte Carlo,random effects,sample size,type II error},
	langid = {english},
	number = {4},
	pages = {493--498},
	title = {{{SIMR}}: An {{R}} Package for Power Analysis of Generalized Linear Mixed Models by Simulation},
	volume = {7},
	year = {2016},
}

@article{hardwicke2023ReducingBiasIncreasing,
	abstract = {Flexibility in the design, analysis and interpretation of scientific studies creates a multiplicity of possible research outcomes. Scientists are granted considerable latitude to selectively use and report the hypotheses, variables and analyses that create the most positive, coherent and attractive story while suppressing those that are negative or inconvenient. This creates a risk of bias that can lead to scientists fooling themselves and fooling others. Preregistration involves declaring a research plan (for example, hypotheses, design and statistical analyses) in a public registry before the research outcomes are known. Preregistration (1) reduces the risk of bias by encouraging outcome-independent decision-making and (2) increases transparency, enabling others to assess the risk of bias and calibrate their confidence in research outcomes. In this Perspective, we briefly review the historical evolution of preregistration in medicine, psychology and other domains, clarify its pragmatic functions, discuss relevant meta-research, and provide recommendations for scientists and journal editors.},
	author = {Hardwicke, Tom E. and Wagenmakers, Eric-Jan},
	doi = {10.1038/s41562-022-01497-2},
	journal = {Nature Human Behaviour},
	keywords = {Science,Scientific community,technology and society},
	langid = {english},
	month = jan,
	number = {1},
	pages = {15--26},
	publisher = {Nature Publishing Group},
	title = {Reducing Bias, Increasing Transparency and Calibrating Confidence with Preregistration},
	volume = {7},
	year = {2023},
}

@article{harwell2018SurveyReportingPractices,
	abstract = {Computer simulation studies represent an important tool for investigating processes difficult or impossible to study using mathematical theory or real data. Hoaglin and Andrews recommended these studies be treated as statistical sampling experiments subject to established principles of design and data analysis, but the survey of Hauck and Anderson suggested these recommendations had, at that point in time, generally been ignored. We update the survey results of Hauck and Anderson using a sample of studies applying simulation methods in statistical research to assess the extent to which the recommendations of Hoaglin and Andrews and others for conducting simulation studies have been adopted. The important role of statistical applications of computer simulation studies in enhancing the reproducibility of scientific findings is also discussed. The results speak to the state of the art and the extent to which these studies are realizing their potential to inform statistical practice and a program of statistical research.},
	author = {Harwell, Michael and Kohli, Nidhi and {Peralta-Torres}, Yadira},
	doi = {10.1080/00031305.2017.1342692},
	journal = {The American Statistician},
	keywords = {Computer simulation,Design and data analysis,Survey},
	month = oct,
	number = {4},
	pages = {321--327},
	publisher = {Taylor \& Francis},
	title = {A {{Survey}} of {{Reporting Practices}} of {{Computer Simulation Studies}} in {{Statistical Research}}},
	volume = {72},
	year = {2018},
}

@article{hoogland1998RobustnessStudiesCovariance,
	abstract = {In covariance structure modeling, several estimation methods are available. The robustness of an estimator against specific violations of assumptions can be determined empirically by means of a Monte Carlo study. Many such studies in covariance structure analysis have been published, but the conclusions frequently seem to contradict each other. An overview of robustness studies in covariance structure analysis is given, and an attempt is made to generalize findings. Robustness studies are described and distinguished from each other systematically by means of certain characteristics. These characteristics serve as explanatory variables in a meta-analysis concerning the behavior of parameter estimators, standard error estimators, and goodness-of-fit statistics when the model is correctly specified.},
	author = {HOOGLAND, JEFFREY J. and BOOMSMA, {\relax ANNE}},
	doi = {10.1177/0049124198026003003},
	journal = {Sociological Methods \& Research},
	langid = {english},
	month = feb,
	number = {3},
	pages = {329--367},
	publisher = {SAGE Publications Inc},
	title = {Robustness {{Studies}} in {{Covariance Structure Modeling}}: {{An Overview}} and a {{Meta-Analysis}}},
	volume = {26},
	year = {1998},
}

@article{huang2016GeneralizedEstimatingEquations,
	abstract = {Background/aims:               Generalized estimating equations are a common modeling approach used in cluster randomized trials to account for within-cluster correlation. It is well known that the sandwich variance estimator is biased when the number of clusters is small ({$\leq$}40), resulting in an inflated type I error rate. Various bias correction methods have been proposed in the statistical literature, but how adequately they are utilized in current practice for cluster randomized trials is not clear. The aim of this study is to evaluate the use of generalized estimating equation bias correction methods in recently published cluster randomized trials and demonstrate the necessity of such methods when the number of clusters is small.                                         Methods:               Review of cluster randomized trials published between August 2013 and July 2014 and using generalized estimating equations for their primary analyses. Two independent reviewers collected data from each study using a standardized, pre-piloted data extraction template. A two-arm cluster randomized trial was simulated under various scenarios to show the potential effect of a small number of clusters on type I error rate when estimating the treatment effect. The nominal level was set at 0.05 for the simulation study.                                         Results:               Of the 51 included trials, 28 (54.9\%) analyzed 40 or fewer clusters with a minimum of four total clusters. Of these 28 trials, only one trial used a bias correction method for generalized estimating equations. The simulation study showed that with four clusters, the type I error rate ranged between 0.43 and 0.47. Even though type I error rate moved closer to the nominal level as the number of clusters increases, it still ranged between 0.06 and 0.07 with 40 clusters.                                         Conclusions:               Our results showed that statistical issues arising from small number of clusters in generalized estimating equations is currently inadequately handled in cluster randomized trials. Potential for type I error inflation could be very high when the sandwich estimator is used without bias correction.},
	author = {Huang, Shuang and Fiero, Mallorie H and Bell, Melanie L},
	doi = {10.1177/1740774516643498},
	journal = {Clinical Trials},
	langid = {english},
	month = aug,
	number = {4},
	pages = {445--449},
	shorttitle = {Generalized Estimating Equations in Cluster Randomized Trials with a Small Number of Clusters},
	title = {Generalized Estimating Equations in Cluster Randomized Trials with a Small Number of Clusters: {{Review}} of Practice and Simulation Study},
	volume = {13},
	year = {2016},
}

@article{hussey2007DesignAnalysisStepped,
	abstract = {Cluster randomized trials (CRT) are often used to evaluate therapies or interventions in situations where individual randomization is not possible or not desirable for logistic, financial or ethical reasons. While a significant and rapidly growing body of literature exists on CRTs utilizing a ``parallel'' design (i.e. I clusters randomized to each treatment), only a few examples of CRTs using crossover designs have been described. In this article we discuss the design and analysis of a particular type of crossover CRT -- the stepped wedge -- and provide an example of its use.},
	author = {Hussey, Michael A. and Hughes, James P.},
	doi = {10.1016/j.cct.2006.05.007},
	journal = {Contemporary Clinical Trials},
	keywords = {Cluster randomized trial,Prevention trials,Stepped wedge design},
	month = feb,
	number = {2},
	pages = {182--191},
	title = {Design and Analysis of Stepped Wedge Cluster Randomized Trials},
	volume = {28},
	year = {2007},
}

@book{jones2012IntroductionScientificProgramming,
	address = {New York},
	author = {Jones, Owen and Maillardet, Robert and Robinson, Andrew},
	doi = {10.1201/9781420068740},
	publisher = {{Chapman and Hall/CRC}},
	title = {Introduction to {{Scientific Programming}} and {{Simulation Using R}}},
	year = {2012},
}

@misc{joshi2022SimhelpersHelperFunctions,
	author = {Joshi, Megha and Pustejovsky, James E.},
	keywords = {cited},
	title = {Simhelpers: {{Helper Functions}} for {{Simulation Studies}}},
	year = {2022}}

@book{kalos2009MonteCarloMethods,
	abstract = {This introduction to Monte Carlo methods seeks to identify and study the unifying elements that underlie their effective application. Initial chapters provide a short treatment of the probability and statistics needed as background, enabling those without experience in Monte Carlo techniques to apply these ideas to their research. The book focuses on two basic themes: The first is the importance of random walks as they occur both in natural stochastic systems and in their relationship to integral and differential equations. The second theme is that of variance reduction in general and importance sampling in particular as a technique for efficient use of the methods. Random walks are introduced with an elementary example in which the modeling of radiation transport arises directly from a schematic probabilistic description of the interaction of radiation with matter. Building on this example, the relationship between random walks and integral equations is outlined. The applicability of these ideas to other problems is shown by a clear and elementary introduction to the solution of the Schrodinger equation by random walks. The text includes sample problems that readers can solve by themselves to illustrate the content of each chapter.  This is the second, completely revised and extended edition of the successful monograph, which brings the treatment up to date and incorporates the many advances in Monte Carlo techniques and their applications, while retaining the original elementary but general approach.},
	author = {Kalos, Malvin H. and Whitlock, Paula A.},
	isbn = {978-3-527-62622-9},
	keywords = {Science / Physics / General,Science / Physics / Mathematical & Computational},
	langid = {english},
	month = jun,
	publisher = {John Wiley \& Sons},
	title = {Monte {{Carlo Methods}}},
	year = {2009}}

@misc{kenny2023SimEngineModularFramework,
	author = {Kenny, Avi and Wolock, Charles},
	keywords = {cited},
	month = oct,
	title = {{{SimEngine}}: {{A Modular Framework}} for {{Statistical Simulations}} in {{R}}},
	year = {2023}}

@article{kern2016AssessingMethodsGeneralizing,
	abstract = {Randomized experiments are considered the gold standard for causal inference because they can provide unbiased estimates of treatment effects for the experimental participants. However, researchers and policymakers are often interested in using a specific experiment to inform decisions about other target populations. In education research, increasing attention is being paid to the potential lack of generalizability of randomized experiments because the experimental participants may be unrepresentative of the target population of interest. This article examines whether generalization may be assisted by statistical methods that adjust for observed differences between the experimental participants and members of a target population. The methods examined include approaches that reweight the experimental data so that participants more closely resemble the target population and methods that utilize models of the outcome. Two simulation studies and one empirical analysis investigate and compare the methods' performance. One simulation uses purely simulated data while the other utilizes data from an evaluation of a school-based dropout prevention program. Our simulations suggest that machine learning methods outperform regression-based methods when the required structural (ignorability) assumptions are satisfied. When these assumptions are violated, all of the methods examined perform poorly. Our empirical analysis uses data from a multisite experiment to assess how well results from a given site predict impacts in other sites. Using a variety of extrapolation methods, predicted effects for each site are compared to actual benchmarks. Flexible modeling approaches perform best, although linear regression is not far behind. Taken together, these results suggest that flexible modeling techniques can aid generalization while underscoring the fact that even state-of-the-art statistical techniques still rely on strong assumptions.},
	author = {Kern, Holger L. and Stuart, Elizabeth A. and Hill, Jennifer and Green, Donald P.},
	doi = {10.1080/19345747.2015.1060282},
	journal = {Journal of Research on Educational Effectiveness},
	keywords = {Bayesian Additive Regression Trees external validity generalizability propensity score weighting},
	month = jan,
	number = {1},
	pages = {103--127},
	title = {Assessing {{Methods}} for {{Generalizing Experimental Impact Estimates}} to {{Target Populations}}},
	volume = {9},
	year = {2016},
}

@article{koehler2009AssessmentMonteCarlo,
	abstract = {Statistical experiments, more commonly referred to as Monte Carlo or simulation studies, are used to study the behavior of statistical methods and measures under controlled situations. Whereas recent computing and methodological advances have permitted increased efficiency in the simulation process, known as variance reduction, such experiments remain limited by their finite nature and hence are subject to uncertainty; when a simulation is run more than once, different results are obtained. However, virtually no emphasis has been placed on reporting the uncertainty, referred to here as Monte Carlo error, associated with simulation results in the published literature, or on justifying the number of replications used. These deserve broader consideration. Here we present a series of simple and practical methods for estimating Monte Carlo error as well as determining the number of replications required to achieve a desired level of accuracy. The issues and methods are demonstrated with two simple examples, one evaluating operating characteristics of the maximum likelihood estimator for the parameters in logistic regression and the other in the context of using the bootstrap to obtain 95\% confidence intervals. The results suggest that in many settings, Monte Carlo error may be more substantial than traditionally thought.},
	author = {Koehler, Elizabeth and Brown, Elizabeth and Haneuse, Sebastien J.-P. A.},
	doi = {10.1198/tast.2009.0030},
	journal = {The American Statistician},
	keywords = {Bootstrap,cited,Jackknife,Replication},
	month = may,
	number = {2},
	pages = {155--162},
	publisher = {Taylor \& Francis},
	title = {On the {{Assessment}} of {{Monte Carlo Error}} in {{Simulation-Based Statistical Analyses}}},
	volume = {63},
	year = {2009},
}

@misc{leschinski2019MonteCarloAutomaticParallelized,
	author = {Leschinski, Christian Hendrik},
	keywords = {cited},
	month = jan,
	title = {{{MonteCarlo}}: {{Automatic Parallelized Monte Carlo Simulations}}},
	year = {2019}}

@article{leyrat2013PropensityScoresUsed,
	abstract = {Cluster randomized trials (CRTs) are often prone to selection bias despite randomization. Using a simulation study, we investigated the use of propensity score (PS) based methods in estimating treatment effects in CRTs with selection bias when the outcome is quantitative. Of four PS-based methods (adjustment on PS, inverse weighting, stratification, and optimal full matching method), three successfully corrected the bias, as did an approach using classical multivariable regression. However, they showed poorer statistical efficiency than classical methods, with higher standard error for the treatment effect, and type I error much smaller than the 5\% nominal level. Copyright {\copyright} 2013 John Wiley \& Sons, Ltd.},
	author = {Leyrat, C. and Caille, A. and Donner, A. and Giraudeau, B.},
	doi = {10.1002/sim.5795},
	journal = {Statistics in Medicine},
	keywords = {cluster randomized trial,Monte-Carlo simulations,propensity score,selection bias},
	langid = {english},
	number = {19},
	pages = {3357--3372},
	title = {Propensity Scores Used for Analysis of Cluster Randomized Trials with Selection Bias: A Simulation Study},
	volume = {32},
	year = {2013},
}

@article{lohmann2022ItTimeTen,
	abstract = {The quantitative analysis of research data is a core element of empirical research. The performance of statistical methods that are used for analyzing empirical data can be evaluated and compared using computer simulations. A single simulation study can influence the analyses of thousands of empirical studies to follow. With great power comes great responsibility. Here, we argue that this responsibility includes replication of simulation studies to ensure a sound foundation for data analytical decisions. Furthermore, being designed, run, and reported by humans, simulation studies face challenges similar to other experimental empirical research and hence should not be exempt from replication attempts. We highlight that the potential replicability of simulation studies is an opportunity quantitative methodology as a field should pay more attention to.},
	author = {Lohmann, Anna and Astivia, Oscar L. O. and Morris, Tim P. and Groenwold, Rolf H. H.},
	journal = {Frontiers in Epidemiology},
	title = {It's Time! {{Ten}} Reasons to Start Replicating Simulation Studies},
	volume = {2},
	year = {2022}}

@article{miratrix2021applied,
	author = {Miratrix, Luke W. and Weiss, Michael J. and Henderson, Brit},
	doi = {10.1080/19345747.2020.1831115},
	journal = {Journal of Research on Educational Effectiveness},
	langid = {english},
	month = jan,
	number = {1},
	pages = {270--308},
	title = {An {{Applied Researcher}}'s {{Guide}} to {{Estimating Effects}} from {{Multisite Individually Randomized Trials}}: {{Estimands}}, {{Estimators}}, and {{Estimates}}},
	volume = {14},
	year = {2021},
}

@book{miratrix2023DesigningMonteCarlo,
	author = {Miratrix, Luke W. and Pustejovsky, Jame E.},
	keywords = {cited},
	month = nov,
	title = {Designing {{Monte Carlo Simulations}} in {{R}}},
	year = {2023}}

@article{moerbeek2019WhatAreStatistical,
	abstract = {Subjects in randomized controlled trials do not always comply to the treatment condition they have been assigned to. This may cause the estimated effect of the intervention to be biased and also affect efficiency, coverage of confidence intervals, and statistical power. In cluster randomized trials non-compliance may occur at the subject level but also at the cluster level. In the latter case, all subjects within the same cluster have the same compliance status. The purpose of this study is to investigate the statistical implications of non-compliance in cluster randomized trials. A simulation study was conducted with varying degrees of non-compliance at either the cluster level or subject level. The probability of non-compliance depends on a covariate at the cluster or subject level. Various realistic values of the intraclass correlation coefficient and cluster size are used. The data are analyzed by intention to treat, as treated, per protocol and the instrumental variable approach. The results show non-compliance may result in downward biased estimates of the intervention effect and an under- or overestimate of its standard deviation. The coverage of the confidence intervals may be too small, and in most cases, empirical power is too small. The results are more severe when the probability of non-compliance increases and the covariate that affects compliance is unobserved. It is advocated to avoid non-compliance. If this is not possible, compliance status and covariates that affect compliance should be measured and included in the statistical model.},
	author = {Moerbeek, Mirjam and van Schie, Sander},
	doi = {10.1002/sim.8351},
	journal = {Statistics in Medicine},
	keywords = {cluster randomized trial,simulation study,treatment non-compliance},
	langid = {english},
	number = {26},
	pages = {5071--5084},
	title = {What Are the Statistical Implications of Treatment Non-Compliance in Cluster Randomized Trials: {{A}} Simulation Study},
	urldate = {2024-01-05},
	volume = {38},
	year = {2019},
}

@book{mooney1997MonteCarloSimulation,
	author = {Mooney, Christopher Z},
	keywords = {cited},
	number = {116},
	publisher = {Sage},
	title = {Monte {{Carlo Simulation}}},
	year = {1997}}

@article{morris2019UsingSimulationStudies,
	author = {Morris, Tim P. and White, Ian R. and Crowther, Michael J.},
	doi = {10.1002/sim.8086},
	journal = {Statistics in Medicine},
	keywords = {cited},
	langid = {english},
	month = jan,
	title = {Using Simulation Studies to Evaluate Statistical Methods},
	urldate = {2019-01-26},
	year = {2019},
}

@misc{nguyen2022MpowerPackagePower,
	abstract = {Estimating sample size and statistical power is an essential part of a good study design. This R package allows users to conduct power analysis based on Monte Carlo simulations in settings in which consideration of the correlations between predictors is important. It runs power analyses given a data generative model and an inference model. It can set up a data generative model that preserves dependence structures among variables given existing data (continuous, binary, or ordinal) or high-level descriptions of the associations. Users can generate power curves to assess the trade-offs between sample size, effect size, and power of a design. This paper presents tutorials and examples focusing on applications for environmental mixture studies when predictors tend to be moderately to highly correlated. It easily interfaces with several existing and newly developed analysis strategies for assessing associations between exposures and health outcomes. However, the package is sufficiently general to facilitate power simulations in a wide variety of settings.},
	author = {Nguyen, Phuc H. and Engel, Stephanie M. and Herring, Amy H.},
	howpublished = {https://arxiv.org/abs/2209.08036v1},
	journal = {arXiv.org},
	langid = {english},
	month = sep,
	shorttitle = {Mpower},
	title = {Mpower: {{An R Package}} for {{Power Analysis}} via {{Simulation}} for {{Correlated Data}}},
	year = {2022}}

@article{orcan2021MonteCarloSEMPackageSimulate,
	abstract = {Monte Carlo simulation is a useful tool for researchers to estimated accuracy of a statistical model. It is usually used for investigating parameter estimation procedure or violation of assumption for some given conditions. To run a simulation either the paid software or open source but free program such as R is need to be used. For that, researchers must have a good knowledge about the theoretical procedures. This paper introduces the R package called MonteCarloSEM. The package helps to simulate and analyze data sets for some simulation condition such as sample size and normality for a given model. Also, an example is given to show how the functions within the package works.},
	author = {Or{\c c}an, Fatih},
	journal = {International Journal of Assessment Tools in Education},
	keywords = {cited},
	langid = {english},
	month = sep,
	number = {3},
	pages = {704--713},
	title = {{{MonteCarloSEM}}: {{An R Package}} to {{Simulate Data}} for {{SEM}}},
	volume = {8},
	year = {2021}}

@article{paxton2001MonteCarloExperiments,
	abstract = {The use of Monte Carlo simulations for the empirical assessment of statistical estimators is becoming more common in structural equation modeling research. Yet, there is little guidance for the researcher interested in using the technique. In this article we illustrate both the design and implementation of Monte Carlo simulations. We present 9 steps in planning and performing a Monte Carlo analysis: (1) developing a theoretically derived research question of interest, (2) creating a valid model, (3) designing specific experimental conditions, (4) choosing values of population parameters, (5) choosing an appropriate software package, (6) executing the simulations, (7) file storage, (8) troubleshooting and verification, and (9) summarizing results. Throughout the article, we use as a running example a Monte Carlo simulation that we performed to illustrate many of the relevant points with concrete information and detail.},
	author = {Paxton, Pamela and Curran, Patrick J. and Bollen, Kenneth A. and Kirby, Jim and Chen, Feinian},
	doi = {10.1207/S15328007SEM0802_7},
	journal = {Structural Equation Modeling: A Multidisciplinary Journal},
	keywords = {cited},
	month = apr,
	number = {2},
	pages = {287--312},
	publisher = {Routledge},
	title = {Monte {{Carlo Experiments}}: {{Design}} and {{Implementation}}},
	volume = {8},
	year = {2001},
}

@book{robert2010IntroducingMonteCarlo,
	address = {New York, NY},
	author = {Robert, Christian and Casella, George},
	doi = {10.1007/978-1-4419-1576-4},
	isbn = {978-1-4419-1582-5 978-1-4419-1576-4},
	keywords = {bayesian statistics,Markov chain,Mathematica,Monte Carlo,Monte Carlo method,Random variable,simulation,STATISTICA},
	langid = {english},
	publisher = {Springer},
	title = {Introducing {{Monte Carlo Methods}} with {{R}}},
	year = {2010},
}

@misc{scheer2020SimToolConductSimulation,
	author = {Scheer, Marcel},
	keywords = {cited},
	month = sep,
	title = {{{simTool}}: {{Conduct Simulation Studies}} with a {{Minimal Amount}} of {{Source Code}}},
	year = {2020}}

@article{siepe2024SimulationStudiesMethodological,
	abstract = {Simulation studies are widely used for evaluating the performance of statistical methods in psychology. However, the quality of simulation studies can vary widely in terms of their design, execution, and reporting. In order to assess the quality of typical simulation studies in psychology, we reviewed 321 articles published in Psychological Methods, Behavioral Research Methods, and Multivariate Behavioral Research in 2021 and 2022, among which 100/321 = 31.2\% report a simulation study. We find that many articles do not provide complete and transparent information about key aspects of the study, such as justifications for the number of simulation repetitions, Monte Carlo uncertainty estimates, or code and data to reproduce the simulation studies. To address this problem, we provide a summary of the ADEMP (Aims, Data-generating mechanism, Estimands and other targets, Methods, Performance measures) design and reporting framework from Morris, White, and Crowther (2019) adapted to simulation studies in psychology. Based on this framework, we provide ADEMP-PreReg, a step-by-step template for researchers to use when designing, potentially preregistering, and reporting their simulation studies. We give formulae for estimating common performance measures, their Monte Carlo standard errors, and for calculating the number of simulation repetitions to achieve a desired Monte Carlo standard error. Finally, we give a detailed tutorial on how to apply the ADEMP framework in practice using an example simulation study on the evaluation of methods for the analysis of pre--post measurement experiments.},
	author = {Siepe, Bj{\"o}rn S. and Barto{\v s}, Franti{\v s}ek and Morris, Tim and Boulesteix, Anne-Laure and Heck, Daniel W. and Pawel, Samuel},
	doi = {10.31234/osf.io/ufgy6},
	keywords = {cited},
	langid = {american},
	month = jan,
	title = {Simulation Studies for Methodological Research in Psychology: A Standardized Template for Planning, Preregistration, and Reporting},
	year = {2024},
}

@article{sigal2016PlayItAgain,
	abstract = {Monte Carlo simulations (MCSs) provide important information about statistical phenomena that would be impossible to assess otherwise. This article introduces MCS methods and their applications to research and statistical pedagogy using a novel software package for the R Project for Statistical Computing constructed to lessen the often steep learning curve when organizing simulation code. A primary goal of this article is to demonstrate how well-suited MCS designs are to classroom demonstrations, and how they provide a hands-on method for students to become acquainted with complex statistical concepts. In this article, essential programming aspects for writing MCS code in R are overviewed, multiple applied examples with relevant code are provided, and the benefits of using a generate--analyze--summarize coding structure over the typical ``for-loop'' strategy are discussed.},
	author = {Sigal, Matthew J. and Chalmers, R. Philip},
	doi = {10.1080/10691898.2016.1246953},
	journal = {Journal of Statistics Education},
	keywords = {Active learning,R,Simulation,Statistical computing},
	month = sep,
	number = {3},
	pages = {136--156},
	publisher = {Taylor \& Francis},
	title = {Play {{It Again}}: {{Teaching Statistics With Monte Carlo Simulation}}},
	volume = {24},
	year = {2016},
}

@article{skrondal2000DesignAnalysisMonte,
	abstract = {The design and analysis of Monte Carlo experiments, with special reference to structural equation modelling, is discussed in this article. These topics merit consideration, since the validity of the conclusions drawn from a Monte Carlo study clearly hinges on these features. It is argued that comprehensive Monte Carlo experiments can be implemented on a PC if the experiments are adequately designed. This is especially important when investigating modern computer intensive methodologies like resampling and Markov Chain Monte Carlo methods. We are faced with three fundamental challenges in Monte Carlo experimentation. The first problem is statistical precision, which concerns the reliability of the obtained results. External validity, on the other hand, depends on the number of experimental conditions, and is crucial for the prospects of generalising the results beyond the specific experiment. Finally, we face the constraint on available computer resources. The conventional wisdom in designing and analysing Monte Carlo experiments embodies no explicit specification of meta-model for analysing the output of the experiment, the use of case studies or full factorial designs as experimental plans, no use of variance reduction techniques, a large number of replications, and "eyeballing" of the results. A critical examination of the conventional wisdom is presented in this article. We suggest that the following alternative procedures should be considered. First of all, we argue that it is profitable to specify explicit meta-models, relating the chosen performance statistics and experimental conditions. Regarding the experimental plan, we recommend the use of incomplete designs, which will often result in considerable savings. We also consider the use of common random numbers in the simulation phase, since this may enhance the precision in estimating meta-models. The use of fewer replications per trial, enabling us to investigate an increased number of experimental conditions, should also be considered in order to improve the external validity at the cost of the conventionally excessive precision.},
	author = {Skrondal, Anders},
	doi = {10.1207/S15327906MBR3502_1},
	journal = {Multivariate Behavioral Research},
	keywords = {cited},
	month = apr,
	number = {2},
	pages = {137--167},
	title = {Design and {{Analysis}} of {{Monte Carlo Experiments}}: {{Attacking}} the {{Conventional Wisdom}}},
	volume = {35},
	year = {2000},
}

@article{smith1973MonteCarloMethods,
	author = {Smith, Vincent Kerry},
	journal = {(No Title)},
	langid = {english},
	shorttitle = {Monte {{Carlo}} Methods},
	title = {Monte {{Carlo}} Methods : {{Their Role}} for {{Econometrics}}},
	year = {1973}}

@article{sofrygin2017SimcausalPackageConducting,
	author = {Sofrygin, Oleg and van der Laan, Mark J. and Neugebauer, Romain},
	doi = {10.18637/jss.v081.i02},
	journal = {Journal of Statistical Software},
	keywords = {cited},
	number = {2},
	pages = {1--47},
	title = {Simcausal {{R Package}}: {{Conducting Transparent}} and {{Reproducible Simulation Studies}} of {{Causal Effect Estimation}} with {{Complex Longitudinal Data}}},
	volume = {81},
	year = {2017},
}

@article{vevea1995general,
	author = {Vevea, Jack L and Hedges, Larry V},
	doi = {10.1007/BF02294384},
	journaltitle = {Psychometrika},
	number = {3},
	pages = {419--435},
	shortjournal = {Psychometrika},
	title = {A general linear model for estimating effect size in the presence of publication bias},
	volume = {60},
	year = {1995},
}

@article{white2023HowCheckSimulation,
	abstract = {Simulation studies are powerful tools in epidemiology and biostatistics, but they can be hard to conduct successfully. Sometimes unexpected results are obtained. We offer advice on how to check a simulation study when this occurs, and how to design and conduct the study to give results that are easier to check. Simulation studies should be designed to include some settings in which answers are already known. They should be coded in stages, with data-generating mechanisms checked before simulated data are analysed. Results should be explored carefully, with scatterplots of standard error estimates against point estimates surprisingly powerful tools. Failed estimation and outlying estimates should be identified and dealt with by changing data-generating mechanisms or coding realistic hybrid analysis procedures. Finally, we give a series of ideas that have been useful to us in the past for checking unexpected results. Following our advice may help to prevent errors and to improve the quality of published simulation studies.},
	author = {White, Ian R and Pham, Tra My and Quartagno, Matteo and Morris, Tim P},
	doi = {10.1093/ije/dyad134},
	journal = {International Journal of Epidemiology},
	keywords = {cited},
	month = oct,
	pages = {dyad134},
	title = {How to Check a Simulation Study},
	year = {2023},
}
