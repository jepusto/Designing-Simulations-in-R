%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Luke Miratrix at 2024-06-12 14:12:36 -0400 


%% Saved with string encoding Unicode (UTF-8) 


@article{Wickham2014tidydata,
 title={Tidy Data},
 volume={59},
 url={https://www.jstatsoft.org/index.php/jss/article/view/v059i10},
 doi={10.18637/jss.v059.i10},
 abstract={A huge amount of effort is spent cleaning data to get it ready for analysis, but there has been little research on how to make data cleaning as easy and effective as possible. This paper tackles a small, but important, component of data cleaning: data tidying. Tidy datasets are easy to manipulate, model and visualize, and have a specific structure: each variable is a column, each observation is a row, and each type of observational unit is a table. This framework makes it easy to tidy messy datasets because only a small set of tools are needed to deal with a wide range of un-tidy datasets. This structure also makes it easier to develop tidy tools for data analysis, tools that both input and output tidy datasets. The advantages of a consistent data structure and matching tools are demonstrated with a case study free from mundane data manipulation chores.},
 number={10},
 journal={Journal of Statistical Software},
 author={Wickham, Hadley},
 year={2014},
 pages={1–23}
}

@book{westfall2013understanding,
	author = {Westfall, Peter H and Henning, Kevin SS},
	date-added = {2024-06-12 14:12:32 -0400},
	date-modified = {2024-06-12 14:12:32 -0400},
	publisher = {CRC Press Boca Raton, FL},
	title = {Understanding advanced statistical methods},
	volume = {543},
	year = {2013}}

@article{Bloom:2016um,
	abstract = {{The present article considers a fundamental question in evaluation research: ``By how much do program effects vary across sites?'' The article first presents a theoretical model of cross-site impact variation and a related estimation model with a random treatment coefficient and fixed site-specific intercepts. This approach eliminates several biases that can arise from unbalanced sample designs for multisite randomized trials. The article then describes how the approach operates, explores its assumptions, and applies the approach to data from three large welfare-to-work trials. The article also illustrates how to report cross-site impact findings and presents diagnostics for assessing these findings. To keep the article manageable, it focuses on experimental estimates of effects of program assignment (effects of intent to treat), although the ideas presented can be extended to analyses of multisite quasi-experiments and experimental estimates of effects of program participation (complier average causal effects).}},
	author = {Bloom, Howard S. and Raudenbush, Stephen W. and Weiss, Michael J. and Porter, Kristin},
	date-added = {2024-06-11 06:55:53 -0400},
	date-modified = {2024-06-11 06:55:53 -0400},
	doi = {10.1080/19345747.2016.1264518},
	issn = {1934-5747},
	journal = {Journal of Research on Educational Effectiveness},
	keywords = {FIRC},
	local-url = {file://localhost/Users/lmiratrix/Documents/Papers%20Library/Bloom/Journal%20of%20Research%20on%20Educational%20Effectiveness_Porter_1.pdf},
	month = {11},
	number = {4},
	pages = {0--0},
	rating = {5},
	title = {{Using Multisite Experiments to Study Cross-Site Variation in Treatment Effects: A Hybrid Approach With Fixed Intercepts and a Random Treatment Coefficient}},
	url = {https://mail.google.com/mail/u/0/},
	volume = {10},
	year = {2016},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxBoLi4vLi4vLi4vLi4vRG9jdW1lbnRzL1BhcGVycyBMaWJyYXJ5L0Jsb29tL0pvdXJuYWwgb2YgUmVzZWFyY2ggb24gRWR1Y2F0aW9uYWwgRWZmZWN0aXZlbmVzc19Qb3J0ZXJfMS5wZGZPEQIwAAAAAAIwAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAADiX0oYQkQAAf////8fSm91cm5hbCBvZiBSZXNlYXJjI0ZGRkZGRkZGLnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA/////9TBzz4AAAAAAAAAAAAEAAQAAAogY3UAAAAAAAAAAAAAAAAABUJsb29tAAACAG4vOlVzZXJzOmxtaXJhdHJpeDpEb2N1bWVudHM6UGFwZXJzIExpYnJhcnk6Qmxvb206Sm91cm5hbCBvZiBSZXNlYXJjaCBvbiBFZHVjYXRpb25hbCBFZmZlY3RpdmVuZXNzX1BvcnRlcl8xLnBkZgAOAHwAPQBKAG8AdQByAG4AYQBsACAAbwBmACAAUgBlAHMAZQBhAHIAYwBoACAAbwBuACAARQBkAHUAYwBhAHQAaQBvAG4AYQBsACAARQBmAGYAZQBjAHQAaQB2AGUAbgBlAHMAcwBfAFAAbwByAHQAZQByAF8AMQAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIAbFVzZXJzL2xtaXJhdHJpeC9Eb2N1bWVudHMvUGFwZXJzIExpYnJhcnkvQmxvb20vSm91cm5hbCBvZiBSZXNlYXJjaCBvbiBFZHVjYXRpb25hbCBFZmZlY3RpdmVuZXNzX1BvcnRlcl8xLnBkZgATAAEvAAAVAAIAEP//AAAACAANABoAJACPAAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAAAsM=},
	bdsk-url-1 = {https://mail.google.com/mail/u/0/},
	bdsk-url-2 = {https://doi.org/10.1080/19345747.2016.1264518}}

@article{brown1974SmallSampleBehavior,
  title = {The {{Small Sample Behavior}} of {{Some Statistics Which Test}} the {{Equality}} of {{Several Means}}},
  author = {Brown, Morton B. and Forsythe, Alan B.},
  year = {1974},
  month = feb,
  journal = {Technometrics},
  volume = {16},
  number = {1},
  pages = {129--132},
  issn = {0040-1706, 1537-2723},
  doi = {10.1080/00401706.1974.10489158},
  urldate = {2024-06-27},
  langid = {english}
}

@article{mehrotra1997ImprovingBrownforsytheSolution,
  title = {Improving the Brown-Forsythe Solution to the Generalized Behrens-Fisher Problem},
  author = {Mehrotra, Devan V.},
  year = {1997},
  month = jan,
  journal = {Communications in Statistics - Simulation and Computation},
  volume = {26},
  number = {3},
  pages = {1139--1145},
  publisher = {Taylor \& Francis},
  issn = {0361-0918},
  doi = {10.1080/03610919708813431},
  urldate = {2024-06-27},
  abstract = {Over two decades ago, Brown and Forsythe (B-F) (1974) proposed an innovative solution to the problem of comparing independent normal means under heteroscedasticity. Since then, their testing procedure has gained in popularity and authors have published various articles in which the B-F test has formed the basis of their research. The purpose of this paper is to point out, and correct, a flaw in the B-F testing procedure. Specifically, it is shown that the approximation proposed by B-F for the null distribution of their test statistic is inadequate. An improved approximation is provided and the small sample null properties of the modified B-F test are studied via simulation. The empirical findings support the theoretical result that the modified B-F test does a better job of preserving the test size compared to the original B-F test.},
  keywords = {ANOVA,heteroscedasticity,Satterthwaite approximation}
}


@article{Kern_calibrated,
	abstract = {{Randomized experiments are considered the gold standard for causal inference because they can provide unbiased estimates of treatment effects for the experimental participants. However, researchers and policymakers are often interested in using a specific experiment to inform decisions about other target populations. In education research, increasing attention is being paid to the potential lack of generalizability of randomized experiments because the experimental participants may be unrepresentative of the target population of interest. This article examines whether generalization may be assisted by statistical methods that adjust for observed differences between the experimental participants and members of a target population. The methods examined include approaches that reweight the experimental data so that participants more closely resemble the target population and methods that utilize models of the outcome. Two simulation studies and one empirical analysis investigate and compare the methods' performance. One simulation uses purely simulated data while the other utilizes data from an evaluation of a school-based dropout prevention program. Our simulations suggest that machine learning methods outperform regression-based methods when the required structural (ignorability) assumptions are satisfied. When these assumptions are violated, all of the methods examined perform poorly. Our empirical analysis uses data from a multisite experiment to assess how well results from a given site predict impacts in other sites. Using a variety of extrapolation methods, predicted effects for each site are compared to actual benchmarks. Flexible modeling approaches perform best, although linear regression is not far behind. Taken together, these results suggest that flexible modeling techniques can aid generalization while underscoring the fact that even state-of-the-art statistical techniques still rely on strong assumptions.}},
	author = {Kern, Holger L. and Stuart, Elizabeth A. and Hill, Jennifer and Green, Donald P.},
	date-added = {2022-07-22 13:11:01 +0000},
	date-modified = {2022-07-22 13:11:09 +0000},
	doi = {10.1080/19345747.2015.1060282},
	issn = {1934-5747},
	journal = {Journal of Research on Educational Effectiveness},
	keywords = {generalizability},
	local-url = {file://localhost/Users/lmiratrix/Documents/Papers%20Library/Kern/Journal%20of%20Research%20on%20Educational%20Effectiveness_Green_1.pdf},
	month = {03},
	number = {1},
	pages = {103--127},
	pmid = {27668031},
	title = {{Assessing Methods for Generalizing Experimental Impact Estimates to Target Populations}},
	volume = {9},
	year = {2014},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxBmLi4vLi4vLi4vLi4vRG9jdW1lbnRzL1BhcGVycyBMaWJyYXJ5L0tlcm4vSm91cm5hbCBvZiBSZXNlYXJjaCBvbiBFZHVjYXRpb25hbCBFZmZlY3RpdmVuZXNzX0dyZWVuXzEucGRmTxECKAAAAAACKAACAAAMTWFjaW50b3NoIEhEAAAAAAAAAAAAAAAAAAAA4l9KGEJEAAH/////H0pvdXJuYWwgb2YgUmVzZWFyYyNGRkZGRkZGRi5wZGYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAP/////WfMv8AAAAAAAAAAAABAAEAAAKIGN1AAAAAAAAAAAAAAAAAARLZXJuAAIAbC86VXNlcnM6bG1pcmF0cml4OkRvY3VtZW50czpQYXBlcnMgTGlicmFyeTpLZXJuOkpvdXJuYWwgb2YgUmVzZWFyY2ggb24gRWR1Y2F0aW9uYWwgRWZmZWN0aXZlbmVzc19HcmVlbl8xLnBkZgAOAHoAPABKAG8AdQByAG4AYQBsACAAbwBmACAAUgBlAHMAZQBhAHIAYwBoACAAbwBuACAARQBkAHUAYwBhAHQAaQBvAG4AYQBsACAARQBmAGYAZQBjAHQAaQB2AGUAbgBlAHMAcwBfAEcAcgBlAGUAbgBfADEALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASAGpVc2Vycy9sbWlyYXRyaXgvRG9jdW1lbnRzL1BhcGVycyBMaWJyYXJ5L0tlcm4vSm91cm5hbCBvZiBSZXNlYXJjaCBvbiBFZHVjYXRpb25hbCBFZmZlY3RpdmVuZXNzX0dyZWVuXzEucGRmABMAAS8AABUAAgAQ//8AAAAIAA0AGgAkAI0AAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAACuQ==},
	bdsk-url-1 = {https://doi.org/10.1080/19345747.2015.1060282}}

@article{White1980heteroskedasticity,
  title = {A Heteroskedasticity-Consistent Covariance Matrix Estimator and a Direct Test for Heteroskedasticity},
  author = {White, Halbert},
  year = {1980},
  journal = {Econometrica},
  volume = {48},
  number = {4},
  pages = {817--838}
}

@article{dong2013PowerUpToolCalculating,
  title = {{{{\emph{PowerUp}}}}{\emph{!}} : {{A Tool}} for {{Calculating Minimum Detectable Effect Sizes}} and {{Minimum Required Sample Sizes}} for {{Experimental}} and {{Quasi-Experimental Design Studies}}},
  shorttitle = {{{{\emph{PowerUp}}}}{\emph{!}}},
  author = {Dong, Nianbo and Maynard, Rebecca},
  year = {2013},
  month = jan,
  journal = {Journal of Research on Educational Effectiveness},
  volume = {6},
  number = {1},
  pages = {24--67},
  issn = {1934-5747, 1934-5739},
  doi = {10.1080/19345747.2012.673143},
  urldate = {2024-06-27},
  langid = {english},
  file = {C:\Users\jamespustejovsky\Zotero\storage\KID6WKQ6\Dong and Maynard - 2013 - PowerUp!  A Tool for Calculating Minimum D.pdf}
}

@article{tipton2014stratified,
author = {Elizabeth Tipton},
title ={Stratified Sampling Using Cluster Analysis: A Sample Selection Strategy for Improved Generalizations From Experiments},

journal = {Evaluation Review},
volume = {37},
number = {2},
pages = {109-139},
year = {2013},
doi = {10.1177/0193841X13516324},
    note ={PMID: 24647924},

URL = { 
    
        https://doi.org/10.1177/0193841X13516324
    
    

},
eprint = { 
    
        https://doi.org/10.1177/0193841X13516324
    
    

}
,
    abstract = { Background:An important question in the design of experiments is how to ensure that the findings from the experiment are generalizable to a larger population. This concern with generalizability is particularly important when treatment effects are heterogeneous and when selecting units into the experiment using random sampling is not possible—two conditions commonly met in large-scale educational experiments.Method:This article introduces a model-based balanced-sampling framework for improving generalizations, with a focus on developing methods that are robust to model misspecification. Additionally, the article provides a new method for sample selection within this framework: First units in an inference population are divided into relatively homogenous strata using cluster analysis, and then the sample is selected using distance rankings.Result:In order to demonstrate and evaluate the method, a reanalysis of a completed experiment is conducted. This example compares samples selected using the new method with the actual sample used in the experiment. Results indicate that even under high nonresponse, balance is better on most covariates and that fewer coverage errors result.Conclusion:The article concludes with a discussion of additional benefits and limitations of the method. }
}

@article{faul2009StatisticalPowerAnalyses,
  title = {Statistical Power Analyses Using {{G}}*{{Power}} 3.1: {{Tests}} for Correlation and Regression Analyses},
  shorttitle = {Statistical Power Analyses Using {{G}}*{{Power}} 3.1},
  author = {Faul, Franz and Erdfelder, Edgar and Buchner, Axel and Lang, Albert-Georg},
  year = {2009},
  month = nov,
  journal = {Behavior Research Methods},
  volume = {41},
  number = {4},
  pages = {1149--1160},
  issn = {1554-351X, 1554-3528},
  doi = {10.3758/BRM.41.4.1149},
  urldate = {2024-06-27},
  copyright = {http://www.springer.com/tdm},
  langid = {english},
  file = {C:\Users\jamespustejovsky\Zotero\storage\PH3MJZ2H\Faul et al. - 2009 - Statistical power analyses using GPower 3.1 Test.pdf}
}




@article{longUsingHeteroscedasticityConsistent2000,
  title = {Using Heteroscedasticity Consistent Standard Errors in the Linear Regression Model},
  author = {Long, J. Scott and Ervin, Laurie H.},
  year = {2000},
  month = aug,
  journal = {The American Statistician},
  volume = {54},
  number = {3},
  pages = {217--224},
  publisher = {Taylor \& Francis},
  issn = {0003-1305},
  doi = {10.1080/00031305.2000.10474549},
  urldate = {2023-12-19},
  abstract = {In the presence of heteroscedasticity, ordinary least squares (OLS) estimates are unbiased, but the usual tests of significance are generally inappropriate and their use can lead to incorrect inferences. Tests based on a heteroscedasticity consistent covariance matrix (HCCM), however, are consistent even in the presence of heteroscedasticity of an unknown form. Most applications that use a HCCM appear to rely on the asymptotic version known as HC0. Our Monte Carlo simulations show that HC0 often results in incorrect inferences when N {$\leq$} 250, while three relatively unknown, small sample versions of the HCCM, and especially a version known as HC3, work well even for N's as small as 25. We recommend that: (1) data analysts should correct for heteroscedasticity using a HCCM whenever there is reason to suspect heteroscedasticity; (2) the decision to use HCCM-based tests should not be determined by a screening test for heteroscedasticity; and (3) when N {$\leq$} 250, the HCCM known as HC3 should be used. Since HC3 is simple to compute, we encourage authors of statistical software to add this estimator to their programs.},
  keywords = {Heteroscedasticity,Heteroscedasticity consistent covariance matrix},
  file = {C:\Users\jamespustejovsky\Zotero\storage\URPFHBJT\Long and Ervin - 2000 - Using Heteroscedasticity Consistent Standard Error.pdf}
}

@book{GerberGreen,
	author = {Gerber, Alan S and Green, Donald P},
	date-added = {2022-06-30 20:27:18 +0000},
	date-modified = {2022-07-03 14:19:03 +0000},
	publisher = {W. W. Norton \& Company},
	series = {W. W. Norton \& Company},
	title = {{Field Experiments: Design, Analysis, and Interpretation}},
	year = {2012}}

@article{sundberg2003conditional,
	author = {Sundberg, Rolf},
	date-added = {2022-06-30 20:25:30 +0000},
	date-modified = {2022-06-30 20:25:30 +0000},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	month = {00},
	number = {1},
	pages = {299 -- 315},
	title = {{Conditional statistical inference and quantification of relevance}},
	volume = {65},
	year = {2003}}

@article{staiger2010searching,
	author = {Staiger, Douglas O and Rockoff, Jonah E},
	date-added = {2022-06-28 21:30:20 +0000},
	date-modified = {2022-06-28 21:30:20 +0000},
	journal = {Journal of Economic perspectives},
	number = {3},
	pages = {97--118},
	title = {Searching for effective teachers with imperfect information},
	volume = {24},
	year = {2010}}

@article{abdulkadirouglu2017research,
	author = {Abdulkadiro{\u{g}}lu, Atila and Angrist, Joshua D and Narita, Yusuke and Pathak, Parag A},
	date-added = {2022-06-28 21:29:08 +0000},
	date-modified = {2022-06-28 21:29:08 +0000},
	journal = {Econometrica},
	number = {5},
	pages = {1373--1432},
	publisher = {Wiley Online Library},
	title = {Research design meets market design: Using centralized assignment for impact evaluation},
	volume = {85},
	year = {2017}}
	
@misc{fryda2014H2oInterfaceH2O,
  title = {H2o: {{R Interface}} for the '{{H2O}}' {{Scalable Machine Learning Platform}}},
  shorttitle = {H2o},
  author = {Fryda, Tomas and LeDell, Erin and Gill, Navdeep and Aiello, Spencer and Fu, Anqi and Candel, Arno and Click, Cliff and Kraljevic, Tom and Nykodym, Tomas and Aboyoun, Patrick and Kurka, Michal and Malohlava, Michal and Poirier, Sebastien and Wong, Wendy},
  year = {2014},
  month = jun,
  pages = {3.44.0.3},
  publisher = {Comprehensive R Archive Network},
  doi = {10.32614/CRAN.package.h2o},
  urldate = {2024-06-28},
  abstract = {R interface for 'H2O', the scalable open source machine learning platform that offers parallelized implementations of many supervised and unsupervised machine learning algorithms such as Generalized Linear Models (GLM), Gradient Boosting Machines (including XGBoost), Random Forests, Deep Neural Networks (Deep Learning), Stacked Ensembles, Naive Bayes, Generalized Additive Models (GAM), ANOVA GLM, Cox Proportional Hazards, K-Means, PCA, ModelSelection, Word2Vec, as well as a fully automatic machine learning algorithm (H2O AutoML).},
  langid = {english}
}


@article{miratrix2021applied,
	author = {Miratrix, Luke W and Weiss, Michael J and Henderson, Brit},
	date-added = {2022-06-28 21:25:41 +0000},
	date-modified = {2022-06-28 21:25:41 +0000},
	journal = {Journal of Research on Educational Effectiveness},
	number = {1},
	pages = {270--308},
	publisher = {Taylor \& Francis},
	title = {An applied researcher's guide to estimating effects from multisite individually randomized trials: Estimands, estimators, and estimates},
	volume = {14},
	year = {2021}}

@book{xie2015,
	address = {Boca Raton, Florida},
	author = {Yihui Xie},
	edition = {2nd},
	note = {ISBN 978-1498716963},
	publisher = {Chapman and Hall/CRC},
	title = {Dynamic Documents with {R} and knitr},
	url = {http://yihui.name/knitr/},
	year = {2015},
	bdsk-url-1 = {http://yihui.name/knitr/}}
	
@article{alfons2010ObjectOrientedFrameworkStatistical,
  title = {An {{Object-Oriented Framework}} for {{Statistical Simulation}}: {{The R Package simFrame}}},
  author = {Alfons, Andreas and Templ, Matthias and Filzmoser, Peter},
  year = {2010},
  journal = {Journal of Statistical Software},
  volume = {37},
  number = {3},
  pages = {1--36},
  doi = {10.18637/jss.v037.i03},
  keywords = {cited}
}

@article{bien2016SimulatorEngineStreamline,
  title = {The Simulator: {{An Engine}} to {{Streamline Simulations}}},
  author = {Bien, Jacob},
  year = {2016},
  journal = {arXiv:1607.00021},
  eprint = {1607.00021},
  archiveprefix = {arXiv},
  keywords = {cited}
}

@article{blair2019DeclaringDiagnosingResearch,
  title = {Declaring and {{Diagnosing Research Designs}}},
  author = {Blair, Graeme and Cooper, Jasper and Coppock, Alexander and Humphreys, Macartan},
  year = {2019},
  month = aug,
  journal = {American Political Science Review},
  volume = {113},
  number = {3},
  pages = {838--859},
  publisher = {Cambridge University Press},
  issn = {0003-0554, 1537-5943},
  doi = {10.1017/S0003055419000194},
  urldate = {2024-01-01},
  abstract = {Researchers need to select high-quality research designs and communicate those designs clearly to readers. Both tasks are difficult. We provide a framework for formally ``declaring'' the analytically relevant features of a research design in a demonstrably complete manner, with applications to qualitative, quantitative, and mixed methods research. The approach to design declaration we describe requires defining a model of the world (M), an inquiry (I), a data strategy (D), and an answer strategy (A). Declaration of these features in code provides sufficient information for researchers and readers to use Monte Carlo techniques to diagnose properties such as power, bias, accuracy of qualitative causal inferences, and other ``diagnosands.'' Ex ante declarations can be used to improve designs and facilitate preregistration, analysis, and reconciliation of intended and actual analyses. Ex post declarations are useful for describing, sharing, reanalyzing, and critiquing existing designs. We provide open-source software, DeclareDesign, to implement the proposed approach.},
  langid = {english},
  keywords = {cited},
  file = {C:\Users\jamespustejovsky\Zotero\storage\CIS3IBN8\Blair et al. - 2019 - Declaring and Diagnosing Research Designs.pdf}
}

@book{blair2023ResearchDesignSocial,
  title = {Research {{Design}} in the {{Social Sciences}}: {{Declaration}}, {{Diagnosis}}, and {{Redesign}}},
  author = {Blair, Graeme and Coppock, Alexander and Humphreys, Macartan},
  year = {2023},
  publisher = {Princeton University Press},
  address = {Princeton},
  keywords = {cited}
}

@article{boos2015AssessingVariabilityComplex,
  title = {Assessing {{Variability}} of {{Complex Descriptive Statistics}} in {{Monte Carlo Studies Using Resampling Methods}}},
  author = {Boos, Dennis D. and Osborne, Jason A.},
  year = {2015},
  journal = {International Statistical Review},
  volume = {83},
  number = {2},
  pages = {228--238},
  issn = {1751-5823},
  doi = {10.1111/insr.12087},
  urldate = {2024-01-01},
  abstract = {SummaryGood statistical practice dictates that summaries in Monte Carlo studies should always be accompanied by standard errors. Those standard errors are easy to provide for summaries that are sample means over the replications of the Monte Carlo output: for example, bias estimates, power estimates for tests and mean squared error estimates. But often more complex summaries are of interest: medians (often displayed in boxplots), sample variances, ratios of sample variances and non-normality measures such as skewness and kurtosis. In principle, standard errors for most of these latter summaries may be derived from the Delta Method, but that extra step is often a barrier for standard errors to be provided. Here, we highlight the simplicity of using the jackknife and bootstrap to compute these standard errors, even when the summaries are somewhat complicated. {\copyright} 2014 The Authors. International Statistical Review {\copyright} 2014 International Statistical Institute},
  copyright = {{\copyright}2014\,The Authors. International Statistical Review {\copyright} 2014\,International Statistical Institute},
  langid = {english},
  keywords = {Bootstrap,cited,coefficient of variation,delta method,influence curve,jackknife,standard errors,variability of ratios},
  file = {C\:\\Users\\jamespustejovsky\\Zotero\\storage\\CU9IPBXZ\\Boos and Osborne - 2015 - Assessing Variability of Complex Descriptive Stati.pdf;C\:\\Users\\jamespustejovsky\\Zotero\\storage\\T3ELDFPE\\insr.html}
}

@article{boulesteix2020IntroductionStatisticalSimulations,
  title = {Introduction to Statistical Simulations in Health Research},
  author = {Boulesteix, Anne-Laure and Groenwold, Rolf HH and Abrahamowicz, Michal and Binder, Harald and Briel, Matthias and Hornung, Roman and Morris, Tim P. and Rahnenf{\"u}hrer, J{\"o}rg and Sauerbrei, Willi},
  year = {2020},
  month = dec,
  journal = {BMJ Open},
  volume = {10},
  number = {12},
  pages = {e039921},
  publisher = {British Medical Journal Publishing Group},
  issn = {2044-6055, 2044-6055},
  doi = {10.1136/bmjopen-2020-039921},
  urldate = {2024-01-01},
  abstract = {In health research, statistical methods are frequently used to address a wide variety of research questions. For almost every analytical challenge, different methods are available. But how do we choose between different methods and how do we judge whether the chosen method is appropriate for our specific study? Like in any science, in statistics, experiments can be run to find out which methods should be used under which circumstances. The main objective of this paper is to demonstrate that simulation studies, that is, experiments investigating synthetic data with known properties, are an invaluable tool for addressing these questions. We aim to provide a first introduction to simulation studies for data analysts or, more generally, for researchers involved at different levels in the analyses of health data, who (1) may rely on simulation studies published in statistical literature to choose their statistical methods and who, thus, need to understand the criteria of assessing the validity and relevance of simulation results and their interpretation; and/or (2) need to understand the basic principles of designing statistical simulations in order to efficiently collaborate with more experienced colleagues or start learning to conduct their own simulations. We illustrate the implementation of a simulation study and the interpretation of its results through a simple example inspired by recent literature, which is completely reproducible using the R-script available from online supplemental file 1.},
  chapter = {Epidemiology},
  copyright = {{\copyright} Author(s) (or their employer(s)) 2020. Re-use permitted under CC BY-NC. No commercial re-use. See rights and permissions. Published by BMJ.. http://creativecommons.org/licenses/by-nc/4.0/This is an open access article distributed in accordance with the Creative Commons Attribution Non Commercial (CC BY-NC 4.0) license, which permits others to distribute, remix, adapt, build upon this work non-commercially, and license their derivative works on different terms, provided the original work is properly cited, appropriate credit is given, any changes made indicated, and the use is non-commercial. See:~http://creativecommons.org/licenses/by-nc/4.0/.},
  langid = {english},
  pmid = {33318113},
  keywords = {cited,epidemiology,protocols & guidelines,statistics & research methods},
  file = {C:\Users\jamespustejovsky\Zotero\storage\YC92XY9J\Boulesteix et al. - 2020 - Introduction to statistical simulations in health .pdf}
}

@article{boulesteix2020ReplicationCrisisMethodological,
  title = {A {{Replication Crisis}} in {{Methodological Research}}?},
  author = {Boulesteix, Anne-Laure and Hoffmann, Sabine and Charlton, Alethea and Seibold, Heidi},
  year = {2020},
  month = oct,
  journal = {Significance},
  volume = {17},
  number = {5},
  pages = {18--21},
  issn = {1740-9705},
  doi = {10.1111/1740-9713.01444},
  urldate = {2024-01-01},
  abstract = {Statisticians have been keen to critique statistical aspects of the ``replication crisis'' in other scientific disciplines. But new statistical tools are often published and promoted without any thought to replicability. This needs to change, argue Anne-Laure Boulesteix, Sabine Hoffmann, Alethea Charlton and Heidi Seibold},
  file = {C\:\\Users\\jamespustejovsky\\Zotero\\storage\\KBM9KZ63\\Boulesteix et al. - 2020 - A Replication Crisis in Methodological Research.pdf;C\:\\Users\\jamespustejovsky\\Zotero\\storage\\SS7MWV8I\\7038554.html}
}

@misc{brown2023SimprFlexibleTidyverse,
  title = {Simpr: {{Flexible}} '{{Tidyverse}}'-{{Friendly Simulations}}},
  author = {Brown, Ethan},
  year = {2023},
  keywords = {cited}
}

@book{carsey2013MonteCarloSimulation,
  title = {Monte {{Carlo Simulation}} and {{Resampling Methods}} for {{Social Science}}},
  author = {Carsey, Thomas M and Harden, Jeffrey J},
  year = {2013},
  publisher = {Sage Publications},
  keywords = {cited}
}

@article{chalmers2020WritingEffectiveReliable,
  title = {Writing {{Effective}} and {{Reliable Monte Carlo Simulations}} with the {{SimDesign Package}}},
  author = {Chalmers, R. Philip and Adkins, Mark C.},
  year = {2020},
  month = may,
  journal = {The Quantitative Methods for Psychology},
  volume = {16},
  number = {4},
  pages = {248--280},
  issn = {2292-1354},
  doi = {10.20982/tqmp.16.4.p248},
  urldate = {2024-01-01},
  keywords = {cited},
  file = {C:\Users\jamespustejovsky\Zotero\storage\FQG3FM6J\Chalmers and Adkins - 2020 - Writing Effective and Reliable Monte Carlo Simulat.pdf}
}

@book{chang2010MonteCarloSimulation,
  title = {Monte {{Carlo Simulation}} for the {{Pharmaceutical Industry}}: {{Concepts}}, {{Algorithms}}, and {{Case Studies}}},
  shorttitle = {Monte {{Carlo Simulation}} for the {{Pharmaceutical Industry}}},
  author = {Chang, Mark},
  year = {2010},
  month = sep,
  publisher = {CRC Press},
  abstract = {Helping you become a creative, logical thinker and skillful "simulator," Monte Carlo Simulation for the Pharmaceutical Industry: Concepts, Algorithms, and Case Studies provides broad coverage of the entire drug development process, from drug discovery to preclinical and clinical trial aspects to commercialization. It presents the theories and metho},
  googlebooks = {MVuFoSMPZC8C},
  isbn = {978-1-4398-3593-7},
  langid = {english},
  keywords = {cited,Mathematics / Probability & Statistics / General,Medical / Pharmacology}
}

@article{claesen2021ComparingDreamReality,
  title = {Comparing Dream to Reality: An Assessment of Adherence of the First Generation of Preregistered Studies},
  shorttitle = {Comparing Dream to Reality},
  author = {Claesen, Aline and Gomes, Sara and Tuerlinckx, Francis and Vanpaemel, Wolf},
  year = {2021},
  month = oct,
  journal = {Royal Society Open Science},
  volume = {8},
  number = {10},
  pages = {211037},
  publisher = {Royal Society},
  doi = {10.1098/rsos.211037},
  urldate = {2024-01-08},
  abstract = {Preregistration is a method to increase research transparency by documenting research decisions on a public, third-party repository prior to any influence by data. It is becoming increasingly popular in all subfields of psychology and beyond. Adherence to the preregistration plan may not always be feasible and even is not necessarily desirable, but without disclosure of deviations, readers who do not carefully consult the preregistration plan might get the incorrect impression that the study was exactly conducted and reported as planned. In this paper, we have investigated adherence and disclosure of deviations for all articles published with the Preregistered badge in Psychological Science between February 2015 and November 2017 and shared our findings with the corresponding authors for feedback. Two out of 27 preregistered studies contained no deviations from the preregistration plan. In one study, all deviations were disclosed. Nine studies disclosed none of the deviations. We mainly observed (un)disclosed deviations from the plan regarding the reported sample size, exclusion criteria and statistical analysis. This closer look at preregistrations of the first generation reveals possible hurdles for reporting preregistered studies and provides input for future reporting guidelines. We discuss the results and possible explanations, and provide recommendations for preregistered research.},
  keywords = {open science,preregistration,psychological science,researcher degrees of freedom,transparency},
  file = {C:\Users\jamespustejovsky\Zotero\storage\WX4BM5R7\Claesen et al. - 2021 - Comparing dream to reality an assessment of adher.pdf}
}

@article{cruwell2023WhatBadgeComputational,
  title = {What's in a {{Badge}}? {{A Computational Reproducibility Investigation}} of the {{Open Data Badge Policy}} in {{One Issue}} of {{Psychological Science}}},
  shorttitle = {What's in a {{Badge}}?},
  author = {Cr{\"u}well, Sophia and Apthorp, Deborah and Baker, Bradley J. and Colling, Lincoln and Elson, Malte and Geiger, Sandra J. and Lobentanzer, Sebastian and Mon{\'e}ger, Jean and Patterson, Alex and Schwarzkopf, D. Samuel and Zaneva, Mirela and Brown, Nicholas J. L.},
  year = {2023},
  month = apr,
  journal = {Psychological Science},
  volume = {34},
  number = {4},
  pages = {512--522},
  publisher = {SAGE Publications Inc},
  issn = {0956-7976},
  doi = {10.1177/09567976221140828},
  urldate = {2024-01-08},
  abstract = {In April 2019, Psychological Science published its first issue in which all Research Articles received the Open Data badge. We used that issue to investigate the effectiveness of this badge, focusing on the adherence to its aim at Psychological Science: sharing both data and code to ensure reproducibility of results. Twelve researchers of varying experience levels attempted to reproduce the results of the empirical articles in the target issue (at least three researchers per article). We found that all 14 articles provided at least some data and six provided analysis code, but only one article was rated to be exactly reproducible, and three were rated as essentially reproducible with minor deviations. We suggest that researchers should be encouraged to adhere to the higher standard in force at Psychological Science. Moreover, a check of reproducibility during peer review may be preferable to the disclosure method of awarding badges.},
  langid = {english},
  file = {C:\Users\jamespustejovsky\Zotero\storage\IQU35GF2\Crüwell et al. - 2023 - What’s in a Badge A Computational Reproducibility.pdf}
}

@book{davison1997BootstrapMethodsTheir,
  title = {Bootstrap Methods and Their Applications},
  author = {Davison, A. C. and Hinkley, D. V.},
  year = {1997},
  publisher = {Cambridge University Press},
  address = {Cambridge}
}


@article{feiveson2002PowerSimulation,
  title = {Power by {{Simulation}}},
  author = {Feiveson, A. H.},
  year = {2002},
  month = jun,
  journal = {The Stata Journal},
  volume = {2},
  number = {2},
  pages = {107--124},
  publisher = {SAGE Publications},
  issn = {1536-867X},
  doi = {10.1177/1536867X0200200201},
  urldate = {2023-12-31},
  abstract = {This paper describes how to write Stata programs to estimate the power of virtually any statistical test that Stata can perform. Examples given include the t test, Poisson regression, Cox regression, and the nonparametric rank-sum test.},
  langid = {english},
  file = {C:\Users\jamespustejovsky\Zotero\storage\ZT8R83PS\Feiveson - 2002 - Power by Simulation.pdf}
}

@book{gamma1995DesignPatternsElements,
  title = {Design {{Patterns}}: {{Elements}} of {{Reusable Object-Oriented Software}}},
  shorttitle = {Design {{Patterns}}},
  author = {Gamma, Erich and Helm, Richard and Johnson, Ralph and Vlissides, John},
  year = {1995},
  publisher = {Addison-Wesley Publishing Co.},
  address = {Reading, MA},
  urldate = {2024-01-05},
  abstract = {A book review of Design Patterns: Elements of Reusable Object-Oriented Software by Erich Gamma, Richard Helm, Ralph Johnson, and John Vlissides is presented.},
  chapter = {Books},
  copyright = {Copyright International Business Machines Corporation 1995},
  isbn = {0-201-63361-2},
  keywords = {Design pattern,Design Patterns,Object-oriented programming},
  file = {C:\Users\jamespustejovsky\Zotero\storage\KQSAGQPV\Beck - 1995 - Design Patterns Elements of Reusable Object-Orien.pdf}
}

@article{gasparini2018RsimsumSummariseResults,
  title = {Rsimsum: {{Summarise}} Results from {{Monte Carlo}} Simulation Studies},
  author = {Gasparini, Alessandro},
  year = {2018},
  journal = {Journal of Open Source Software},
  volume = {3},
  number = {26},
  pages = {739},
  doi = {10.21105/joss.00739},
  keywords = {cited}
}

@book{gelman2013BayesianDataAnalysis,
  title = {Bayesian {{Data Analysis}}},
  author = {Gelman, Andrew and Carlin, John B. and Stern, Hal S. and Dunson, David B. and Vehtari, Aki and Rubin, Donald B.},
  year = {2013},
  month = nov,
  edition = {0},
  publisher = {{Chapman and Hall/CRC}},
  doi = {10.1201/b16018},
  urldate = {2024-06-26},
  isbn = {978-0-429-11307-9},
  langid = {english}
}

@article{gelman2014StatisticalCrisisScience,
  title = {The Statistical Crisis in Science: Data-Dependent Analysis--a \&quot;Garden of Forking Paths\&quot;--Explains Why Many Statistically Significant Comparisons Don't Hold Up},
  shorttitle = {The Statistical Crisis in Science},
  author = {Gelman, Andrew and Loken, Eric},
  year = {2014},
  month = nov,
  journal = {American Scientist},
  volume = {102},
  number = {6},
  pages = {460--466},
  publisher = {Sigma Xi, The Scientific Research Society},
  issn = {00030996},
  urldate = {2024-01-08},
  abstract = {{$<$}em{$>$}Gale{$<$}/em{$>$} Academic OneFile includes The statistical crisis in science: data-dependent analy by Andrew Gelman and Eric Loken. Click to explore.},
  langid = {english},
  file = {C:\Users\jamespustejovsky\Zotero\storage\89I3TV76\i.html}
}

@article{goldfeld2020SimstudyIlluminatingResearch,
  title = {Simstudy: {{Illuminating}} Research Methods through Data Generation},
  author = {Goldfeld, Keith and {Wujciak-Jens}, Jacob},
  year = {2020},
  journal = {Journal of Open Source Software},
  volume = {5},
  number = {54},
  pages = {2763},
  publisher = {The Open Journal},
  doi = {10.21105/joss.02763},
  keywords = {cited}
}

@article{green2016SIMRPackagePower,
  title = {{{SIMR}}: An {{R}} Package for Power Analysis of Generalized Linear Mixed Models by Simulation},
  shorttitle = {{{SIMR}}},
  author = {Green, Peter and MacLeod, Catriona J.},
  year = {2016},
  journal = {Methods in Ecology and Evolution},
  volume = {7},
  number = {4},
  pages = {493--498},
  issn = {2041-210X},
  doi = {10.1111/2041-210X.12504},
  urldate = {2023-12-31},
  abstract = {The r package simr allows users to calculate power for generalized linear mixed models from the lme4 package. The power calculations are based on Monte Carlo simulations. It includes tools for (i) running a power analysis for a given model and design; and (ii) calculating power curves to assess trade-offs between power and sample size. This paper presents a tutorial using a simple example of count data with mixed effects (with structure representative of environmental monitoring data) to guide the user along a gentle learning curve, adding only a few commands or options at a time.},
  copyright = {{\copyright} 2015 The Authors. Methods in Ecology and Evolution {\copyright} 2015 British Ecological Society},
  langid = {english},
  keywords = {cited,experimental design,glmm,Monte Carlo,random effects,sample size,type II error},
  file = {C\:\\Users\\jamespustejovsky\\Zotero\\storage\\QDLKYJ6L\\Green and MacLeod - 2016 - SIMR an R package for power analysis of generaliz.pdf;C\:\\Users\\jamespustejovsky\\Zotero\\storage\\HEIG34AA\\2041-210X.html}
}

@article{hardwicke2023ReducingBiasIncreasing,
  title = {Reducing Bias, Increasing Transparency and Calibrating Confidence with Preregistration},
  author = {Hardwicke, Tom E. and Wagenmakers, Eric-Jan},
  year = {2023},
  month = jan,
  journal = {Nature Human Behaviour},
  volume = {7},
  number = {1},
  pages = {15--26},
  publisher = {Nature Publishing Group},
  issn = {2397-3374},
  doi = {10.1038/s41562-022-01497-2},
  urldate = {2024-01-08},
  abstract = {Flexibility in the design, analysis and interpretation of scientific studies creates a multiplicity of possible research outcomes. Scientists are granted considerable latitude to selectively use and report the hypotheses, variables and analyses that create the most positive, coherent and attractive story while suppressing those that are negative or inconvenient. This creates a risk of bias that can lead to scientists fooling themselves and fooling others. Preregistration involves declaring a research plan (for example, hypotheses, design and statistical analyses) in a public registry before the research outcomes are known. Preregistration (1) reduces the risk of bias by encouraging outcome-independent decision-making and (2) increases transparency, enabling others to assess the risk of bias and calibrate their confidence in research outcomes. In this Perspective, we briefly review the historical evolution of preregistration in medicine, psychology and other domains, clarify its pragmatic functions, discuss relevant meta-research, and provide recommendations for scientists and journal editors.},
  copyright = {2022 Springer Nature Limited},
  langid = {english},
  keywords = {Science,Scientific community,technology and society},
  file = {C:\Users\jamespustejovsky\Zotero\storage\SKQY4R7Q\Hardwicke and Wagenmakers - 2023 - Reducing bias, increasing transparency and calibra.pdf}
}

@article{harwell2018SurveyReportingPractices,
  title = {A {{Survey}} of {{Reporting Practices}} of {{Computer Simulation Studies}} in {{Statistical Research}}},
  author = {Harwell, Michael and Kohli, Nidhi and {Peralta-Torres}, Yadira},
  year = {2018},
  month = oct,
  journal = {The American Statistician},
  volume = {72},
  number = {4},
  pages = {321--327},
  publisher = {Taylor \& Francis},
  issn = {0003-1305},
  doi = {10.1080/00031305.2017.1342692},
  urldate = {2024-01-02},
  abstract = {Computer simulation studies represent an important tool for investigating processes difficult or impossible to study using mathematical theory or real data. Hoaglin and Andrews recommended these studies be treated as statistical sampling experiments subject to established principles of design and data analysis, but the survey of Hauck and Anderson suggested these recommendations had, at that point in time, generally been ignored. We update the survey results of Hauck and Anderson using a sample of studies applying simulation methods in statistical research to assess the extent to which the recommendations of Hoaglin and Andrews and others for conducting simulation studies have been adopted. The important role of statistical applications of computer simulation studies in enhancing the reproducibility of scientific findings is also discussed. The results speak to the state of the art and the extent to which these studies are realizing their potential to inform statistical practice and a program of statistical research.},
  keywords = {Computer simulation,Design and data analysis,Survey},
  file = {C:\Users\jamespustejovsky\Zotero\storage\H9BDZ3WT\Harwell et al. - 2018 - A Survey of Reporting Practices of Computer Simula.pdf}
}

@article{hoogland1998RobustnessStudiesCovariance,
  title = {Robustness {{Studies}} in {{Covariance Structure Modeling}}: {{An Overview}} and a {{Meta-Analysis}}},
  shorttitle = {Robustness {{Studies}} in {{Covariance Structure Modeling}}},
  author = {HOOGLAND, JEFFREY J. and BOOMSMA, {\relax ANNE}},
  year = {1998},
  month = feb,
  journal = {Sociological Methods \& Research},
  volume = {26},
  number = {3},
  pages = {329--367},
  publisher = {SAGE Publications Inc},
  issn = {0049-1241},
  doi = {10.1177/0049124198026003003},
  urldate = {2024-01-02},
  abstract = {In covariance structure modeling, several estimation methods are available. The robustness of an estimator against specific violations of assumptions can be determined empirically by means of a Monte Carlo study. Many such studies in covariance structure analysis have been published, but the conclusions frequently seem to contradict each other. An overview of robustness studies in covariance structure analysis is given, and an attempt is made to generalize findings. Robustness studies are described and distinguished from each other systematically by means of certain characteristics. These characteristics serve as explanatory variables in a meta-analysis concerning the behavior of parameter estimators, standard error estimators, and goodness-of-fit statistics when the model is correctly specified.},
  langid = {english},
  file = {C:\Users\jamespustejovsky\Zotero\storage\D23SYWER\HOOGLAND and BOOMSMA - 1998 - Robustness Studies in Covariance Structure Modelin.pdf}
}

@article{huang2016GeneralizedEstimatingEquations,
  title = {Generalized Estimating Equations in Cluster Randomized Trials with a Small Number of Clusters: {{Review}} of Practice and Simulation Study},
  shorttitle = {Generalized Estimating Equations in Cluster Randomized Trials with a Small Number of Clusters},
  author = {Huang, Shuang and Fiero, Mallorie H and Bell, Melanie L},
  year = {2016},
  month = aug,
  journal = {Clinical Trials},
  volume = {13},
  number = {4},
  pages = {445--449},
  issn = {1740-7745, 1740-7753},
  doi = {10.1177/1740774516643498},
  urldate = {2024-01-05},
  abstract = {Background/aims:               Generalized estimating equations are a common modeling approach used in cluster randomized trials to account for within-cluster correlation. It is well known that the sandwich variance estimator is biased when the number of clusters is small ({$\leq$}40), resulting in an inflated type I error rate. Various bias correction methods have been proposed in the statistical literature, but how adequately they are utilized in current practice for cluster randomized trials is not clear. The aim of this study is to evaluate the use of generalized estimating equation bias correction methods in recently published cluster randomized trials and demonstrate the necessity of such methods when the number of clusters is small.                                         Methods:               Review of cluster randomized trials published between August 2013 and July 2014 and using generalized estimating equations for their primary analyses. Two independent reviewers collected data from each study using a standardized, pre-piloted data extraction template. A two-arm cluster randomized trial was simulated under various scenarios to show the potential effect of a small number of clusters on type I error rate when estimating the treatment effect. The nominal level was set at 0.05 for the simulation study.                                         Results:               Of the 51 included trials, 28 (54.9\%) analyzed 40 or fewer clusters with a minimum of four total clusters. Of these 28 trials, only one trial used a bias correction method for generalized estimating equations. The simulation study showed that with four clusters, the type I error rate ranged between 0.43 and 0.47. Even though type I error rate moved closer to the nominal level as the number of clusters increases, it still ranged between 0.06 and 0.07 with 40 clusters.                                         Conclusions:               Our results showed that statistical issues arising from small number of clusters in generalized estimating equations is currently inadequately handled in cluster randomized trials. Potential for type I error inflation could be very high when the sandwich estimator is used without bias correction.},
  langid = {english}
}

@misc{hunter2023PowerMultiplicityProject,
  title = {Power {{Under Multiplicity Project}} ({{PUMP}}): {{Estimating Power}}, {{Minimum Detectable Effect Size}}, and {{Sample Size When Adjusting}} for {{Multiple Outcomes}} in {{Multi-level Experiments}}},
  shorttitle = {Power {{Under Multiplicity Project}} ({{PUMP}})},
  author = {Hunter, Kristen and Miratrix, Luke and Porter, Kristin},
  year = {2023},
  month = may,
  number = {arXiv:2112.15273},
  eprint = {2112.15273},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2112.15273},
  urldate = {2024-01-05},
  abstract = {For randomized controlled trials (RCTs) with a single intervention being measured on multiple outcomes, researchers often apply a multiple testing procedure (such as Bonferroni or Benjamini-Hochberg) to adjust \$p\$-values. Such an adjustment reduces the likelihood of spurious findings, but also changes the statistical power, sometimes substantially, which reduces the probability of detecting effects when they do exist. However, this consideration is frequently ignored in typical power analyses, as existing tools do not easily accommodate the use of multiple testing procedures. We introduce the PUMP R package as a tool for analysts to estimate statistical power, minimum detectable effect size, and sample size requirements for multi-level RCTs with multiple outcomes. Multiple outcomes are accounted for in two ways. First, power estimates from PUMP properly account for the adjustment in \$p\$-values from applying a multiple testing procedure. Second, as researchers change their focus from one outcome to multiple outcomes, different definitions of statistical power emerge. PUMP allows researchers to consider a variety of definitions of power, as some may be more appropriate for the goals of their study. The package estimates power for frequentist multi-level mixed effects models, and supports a variety of commonly-used RCT designs and models and multiple testing procedures. In addition to the main functionality of estimating power, minimum detectable effect size, and sample size requirements, the package allows the user to easily explore sensitivity of these quantities to changes in underlying assumptions.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology},
  file = {C\:\\Users\\jamespustejovsky\\Zotero\\storage\\39E7JMUN\\Hunter et al. - 2023 - Power Under Multiplicity Project (PUMP) Estimatin.pdf;C\:\\Users\\jamespustejovsky\\Zotero\\storage\\XJQH5V4I\\2112.html}
}

@article{hussey2007DesignAnalysisStepped,
  title = {Design and Analysis of Stepped Wedge Cluster Randomized Trials},
  author = {Hussey, Michael A. and Hughes, James P.},
  year = {2007},
  month = feb,
  journal = {Contemporary Clinical Trials},
  volume = {28},
  number = {2},
  pages = {182--191},
  issn = {1551-7144},
  doi = {10.1016/j.cct.2006.05.007},
  urldate = {2024-01-05},
  abstract = {Cluster randomized trials (CRT) are often used to evaluate therapies or interventions in situations where individual randomization is not possible or not desirable for logistic, financial or ethical reasons. While a significant and rapidly growing body of literature exists on CRTs utilizing a ``parallel'' design (i.e. I clusters randomized to each treatment), only a few examples of CRTs using crossover designs have been described. In this article we discuss the design and analysis of a particular type of crossover CRT -- the stepped wedge -- and provide an example of its use.},
  keywords = {Cluster randomized trial,Prevention trials,Stepped wedge design},
  file = {C:\Users\jamespustejovsky\Zotero\storage\ZX4WVRHJ\S1551714406000632.html}
}

@book{jones2012IntroductionScientificProgramming,
  title = {Introduction to {{Scientific Programming}} and {{Simulation Using R}}},
  author = {Jones, Owen and Maillardet, Robert and Robinson, Andrew},
  year = {2012},
  month = oct,
  publisher = {{Chapman and Hall/CRC}},
  address = {New York},
  doi = {10.1201/9781420068740},
  abstract = {Known for its versatility, the free programming language R is widely used for statistical computing and graphics, but is also a fully functional programming language well suited to scientific programming.An Introduction to Scientific Programming and Simulation Using R teaches the skills needed to perform scientific programming while also introducin},
  isbn = {978-0-429-14333-5}
}

@misc{joshi2022SimhelpersHelperFunctions,
  title = {Simhelpers: {{Helper Functions}} for {{Simulation Studies}}},
  author = {Joshi, Megha and Pustejovsky, James},
  year = {2022},
  keywords = {cited}
}

@book{kalos2009MonteCarloMethods,
  title = {Monte {{Carlo Methods}}},
  author = {Kalos, Malvin H. and Whitlock, Paula A.},
  year = {2009},
  month = jun,
  publisher = {John Wiley \& Sons},
  abstract = {This introduction to Monte Carlo methods seeks to identify and study the unifying elements that underlie their effective application. Initial chapters provide a short treatment of the probability and statistics needed as background, enabling those without experience in Monte Carlo techniques to apply these ideas to their research. The book focuses on two basic themes: The first is the importance of random walks as they occur both in natural stochastic systems and in their relationship to integral and differential equations. The second theme is that of variance reduction in general and importance sampling in particular as a technique for efficient use of the methods. Random walks are introduced with an elementary example in which the modeling of radiation transport arises directly from a schematic probabilistic description of the interaction of radiation with matter. Building on this example, the relationship between random walks and integral equations is outlined. The applicability of these ideas to other problems is shown by a clear and elementary introduction to the solution of the Schrodinger equation by random walks. The text includes sample problems that readers can solve by themselves to illustrate the content of each chapter.  This is the second, completely revised and extended edition of the successful monograph, which brings the treatment up to date and incorporates the many advances in Monte Carlo techniques and their applications, while retaining the original elementary but general approach.},
  isbn = {978-3-527-62622-9},
  langid = {english},
  keywords = {Science / Physics / General,Science / Physics / Mathematical & Computational}
}

@misc{kenny2023SimEngineModularFramework,
  title = {{{SimEngine}}: {{A Modular Framework}} for {{Statistical Simulations}} in {{R}}},
  author = {Kenny, Avi and Wolock, Charles},
  year = {2023},
  month = oct,
  keywords = {cited}
}

@article{kern2016AssessingMethodsGeneralizing,
  title = {Assessing {{Methods}} for {{Generalizing Experimental Impact Estimates}} to {{Target Populations}}},
  author = {Kern, Holger L. and Stuart, Elizabeth A. and Hill, Jennifer and Green, Donald P.},
  year = {2016},
  month = jan,
  journal = {Journal of Research on Educational Effectiveness},
  volume = {9},
  number = {1},
  pages = {103--127},
  publisher = {Routledge},
  issn = {1934-5747},
  doi = {10.1080/19345747.2015.1060282},
  urldate = {2024-01-01},
  abstract = {Randomized experiments are considered the gold standard for causal inference because they can provide unbiased estimates of treatment effects for the experimental participants. However, researchers and policymakers are often interested in using a specific experiment to inform decisions about other target populations. In education research, increasing attention is being paid to the potential lack of generalizability of randomized experiments because the experimental participants may be unrepresentative of the target population of interest. This article examines whether generalization may be assisted by statistical methods that adjust for observed differences between the experimental participants and members of a target population. The methods examined include approaches that reweight the experimental data so that participants more closely resemble the target population and methods that utilize models of the outcome. Two simulation studies and one empirical analysis investigate and compare the methods' performance. One simulation uses purely simulated data while the other utilizes data from an evaluation of a school-based dropout prevention program. Our simulations suggest that machine learning methods outperform regression-based methods when the required structural (ignorability) assumptions are satisfied. When these assumptions are violated, all of the methods examined perform poorly. Our empirical analysis uses data from a multisite experiment to assess how well results from a given site predict impacts in other sites. Using a variety of extrapolation methods, predicted effects for each site are compared to actual benchmarks. Flexible modeling approaches perform best, although linear regression is not far behind. Taken together, these results suggest that flexible modeling techniques can aid generalization while underscoring the fact that even state-of-the-art statistical techniques still rely on strong assumptions.},
  pmid = {27668031},
  keywords = {Bayesian Additive Regression Trees external validity generalizability propensity score weighting},
  file = {C:\Users\jamespustejovsky\Zotero\storage\2F7WKUXW\Kern et al. - 2016 - Assessing Methods for Generalizing Experimental Im.pdf}
}

@article{koehler2009AssessmentMonteCarlo,
  title = {On the {{Assessment}} of {{Monte Carlo Error}} in {{Simulation-Based Statistical Analyses}}},
  author = {Koehler, Elizabeth and Brown, Elizabeth and Haneuse, Sebastien J.-P. A.},
  year = {2009},
  month = may,
  journal = {The American Statistician},
  volume = {63},
  number = {2},
  pages = {155--162},
  publisher = {Taylor \& Francis},
  issn = {0003-1305},
  doi = {10.1198/tast.2009.0030},
  urldate = {2024-01-02},
  abstract = {Statistical experiments, more commonly referred to as Monte Carlo or simulation studies, are used to study the behavior of statistical methods and measures under controlled situations. Whereas recent computing and methodological advances have permitted increased efficiency in the simulation process, known as variance reduction, such experiments remain limited by their finite nature and hence are subject to uncertainty; when a simulation is run more than once, different results are obtained. However, virtually no emphasis has been placed on reporting the uncertainty, referred to here as Monte Carlo error, associated with simulation results in the published literature, or on justifying the number of replications used. These deserve broader consideration. Here we present a series of simple and practical methods for estimating Monte Carlo error as well as determining the number of replications required to achieve a desired level of accuracy. The issues and methods are demonstrated with two simple examples, one evaluating operating characteristics of the maximum likelihood estimator for the parameters in logistic regression and the other in the context of using the bootstrap to obtain 95\% confidence intervals. The results suggest that in many settings, Monte Carlo error may be more substantial than traditionally thought.},
  pmid = {22544972},
  keywords = {Bootstrap,cited,Jackknife,Replication},
  file = {C:\Users\jamespustejovsky\Zotero\storage\BZFE3YSE\Koehler et al. - 2009 - On the Assessment of Monte Carlo Error in Simulati.pdf}
}

@misc{leschinski2019MonteCarloAutomaticParallelized,
  title = {{{MonteCarlo}}: {{Automatic Parallelized Monte Carlo Simulations}}},
  author = {Leschinski, Christian Hendrik},
  year = {2019},
  month = jan,
  keywords = {cited}
}

@article{leyrat2013PropensityScoresUsed,
  title = {Propensity Scores Used for Analysis of Cluster Randomized Trials with Selection Bias: A Simulation Study},
  shorttitle = {Propensity Scores Used for Analysis of Cluster Randomized Trials with Selection Bias},
  author = {Leyrat, C. and Caille, A. and Donner, A. and Giraudeau, B.},
  year = {2013},
  journal = {Statistics in Medicine},
  volume = {32},
  number = {19},
  pages = {3357--3372},
  issn = {1097-0258},
  doi = {10.1002/sim.5795},
  urldate = {2024-01-05},
  abstract = {Cluster randomized trials (CRTs) are often prone to selection bias despite randomization. Using a simulation study, we investigated the use of propensity score (PS) based methods in estimating treatment effects in CRTs with selection bias when the outcome is quantitative. Of four PS-based methods (adjustment on PS, inverse weighting, stratification, and optimal full matching method), three successfully corrected the bias, as did an approach using classical multivariable regression. However, they showed poorer statistical efficiency than classical methods, with higher standard error for the treatment effect, and type I error much smaller than the 5\% nominal level. Copyright {\copyright} 2013 John Wiley \& Sons, Ltd.},
  copyright = {Copyright {\copyright} 2013 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {cluster randomized trial,Monte-Carlo simulations,propensity score,selection bias},
  file = {C\:\\Users\\jamespustejovsky\\Zotero\\storage\\VHHHRSDD\\Leyrat et al. - 2013 - Propensity scores used for analysis of cluster ran.pdf;C\:\\Users\\jamespustejovsky\\Zotero\\storage\\FU2REDBN\\sim.html}
}

@article{lohmann2022ItTimeTen,
  title = {It's Time! {{Ten}} Reasons to Start Replicating Simulation Studies},
  author = {Lohmann, Anna and Astivia, Oscar L. O. and Morris, Tim P. and Groenwold, Rolf H. H.},
  year = {2022},
  journal = {Frontiers in Epidemiology},
  volume = {2},
  issn = {2674-1199},
  urldate = {2024-01-01},
  abstract = {The quantitative analysis of research data is a core element of empirical research. The performance of statistical methods that are used for analyzing empirical data can be evaluated and compared using computer simulations. A single simulation study can influence the analyses of thousands of empirical studies to follow. With great power comes great responsibility. Here, we argue that this responsibility includes replication of simulation studies to ensure a sound foundation for data analytical decisions. Furthermore, being designed, run, and reported by humans, simulation studies face challenges similar to other experimental empirical research and hence should not be exempt from replication attempts. We highlight that the potential replicability of simulation studies is an opportunity quantitative methodology as a field should pay more attention to.},
  file = {C:\Users\jamespustejovsky\Zotero\storage\GBRU4F33\Lohmann et al. - 2022 - It's time! Ten reasons to start replicating simula.pdf}
}

@article{miratrix2021AppliedResearcherGuide,
  title = {An {{Applied Researcher}}'s {{Guide}} to {{Estimating Effects}} from {{Multisite Individually Randomized Trials}}: {{Estimands}}, {{Estimators}}, and {{Estimates}}},
  shorttitle = {An {{Applied Researcher}}'s {{Guide}} to {{Estimating Effects}} from {{Multisite Individually Randomized Trials}}},
  author = {Miratrix, Luke W. and Weiss, Michael J. and Henderson, Brit},
  year = {2021},
  month = jan,
  journal = {Journal of Research on Educational Effectiveness},
  volume = {14},
  number = {1},
  pages = {270--308},
  issn = {1934-5747, 1934-5739},
  doi = {10.1080/19345747.2020.1831115},
  urldate = {2024-01-05},
  langid = {english}
}

@book{miratrix2023DesigningMonteCarlo,
  title = {Designing {{Monte Carlo Simulations}} in {{R}}},
  author = {Miratrix, Luke W. and Pustejovsky, Jame E.},
  year = {2023},
  month = nov,
  keywords = {cited}
}

@article{moerbeek2019WhatAreStatistical,
  title = {What Are the Statistical Implications of Treatment Non-Compliance in Cluster Randomized Trials: {{A}} Simulation Study},
  shorttitle = {What Are the Statistical Implications of Treatment Non-Compliance in Cluster Randomized Trials},
  author = {Moerbeek, Mirjam and van Schie, Sander},
  year = {2019},
  journal = {Statistics in Medicine},
  volume = {38},
  number = {26},
  pages = {5071--5084},
  issn = {1097-0258},
  doi = {10.1002/sim.8351},
  urldate = {2024-01-05},
  abstract = {Subjects in randomized controlled trials do not always comply to the treatment condition they have been assigned to. This may cause the estimated effect of the intervention to be biased and also affect efficiency, coverage of confidence intervals, and statistical power. In cluster randomized trials non-compliance may occur at the subject level but also at the cluster level. In the latter case, all subjects within the same cluster have the same compliance status. The purpose of this study is to investigate the statistical implications of non-compliance in cluster randomized trials. A simulation study was conducted with varying degrees of non-compliance at either the cluster level or subject level. The probability of non-compliance depends on a covariate at the cluster or subject level. Various realistic values of the intraclass correlation coefficient and cluster size are used. The data are analyzed by intention to treat, as treated, per protocol and the instrumental variable approach. The results show non-compliance may result in downward biased estimates of the intervention effect and an under- or overestimate of its standard deviation. The coverage of the confidence intervals may be too small, and in most cases, empirical power is too small. The results are more severe when the probability of non-compliance increases and the covariate that affects compliance is unobserved. It is advocated to avoid non-compliance. If this is not possible, compliance status and covariates that affect compliance should be measured and included in the statistical model.},
  copyright = {{\copyright} 2019 The Authors. Statistics~in~Medicine Published by John Wiley \& Sons Ltd.},
  langid = {english},
  keywords = {cluster randomized trial,simulation study,treatment non-compliance},
  file = {C\:\\Users\\jamespustejovsky\\Zotero\\storage\\3D57RHNZ\\Moerbeek and Schie - 2019 - What are the statistical implications of treatment.pdf;C\:\\Users\\jamespustejovsky\\Zotero\\storage\\5VFMRPKP\\sim.html}
}

@book{mooney1997MonteCarloSimulation,
  title = {Monte {{Carlo Simulation}}},
  author = {Mooney, Christopher Z},
  year = {1997},
  number = {116},
  publisher = {Sage},
  keywords = {cited}
}

@article{morris2019UsingSimulationStudies,
  title = {Using Simulation Studies to Evaluate Statistical Methods},
  shorttitle = {Using Simulation Studies to Evaluate Statistical Methods},
  author = {Morris, Tim P. and White, Ian R. and Crowther, Michael J.},
  year = {2019},
  month = jan,
  journal = {Statistics in Medicine},
  issn = {02776715},
  doi = {10.1002/sim.8086},
  urldate = {2019-01-26},
  langid = {english},
  keywords = {cited},
  file = {C:\Users\jamespustejovsky\Zotero\storage\VNK7VV22\Morris et al. - 2019 - Using simulation studies to evaluate statistical m.pdf}
}

@misc{nguyen2022MpowerPackagePower,
  title = {Mpower: {{An R Package}} for {{Power Analysis}} via {{Simulation}} for {{Correlated Data}}},
  shorttitle = {Mpower},
  author = {Nguyen, Phuc H. and Engel, Stephanie M. and Herring, Amy H.},
  year = {2022},
  month = sep,
  journal = {arXiv.org},
  urldate = {2024-01-01},
  abstract = {Estimating sample size and statistical power is an essential part of a good study design. This R package allows users to conduct power analysis based on Monte Carlo simulations in settings in which consideration of the correlations between predictors is important. It runs power analyses given a data generative model and an inference model. It can set up a data generative model that preserves dependence structures among variables given existing data (continuous, binary, or ordinal) or high-level descriptions of the associations. Users can generate power curves to assess the trade-offs between sample size, effect size, and power of a design. This paper presents tutorials and examples focusing on applications for environmental mixture studies when predictors tend to be moderately to highly correlated. It easily interfaces with several existing and newly developed analysis strategies for assessing associations between exposures and health outcomes. However, the package is sufficiently general to facilitate power simulations in a wide variety of settings.},
  howpublished = {https://arxiv.org/abs/2209.08036v1},
  langid = {english},
  file = {C:\Users\jamespustejovsky\Zotero\storage\ZV6P78KM\Nguyen et al. - 2022 - mpower An R Package for Power Analysis via Simula.pdf}
}

@article{orcan2021MonteCarloSEMPackageSimulate,
  title = {{{MonteCarloSEM}}: {{An R Package}} to {{Simulate Data}} for {{SEM}}},
  shorttitle = {{{MonteCarloSEM}}},
  author = {Or{\c c}an, Fatih},
  year = {2021},
  month = sep,
  journal = {International Journal of Assessment Tools in Education},
  volume = {8},
  number = {3},
  pages = {704--713},
  publisher = {{\.I}zzet KARA},
  issn = {2148-7456},
  urldate = {2024-01-02},
  abstract = {Monte Carlo simulation is a useful tool for researchers to estimated accuracy of a statistical model. It is usually used for investigating parameter estimation procedure or violation of assumption for some given conditions. To run a simulation either the paid software or open source but free program such as R is need to be used. For that, researchers must have a good knowledge about the theoretical procedures. This paper introduces the R package called MonteCarloSEM. The package helps to simulate and analyze data sets for some simulation condition such as sample size and normality for a given model. Also, an example is given to show how the functions within the package works.},
  langid = {english},
  keywords = {cited},
  file = {C:\Users\jamespustejovsky\Zotero\storage\DRYK6I84\Orçan - 2021 - MonteCarloSEM An R Package to Simulate Data for S.pdf}
}

@article{paxton2001MonteCarloExperiments,
  title = {Monte {{Carlo Experiments}}: {{Design}} and {{Implementation}}},
  shorttitle = {Monte {{Carlo Experiments}}},
  author = {Paxton, Pamela and Curran, Patrick J. and Bollen, Kenneth A. and Kirby, Jim and Chen, Feinian},
  year = {2001},
  month = apr,
  journal = {Structural Equation Modeling: A Multidisciplinary Journal},
  volume = {8},
  number = {2},
  pages = {287--312},
  publisher = {Routledge},
  issn = {1070-5511},
  doi = {10.1207/S15328007SEM0802_7},
  urldate = {2024-01-02},
  abstract = {The use of Monte Carlo simulations for the empirical assessment of statistical estimators is becoming more common in structural equation modeling research. Yet, there is little guidance for the researcher interested in using the technique. In this article we illustrate both the design and implementation of Monte Carlo simulations. We present 9 steps in planning and performing a Monte Carlo analysis: (1) developing a theoretically derived research question of interest, (2) creating a valid model, (3) designing specific experimental conditions, (4) choosing values of population parameters, (5) choosing an appropriate software package, (6) executing the simulations, (7) file storage, (8) troubleshooting and verification, and (9) summarizing results. Throughout the article, we use as a running example a Monte Carlo simulation that we performed to illustrate many of the relevant points with concrete information and detail.},
  keywords = {cited}
}

@book{robert2010IntroducingMonteCarlo,
  title = {Introducing {{Monte Carlo Methods}} with {{R}}},
  author = {Robert, Christian and Casella, George},
  year = {2010},
  publisher = {Springer},
  address = {New York, NY},
  doi = {10.1007/978-1-4419-1576-4},
  urldate = {2024-01-02},
  isbn = {978-1-4419-1582-5 978-1-4419-1576-4},
  langid = {english},
  keywords = {bayesian statistics,Markov chain,Mathematica,Monte Carlo,Monte Carlo method,Random variable,simulation,STATISTICA},
  file = {C:\Users\jamespustejovsky\Zotero\storage\RX3A54TU\Robert and Casella - 2010 - Introducing Monte Carlo Methods with R.pdf}
}

@misc{scheer2020SimToolConductSimulation,
  title = {{{simTool}}: {{Conduct Simulation Studies}} with a {{Minimal Amount}} of {{Source Code}}},
  author = {Scheer, Marcel},
  year = {2020},
  month = sep,
  keywords = {cited}
}

@article{siepe2024SimulationStudiesMethodological,
  title = {Simulation {{Studies}} for {{Methodological Research}} in {{Psychology}}: {{A Standardized Template}} for {{Planning}}, {{Preregistration}}, and {{Reporting}}},
  shorttitle = {Simulation {{Studies}} for {{Methodological Research}} in {{Psychology}}},
  author = {Siepe, Bj{\"o}rn S. and Barto{\v s}, Franti{\v s}ek and Morris, Tim and Boulesteix, Anne-Laure and Heck, Daniel W. and Pawel, Samuel},
  year = {2024},
  month = jan,
  publisher = {OSF},
  doi = {10.31234/osf.io/ufgy6},
  urldate = {2024-01-01},
  abstract = {Simulation studies are widely used for evaluating the performance of statistical methods in psychology. However, the quality of simulation studies can vary widely in terms of their design, execution, and reporting. In order to assess the quality of typical simulation studies in psychology, we reviewed 321 articles published in Psychological Methods, Behavioral Research Methods, and Multivariate Behavioral Research in 2021 and 2022, among which 100/321 = 31.2\% report a simulation study. We find that many articles do not provide complete and transparent information about key aspects of the study, such as justifications for the number of simulation repetitions, Monte Carlo uncertainty estimates, or code and data to reproduce the simulation studies. To address this problem, we provide a summary of the ADEMP (Aims, Data-generating mechanism, Estimands and other targets, Methods, Performance measures) design and reporting framework from Morris, White, and Crowther (2019) adapted to simulation studies in psychology. Based on this framework, we provide ADEMP-PreReg, a step-by-step template for researchers to use when designing, potentially preregistering, and reporting their simulation studies. We give formulae for estimating common performance measures, their Monte Carlo standard errors, and for calculating the number of simulation repetitions to achieve a desired Monte Carlo standard error. Finally, we give a detailed tutorial on how to apply the ADEMP framework in practice using an example simulation study on the evaluation of methods for the analysis of pre--post measurement experiments.},
  langid = {american},
  keywords = {cited},
  file = {C\:\\Users\\jamespustejovsky\\Zotero\\storage\\3WUS7RGS\\Siepe et al. - 2024 - Simulation Studies for Methodological Research in .pdf;C\:\\Users\\jamespustejovsky\\Zotero\\storage\\ZA4S3LP9\\ufgy6.html}
}

@article{sigal2016PlayItAgain,
  title = {Play {{It Again}}: {{Teaching Statistics With Monte Carlo Simulation}}},
  shorttitle = {Play {{It Again}}},
  author = {Sigal, Matthew J. and Chalmers, R. Philip},
  year = {2016},
  month = sep,
  journal = {Journal of Statistics Education},
  volume = {24},
  number = {3},
  pages = {136--156},
  publisher = {Taylor \& Francis},
  issn = {null},
  doi = {10.1080/10691898.2016.1246953},
  urldate = {2024-01-01},
  abstract = {Monte Carlo simulations (MCSs) provide important information about statistical phenomena that would be impossible to assess otherwise. This article introduces MCS methods and their applications to research and statistical pedagogy using a novel software package for the R Project for Statistical Computing constructed to lessen the often steep learning curve when organizing simulation code. A primary goal of this article is to demonstrate how well-suited MCS designs are to classroom demonstrations, and how they provide a hands-on method for students to become acquainted with complex statistical concepts. In this article, essential programming aspects for writing MCS code in R are overviewed, multiple applied examples with relevant code are provided, and the benefits of using a generate--analyze--summarize coding structure over the typical ``for-loop'' strategy are discussed.},
  keywords = {Active learning,R,Simulation,Statistical computing},
  file = {C:\Users\jamespustejovsky\Zotero\storage\VJFLBCD7\Sigal and Chalmers - 2016 - Play It Again Teaching Statistics With Monte Carl.pdf}
}

@article{skrondal2000DesignAnalysisMonte,
  title = {Design and {{Analysis}} of {{Monte Carlo Experiments}}: {{Attacking}} the {{Conventional Wisdom}}},
  shorttitle = {Design and {{Analysis}} of {{Monte Carlo Experiments}}},
  author = {Skrondal, Anders},
  year = {2000},
  month = apr,
  journal = {Multivariate Behavioral Research},
  volume = {35},
  number = {2},
  pages = {137--167},
  publisher = {Routledge},
  issn = {0027-3171},
  doi = {10.1207/S15327906MBR3502_1},
  urldate = {2024-01-02},
  abstract = {The design and analysis of Monte Carlo experiments, with special reference to structural equation modelling, is discussed in this article. These topics merit consideration, since the validity of the conclusions drawn from a Monte Carlo study clearly hinges on these features. It is argued that comprehensive Monte Carlo experiments can be implemented on a PC if the experiments are adequately designed. This is especially important when investigating modern computer intensive methodologies like resampling and Markov Chain Monte Carlo methods. We are faced with three fundamental challenges in Monte Carlo experimentation. The first problem is statistical precision, which concerns the reliability of the obtained results. External validity, on the other hand, depends on the number of experimental conditions, and is crucial for the prospects of generalising the results beyond the specific experiment. Finally, we face the constraint on available computer resources. The conventional wisdom in designing and analysing Monte Carlo experiments embodies no explicit specification of meta-model for analysing the output of the experiment, the use of case studies or full factorial designs as experimental plans, no use of variance reduction techniques, a large number of replications, and "eyeballing" of the results. A critical examination of the conventional wisdom is presented in this article. We suggest that the following alternative procedures should be considered. First of all, we argue that it is profitable to specify explicit meta-models, relating the chosen performance statistics and experimental conditions. Regarding the experimental plan, we recommend the use of incomplete designs, which will often result in considerable savings. We also consider the use of common random numbers in the simulation phase, since this may enhance the precision in estimating meta-models. The use of fewer replications per trial, enabling us to investigate an increased number of experimental conditions, should also be considered in order to improve the external validity at the cost of the conventionally excessive precision.},
  pmid = {26754081},
  keywords = {cited}
}

@article{smith1973MonteCarloMethods,
  title = {Monte {{Carlo}} Methods : {{Their Role}} for {{Econometrics}}},
  shorttitle = {Monte {{Carlo}} Methods},
  author = {Smith, Vincent Kerry},
  year = {1973},
  journal = {(No Title)},
  urldate = {2024-01-02},
  langid = {english},
  file = {C:\Users\jamespustejovsky\Zotero\storage\WC9SPKTS\1130000796834682624.html}
}

@article{sofrygin2017SimcausalPackageConducting,
  title = {Simcausal {{R Package}}: {{Conducting Transparent}} and {{Reproducible Simulation Studies}} of {{Causal Effect Estimation}} with {{Complex Longitudinal Data}}},
  author = {Sofrygin, Oleg and van der Laan, Mark J. and Neugebauer, Romain},
  year = {2017},
  journal = {Journal of Statistical Software},
  volume = {81},
  number = {2},
  pages = {1--47},
  doi = {10.18637/jss.v081.i02},
  keywords = {cited}
}

@article{white2023HowCheckSimulation,
  title = {How to Check a Simulation Study},
  author = {White, Ian R and Pham, Tra My and Quartagno, Matteo and Morris, Tim P},
  year = {2023},
  month = oct,
  journal = {International Journal of Epidemiology},
  pages = {dyad134},
  issn = {0300-5771},
  doi = {10.1093/ije/dyad134},
  urldate = {2024-01-01},
  abstract = {Simulation studies are powerful tools in epidemiology and biostatistics, but they can be hard to conduct successfully. Sometimes unexpected results are obtained. We offer advice on how to check a simulation study when this occurs, and how to design and conduct the study to give results that are easier to check. Simulation studies should be designed to include some settings in which answers are already known. They should be coded in stages, with data-generating mechanisms checked before simulated data are analysed. Results should be explored carefully, with scatterplots of standard error estimates against point estimates surprisingly powerful tools. Failed estimation and outlying estimates should be identified and dealt with by changing data-generating mechanisms or coding realistic hybrid analysis procedures. Finally, we give a series of ideas that have been useful to us in the past for checking unexpected results. Following our advice may help to prevent errors and to improve the quality of published simulation studies.},
  keywords = {cited},
  file = {C\:\\Users\\jamespustejovsky\\Zotero\\storage\\MI2FJWW8\\White et al. - 2023 - How to check a simulation study.pdf;C\:\\Users\\jamespustejovsky\\Zotero\\storage\\PP2MIHSI\\7313663.html}
}
	
